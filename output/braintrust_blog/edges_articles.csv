source,target,weight,anchors
https://www.braintrust.dev/blog/ab-testing-evals,https://www.braintrust.dev/blog/ab-testing-evals,10,A/B testing can't keep up with AI | The critical difference | The new operating model | Understanding evals through A/B testing | Why this matters now
https://www.braintrust.dev/blog/graphite,https://www.braintrust.dev/blog/graphite,32,1. Acceptance rate | 1. Line range validation | 2. Semantic similarity | 2. Upvote rate | 3. Binary feedback | 3. Downvote rate | Dataset curation | How Graphite builds reliable AI code review at scale | Key takeaways | Manual evaluation approach | Measuring what matters: Diamond's key metrics | Putting it all together | Results | Scoring functions | Systematic evaluation with Braintrust | The technical challenge of AI code review
https://www.braintrust.dev/blog/async-programming,https://www.braintrust.dev/blog/async-programming,18,1. Clear problem definitions | 2. Automated verification | 3. Detailed code review | Async programming at Braintrust | The async programming workflow | The rise of async programming | The three pillars of async programming | Where this is heading | Why async programming works
https://www.braintrust.dev/blog/gpt-5-vs-claude-opus,https://www.braintrust.dev/blog/gpt-5-vs-claude-opus,12,GPT-5 vs. Claude Opus 4.1 | General breakdown | How customers have been reacting to GPT-5 | How to know for sure which to deploy | Reasoning effort | Running HLE benchmarks
https://www.braintrust.dev/blog/agent-while-loop,https://www.braintrust.dev/blog/agent-while-loop,10,Context engineering matters | Evaluation as a foundation | The canonical agent architecture: A while loop with tools | The path through complexity | Tool design sets up the LLM for success
https://www.braintrust.dev/blog/five-lessons-evals,https://www.braintrust.dev/blog/five-lessons-evals,26,"1: Effective evals speak for themselves | 2: Great evals must be engineered | 3: Context beats prompts | 4: Be ready for new models to change everything | 5: Optimize the whole loop, not just the prompt | Custom scoring | Data engineering | Five hard-learned lessons about AI evals | Meet Loop: your eval copilot | Playing offense | Rapid model adoption | Summing it up | Userâ€‘feedback input"
https://www.braintrust.dev/blog/braintrust-not-eval-framework,https://www.braintrust.dev/blog/braintrust-not-eval-framework,14,Braintrust is not an eval framework | Core components | Playgrounds: where infrastructure becomes invisible | The bottom line | The case for infrastructure | What about frameworks? | What is eval infrastructure?
https://www.braintrust.dev/blog/braintrust-not-eval-framework,https://www.braintrust.dev/blog/brainstore,1,Brainstore
https://www.braintrust.dev/blog/braintrust-not-eval-framework,https://www.braintrust.dev/blog/faster-experiments,1,how experiment data is loaded in the UI
https://www.braintrust.dev/blog/grok-4,https://www.braintrust.dev/blog/grok-4,14,Building with Grok 4 | Run evals | Scoring | Setup | Tasks | The pelican baseline | Whatâ€™s next?
https://www.braintrust.dev/blog/faster-experiments,https://www.braintrust.dev/blog/faster-experiments,18,Aggregate scores and custom columns | Benchmark comparison | Dynamic filtering across all fields | Efficient preview aggregation | Experiments UI: Now 10x faster | Faster experiences across Braintrust | Real-time visibility | The challenge | The solution
https://www.braintrust.dev/blog/faster-experiments,https://www.braintrust.dev/blog/brainstore,1,Brainstore
https://www.braintrust.dev/blog/eval-playgrounds,https://www.braintrust.dev/blog/eval-playgrounds,8,"A UX-first approach to evals | Differentiated by design | Eval playgrounds for faster, focused iteration | Why UX matters"
https://www.braintrust.dev/blog/coursera,https://www.braintrust.dev/blog/coursera,20,"1. Define clear evaluation criteria upfront | 2. Curate targeted datasets | 3. Implement both heuristic and model-based scorers | 4. Run evaluations and iterate rapidly | Evaluating AI features with Braintrust | How Coursera builds next-generation learning tools | Practical lessons for organizations adopting AI evaluation | Results: Better AI features, faster development | Scaling AI evaluation | The business impact of AI features"
https://www.braintrust.dev/blog/best-practices,https://www.braintrust.dev/blog/best-practices,24,"Q: Can Braintrust evaluate multimodal data (images, audio, video)? | Q: Have customers successfully used LLMs to automate scoring guidance? | Q: How do I get started building effective evals? | Q: How do customers integrate evals into continuous integration (CI)? | Q: How does Braintrust handle user feedback and PII in evals? | Q: How should teams balance automated scoring with human review? | Q: What are some common scoring functions teams start with? | Q: What is Brainstore and why was it developed? | Q: What is Braintrustâ€™s approach to multi-step prompt chaining (agents)? | Q: Whatâ€™s your advice on using synthetic data in evals? | The future of evals | Webinar recap: Eval best practices"
https://www.braintrust.dev/blog/resilient-design,https://www.braintrust.dev/blog/resilient-design,12,Built for serverless environments | Confidence through design | Non-blocking logging by default | Reliable proxy service | Resilient observability by design | Resilient prompt management
https://www.braintrust.dev/blog/brainstore-default,https://www.braintrust.dev/blog/brainstore-default,10,A breaking API change | Brainstore is now the default | Changes to expect | Looking ahead | Self-hosted users
https://www.braintrust.dev/blog/brainstore-default,https://www.braintrust.dev/blog/brainstore,1,Brainstore
https://www.braintrust.dev/blog/brainstore,https://www.braintrust.dev/blog/brainstore,24,Architecture | Benchmarks | Brainstore: the purpose-built database for the AI engineering era | Complex queries (full-text search) | Customer testimonials | Huge data volumes | Introducing Brainstore | Looking ahead | Rollout and next steps | Security and privacy constraints | The breakpoint for classic observability | Why is it so much faster?
https://www.braintrust.dev/blog/model-updates,https://www.braintrust.dev/blog/model-updates,10,"Additional updates | Bedrock, Vertex AI, and universal structured outputs support | Consistent structured outputs | Expanded Bedrock and Vertex AI support | Simplified model selection in the playground"
https://www.braintrust.dev/blog/fintool,https://www.braintrust.dev/blog/fintool,16,1. Define quality standards and format rules | 2. Curate golden datasets | 3. Automate evals with LLM-as-a-judge | 4. Add human in the loop oversight | Conclusion | Fintool's continuous evaluation workflow | How Fintool generates millions of financial insights | Results
https://www.braintrust.dev/blog/loom,https://www.braintrust.dev/blog/loom,16,"1. Identify the traits of great video titles | 2. Check for common measures of quality | 3. Implement objective measures with code | 4. Create initial scorers and iterate | How Loom auto-generates video titles | How do I figure out what scorers to create? | The results: faster, better AI features | Why it works"
https://www.braintrust.dev/blog/evaluating-agents,https://www.braintrust.dev/blog/evaluating-agents,28,Best practices | Building block: the augmented LLM | Choosing eval metrics | Evaluating agents | Evaluator-optimizer | Fully autonomous agent | Iterative evaluation process | Next steps | Orchestrator-workers | Parallelization | Prompt chaining | Quantitative vs. qualitative metrics | Routing | Why run evaluations?
https://www.braintrust.dev/blog/evaluating-agents,https://www.braintrust.dev/blog/after-evals,1,how you might improve your application
https://www.braintrust.dev/blog/hybrid-deployment,https://www.braintrust.dev/blog/hybrid-deployment,12,Advantages of hybrid deployment | Constraining the SDK | Data storage | Getting started | Keeping your data secure | Our approach to hybrid deployment
https://www.braintrust.dev/blog/2024,https://www.braintrust.dev/blog/2024,24,"1. Custom scorer, tool, and prompt functions | 10. Flexible visualizations | 2. AI proxy and hybrid self-hosting | 3. Structured outputs in the playground | 4. Monitoring improvements | 5. Faster experiment and log loading | 6. Human review | 7. Custom provider configuration | 8. Improved logs and search | 9. Attachment uploads and previews, plus Realtime API support | Thank you | The top 10 most loved features of 2024"
https://www.braintrust.dev/blog/monitor,https://www.braintrust.dev/blog/monitor,6,Flexible data analysis | Getting started | New monitor page for easy analytics
https://www.braintrust.dev/blog/new-model,https://www.braintrust.dev/blog/new-model,12,Benchmark against real data | Develop a baseline | Monitor your application | Swap your model in production | What to do when a new AI model comes out | Whatâ€™s next
https://www.braintrust.dev/blog/rag-mongodb,https://www.braintrust.dev/blog/rag-mongodb,24,Building a RAG app with MongoDB Atlas | Create a playground | Creating a RAG tool | Define a scorer | Getting started | Import a dataset | Next steps | Run full experiments | Try out the tool | Tweak the prompt | Upload vectors | Writing a prompt
https://www.braintrust.dev/blog/gemini,https://www.braintrust.dev/blog/gemini,10,Evaluating Gemini models for vision | Gemini for document extraction | Getting started with Gemini | Key takeaways | Working with multimodal models
https://www.braintrust.dev/blog/python-tools-uv,https://www.braintrust.dev/blog/python-tools-uv,2,Support for Python tool functions
https://www.braintrust.dev/blog/realtime-api,https://www.braintrust.dev/blog/realtime-api,12,Building secure and scalable production apps with OpenAIâ€™s Realtime API | Configuring your app | Infrastructure challenges | Logging | Using the AI proxy | Whatâ€™s next
https://www.braintrust.dev/blog/realtime-api,https://www.braintrust.dev/blog/attachments,1,attachment
https://www.braintrust.dev/blog/attachments,https://www.braintrust.dev/blog/attachments,8,Logging with attachments | Uploading an attachment | Viewing an attachment in a trace | Whatâ€™s next
https://www.braintrust.dev/blog/after-evals,https://www.braintrust.dev/blog/after-evals,22,Decide on an improvement | Fix a specific set of bad outputs | How to choose your focus | How to improve your AI application | How to improve your evals | I ran an eval. Now what? | Ideal workflow | Increase a score | Make the update and re-run your evals to verify results | Repeat! | Think through how to make that improvement
https://www.braintrust.dev/blog/after-evals,https://www.braintrust.dev/blog/eval-feedback-loops,1,Structuring evals
https://www.braintrust.dev/blog/after-evals,https://www.braintrust.dev/blog/getting-started-evals,1,Getting started with automated evals
https://www.braintrust.dev/blog/after-evals,https://www.braintrust.dev/blog/improve-evals,1,check out the full blog post
https://www.braintrust.dev/blog/after-evals,https://www.braintrust.dev/blog/notion,1,how Notion develops world-class AI features
https://www.braintrust.dev/blog/notion,https://www.braintrust.dev/blog/notion,20,1. Decide on an improvement | 2. Curate targeted datasets | 3. Tie these datasets to specific scoring functions | 4. Run evals & inspect results | 5. Iterate quickly | Conclusion | Evaluating Notion Q&A | How Notion develops world-class AI features | Notionâ€™s evals today | The results
https://www.braintrust.dev/blog/functions,https://www.braintrust.dev/blog/functions,12,Custom scoring functions | Function composition | Functions in Braintrust | Functions: flexible AI engineering primitives | Prompts are functions | The new software engineering lifecycle
https://www.braintrust.dev/blog/functions,https://www.braintrust.dev/blog/custom-scorers,1,full blog post
https://www.braintrust.dev/blog/custom-scorers,https://www.braintrust.dev/blog/custom-scorers,8,Custom scoring functions in the Braintrust Playground | How to create custom scorers | Uploading custom scorers | What's next
https://www.braintrust.dev/blog/soc2,https://www.braintrust.dev/blog/soc2,2,Braintrust achieves SOC 2 Type II compliance
https://www.braintrust.dev/blog/improve-evals,https://www.braintrust.dev/blog/improve-evals,16,Add new test cases to your dataset | Conclusion | How to improve your evals | How to improve your evaluations | Identify new and useful evaluators | Improve your existing scorers | What are evaluations? | What you should work towards
https://www.braintrust.dev/blog/improve-evals,https://www.braintrust.dev/blog/getting-started-evals,1,guide to getting started with automated evals
https://www.braintrust.dev/blog/improve-evals,https://www.braintrust.dev/blog/eval-feedback-loops,1,here
https://www.braintrust.dev/blog/zapier-ai,https://www.braintrust.dev/blog/zapier-ai,20,1. Prototype & build v0 | 2. Ship v1 | 3. Collect user feedback | 4. Establish a set of evals | 5. Iterate to improve product quality | 6. Take a moment to be proud | 7. Optimize | Conclusion | How Zapier builds production-ready AI products | Zapier's AI development process
https://www.braintrust.dev/blog/ai-development-loops,https://www.braintrust.dev/blog/ai-development-loops,10,AI development loops | Conclusion | Loop 1: Exploration | Loop 2: Evaluation | Loop 3: Data Collection
https://www.braintrust.dev/blog/getting-started-evals,https://www.braintrust.dev/blog/getting-started-evals,14,Automated evaluations | Comparative evals | Continuous iteration | Getting started with automated evaluations | Heuristics | LLM evaluators | The state of evals today
https://www.braintrust.dev/blog/eval-feedback-loops,https://www.braintrust.dev/blog/eval-feedback-loops,12,Capturing and utilizing logs | Eval feedback loops | Initial steps | Putting this to practice | Scaling | Structuring your evals
https://www.braintrust.dev/blog/wing-30,https://www.braintrust.dev/blog/wing-30,2,Braintrust selected to be in the Enterprise Tech 30
https://www.braintrust.dev/blog/hostinger-evals,https://www.braintrust.dev/blog/hostinger-evals,8,Hostinger's Approach to Evaluations (0:10) | How Braintrust Supports Hostinger (2:40) | How Hostinger evaluates AI applications with Braintrust | What Evaluations does Hostinger Run? (1:47)
https://www.braintrust.dev/blog/2023-summary,https://www.braintrust.dev/blog/2023-summary,2,"2023, a year in review"
https://www.braintrust.dev/blog/seed-round,https://www.braintrust.dev/blog/seed-round,2,Braintrust's seed round: $5m to build infrastructure for AI products
https://www.braintrust.dev/blog/open-sourcing-proxy,https://www.braintrust.dev/blog/open-sourcing-proxy,18,API key management | Additional features | Azure OpenAI | Benchmarks | Caching | Deployment options | Load balancing | Open sourcing the AI proxy | What's next
https://www.braintrust.dev/blog/open-sourcing-proxy,https://www.braintrust.dev/blog/ai-proxy,1,Braintrust AI Proxy
https://www.braintrust.dev/blog/ai-proxy,https://www.braintrust.dev/blog/ai-proxy,12,AI proxy: fostering a more open ecosystem | API keys | Architecture | Availability | Caching | Just let me try it!
https://www.braintrust.dev/blog/state-of-ai,https://www.braintrust.dev/blog/state-of-ai,2,State of AI development 2023
https://www.braintrust.dev/blog/journey,https://www.braintrust.dev/blog/journey,16,1. Start with a prototype | 2. Manually testing | 3. Get some friends to test | 4. Fix bugs and add features | 5. Evaluations enlightenment | 6. Launch ðŸš€ | Conclusion | The AI product development journey
https://www.braintrust.dev/blog/update-6,https://www.braintrust.dev/blog/update-6,8,Braintrust Weekly Update | Fun links and mentions: | Release notes : | We added function calling to our playground
https://www.braintrust.dev/blog/update-5,https://www.braintrust.dev/blog/update-5,8,Braintrust Weekly Update | Fun links and mentions: | New OpenAI and Open Source models in playground | Release notes :
https://www.braintrust.dev/blog/update-4,https://www.braintrust.dev/blog/update-4,8,Braintrust Weekly Update | Experiment sidebar is now resizable | Fun links and mentions: | Release notes :
https://www.braintrust.dev/blog/update-3,https://www.braintrust.dev/blog/update-3,10,Braintrust Weekly Update | Fun links and mentions: | Prompt playground variable improvements | Release notes : | Time duration summary metrics for experiments
https://www.braintrust.dev/blog/update-2,https://www.braintrust.dev/blog/update-2,8,Braintrust Weekly Update | Experiment dashboard customization | Release notes: | Tracing: log and visualize complex LLM chains and executions.
https://www.braintrust.dev/blog/update-1,https://www.braintrust.dev/blog/update-1,8,Braintrust Weekly Update | Release notes | We evaluated the Alpaca evals leaderboard in Braintrust | We improved Datasets. See when they were last edited and the version number from the UI.
https://www.braintrust.dev/blog/reliable-ai,https://www.braintrust.dev/blog/reliable-ai,14,"Automated evaluations you can trust | Collaborative playground for rapid testing | Dataset management | Get started for free | It's time to build reliable AI | The end of predictable software | Tomorrow, together"
