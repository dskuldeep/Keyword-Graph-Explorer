{"https://www.braintrust.dev/blog": {"url": "https://www.braintrust.dev/blog", "title": "Latest Braintrust news - Braintrust", "text": "Latest Braintrust news\nA/B testing can't keep up with AI\nHow evals can replace traditional A/B testing in product development.\n2025-09-03\nRead\nRead\nHow Graphite builds reliable AI code review at scale\nLearn how Graphite's engineering team uses systematic evaluation to build Diamond, their AI code reviewer that provides actionable feedback without hallucinations.\n25 August 2025\nRead\nThe rise of async programming\nThe workflow that's changing how software gets built.\n19 August 2025\nRead\nGPT-5 vs. Claude Opus 4.1\nWhich one you should ship with, and how to know for sure.\n8 August 2025\nRead\nThe canonical agent architecture: A while loop with tools\nWhy the best AI agents are just loops that call functions.\n7 August 2025\nRead\nFive hard-learned lessons about AI evals\nWhat our customers have taught us about running evals at scale.\n17 July 2025\nRead\nBraintrust is not an eval framework\nWhy we built infrastructure for AI products, not just another evaluation tool.\n14 July 2025\nRead\nBuilding with Grok 4\nxAI recently announced Grok 4. We put it to the ultimate test.\n11 July 2025\nRead\nExperiments UI: Now 10x faster\nBrainstore speeds up experiments, datasets, and logs.\n3 June 2025\nRead\nEval playgrounds for faster, focused iteration\nRun full evals directly in a powerful editor UI.\n27 May 2025\nRead\nHow Coursera builds next-generation learning tools\nKey learnings from the Coursera AI engineering team.\n12 May 2025\nRead\nWebinar recap: Eval best practices\nA recap of our technical Q&A hosted by CEO Ankur Goyal.\n22 April 2025\nRead\nResilient observability by design\nHow we built Braintrust to ensure no impact on downtime.\n3 April 2025\nRead\nBrainstore is now on by default\nBrainstore is now the default in both our UI and API. Learn what's changing and coming next.\n31 March 2025\nRead\nBrainstore: the database designed for the AI engineering era\nLLM observability, now 80x faster.\n3 March 2025\nRead\nBedrock, Vertex AI, and universal structured outputs\nFull support for Bedrock, Vertex AI, and structured outputs in the AI proxy and playground.\n14 February 2025\nRead\nHow Fintool generates millions of financial insights\nLearn to build trusted and scalable LLM apps from the team at Fintool.\n31 January 2025\nRead\nHow Loom auto-generates video titles\nLearn scoring best practices from the software engineering team at Loom.\n27 January 2025\nRead\nEvaluating agents\nLearn best practices for scoring agentic systems.\n22 January 2025\nRead\nOur approach to hybrid deployment\nThe easiest way to self-host Braintrust.\n8 January 2025\nRead\nThe top 10 most loved features of 2024\nOur year in review.\n31 December 2024\nRead\nNew monitor page for easy analytics\nMore visibility into performance across logs and experiments.\n18 December 2024\nRead\nWhat to do when a new AI model comes out\nHow to decide if you should use a new model in production.\n4 December 2024\nRead\nBuilding a RAG app with MongoDB Atlas\nHow to iterate on AI applications without redeploying code.\n18 November 2024\nRead\nEvaluating Gemini models for vision\nFaster, more efficient, and highly accurate for real-world applications.\n14 November 2024\nRead\nPython tool functions: powered by uv\nHow we used the uv library to build Python tools.\n13 November 2024\nRead\nBuilding serverless apps with the OpenAI Realtime API\nNo server setup or configuration necessary.\n4 November 2024\nRead\nLogging with attachments\nObservability for advanced AI applications.\n24 October 2024\nRead\nI ran an eval. Now what?\nA guide to next steps after your first eval and best practices for your workflows.\n17 October 2024\nRead\nHow Notion develops world-class AI features\nLearn how Notion refined their development workflow with Braintrust.\n9 October 2024\nRead\nAnnouncing our $36M Series A\nWe\u2019re thrilled to announce that we've raised $36 million to advance the future of AI software engineering, bringing our total funding to $45 million.\n8 October 2024\nRead\nFunctions: flexible AI engineering primitives\nIntroducing functions, a general-purpose primitive for building, evaluating, and observing AI products.\n8 October 2024\nRead\nCustom scoring functions in the Braintrust Playground\nCreate custom scorers and access them via the Braintrust UI and API.\n16 September 2024\nRead\nBraintrust achieves SOC 2 Type II compliance\nWe are excited to announce that Braintrust has achieved SOC 2 Type II compliance.\n15 July 2024\nRead\nHow to improve your evaluations\nLearn how to improve your evals by identifying new evaluators, iterating on existing scorers, and adding new test cases.\n2024-06-20\nRead\nHow Zapier builds production-ready AI products\nZapier was one of the earliest adopters of GenAI. In this post, we share insights from Mike Knoop, Co-founder & Head of AI at Zapier.\n2024-05-30\nRead\nAI development loops\nKey activities that enable fast feedback and clear signal when developing AI features.\n6 May 2024\nRead\nGetting started with automated evaluations\nThree actionable approaches for engineering teams to get started with automated evaluations.\n2024-04-24\nRead\nEval feedback loops\nLearn how to build robust eval feedback loops for AI products by connecting real-world log data to your evals. Discover best practices for structuring evals, flowing production logs into eval datasets, and using Braintrust to streamline the process.\n17 April 2024\nRead\nBraintrust selected to be in the Enterprise Tech 30\nThe Enterprise Tech 30 by Wing Venture Capital names the highest potential private companies in enterprise technology.\n9 April 2024\nRead\nHow Hostinger evaluates AI applications with Braintrust\nLiucija, Senior Data Scientist on the AI team @ Hostinger, provides an overview of how she leverages Braintrust to accelerate Hostinger's AI development process and automate over 40% of customer support chat conversations.\n27 February 2024\nRead\n2023, a year in review\nCheck out your Braintrust 2023 year in review to see how you did this year!\n21 December 2023\nRead\nBraintrust's seed round: $5m to build infrastructure for AI products\nAnnouncing Braintrust's seed round led by Greylock. The round builds on our early traction with customers like Zapier, Coda, Airtable, and Instacart and allows us to accelerate our vision of building world-class infrastructure for AI products. We are hiring for a number of roles, so please check out our careers page if you are interested in joining us.\n13 December 2023\nRead\nOpen sourcing the AI proxy\nThe Braintrust AI Proxy is now open source! We also added support for Azure OpenAI and provider load balancing.\n27 November 2023\nRead\nAI proxy: fostering a more open ecosystem\nIntroducing Braintrust's latest feature: an AI proxy that lets you use open source models like LLaMa 2 and Mistral, as well as all of OpenAI's and Anthropic's models, behind a single interface with caching, security, and API key management built in.\n20 November 2023\nRead\nState of AI development 2023\nRetool recently surveyed over 1,500 workers and how their companies are adopting AI in their State of AI 2023 report. Here's what they are struggling with and how Braintrust can help them.\n15 November 2023\nRead\nThe AI product development journey\nBuilding reliable AI apps is hard. It\u2019s easy to build a cool demo but hard to build an AI app that works in production for real users. In traditional software development, there\u2019s a set of best practices like setting up CI/CD and writing tests to make your software robust and easy to build on. But, with LLM apps it\u2019s not obvious how to create these tests or processes.\n13 November 2023\nRead\nWeekly update 11/13/23\nFunction calling and tool support, new blog posts, and project UI improvements.\n13 November 2023\nRead\nWeekly update 11/06/23\nPerplexity models support, new OpenAI models, reworked diff selector in experiment view.\n06 November 2023\nRead\nWeekly update 10/30/23\nResizable sidebar, new help tooltips, performance optimizations, Replit.\n30 October 2023\nRead\nWeekly update 10/23/23\nAuto input variables in the playground, duration metrics, performance optimizations, partner releases.\n23 October 2023\nRead\nWeekly update 10/16/23\nTracing, experiment dashboard customization, text-block prompts, bigger tables, new eval docs.\n16 October 2023\nRead\nWeekly update 10/09/23\nPerformance improvements, fine tuning tutorial, Alpaca Evals, autocomplete in the playground.\n09 October 2023\nRead\nIt's time to build reliable AI\nIntroducing Braintrust: the enterprise-grade stack for building AI products. From evaluations, to prompt playground, to data management, we take uncertainty and tedium out of incorporating AI into your business.\n12 September 2023", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/blog/atom", "anchor": ""}, {"href": "https://www.braintrust.dev/blog/ab-testing-evals", "anchor": "Latest Braintrust news A/B testing can't keep up with AI How evals can replace traditional A/B testing in product development. 2025-09-03 Read"}, {"href": "https://www.braintrust.dev/blog/graphite", "anchor": "Read How Graphite builds reliable AI code review at scale Learn how Graphite's engineering team uses systematic evaluation to build Diamond, their AI code reviewer that provides actionable feedback without hallucinations. 25 August 2025"}, {"href": "https://www.braintrust.dev/blog/async-programming", "anchor": "Read The rise of async programming The workflow that's changing how software gets built. 19 August 2025"}, {"href": "https://www.braintrust.dev/blog/gpt-5-vs-claude-opus", "anchor": "Read GPT-5 vs. Claude Opus 4.1 Which one you should ship with, and how to know for sure. 8 August 2025"}, {"href": "https://www.braintrust.dev/blog/agent-while-loop", "anchor": "Read The canonical agent architecture: A while loop with tools Why the best AI agents are just loops that call functions. 7 August 2025"}, {"href": "https://www.braintrust.dev/blog/five-lessons-evals", "anchor": "Read Five hard-learned lessons about AI evals What our customers have taught us about running evals at scale. 17 July 2025"}, {"href": "https://www.braintrust.dev/blog/braintrust-not-eval-framework", "anchor": "Read Braintrust is not an eval framework Why we built infrastructure for AI products, not just another evaluation tool. 14 July 2025"}, {"href": "https://www.braintrust.dev/blog/grok-4", "anchor": "Read Building with Grok 4 xAI recently announced Grok 4. We put it to the ultimate test. 11 July 2025"}, {"href": "https://www.braintrust.dev/blog/faster-experiments", "anchor": "Read Experiments UI: Now 10x faster Brainstore speeds up experiments, datasets, and logs. 3 June 2025"}, {"href": "https://www.braintrust.dev/blog/eval-playgrounds", "anchor": "Read Eval playgrounds for faster, focused iteration Run full evals directly in a powerful editor UI. 27 May 2025"}, {"href": "https://www.braintrust.dev/blog/coursera", "anchor": "Read How Coursera builds next-generation learning tools Key learnings from the Coursera AI engineering team. 12 May 2025"}, {"href": "https://www.braintrust.dev/blog/best-practices", "anchor": "Read Webinar recap: Eval best practices A recap of our technical Q&A hosted by CEO Ankur Goyal. 22 April 2025"}, {"href": "https://www.braintrust.dev/blog/resilient-design", "anchor": "Read Resilient observability by design How we built Braintrust to ensure no impact on downtime. 3 April 2025"}, {"href": "https://www.braintrust.dev/blog/brainstore-default", "anchor": "Read Brainstore is now on by default Brainstore is now the default in both our UI and API. Learn what's changing and coming next. 31 March 2025"}, {"href": "https://www.braintrust.dev/blog/brainstore", "anchor": "Read Brainstore: the database designed for the AI engineering era LLM observability, now 80x faster. 3 March 2025"}, {"href": "https://www.braintrust.dev/blog/model-updates", "anchor": "Read Bedrock, Vertex AI, and universal structured outputs Full support for Bedrock, Vertex AI, and structured outputs in the AI proxy and playground. 14 February 2025"}, {"href": "https://www.braintrust.dev/blog/fintool", "anchor": "Read How Fintool generates millions of financial insights Learn to build trusted and scalable LLM apps from the team at Fintool. 31 January 2025"}, {"href": "https://www.braintrust.dev/blog/loom", "anchor": "Read How Loom auto-generates video titles Learn scoring best practices from the software engineering team at Loom. 27 January 2025"}, {"href": "https://www.braintrust.dev/blog/evaluating-agents", "anchor": "Read Evaluating agents Learn best practices for scoring agentic systems. 22 January 2025"}, {"href": "https://www.braintrust.dev/blog/hybrid-deployment", "anchor": "Read Our approach to hybrid deployment The easiest way to self-host Braintrust. 8 January 2025"}, {"href": "https://www.braintrust.dev/blog/2024", "anchor": "Read The top 10 most loved features of 2024 Our year in review. 31 December 2024"}, {"href": "https://www.braintrust.dev/blog/monitor", "anchor": "Read New monitor page for easy analytics More visibility into performance across logs and experiments. 18 December 2024"}, {"href": "https://www.braintrust.dev/blog/new-model", "anchor": "Read What to do when a new AI model comes out How to decide if you should use a new model in production. 4 December 2024"}, {"href": "https://www.braintrust.dev/blog/rag-mongodb", "anchor": "Read Building a RAG app with MongoDB Atlas How to iterate on AI applications without redeploying code. 18 November 2024"}, {"href": "https://www.braintrust.dev/blog/gemini", "anchor": "Read Evaluating Gemini models for vision Faster, more efficient, and highly accurate for real-world applications. 14 November 2024"}, {"href": "https://www.braintrust.dev/blog/python-tools-uv", "anchor": "Read Python tool functions: powered by uv How we used the uv library to build Python tools. 13 November 2024"}, {"href": "https://www.braintrust.dev/blog/realtime-api", "anchor": "Read Building serverless apps with the OpenAI Realtime API No server setup or configuration necessary. 4 November 2024"}, {"href": "https://www.braintrust.dev/blog/attachments", "anchor": "Read Logging with attachments Observability for advanced AI applications. 24 October 2024"}, {"href": "https://www.braintrust.dev/blog/after-evals", "anchor": "Read I ran an eval. Now what? A guide to next steps after your first eval and best practices for your workflows. 17 October 2024"}, {"href": "https://www.braintrust.dev/blog/notion", "anchor": "Read How Notion develops world-class AI features Learn how Notion refined their development workflow with Braintrust. 9 October 2024"}, {"href": "https://www.braintrust.dev/blog/announcing-series-a", "anchor": "Read Announcing our $36M Series A We\u2019re thrilled to announce that we've raised $36 million to advance the future of AI software engineering, bringing our total funding to $45 million. 8 October 2024"}, {"href": "https://www.braintrust.dev/blog/functions", "anchor": "Read Functions: flexible AI engineering primitives Introducing functions, a general-purpose primitive for building, evaluating, and observing AI products. 8 October 2024"}, {"href": "https://www.braintrust.dev/blog/custom-scorers", "anchor": "Read Custom scoring functions in the Braintrust Playground Create custom scorers and access them via the Braintrust UI and API. 16 September 2024"}, {"href": "https://www.braintrust.dev/blog/soc2", "anchor": "Read Braintrust achieves SOC 2 Type II compliance We are excited to announce that Braintrust has achieved SOC 2 Type II compliance. 15 July 2024"}, {"href": "https://www.braintrust.dev/blog/improve-evals", "anchor": "Read How to improve your evaluations Learn how to improve your evals by identifying new evaluators, iterating on existing scorers, and adding new test cases. 2024-06-20"}, {"href": "https://www.braintrust.dev/blog/zapier-ai", "anchor": "Read How Zapier builds production-ready AI products Zapier was one of the earliest adopters of GenAI. In this post, we share insights from Mike Knoop, Co-founder & Head of AI at Zapier. 2024-05-30"}, {"href": "https://www.braintrust.dev/blog/ai-development-loops", "anchor": "Read AI development loops Key activities that enable fast feedback and clear signal when developing AI features. 6 May 2024"}, {"href": "https://www.braintrust.dev/blog/getting-started-evals", "anchor": "Read Getting started with automated evaluations Three actionable approaches for engineering teams to get started with automated evaluations. 2024-04-24"}, {"href": "https://www.braintrust.dev/blog/eval-feedback-loops", "anchor": "Read Eval feedback loops Learn how to build robust eval feedback loops for AI products by connecting real-world log data to your evals. Discover best practices for structuring evals, flowing production logs into eval datasets, and using Braintrust to streamline the process. 17 April 2024"}, {"href": "https://www.braintrust.dev/blog/wing-30", "anchor": "Read Braintrust selected to be in the Enterprise Tech 30 The Enterprise Tech 30 by Wing Venture Capital names the highest potential private companies in enterprise technology. 9 April 2024"}, {"href": "https://www.braintrust.dev/blog/hostinger-evals", "anchor": "Read How Hostinger evaluates AI applications with Braintrust Liucija, Senior Data Scientist on the AI team @ Hostinger, provides an overview of how she leverages Braintrust to accelerate Hostinger's AI development process and automate over 40% of customer support chat conversations. 27 February 2024"}, {"href": "https://www.braintrust.dev/blog/2023-summary", "anchor": "Read 2023, a year in review Check out your Braintrust 2023 year in review to see how you did this year! 21 December 2023"}, {"href": "https://www.braintrust.dev/blog/seed-round", "anchor": "Read Braintrust's seed round: $5m to build infrastructure for AI products Announcing Braintrust's seed round led by Greylock. The round builds on our early traction with customers like Zapier, Coda, Airtable, and Instacart and allows us to accelerate our vision of building world-class infrastructure for AI products. We are hiring for a number of roles, so please check out our careers page if you are interested in joining us. 13 December 2023"}, {"href": "https://www.braintrust.dev/blog/open-sourcing-proxy", "anchor": "Read Open sourcing the AI proxy The Braintrust AI Proxy is now open source! We also added support for Azure OpenAI and provider load balancing. 27 November 2023"}, {"href": "https://www.braintrust.dev/blog/ai-proxy", "anchor": "Read AI proxy: fostering a more open ecosystem Introducing Braintrust's latest feature: an AI proxy that lets you use open source models like LLaMa 2 and Mistral, as well as all of OpenAI's and Anthropic's models, behind a single interface with caching, security, and API key management built in. 20 November 2023"}, {"href": "https://www.braintrust.dev/blog/state-of-ai", "anchor": "Read State of AI development 2023 Retool recently surveyed over 1,500 workers and how their companies are adopting AI in their State of AI 2023 report. Here's what they are struggling with and how Braintrust can help them. 15 November 2023"}, {"href": "https://www.braintrust.dev/blog/journey", "anchor": "Read The AI product development journey Building reliable AI apps is hard. It\u2019s easy to build a cool demo but hard to build an AI app that works in production for real users. In traditional software development, there\u2019s a set of best practices like setting up CI/CD and writing tests to make your software robust and easy to build on. But, with LLM apps it\u2019s not obvious how to create these tests or processes. 13 November 2023"}, {"href": "https://www.braintrust.dev/blog/update-6", "anchor": "Read Weekly update 11/13/23 Function calling and tool support, new blog posts, and project UI improvements. 13 November 2023"}, {"href": "https://www.braintrust.dev/blog/update-5", "anchor": "Read Weekly update 11/06/23 Perplexity models support, new OpenAI models, reworked diff selector in experiment view. 06 November 2023"}, {"href": "https://www.braintrust.dev/blog/update-4", "anchor": "Read Weekly update 10/30/23 Resizable sidebar, new help tooltips, performance optimizations, Replit. 30 October 2023"}, {"href": "https://www.braintrust.dev/blog/update-3", "anchor": "Read Weekly update 10/23/23 Auto input variables in the playground, duration metrics, performance optimizations, partner releases. 23 October 2023"}, {"href": "https://www.braintrust.dev/blog/update-2", "anchor": "Read Weekly update 10/16/23 Tracing, experiment dashboard customization, text-block prompts, bigger tables, new eval docs. 16 October 2023"}, {"href": "https://www.braintrust.dev/blog/update-1", "anchor": "Read Weekly update 10/09/23 Performance improvements, fine tuning tutorial, Alpaca Evals, autocomplete in the playground. 09 October 2023"}, {"href": "https://www.braintrust.dev/blog/reliable-ai", "anchor": "Read It's time to build reliable AI Introducing Braintrust: the enterprise-grade stack for building AI products. From evaluations, to prompt playground, to data management, we take uncertainty and tedium out of incorporating AI into your business. 12 September 2023"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 0}, "https://www.braintrust.dev/": {"url": "https://www.braintrust.dev/", "title": "Braintrust - The evals and observability platform for building reliable AI agents", "text": "Agents fail in unpredictable ways\nHow do you know your AI feature works?\nEvals test your AI with real data and score the results. You can determine whether changes improve or hurt performance.\nAre bad responses reaching users?\nProduction monitoring tracks live model responses and alerts you when quality drops or incorrect outputs increase.\nCan your team improve quality without guesswork?\nSide-by-side diffs allow you to compare the scores of different prompts and models, and see exactly why one version performs better than another.\nIntuitive mental model\nAll evals are composed of a dataset, task, and scorers. This framework gives teams a shared understanding for testing and improving AI applications systematically.\nCross-functional collaboration\nEngineers write code-based tests. Product managers prototype in the UI. Everyone can review results and debug issues together in real time.\nBuilt for scale\nReliable, fast infrastructure handles high-volume production traffic and complex testing workflows.\n\u201cI've never seen a workflow transformation like the one that incorporates evals into \u2018mainstream engineering\u2019 processes before. It's astonishing.\u201d\nFast prompt engineering\nTune prompts, swap models, edit scorers, and run evaluations directly in the browser. Compare traces side-by-side to see exactly what changed.\nBatch testing\nRun your prompts against hundreds or thousands of real or synthetic examples to understand performance across scenarios.\nAI-assisted workflows\nAutomate writing and optimizing prompts, scorers, and datasets with Loop, our built-in agent.\nQuantifiable progress\nMeasure changes against your own benchmarks to make data-driven decisions.\nQuality and safety gates\nPrevent quality regressions and unsafe outputs from reaching users.\nAutomated and human scoring\nRun automated tests on every change, then layer human feedback to capture the nuance machines miss.\nLive performance monitoring\nTrack latency, cost, and custom quality metrics as real traffic flows through your application.\nAutomations and alerts\nConfigure alerts that trigger when quality thresholds are crossed or safety rails trip.\nScalable log ingestion\nIngest and store all application logs with Brainstore, purpose-built for searching and analyzing AI interactions at enterprise scale.\nLoop\nPrompt optimization\nLoop analyzes your prompts and generates better-performing versions so you can hit your quality targets faster.\nSynthetic data generation\nLoop creates evaluation datasets tailored to your use case with the volume and variety needed for thorough testing.\nScorer building\nLoop builds and refines scorers to measure the specific quality metrics that matter for your application.\nBrainstore is built for AI data\nSecurity and compliance at scale\nGranular permissions\nRole-based access control with org-level permissions and project isolation to meet your security and compliance requirements.\nHybrid deployment\nSelf-hosting options to maintain full control over your AI data and meet strict compliance requirements.\nOutsized impact for the biggest brands in AI\n\u201cEvery new AI project starts with evals in Braintrust\u2014it's a game changer.\u201d", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Start for free"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Get started with evals"}, {"href": "https://www.braintrust.dev/playground", "anchor": "Try playground"}, {"href": "https://www.braintrust.dev/blog/brainstore", "anchor": "View benchmarks"}, {"href": "https://trust.braintrust.dev", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/blog/coursera", "anchor": "How Coursera builds next-generation learning tools"}, {"href": "https://www.braintrust.dev/blog/best-practices", "anchor": "Webinar: Eval best practices"}, {"href": "https://www.braintrust.dev/blog/loom", "anchor": "How Loom auto-generates video titles"}, {"href": "https://www.braintrust.dev/blog/brainstore", "anchor": "Brainstore: the database designed for AI engineering"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 1}, "https://www.braintrust.dev/docs": {"url": "https://www.braintrust.dev/docs", "title": "Get started - Docs - Start - Braintrust", "text": "Get started with Braintrust\nBraintrust is an end-to-end platform for building AI applications. It makes software development with large language models (LLMs) robust and iterative.\nIterative experimentation\nRapidly prototype with different prompts\nand models in the playground\nPerformance insights\nBuilt-in tools to evaluate how models and prompts are performing in production, and dig into specific examples\nReal-time monitoring\nLog, monitor, and take action on real-world interactions with robust and flexible monitoring\nData management\nManage and review data to store and version\nyour test sets centrally\nWhat makes Braintrust powerful is how these tools work together. With Braintrust, developers can move faster, run more experiments, and ultimately build better AI products.", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Get started with Braintrust"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Iterative experimentation"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "playground"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Performance insights"}, {"href": "https://www.braintrust.dev/docs/guides/evals", "anchor": "evaluate"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Real-time monitoring"}, {"href": "https://www.braintrust.dev/docs/guides/logging", "anchor": "Log"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Data management"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Manage"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "review"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}], "depth": 1}, "https://www.braintrust.dev/pricing": {"url": "https://www.braintrust.dev/pricing", "title": "Pricing - Braintrust", "text": "Get started\nPredictable pricing,\ndesigned to scale\nStart building for free, collaborate with your team, and ship quality AI products.\nFree\n$0 / month\n1 million\nTrace spans\n1 GB\nProcessed data\n10,000\nScores and custom metrics\n14 days\nData retention\nUnlimited\nUsers\nPro\n$249 / month\nUnlimited\nTrace spans\n5 GB\nProcessed data ($3/GB thereafter)\n50,000\nScores and custom metrics ($1.50/1,000 thereafter)\n1 month\nData retention ($3/GB retained thereafter)\nUnlimited\nUsers\nEnterprise\nCustom\nPremium support with on-prem or hosted deployment for high volume or privacy-sensitive data.\nContact usTrusted by the best\nPricing calculator\nEstimate your monthly costs based on your usage\nUsage\nFREE\n1,000,000Tokens per month\n10%Scoring percentage\nStorage\n0.01 GB of 1 GB$0.00\nScores\n38 of 10,000 scores$0.00\nEstimated monthly total\n$0.00\nHow is this calculated?\nFrequently asked questions\nIf you have any additional questions, please contact us", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Get started for free"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/contact", "anchor": "contact us"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 1}, "https://www.braintrust.dev/careers": {"url": "https://www.braintrust.dev/careers", "title": "Careers - Braintrust", "text": "Careers\nBuild with us\nBraintrust is a small team of builders passionate about empowering developers working with AI. We\u2019re looking for highly independent, self-motivated, and creative people to join us.\nCloud Infrastructure EngineerCommercial Account ExecutiveCustomer Solutions ArchitectDesign EngineerEnterprise Account Executive Head of Revenue Operations Open Source Engineer - GoOpen Source Engineer - Java Open Source Engineer - PythonOpen Source Engineer - RubyOpen Source Engineer - Typescript/JavascriptRecruiting CoordinatorSales Development RepresentativeSoftware Engineer, BackendSoftware Engineer, ProductSoftware Engineer, SystemsSolutions EngineerTalent SourcerTechnical Support Engineer", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/careers?ashby_jid=bade7f86-8304-4515-bb39-5671bed35010", "anchor": "Cloud Infrastructure Engineer"}, {"href": "https://www.braintrust.dev/careers?ashby_jid=d0ba915c-85c4-4ac5-bedf-7dd7459ec14d", "anchor": "Commercial Account Executive"}, {"href": "https://www.braintrust.dev/careers?ashby_jid=b2131234-080c-4c5b-85e1-56e3edafa4e3", "anchor": "Customer Solutions Architect"}, {"href": "https://www.braintrust.dev/careers?ashby_jid=57487086-68ae-4c81-8c62-c6157c20e4ac", "anchor": "Design Engineer"}, {"href": "https://www.braintrust.dev/careers?ashby_jid=3ae60654-eeaa-4827-8724-8ff060e2e2e4", "anchor": "Enterprise Account Executive"}, {"href": "https://www.braintrust.dev/careers?ashby_jid=002956ad-7692-4159-a54d-a211c972524f", "anchor": "Head of Revenue Operations"}, {"href": "https://www.braintrust.dev/careers?ashby_jid=2a6c4ee9-063f-45d4-83ba-faf64b1f1a60", "anchor": "Open Source Engineer - Go"}, {"href": "https://www.braintrust.dev/careers?ashby_jid=5d761ea6-bb57-4ae6-ad7d-de39edb0aed6", "anchor": "Open Source Engineer - Java"}, {"href": "https://www.braintrust.dev/careers?ashby_jid=ba4d6676-f65c-42c9-84ee-285f2da15e0d", "anchor": "Open Source Engineer - Python"}, {"href": "https://www.braintrust.dev/careers?ashby_jid=48cf6a86-8b76-45ea-8fdc-033e49204bef", "anchor": "Open Source Engineer - Ruby"}, {"href": "https://www.braintrust.dev/careers?ashby_jid=f93a8de6-f9c4-47dc-a8a9-df8f8dec7223", "anchor": "Open Source Engineer - Typescript/Javascript"}, {"href": "https://www.braintrust.dev/careers?ashby_jid=2dc80993-3029-49bd-b95c-c8f832002619", "anchor": "Recruiting Coordinator"}, {"href": "https://www.braintrust.dev/careers?ashby_jid=319c6491-53c1-46cf-868d-f3b4acba650c", "anchor": "Sales Development Representative"}, {"href": "https://www.braintrust.dev/careers?ashby_jid=9728653e-49b9-4f5c-b6cd-7dbc6a6d5fcc", "anchor": "Software Engineer, Backend"}, {"href": "https://www.braintrust.dev/careers?ashby_jid=7d0a5b62-a554-48f0-b2f7-8cbac0e98ae6", "anchor": "Software Engineer, Product"}, {"href": "https://www.braintrust.dev/careers?ashby_jid=8b9cfa26-627f-442c-a358-783b0e4ef930", "anchor": "Software Engineer, Systems"}, {"href": "https://www.braintrust.dev/careers?ashby_jid=f877a92d-e7ab-4664-abf5-c60fafa789ef", "anchor": "Solutions Engineer"}, {"href": "https://www.braintrust.dev/careers?ashby_jid=6a9b8666-77f2-4405-8ef6-09488074caf4", "anchor": "Talent Sourcer"}, {"href": "https://www.braintrust.dev/careers?ashby_jid=5b64bc4a-7005-4134-9718-8b9772782194", "anchor": "Technical Support Engineer"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 1}, "https://www.braintrust.dev/contact": {"url": "https://www.braintrust.dev/contact", "title": "Contact us - Braintrust", "text": "Contact\nChat with us\nWe would love to hear from you. Fill out the form or email us at info@braintrust.dev. Have a technical question? Get help fast in our Discord channel.\nWe would love to hear from you. Fill out the form or email us at info@braintrust.dev. Have a technical question? Get help fast in our Discord channel.", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "mailto:info@braintrust.dev", "anchor": "info@braintrust.dev"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 1}, "https://www.braintrust.dev/signin": {"url": "https://www.braintrust.dev/signin", "title": "Braintrust - The evals and observability platform for building reliable AI agents", "text": "Sign in to Braintrust\nEmail address\nContinue\nOr\nContinue with Google\nDon't have an account?\nSign up\nBraintrust - The evals and observability platform for building reliable AI agents", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up"}], "depth": 1}, "https://www.braintrust.dev/signup": {"url": "https://www.braintrust.dev/signup", "title": "Braintrust - The evals and observability platform for building reliable AI agents", "text": "Create your Braintrust account\nEmail address\nPassword\nContinue\nOr\nContinue with Google\nAlready have a Braintrust account?\nSign in\nBy signing up, you agree to our\nPrivacy Policy\nand\nTerms of Service\nTrusted by leading AI teams\nBraintrust - The evals and observability platform for building reliable AI agents", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}], "depth": 1}, "https://www.braintrust.dev/blog/atom": {"url": "https://www.braintrust.dev/blog/atom", "title": "Braintrust Blog", "text": "", "links": [], "depth": 1}, "https://www.braintrust.dev/blog/ab-testing-evals": {"url": "https://www.braintrust.dev/blog/ab-testing-evals", "title": "A/B testing can't keep up with AI - Blog - Braintrust", "text": "A/B testing can't keep up with AI\nEvals are still a new practice for many product teams. Engineers often compare them to unit or integration tests, but those analogies don't capture their full potential.\nWhat's really happening is a broader shift: we're moving from one era of product experimentation to another. In a world where interfaces, content, and features can adapt dynamically to each individual, why keep optimizing for the best average experience? The recent acquisitions of Statsig by OpenAI and Eppo by Datadog hint at the turning point: A/B testing is no longer sufficient for AI product optimization. The future is evals.\nUnderstanding evals through A/B testing\nBoth evals and A/B testing start from the same place: you've noticed, heard, or hypothesized that the user experience of your product could be better. Whether you're testing a new onboarding flow or refining AI responses, the goal is to collect data. You want users to be more likely to stay because of better features, more relevant responses, or smoother interactions.\nThe process is remarkably similar too:\n- Start with a hypothesis: In A/B testing, you might hypothesize that \"Users get confused about our getting started page and can't immediately see our value. A more targeted tutorial will improve their experience.\" With evals, it might be: \"Users are rejecting responses because of formatting issues, so the format part of our prompt needs to be fixed.\"\n- Define success criteria: In A/B testing, this could be \"users will use the product more frequently.\" For evals: \"AI responses will better respect users' formatting preferences and users are more likely to accept the change.\"\n- Establish metrics: This might be something like increasing engagement days, or using scorers to evaluate prompt changes.\n- Target specific segments: Focusing your analysis where it matters most, whether it's specific user cohorts or a distinct type of query.\nThe critical difference\nA/B testing assumes it's expensive to create variants. Let's say you're building a new onboarding flow for your banking app. You have two hypotheses:\n- Users will have a better experience if they complete a financial goals assessment first\n- Users want to just jump straight into account setup\nTo test these with an A/B experiment approach, you build each experience out, expose a fraction of your users to each, and compare conversion rates. Each variant requires significant design and engineering work. You really can't explore 20 options at once.\nAI eliminates this constraint. When AI can automatically update your onboarding flow (or itself through prompt modifications), the core assumption behind A/B testing dissolves. You can now have 20 variants, or as many variants as you have users, or just one that updates automatically every 30 minutes based on real-user feedback. This points toward AI-generated interfaces that personalize dynamically for each user rather than optimizing for statistical averages.\nThe new operating model\nInstead of testing a handful of options and waiting weeks for results, teams can now test dozens of variations and see what works immediately. The system learns and improves continuously through the eval feedback loop. Where traditional testing required careful planning and complex analysis, evals give teams direct feedback that anyone can understand.\nMany existing skills from A/B testing still apply: forming hypotheses, defining success criteria, analyzing different user segments, and understanding what improvements actually matter. What's fundamentally different is the shift from building every feature by hand to defining the rules that let systems build themselves. Teams become architects of automated improvement rather than craftspeople manually tweaking each detail.\nWhy this matters now\nWe're not yet at the point where most products automatically generate a different experience for every user, since implementing evals well is non-trivial. But that's clearly the direction we're headed.\nUnderstanding evals through the lens of A/B testing provides a familiar bridge to this new world while highlighting the unique advantages: rapid iteration, infinite variants, immediate feedback, and the ability to optimize continuously rather than in discrete experimental cycles.\nWhen you can use evals instead of A/B tests, you should. Think evals first and put as much optimization as possible there. Of course, there are still important cases where traditional A/B testing remains valuable, like model selection with real-world constraints (use evals to optimize each model's performance, then A/B test how latency differences affect real users) or optimizing non-AI product areas where it's still impractical to use LLMs.\nAt the end of the day, the companies that evolve to think in evals will build products that improve faster than their competitors can run a single A/B test.", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Latest news"}, {"href": "https://www.braintrust.dev/blog/ab-testing-evals", "anchor": "A/B testing can't keep up with AI"}, {"href": "https://www.braintrust.dev/blog/ab-testing-evals", "anchor": "Understanding evals through A/B testing"}, {"href": "https://www.braintrust.dev/blog/ab-testing-evals", "anchor": "The critical difference"}, {"href": "https://www.braintrust.dev/blog/ab-testing-evals", "anchor": "The new operating model"}, {"href": "https://www.braintrust.dev/blog/ab-testing-evals", "anchor": "Why this matters now"}, {"href": "https://www.braintrust.dev/blog/ab-testing-evals", "anchor": "A/B testing can't keep up with AI"}, {"href": "https://www.braintrust.dev/blog/ab-testing-evals", "anchor": "Understanding evals through A/B testing"}, {"href": "https://www.braintrust.dev/blog/ab-testing-evals", "anchor": "The critical difference"}, {"href": "https://www.braintrust.dev/blog/ab-testing-evals", "anchor": "The new operating model"}, {"href": "https://www.braintrust.dev/blog/ab-testing-evals", "anchor": "Why this matters now"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 1}, "https://www.braintrust.dev/blog/graphite": {"url": "https://www.braintrust.dev/blog/graphite", "title": "How Graphite builds reliable AI code review at scale - Blog - Braintrust", "text": "How Graphite builds reliable AI code review at scale\nGraphite is transforming how developers collaborate on code through their suite of developer tools, including their AI-powered code reviewer, Diamond. As countless developers rely on Diamond to provide intelligent comments on their pull requests, the engineering team faces the challenge of building AI that provides consistently actionable, relevant feedback without hallucinations.\nIn this case study, we'll explore how Graphite moved from ad-hoc manual evaluation to a systematic approach that enables them to ship reliable AI features quickly and measure their real-world impact.\nThe technical challenge of AI code review\nBuilding AI code review systems requires solving several complex technical problems. Unlike other AI applications where occasional errors might be tolerable, code review demands precision and relevance so developers can trust the feedback they receive. Diamond faces four key challenges:\n- Contextual relevance: Understanding code changes within their broader project context\n- Actionability: Providing feedback that developers can actually implement\n- Precision: Avoiding false positives that waste developer time and reduce trust\n- Consistency: Maintaining quality across different codebases, languages, and coding styles\nMeasuring what matters: Diamond's key metrics\nTo build effective AI code review, Graphite focuses on metrics that directly correlate with developer satisfaction and trust. Their evaluation framework centers around three critical measurements:\n1. Acceptance rate\nAcceptance rate tracks comments that result in developer action. When a developer sees a Diamond comment and commits the suggested change, this signals that the feedback was both accurate and valuable. Graphite considers this their most important metric because it directly measures whether comments are actionable and relevant.\n2. Upvote rate\nUpvote rate measures comments that developers explicitly mark as valuable. Upvoted comments indicate that developers found the feedback helpful, even if they don't immediately implement the suggestion. This captures broader developer satisfaction with Diamond's suggestions.\n3. Downvote rate\nDownvote rate identifies comments that developers explicitly mark as poor quality. When users downvote a comment, it signals that the feedback was irrelevant, incorrect, or unhelpful. This metric helps the team identify patterns in problematic feedback and areas for improvement.\nManual evaluation approach\nBefore starting systematic evaluation with Braintrust, Graphite's team relied on manual processes that became unmanageable as Diamond scaled.\nTheir initial approach involved:\n- Spreadsheet-based evaluation: They were manually annotating AI-generated comments in spreadsheets\n- Ad-hoc scoring: There was no standardized criteria for what constituted \"good\" feedback\n- Limited collaboration: It was for team members to share and build upon evaluation work\n- Attempted in-house tool: They started building internal evaluation tooling, but the user experience wasn't ideal\nUltimately, the manual process couldn't keep pace with the volume of evaluation needed to iterate quickly.\nSystematic evaluation with Braintrust\nDataset curation\nGraphite developed a sophisticated approach to building evaluation datasets by leveraging their own internal usage of Diamond. The team uses their own codebase as a testing ground, collecting data from Diamond's comments on internal pull requests. They track whether each comment gets accepted or rejected by developers, building a comprehensive dataset from real developer interactions with Diamond's suggestions.\nThis access to real-world data from actual developer interactions gives them realistic insights into how Diamond performs in practice. They can build balanced datasets that include both positive and negative examples to train more robust evaluation systems. Since every internal pull request generates new evaluation data, they have a continuous stream of fresh examples to work with. Most importantly, comments are evaluated in their authentic context where they were actually used, providing more accurate assessments of Diamond's effectiveness.\nThe team maintains separate datasets for different types of feedback: thumbs up comments, thumbs down comments, and accepted versus unaccepted comments. This multi-dimensional approach ensures they can evaluate different aspects of comment quality.\nRather than relying on synthetic metrics, Graphite has built their entire evaluation framework around how developers actually interact with Diamond's suggestions. This user-driven approach creates a continuous improvement cycle where developer actions (accept/ignore) and explicit feedback (upvote/downvote) become training data for evaluation functions, which then guide model improvements.\nScoring functions\nGraphite implemented three primary scoring functions for their specific use case:\n1. Line range validation\nThe line range validation scorer validates that Diamond places comments at the correct line ranges in the code. If their evaluation dataset indicates comments should appear on specific lines, the scorer checks that Diamond's output intersects with those expected ranges. This prevents irrelevant feedback on unrelated code sections.\n2. Semantic similarity\nThe semantic similarity scorer uses Braintrust's Similarity autoeval to ensure consistency across Diamond's feedback. When the team makes changes to their CI integration pipeline, they want to verify that the new LLM pipeline produces comments semantically similar to previous versions. This maintains consistency in Diamond's feedback when processing similar code changes across iterations.\n3. Binary feedback\nThe binary feedback scorer provides simple binary scoring based on explicit user feedback like thumbs up or thumbs down actions. The scorer returns either a 1 or 0, creating clear pass/fail evaluation and direct accountability for comment quality based on actual user responses.\nPutting it all together\nWith curated datasets and scoring functions set up, Graphite implemented a systematic feedback loop:\n- Dataset curation: Collect real Diamond comments from internal PR usage\n- Evaluation: Run new model variants against curated datasets locally and send eval results to Braintrust\n- Analysis: Use Braintrust's comparison views to measure improvements\n- Iteration: Make changes based on insights and repeat the process\nResults\nAfter running evals and iterating on their product, the team has already seen significant improvements in both Diamond's performance and the AI app development experience. For Diamond's custom rule detection feature, where customers define specific coding standards they want enforced, the team observed a 5% reduction in negative rules generated.\nBeyond performance improvements, Graphite has started using Braintrust to make strategic decisions about model deployment. When choosing between different models for specific filters, they run evaluations on both options using their annotated datasets in Braintrust, then compare the results directly through the UI to make data-driven deployment decisions. This systematic approach ensures that every deployment decision is backed by quantitative evidence rather than intuition.\nThe systematic evaluation approach has also significantly accelerated the team's development pace. Having a streamlined process with clear visibility into each data point has made evaluation and dataset annotation much more efficient. The team now benefits from faster iteration cycles with quick feedback on model changes, better decision making through data-driven model selection and feature development, improved collaboration with a shared evaluation framework across the team, and enhanced confidence through quantitative validation before deployment. The ability to visualize evaluation results through Braintrust's UI and benchmark against historical performance has dramatically sped up their iteration cycles.\nKey takeaways\nGraphite's approach demonstrates that building trustworthy AI systems requires moving beyond ad-hoc testing to systematic evaluation workflows. Their success offers a repeatable framework for other teams building AI-powered developer tools:\n- Start by identifying metrics that correlate with real user satisfaction rather than abstract quality measures.\n- Use your own product internally to generate authentic evaluation datasets from actual user interactions.\n- Build custom scoring functions that match your specific domain challenges rather than relying solely on generic metrics.\n- Implement systematic comparison processes to measure improvements objectively and eval new models quickly.\nLearn more about Graphite and Braintrust.\nThank you to Calvin for sharing these insights!", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Latest news"}, {"href": "https://www.braintrust.dev/blog/graphite", "anchor": "How Graphite builds reliable AI code review at scale"}, {"href": "https://www.braintrust.dev/blog/graphite", "anchor": "The technical challenge of AI code review"}, {"href": "https://www.braintrust.dev/blog/graphite", "anchor": "Measuring what matters: Diamond's key metrics"}, {"href": "https://www.braintrust.dev/blog/graphite", "anchor": "1. Acceptance rate"}, {"href": "https://www.braintrust.dev/blog/graphite", "anchor": "2. Upvote rate"}, {"href": "https://www.braintrust.dev/blog/graphite", "anchor": "3. Downvote rate"}, {"href": "https://www.braintrust.dev/blog/graphite", "anchor": "Manual evaluation approach"}, {"href": "https://www.braintrust.dev/blog/graphite", "anchor": "Systematic evaluation with Braintrust"}, {"href": "https://www.braintrust.dev/blog/graphite", "anchor": "Dataset curation"}, {"href": "https://www.braintrust.dev/blog/graphite", "anchor": "Scoring functions"}, {"href": "https://www.braintrust.dev/blog/graphite", "anchor": "1. Line range validation"}, {"href": "https://www.braintrust.dev/blog/graphite", "anchor": "2. Semantic similarity"}, {"href": "https://www.braintrust.dev/blog/graphite", "anchor": "3. Binary feedback"}, {"href": "https://www.braintrust.dev/blog/graphite", "anchor": "Putting it all together"}, {"href": "https://www.braintrust.dev/blog/graphite", "anchor": "Results"}, {"href": "https://www.braintrust.dev/blog/graphite", "anchor": "Key takeaways"}, {"href": "https://www.braintrust.dev/", "anchor": "Braintrust"}, {"href": "https://www.braintrust.dev/blog/graphite", "anchor": "How Graphite builds reliable AI code review at scale"}, {"href": "https://www.braintrust.dev/blog/graphite", "anchor": "The technical challenge of AI code review"}, {"href": "https://www.braintrust.dev/blog/graphite", "anchor": "Measuring what matters: Diamond's key metrics"}, {"href": "https://www.braintrust.dev/blog/graphite", "anchor": "1. Acceptance rate"}, {"href": "https://www.braintrust.dev/blog/graphite", "anchor": "2. Upvote rate"}, {"href": "https://www.braintrust.dev/blog/graphite", "anchor": "3. Downvote rate"}, {"href": "https://www.braintrust.dev/blog/graphite", "anchor": "Manual evaluation approach"}, {"href": "https://www.braintrust.dev/blog/graphite", "anchor": "Systematic evaluation with Braintrust"}, {"href": "https://www.braintrust.dev/blog/graphite", "anchor": "Dataset curation"}, {"href": "https://www.braintrust.dev/blog/graphite", "anchor": "Scoring functions"}, {"href": "https://www.braintrust.dev/blog/graphite", "anchor": "1. Line range validation"}, {"href": "https://www.braintrust.dev/blog/graphite", "anchor": "2. Semantic similarity"}, {"href": "https://www.braintrust.dev/blog/graphite", "anchor": "3. Binary feedback"}, {"href": "https://www.braintrust.dev/blog/graphite", "anchor": "Putting it all together"}, {"href": "https://www.braintrust.dev/blog/graphite", "anchor": "Results"}, {"href": "https://www.braintrust.dev/blog/graphite", "anchor": "Key takeaways"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 1}, "https://www.braintrust.dev/blog/async-programming": {"url": "https://www.braintrust.dev/blog/async-programming", "title": "The rise of async programming - Blog - Braintrust", "text": "The rise of async programming\nI spend a decent amount of time reviewing code I didn't write. An AI agent takes a detailed problem description, writes code (primarily Typescript, Rust, and Python), adds tests, and commits the changes to a branch. I tap back in when everything's ready for review.\nThis used to feel like a futuristic scenario, but it's how I work now, and it's how many developers are starting to work. The shift is subtle but powerful: instead of writing code line by line, we're learning to describe problems clearly and let tools solve them in the background.\nThe async programming workflow\nThis version of \"async programming\" is different from the classic definition. It's about how developers approach building software.\nThe workflow looks like this:\n- Define the problem clearly. Write a detailed specification of what needs to be built, including edge cases, constraints, and success criteria.\n- Hand it off. Delegate the implementation to an AI agent, a teammate, or even your future self with comprehensive notes.\n- Return later. Come back to review results, provide feedback, and decide on next steps.\nThe key difference from traditional programming is the time separation between problem definition and implementation. Instead of immediate feedback loops, you have background problem solving driven by clear requirements and automated verification.\nAsync programming is not vibe coding. Vibe coding enables you to write code without getting into the nitty gritty details. Async programming is a workflow for developers to solve more complex problems simultaneously, while still understanding the details of the code being written. You're still architecting solutions, reviewing implementations, and maintaining a codebase. You're just not typing a vast majority of characters yourself.\nThe three pillars of async programming\nFor async programming to work in practice, you need three things: a clear definition of the problem you're solving, a way to automatically verify your results, and human-driven code review.\n1. Clear problem definitions\nThe quality of your problem statement determines everything else. Vague requirements produce vague results. Precise specifications produce working code.\nHere's the difference:\nVague: \"Make the search faster\"\nPrecise: \"My goal is to reduce search latency from about 800ms to around 200ms. I suspect the root cause is the heap allocation I'm doing on each batch of rows. Can you try refactoring the allocation to happen once per search, instead, and measure the impact?\"\nThe precise version includes the current state, target outcome, proposed approach, and acceptance criteria. An AI agent (or human teammate) can work independently because the requirements are unambiguous.\nEffective async programming specs read like technical documentation: they include context, constraints, examples, and explicit success criteria. If you can't explain the problem clearly, you probably don't understand it well enough to delegate it.\n2. Automated verification\nAsync programming only works if you can verify results without manual testing every edge case. You need systems that can check the work automatically.\nThis might include:\n- Unit and integration tests that validate core functionality\n- Type checking that catches interface mismatches\n- Performance benchmarks that ensure code meets speed requirements\n- Linting and formatting that enforce style guidelines\nThe goal is developing a process that agents can use to validate their work independently. This takes time. You'll provide significant guidance initially, then develop patterns that allow agents to work autonomously. Setting this up in CI is challenging but enables background agents to perform work outside your development environment.\n3. Detailed code review\nOnce you're not typing every character yourself, code review becomes absolutely crucial. I regularly find PRs that solve the completely wrong problem, make poor design decisions, or have large amounts of code duplication.\nReviewing AI-generated code is valuable, similar to traditional code review. Expect to spend significantly more time on code review than before.\nThe code may not be yours line by line, but the system design and technical decisions should still reflect your judgment.\nWhy async programming works\nMy workflow has changed since adopting async programming. I now work on four or five tasks simultaneously: one complex problem synchronously and three or four in the background. When I context switch, I review in-progress work on each background task, provide guidance, and return to synchronous work or code review.\nAsync programming at Braintrust\nWe've been using async programming to build Braintrust itself, and now we're building tools to translate these ideas to AI engineering.\nTraditional prompt engineering is manual. You write a prompt, test it against examples, observe failures, make small adjustments, and repeat. The process requires expertise but involves significant iteration.\nOur agent, Loop, lets you describe the evaluation problem you're trying to solve and spends time in the background analyzing experiment results, identifying patterns in failed test cases, and suggesting improvements to prompts, datasets, and scorers.\nWhere this is heading\nThe implications of working this way are still emerging. This changes what I optimize for as a developer: less time on IDE shortcuts and typing speed, more time explaining problems clearly and reviewing solutions thoroughly.\nThe implementation work can happen in parallel with other thinking. More developers will likely adopt this approach as tools improve. AI isn't replacing programming, but the most valuable parts of programming are becoming more prominent while routine tasks move to the background.", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Latest news"}, {"href": "https://www.braintrust.dev/blog/async-programming", "anchor": "The rise of async programming"}, {"href": "https://www.braintrust.dev/blog/async-programming", "anchor": "The async programming workflow"}, {"href": "https://www.braintrust.dev/blog/async-programming", "anchor": "The three pillars of async programming"}, {"href": "https://www.braintrust.dev/blog/async-programming", "anchor": "1. Clear problem definitions"}, {"href": "https://www.braintrust.dev/blog/async-programming", "anchor": "2. Automated verification"}, {"href": "https://www.braintrust.dev/blog/async-programming", "anchor": "3. Detailed code review"}, {"href": "https://www.braintrust.dev/blog/async-programming", "anchor": "Why async programming works"}, {"href": "https://www.braintrust.dev/blog/async-programming", "anchor": "Async programming at Braintrust"}, {"href": "https://www.braintrust.dev/blog/async-programming", "anchor": "Where this is heading"}, {"href": "https://www.braintrust.dev/blog/async-programming", "anchor": "The rise of async programming"}, {"href": "https://www.braintrust.dev/blog/async-programming", "anchor": "The async programming workflow"}, {"href": "https://www.braintrust.dev/blog/async-programming", "anchor": "The three pillars of async programming"}, {"href": "https://www.braintrust.dev/blog/async-programming", "anchor": "1. Clear problem definitions"}, {"href": "https://www.braintrust.dev/blog/async-programming", "anchor": "2. Automated verification"}, {"href": "https://www.braintrust.dev/blog/async-programming", "anchor": "3. Detailed code review"}, {"href": "https://www.braintrust.dev/blog/async-programming", "anchor": "Why async programming works"}, {"href": "https://www.braintrust.dev/blog/async-programming", "anchor": "Async programming at Braintrust"}, {"href": "https://www.braintrust.dev/blog/async-programming", "anchor": "Where this is heading"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 1}, "https://www.braintrust.dev/blog/gpt-5-vs-claude-opus": {"url": "https://www.braintrust.dev/blog/gpt-5-vs-claude-opus", "title": "GPT-5 vs. Claude Opus 4.1 - Blog - Braintrust", "text": "GPT-5 vs. Claude Opus 4.1\nTwo of the supposedly most capable large language models, OpenAI's GPT-5 (tested 2025-08-07) and Anthropic's Claude Opus 4.1 (2025-08-05), landed within days of each other this week. Both promise significant improvements in reasoning, comprehension, and adaptability, but deciding which to deploy isn\u2019t straightforward. Benchmarks tell part of the story, customer feedback fills in more of the picture, and ultimately, the right choice depends on your workload and constraints.\nWe ran both models through one of the hardest academic benchmarks available, analyzed the results, and talked to customers already experimenting in production. Here\u2019s what we found.\nWant to see GPT-5\nand Opus 4.1\nin action for yourself? You can use the playground to compare different models and prompts side-by-side.\nGeneral breakdown\nClaude Opus 4.1 is fast and efficient, making it well-suited for high-throughput tasks where speed matters. GPT-5 is slower and more expensive but consistently more accurate, especially on multi-step reasoning challenges. If your workload depends on wringing out every possible correct answer, GPT-5 is the stronger option. If you care more about responsiveness and cost, Claude may fit better.\nRunning HLE benchmarks\nWe used Humanity's Last Exam (HLE) \u2014 2,500 PhD-level multiple-choice questions across math, physics, chemistry, linguistics, and engineering \u2014 to push both models to their limits. HLE is intentionally designed so that even frontier models struggle, making it a useful stress test. Let's break down the results by category:\nAccuracy: GPT-5 outperformed Claude by ~62%, with a 28.81% accuracy score compared to Claude's 17.76%. However, it's worth noting that random chance would get you a ~20% accuracy rate.\nCalibration (self-awareness, or understanding what you don\u2019t know): Both models were very overconfident when wrong, but GPT-5 was slightly more self-aware at 51.10%, while Claude came in at 63.64% (a lower percentage is better).\nEfficiency tradeoff: GPT-5\u2019s accuracy advantage comes at a large time and compute cost - it took 190 seconds per question and ~7,000 tokens, while Claude took only 88 seconds per question and ~2,500 tokens.\nReasoning effort\nWe also tested GPT-5 at different reasoning effort levels to determine how reasoning effort might impact accuracy, calibration, and efficiency:\nWe ultimately found that medium effort is the best balance for most production use cases.\nHow customers have been reacting to GPT-5\nEarly feedback from our customers on GPT-5 has been mixed. Some see it as a clear improvement over Claude on the hardest prompts, particularly for niche problem-solving like debugging tricky Spark SQL or identifying subtle shell script issues. Others feel it\u2019s more of a \u201c4.5\u201d update than a full generational leap, especially for agentic use cases where it still requires multiple prompt iterations to deliver optimal results.\nSpeed has been a recurring point of discussion. GPT-5\u2019s latency is noticeably higher than Claude\u2019s, especially when reasoning effort is cranked up. It can also be finicky and pedantic, requiring prompt tuning to hit the right output style. For workloads where latency and cost dominate, customers often still prefer Claude.\nHow to know for sure which to deploy\nBenchmarks like HLE are useful for comparing models under controlled conditions, but they don\u2019t always reflect the nuances of your production environment. The best way to decide is to run your own evaluation that mirrors reality as closely as possible: real prompts, gold-standard answers, cost and latency tracking, and metrics that matter for your use case.\nIf your application depends on complex reasoning, broad world knowledge, or generating reliable code, start with GPT-5 at medium reasoning effort and run Claude as a challenger on the most critical steps. If your workload is dominated by long-document synthesis, policy enforcement, or editorial summarization, start with Claude and only bring in GPT-5 for tasks that require deeper reasoning or integrated code outputs.\nWith Braintrust, you can swap models in production with a single line of code, compare them side-by-side in the playground, and monitor their performance over time. That means you don\u2019t have to commit to a single model blindly \u2014 you can run controlled tests in production, keep the winner, and continuously validate as models evolve.\nIn the end, the question isn\u2019t just \u201cwhich model is better?\u201d It\u2019s \u201cwhich model is better for my workload, at my cost and latency targets, with my success metrics?\u201d\nTry Braintrust today to evaluate the models yourself.", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Latest news"}, {"href": "https://www.braintrust.dev/blog/gpt-5-vs-claude-opus", "anchor": "GPT-5 vs. Claude Opus 4.1"}, {"href": "https://www.braintrust.dev/blog/gpt-5-vs-claude-opus", "anchor": "General breakdown"}, {"href": "https://www.braintrust.dev/blog/gpt-5-vs-claude-opus", "anchor": "Running HLE benchmarks"}, {"href": "https://www.braintrust.dev/blog/gpt-5-vs-claude-opus", "anchor": "Reasoning effort"}, {"href": "https://www.braintrust.dev/blog/gpt-5-vs-claude-opus", "anchor": "How customers have been reacting to GPT-5"}, {"href": "https://www.braintrust.dev/blog/gpt-5-vs-claude-opus", "anchor": "How to know for sure which to deploy"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Braintrust"}, {"href": "https://www.braintrust.dev/blog/gpt-5-vs-claude-opus", "anchor": "GPT-5 vs. Claude Opus 4.1"}, {"href": "https://www.braintrust.dev/blog/gpt-5-vs-claude-opus", "anchor": "General breakdown"}, {"href": "https://www.braintrust.dev/blog/gpt-5-vs-claude-opus", "anchor": "Running HLE benchmarks"}, {"href": "https://www.braintrust.dev/blog/gpt-5-vs-claude-opus", "anchor": "Reasoning effort"}, {"href": "https://www.braintrust.dev/blog/gpt-5-vs-claude-opus", "anchor": "How customers have been reacting to GPT-5"}, {"href": "https://www.braintrust.dev/blog/gpt-5-vs-claude-opus", "anchor": "How to know for sure which to deploy"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 1}, "https://www.braintrust.dev/blog/agent-while-loop": {"url": "https://www.braintrust.dev/blog/agent-while-loop", "title": "The canonical agent architecture: A while loop with tools - Blog - Braintrust", "text": "The canonical agent architecture: A while loop with tools\nFrom personal assistants to complex automated systems, agents are revolutionizing how we interact with technology. For many developers, building a good agent feels like navigating a maze of frameworks, layers of optimization, and tools, each adding its own overhead.\nSurprisingly, many of the most popular and successful agents, including Claude Code and the OpenAI Agents SDK for example, share a common, straightforward architecture: a while loop that makes tool calls.\nHere's the basic structure:\nThat's it. Each iteration passes the current state into a language model, receives back a decision (usually a tool invocation or text response), and moves forward. The agent is just a system prompt and a handful of well-crafted tools.\nThis pattern wins for the same reason as UNIX pipes and React components: it's simple, composable, and flexible enough to handle complexity without becoming complex itself. It naturally extends to more advanced concepts like sub-agents (a tool call that invokes an independent agent loop) and multi-agents (independent agent loops that perform message passing with tool calls). It also allows you to focus on the problems that matter most: tool design, context engineering, and evaluation.\nTool design sets up the LLM for success\nWhen you expose every API argument to a language model, it can become overloaded with irrelevant details and make mistakes. Instead, define each tool with only the essential parameters and provide a clear description tailored to the agent\u2019s task. Only include inputs that are directly relevant to the agent\u2019s objective.\nA common trap is exposing a REST API as a single tool and letting the agent figure it out. This shifts cognitive burden to the agent when that complexity can be absorbed into your tool design. Instead, you can break complex APIs into simple, well-scoped functions tailored to how the agent thinks about the problem.\nHere's a classic example of a tool with too many arguments:\nFor a specific use case, you can likely get away with something much simpler:\nWhen you evaluate the two sets of tools, it's clear that the specific approach scores better:\nContext engineering matters\nEveryone talks about prompt engineering, but most of the agent's context comes from tool inputs and outputs. In a typical agent conversation, tool responses make up 67.6% of the total tokens, while the system prompt accounts for just 3.4%. Tool definitions add another 10.7%, meaning tools comprise nearly 80% of what the agent actually sees.\nIt's important to design tool outputs just like you would a prompt. That means using concise language, filtering out irrelevant bits of data, and formatting them in an easy-to-read way. If you wouldn't want to read a giant blob of JSON, don't dump that into the tool output either.\nThe transcript, or what the agent sees from previous actions, is where a lot of reasoning actually happens. And it's entirely in your control. The goal is to make the agent's job as easy as possible by engineering the context it receives from each tool interaction.\nA good tool output (from the specific search_users\ntool) might look like this:\nEvaluation as a foundation\nWhen you build an agent, you're really building an evaluatable system. This means that in addition to the agent (or task function) itself, you also need to create a representative dataset and a library of scorers. Together, those three components form the foundation you can measure, benchmark, and continuously improve.\nHere are the steps you can take:\n- Create an end-to-end eval that verifies that the agent can complete ambitious tasks driven by a single user input\n- Build a dataset from individual turns where the agent struggles, like when it picks the wrong tool or gives a poor response, and run more evals on those specific failure patterns\n- Use remote evals to test the agent ad-hoc on a number of inputs at once in a playground\nModels will change, prompts will evolve, and tools will get updated, so your eval needs to be durable. This consistency lets you measure progress and catch regressions as you iterate on the agent.\nThe path through complexity\nThe Bitter Lesson appears to apply to agent design as well. At its core, an agent is an LLM, a system prompt, and tools. Keeping your system design to just those components means your work will stand the test of time as new and more powerful models come out.\nThis pattern also absorbs complexity at the edges (in tool design and context engineering) while keeping the core architecture understandable enough for anyone to modify.\nMany teams discover this pattern through experience. They might start with sophisticated frameworks or experiment with graph-based structures and multi-phase planners, but often find that simpler approaches prove more reliable in production. The agents that work consistently tend to converge on similar architectures under the hood.\nFrameworks can be valuable for specific use cases, but the core components remain consistent: a good prompt that defines the agent's role and capabilities, a small set of clean tools designed for the agent's mental model, a transcript window that maintains conversation state, and a while loop that orchestrates everything.\nThe AI ecosystem is evolving fast, with new models, better tools, and novel abstractions. The agents that adapt well to this change are the ones that embrace simplicity and reliability. To see this pattern in action and run the code shown in this blog post, check out our cookbook for building reliable AI agents.", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Latest news"}, {"href": "https://www.braintrust.dev/blog/agent-while-loop", "anchor": "The canonical agent architecture: A while loop with tools"}, {"href": "https://www.braintrust.dev/blog/agent-while-loop", "anchor": "Tool design sets up the LLM for success"}, {"href": "https://www.braintrust.dev/blog/agent-while-loop", "anchor": "Context engineering matters"}, {"href": "https://www.braintrust.dev/blog/agent-while-loop", "anchor": "Evaluation as a foundation"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "remote evals"}, {"href": "https://www.braintrust.dev/blog/agent-while-loop", "anchor": "The path through complexity"}, {"href": "https://www.braintrust.dev/docs/cookbook/recipes/AgentWhileLoop", "anchor": "building reliable AI agents"}, {"href": "https://www.braintrust.dev/blog/agent-while-loop", "anchor": "The canonical agent architecture: A while loop with tools"}, {"href": "https://www.braintrust.dev/blog/agent-while-loop", "anchor": "Tool design sets up the LLM for success"}, {"href": "https://www.braintrust.dev/blog/agent-while-loop", "anchor": "Context engineering matters"}, {"href": "https://www.braintrust.dev/blog/agent-while-loop", "anchor": "Evaluation as a foundation"}, {"href": "https://www.braintrust.dev/blog/agent-while-loop", "anchor": "The path through complexity"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 1}, "https://www.braintrust.dev/blog/five-lessons-evals": {"url": "https://www.braintrust.dev/blog/five-lessons-evals", "title": "Five hard-learned lessons about AI evals - Blog - Braintrust", "text": "Five hard-learned lessons about AI evals\nThe team at Braintrust spends a lot of time deep in evaluation data, helping teams of all sizes ship reliable LLM\u2011powered products. We run the platform that powers those evals and observability workflows, so we see the successes and the failures up close. On an average day, our customers run \u2248 13 evals per org, and the most advanced teams push 3,000+ evals daily while spending hours poring over trace logs. All that usage has taught us five tough lessons. If you\u2019re searching for the most comprehensive LLM evaluation tools in the market today, or an LLM evaluation platform with detailed trace logs for agent workflows, these lessons will show you what \u201cgood\u201d really looks like, and how we built Braintrust to get you there.\n1: Effective evals speak for themselves\nHere\u2019s how I know an org\u2019s eval loop is truly adding value:\nRapid model adoption\nWhen a new model drops, your evals should let you ship it to prod within 24 hours. Notion\u2019s AI team does exactly that, with every major model release showing up in the product the very next day.\nUser\u2011feedback input\nA user files a bug \u2192 you turn that example into an eval case in minutes. That feedback\u2011to\u2011eval path ensures issues never slip through the cracks.\nPlaying offense\nEvals aren\u2019t just regression tests. We use them to validate brand\u2011new features pre\u2011ship, so we already know success rates before users ever touch the product.\nIf those three boxes aren\u2019t checked yet, just keep tightening the loop until the value is undeniable.\n2: Great evals must be engineered\nData engineering\nSynthetic datasets alone won\u2019t cut it. Real users will always do something you didn\u2019t anticipate, so we continuously add production traces back into our eval datasets.\nCustom scoring\nOur open\u2011source autoevals\nlibrary ships with ready\u2011made metrics, but every serious team eventually writes its own scorers. Think of a scorer as the PRD for your AI\u2019s behavior: if you rely on a generic metric, you\u2019re shipping someone else\u2019s requirements, not yours.\n3: Context beats prompts\nModern agents spend far more tokens on tool calls and outputs than on the system prompt itself. That means:\n-\nDesign tools for the model, not just your API. Sometimes we create entirely new \u201cLLM\u2011friendly\u201d endpoints because the original API shape confuses the model.\n-\nOptimize output formats. Switching one internal tool\u2019s output from\nJSON\ntoYAML\nliterally doubled its success rate.YAML\nwas shorter, easier for the model to parse, and cheaper in tokens.\nIf you haven\u2019t audited your agent\u2019s context lately, do it. Prompt tweaks are great, but the surrounding context often moves the needle more.\n4: Be ready for new models to change everything\nWe manage a set of \u201caspirational\u201d evals, which are tests that current models score 10 % on. Every time a new model lands, we swap it in via the Braintrust Proxy (with zero code changes) and rerun the suite. When Claude 4 Sonnet crossed our success threshold last month, we shipped a brand\u2011new feature two weeks later.\nThat turnaround\u2019s only possible because:\n-\nThe evals were already written.\n-\nThe infra made model swapping trivial.\n-\nThe culture said, \u201cIf a new model enables something, drop everything and ship.\u201d\n5: Optimize the whole loop, not just the prompt\nAn eval = data + task (prompt/agent/tools) + scoring. We ran an internal experiment where we asked an LLM to:\nA) optimize only the prompt, vs.\nB) optimize the entire eval (prompt + data + scorers).\nApproach B crushed A, turning an unviable feature into a viable one. Moral: don\u2019t get tunnel vision. If the score is inflated, tighten the metric. If the dataset is stale, enrich it. Holistic tuning compounds.\nMeet Loop: your eval copilot\nTo bake these lessons into the product, we launched Loop, the first AI agent for evals.\nLoop lives in the playground and auto\u2011improves your evals:\n- \u201cOptimize this prompt.\u201d\n- \u201cWhat data am I missing?\u201d\n- \u201cWhy is my score low?\u201d\n- \u201cWrite me a harsher scorer.\u201d\nBehind the scenes, Loop uses Claude 4 Sonnet (or any model you select) to propose and execute changes.\nSumming it up\n-\nEvals that matter enable 24\u2011hour model swaps, feed on real user bugs, and validate features pre\u2011launch.\n-\nEngineer your data pipelines and scorers with the same rigor as production code.\n-\nContext (tools, formats, flows) often matters more than the prompt itself.\n-\nNew models can upend your roadmap. Stay ready with continuous evals and a provider\u2011agnostic proxy.\n-\nOptimize the full loop (data + prompt + scorers), not just single lines of text.\nWe built Braintrust so you can spend less time doing all of this and more time shipping features your users love. Sign up for a free account today, or reach out to learn more about getting your team on Braintrust.", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Latest news"}, {"href": "https://www.braintrust.dev/blog/five-lessons-evals", "anchor": "Five hard-learned lessons about AI evals"}, {"href": "https://www.braintrust.dev/blog/five-lessons-evals", "anchor": "1: Effective evals speak for themselves"}, {"href": "https://www.braintrust.dev/blog/five-lessons-evals", "anchor": "Rapid model adoption"}, {"href": "https://www.braintrust.dev/blog/five-lessons-evals", "anchor": "User\u2011feedback input"}, {"href": "https://www.braintrust.dev/blog/five-lessons-evals", "anchor": "Playing offense"}, {"href": "https://www.braintrust.dev/blog/five-lessons-evals", "anchor": "2: Great evals must be engineered"}, {"href": "https://www.braintrust.dev/blog/five-lessons-evals", "anchor": "Data engineering"}, {"href": "https://www.braintrust.dev/blog/five-lessons-evals", "anchor": "Custom scoring"}, {"href": "https://www.braintrust.dev/docs/autoevals", "anchor": "autoevals"}, {"href": "https://www.braintrust.dev/blog/five-lessons-evals", "anchor": "3: Context beats prompts"}, {"href": "https://www.braintrust.dev/blog/five-lessons-evals", "anchor": "4: Be ready for new models to change everything"}, {"href": "https://www.braintrust.dev/blog/five-lessons-evals", "anchor": "5: Optimize the whole loop, not just the prompt"}, {"href": "https://www.braintrust.dev/blog/five-lessons-evals", "anchor": "Meet Loop: your eval copilot"}, {"href": "https://www.braintrust.dev/blog/five-lessons-evals", "anchor": "Summing it up"}, {"href": "https://braintrust.dev/signup?utm_source=lessons_blog&utm_medium=blog&utm_campaign=lessons_blog_link", "anchor": "Sign up for a free account"}, {"href": "https://www.braintrust.dev/contact?utm_source=lessons_blog&utm_medium=blog&utm_campaign=lessons_blog_link", "anchor": "reach out"}, {"href": "https://www.braintrust.dev/blog/five-lessons-evals", "anchor": "Five hard-learned lessons about AI evals"}, {"href": "https://www.braintrust.dev/blog/five-lessons-evals", "anchor": "1: Effective evals speak for themselves"}, {"href": "https://www.braintrust.dev/blog/five-lessons-evals", "anchor": "Rapid model adoption"}, {"href": "https://www.braintrust.dev/blog/five-lessons-evals", "anchor": "User\u2011feedback input"}, {"href": "https://www.braintrust.dev/blog/five-lessons-evals", "anchor": "Playing offense"}, {"href": "https://www.braintrust.dev/blog/five-lessons-evals", "anchor": "2: Great evals must be engineered"}, {"href": "https://www.braintrust.dev/blog/five-lessons-evals", "anchor": "Data engineering"}, {"href": "https://www.braintrust.dev/blog/five-lessons-evals", "anchor": "Custom scoring"}, {"href": "https://www.braintrust.dev/blog/five-lessons-evals", "anchor": "3: Context beats prompts"}, {"href": "https://www.braintrust.dev/blog/five-lessons-evals", "anchor": "4: Be ready for new models to change everything"}, {"href": "https://www.braintrust.dev/blog/five-lessons-evals", "anchor": "5: Optimize the whole loop, not just the prompt"}, {"href": "https://www.braintrust.dev/blog/five-lessons-evals", "anchor": "Meet Loop: your eval copilot"}, {"href": "https://www.braintrust.dev/blog/five-lessons-evals", "anchor": "Summing it up"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 1}, "https://www.braintrust.dev/blog/braintrust-not-eval-framework": {"url": "https://www.braintrust.dev/blog/braintrust-not-eval-framework", "title": "Braintrust is not an eval framework - Blog - Braintrust", "text": "Braintrust is not an eval framework\nThere's a recurring theme in conversations about AI development tools lately: \"You don't need an eval framework.\" Sometimes Braintrust gets lumped into this category, but Braintrust is not an eval framework.\nBraintrust is infrastructure for building, scaling, and optimizing AI evaluations. And while you might not need an eval framework, you absolutely need eval infrastructure.\nWhat is eval infrastructure?\nEval infrastructure is the set of systems that make it possible to run, visualize, and learn from evaluations at scale. It's what transforms evaluations from academic exercises into practical tools that actually improve your AI products.\nCore components\nInstrumentation. It's really useful to see detailed traces for each test case in an eval. If you're building an agent, seeing each individual LLM call and tool call, for example, is invaluable. If you're working on RAG, it's very helpful to see the retrieved documents. It's also useful to automatically capture metrics like token counts, cost, latency, and error rates.\nProduction data integration. The best evals use real production data. You need systems to capture traces from your application, extract relevant examples, and manage (versioned) datasets. This problem gets really hard at scale. We put a lot of love into the workflow of finding useful traces and saving them to datasets to power your evals. To make this work at scale, we even had to build our own database called Brainstore, which is specifically optimized for handling AI-shaped data.\nReproducibility and versioning. Every eval run needs to be reproducible. This means capturing the state of the world at the time of the eval, including the model(s), dataset, and code. It also means making it straightforward to play with this state. For example, you can open LLM calls, tweak parameters, and re-run them.\nReal-time visualization and exploration. A spreadsheet works great for 100 rows. But modern evals run on thousands or even millions of test cases, and it's non-trivial to visualize that much data, quickly, while still drilling down all the way to individual LLM calls. We work very hard on designing the eval UI to make this intuitive. For example, you can slice and dice by different attributes, quickly see aggregated scores for each group, and even look at diffs per group across experiments.\nScoring infrastructure. Modern scoring functions aren't just simple metrics: they might call LLMs, run code execution, or implement complex domain-specific logic. Running these at scale, handling failures gracefully, and iterating quickly requires specific infrastructure. We run each scoring function in its own sandbox, and deal with creating/running these securely for you.\nAI optimization deeply integrated with your evals. Braintrust exposes all of its core functionality as tools that our agent, Loop, can use to automate the manual gruntwork of evaluations for you. One of my favorite use cases is adding new data to a dataset while taking into account current eval performance.\nPlaygrounds: where infrastructure becomes invisible\nHere's a fun fact: most of our users don't write eval code anymore. They create playgrounds to define and run evaluations interactively. This shift happened once we added enough infrastructure that you didn't have to write code anymore. Once you don't have to, the experience of testing and evaluating becomes much more accessible and efficient for everyone on the team.\nIn a playground, you can tweak a scoring function and immediately see results across your entire dataset. You can test prompts on thousands of datapoints without writing a line of code. The infrastructure handles all the complexity, like parallelization, caching, and error handling, so you can focus on improving your AI.\nThis is the real test of good infrastructure: it becomes invisible. You stop thinking about how to run evals and start thinking about what to evaluate.\nThe case for infrastructure\nThere are compelling reasons to start with solid eval infrastructure rather than logging traces to JSONL\nfiles or querying them with pandas\n:\n-\nInfrastructure engages users across the technical spectrum. Through tools like playgrounds and human annotation, we provide a way for subject matter experts and domain leads (people who might be intimidated by JSON, spreadsheets, or code) to participate. Everyone knows that failing to involve subject matter experts is the surest way to fail at building evals.\n-\nInfrastructure provides battle-tested conventions. We use simple conventions like inputs, outputs, expected, and metadata to log traces. These conventions are shared across your team, queryable, and handle everything from small traces to large agentic interactions with tool calling and multimodal content.\n-\nInfrastructure scales from tens to millions of traces. This is why we built Brainstore. You can send as much data as you want and query everything fast using BTQL (Braintrust Query Language). With multi-turn conversations and agentic systems, traces are getting bigger all the time. Good infrastructure doesn't care about trace size or volume.\nWhat about frameworks?\nThe beauty of infrastructure is that it works with any framework, or no framework at all. Dozens of eval frameworks integrate with Braintrust because they recognize the same thing we do: frameworks handle the \"what\" of evaluation, but infrastructure handles the \"how.\"\nBut Braintrust does come with its own framework that's quite powerful without being constraining. Our primary innovation was turning the otherwise imperative code structure of an eval into a simple declarative API:\nThis forces you to think about the three fundamental challenges of constructing a good eval: data, task, and scores. It handles parallelization and rate limits automatically. And the Eval\nobject itself can be used in more clever ways than just running it. For example, Braintrust has a feature called Remote evals, that allows you to interactively run any Eval\nin our UI.\nBut here's the key: frameworks are optional patterns for organizing your eval code. Infrastructure is the foundation that makes evals actually useful.\nThe bottom line\nI spend a lot of time working on our product, and it turns out that very little of that time is spent on the framework itself. If you look at the git history for framework.ts and framework.py, you'll see they haven't changed much recently.\nInstead, I focus most of my energy on the problems that impact our customers the most: improving our UI and solving for scale. We recently reworked how experiment data is loaded in the UI to make loading them 10x faster. We're currently reworking the UI for defining scoring functions, so it's easier to test and iterate on them without running a full eval.\nThis is what infrastructure work looks like: solving the hard problems that make evaluations actually useful in practice.\nThe next time someone tells you \"you don't need an eval framework,\" they're probably right. But you absolutely need eval infrastructure. At Braintrust, we're laser-focused on building that infrastructure. Whether you use our framework, bring your own, or write raw Python loops, we make sure your evals actually help you ship better AI. Give Braintrust a try, and you'll see why the best AI teams don't talk about their eval framework\u2014they talk about their eval results.", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Latest news"}, {"href": "https://www.braintrust.dev/blog/braintrust-not-eval-framework", "anchor": "Braintrust is not an eval framework"}, {"href": "https://www.braintrust.dev/blog/braintrust-not-eval-framework", "anchor": "What is eval infrastructure?"}, {"href": "https://www.braintrust.dev/blog/braintrust-not-eval-framework", "anchor": "Core components"}, {"href": "https://www.braintrust.dev/blog/brainstore", "anchor": "Brainstore"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/blog/braintrust-not-eval-framework", "anchor": "Playgrounds: where infrastructure becomes invisible"}, {"href": "https://www.braintrust.dev/blog/braintrust-not-eval-framework", "anchor": "The case for infrastructure"}, {"href": "https://www.braintrust.dev/blog/braintrust-not-eval-framework", "anchor": "What about frameworks?"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/blog/braintrust-not-eval-framework", "anchor": "The bottom line"}, {"href": "https://www.braintrust.dev/blog/faster-experiments", "anchor": "how experiment data is loaded in the UI"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Give Braintrust a try"}, {"href": "https://www.braintrust.dev/blog/braintrust-not-eval-framework", "anchor": "Braintrust is not an eval framework"}, {"href": "https://www.braintrust.dev/blog/braintrust-not-eval-framework", "anchor": "What is eval infrastructure?"}, {"href": "https://www.braintrust.dev/blog/braintrust-not-eval-framework", "anchor": "Core components"}, {"href": "https://www.braintrust.dev/blog/braintrust-not-eval-framework", "anchor": "Playgrounds: where infrastructure becomes invisible"}, {"href": "https://www.braintrust.dev/blog/braintrust-not-eval-framework", "anchor": "The case for infrastructure"}, {"href": "https://www.braintrust.dev/blog/braintrust-not-eval-framework", "anchor": "What about frameworks?"}, {"href": "https://www.braintrust.dev/blog/braintrust-not-eval-framework", "anchor": "The bottom line"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 1}, "https://www.braintrust.dev/blog/grok-4": {"url": "https://www.braintrust.dev/blog/grok-4", "title": "Building with Grok 4 - Blog - Braintrust", "text": "11 July 2025 Wayde Gilliam\nxAI recently released their latest family of Grok models: Grok 4 and the premium Grok 4 Heavy. If you missed it, you can catch a replay of the livestream .\nAccording to the xAI team, these reasoning-only models (you can't turn it off) provide substantial improvements over Grok 3, primarily because they were trained to use tools rather than just generalize on their own.\nWhile presenting the latest model as a means to further \"maximizing truth seeing\", Musk boldly claimed that these new models are \"smarter than almost all graduate students in all disciplines simultaneously\" and \"better than PhD students\" when it comes to academic questions.\nBut the real question is, \"Can it create a good 'pelican riding a bicycle' SVG?\"\nSimon Willison has a rather unique test that he runs on new models, where he asks the model to create an image of a pelican riding a bicycle, followed by a request to describe the image it created.\nQuirky tests like these can help us understand the proclivities of various LLMs. You can read Simon's full writeup on his experience with Grok 4 on his blog .\nWith Braintrust, you can evaluate tests like this in a systematic way. In this post, we'll share how you might set up tasks and scorers to\nunderstand how well each model does on these kind of tasks, starting with Grok 4. To make things interesting, we'll define a custom 'LLM-as-Jury' scorer that combines several LLM-as-a-judge scorers from OpenAI, Anthropic, and xAI.\nFor the purposes of this demo, we'll keep things simple and not worry about aligning the scorers to human judgments .\nThe first thing you need to do is create a Braintrust project .\nNext, we'll import some libraries and set up our OpenAI client to call out to xAI. Make sure you have the appropriate API keys configured in your own .env\nfile.\nPython\nimport base64\nimport json\nimport os\nfrom datetime import datetime\nfrom functools import partial\nfrom textwrap import dedent\nimport braintrust as bt\nimport cairosvg\nfrom anthropic import Anthropic\nfrom dotenv import load_dotenv\nfrom IPython.display import SVG , Image, Markdown, display\nfrom openai import OpenAI\nload_dotenv()\n# Grab our Braintrust project\nbt_project = bt.projects.create( name = \"YOUR_PROJECT_NAME\" )\ngrok_client = OpenAI( api_key = os.getenv( \"XAI_API_KEY\" ), base_url = \"https://api.x.ai/v1\" )\nanthropic_client = OpenAI( api_key = os.getenv( \"ANTHROPIC_API_KEY\" ), base_url = \"https://api.anthropic.com/v1\" )\nopenai_client = OpenAI( api_key = os.getenv( \"OPENAI_API_KEY\" ))\nwrapped_grok_client = bt.wrap_openai(grok_client)\nTo run an eval, you need three things:\nSome data (a list of inputs we want to use to evaluate a task on)\nA task (a function like an LLM call that takes a single example from our data to perform some work)\nA scorer (a means to know how well our task performed)\nSince our data will come by way of queries to create and describe an SVG image, we can move on to defining the task we want to evaluate.\nFirst, we need a method to generate an SVG.\nPython\n@bt.traced ()\ndef create_svg_image (image_description: str , client, model_name: str , generation_kwargs: dict = {}):\nrsp = client.chat.completions.create(\nmodel = model_name,\nmessages = [{ \"role\" : \"user\" , \"content\" : image_description}],\n** generation_kwargs,\n)\n# Extract svg content - handle both markdown wrapped and plain SVG\ncontent = rsp.choices[ 0 ].message.content # type: ignore\n# Remove markdown code blocks if present\n# ...\n# Find SVG content if it's embedded in text\nif \"<svg\" in content:\nstart = content.find( \"<svg\" )\nend = content.find( \"</svg>\" ) + 6\nif start != - 1 and end != 5 : # end != 5 means </svg> was found\ncontent = content[start:end]\nsvg_string = content.strip()\nreturn svg_string\nWhen you run this method with some code like this:\nPython\nsvg_string = create_svg_image(\n\"Generate an SVG of a pelican riding a bicycle\" ,\nclient = wrapped_grok_client,\nmodel_name = \"grok-4-0709\" ,\ngeneration_kwargs = { \"max_tokens\" : 10000 },\n)\ndisplay(SVG( data = svg_string))\n... you'll get something like this:\nSecond, we'll need a task that takes an image and uses the same model to generate a description of the image.\nPython\n@bt.traced ()\ndef describe_image (image_path: str , client, model_name: str , generation_kwargs: dict = {}):\nwith open (image_path, \"rb\" ) as image_file:\nimage_data = base64.b64encode(image_file.read()).decode()\nimage_url = f \"data:image/png;base64, { image_data } \"\nrsp = client.chat.completions.create(\nmodel = model_name,\nmessages = [\n{\n\"role\" : \"system\" ,\n\"content\" : \"Describe this image in markdown format. Include the following sections: Simple Description, Main Subject, Background and Setting, Style and Tone \\n Use bullet points for all sections after the Simple Description section.\" ,\n},\n{\n\"role\" : \"user\" ,\n\"content\" : [\n{ \"type\" : \"text\" , \"text\" : \"Describe this image\" },\n{ \"type\" : \"image_url\" , \"image_url\" : { \"url\" : image_url}},\n],\n},\n],\n** generation_kwargs,\n)\ncontent = rsp.choices[ 0 ].message.content # type: ignore\nreturn image_url, content\nThis returns something like this:\n## Simple Description\nThe image depicts a minimalist cartoon illustration of a white bird-like figure with a yellow beak, small wings, and an orange leg extended downward, appearing to interact with a small gray object via directional arrows, all set against a solid black background.\n## Main Subject\n- A central white, oval-shaped figure resembling a cartoon bird or penguin\n- Features a small yellow beak pointing to the right\n- Small, outstretched white wings on either side of the body\n- An orange leg extending downward from the body, with an arrow along it pointing down\n- A small gray oval or blob-like object at the end of the leg\n- A larger downward arrow below the gray object, suggesting motion or direction\n## Background and Setting\n- Entirely solid black, creating a void-like environment\n- No additional scenery, objects, or details present\n- The setting emphasizes isolation and focus on the central subject\n## Style and Tone\n- Highly simplistic and minimalist, using basic geometric shapes like ovals and lines\n- Cartoonish and illustrative, with flat colors and no shading or depth\n- Neutral to slightly whimsical tone, possibly educational or diagrammatic due to the arrows indicating direction or force\nAnd lastly, we'll need a top-level task that puts these all together:\nPython\n@bt.traced ()\ndef create_and_describe_image (image_description: str , client, model_name: str , generation_kwargs: dict = {}):\n# Create SVG Image\nsvg_string = create_svg_image(\nimage_description, client = client, model_name = model_name, generation_kwargs = generation_kwargs\n)\n# Convert SVG to PNG and save\nos.makedirs( \"_temp\" , exist_ok = True )\npng_data = cairosvg.svg2png( bytestring = svg_string.encode( \"utf-8\" ))\nwith open ( \"_temp/created_image.png\" , \"wb\" ) as f:\nf.write(png_data)\n# Ask model to describe the image it created\nimage_url, description = describe_image(\nimage_path = \"_temp/created_image.png\" , client = client, model_name = model_name, generation_kwargs = generation_kwargs\n)\nreturn { \"image_url\" : image_url, \"description\" : description}\nThe last component required to run an eval is one or more scorers. To demonstrate how to build your own custom scorers, we'll define\nan LLM-as-Jury which uses multiple LLM-as-Judge classifiers to derive a final judgement on how well the model did with describing the image it created.\nIn this example, we define OpenAI, Anthropic, and Grok judges, and average their scores to arrive at a final verdict.\nPython\nclass LikertScale ( BaseModel ):\nscore: int = Field(\n... ,\ndescription = \"A score between 1 and 5 (1 is the worst score and 5 is the best score).\" ,\nmin_value = 1 ,\nmax_value = 5 ,\n) # type: ignore\nrationale: str = Field( ... , description = \"A rationale for the score.\" )\ndef ask_llm_judge_about_image_description (client, model_name, input, output):\ngen_kwargs = { \"response_format\" : LikertScale}\nif model_name.startswith( \"claude\" ):\ngen_kwargs = {}\nrsp = client.chat.completions.parse(\nmodel = model_name,\nmessages = [\n{\n\"role\" : \"system\" ,\n\"content\" : dedent( \"\"\" \\\nYou are a critical expert in determining if a generated image matches what the user asked for and whether or not an AI model did a good job in describing that image.\nThe score must be an integer between 1 and 5. You should respond ONLY with a JSON object with this format: {score : int, rationale:str} . Make sure you escape any characters that are not valid JSON.\nOnly response with a string that can be parsed as JSON using `json.loads()`. Double check your work!\n\"\"\" ),\n},\n{\n\"role\" : \"user\" ,\n\"content\" : [\n{ \"type\" : \"text\" , \"text\" : f \"Here is the image generated from the description: {input} \" },\n{ \"type\" : \"image_url\" , \"image_url\" : { \"url\" : output[ \"image_url\" ]}},\n{\n\"type\" : \"text\" ,\n\"text\" : f \"Here is the description of the generated image: { output[ 'description' ] } \" ,\n},\n{\n\"type\" : \"text\" ,\n\"text\" : \"Return a score between 1 and 5 based on how well the image matches the description and how well the description matches the image. 1 is the worst score and 5 is the best score.\" ,\n},\n],\n},\n],\n** gen_kwargs,\n)\nif model_name.startswith( \"claude\" ):\nparsed = json.loads(rsp.choices[ 0 ].message.content)\nreturn (parsed[ \"score\" ] - 1 ) / 4\nelse :\nparsed: LikertScale = rsp.choices[ 0 ].message.parsed\nreturn (parsed.score - 1 ) / 4\ndef is_good_description (input, output, expected = None , metadata = None ):\noai_judge_score = partial(\nask_llm_judge_about_image_description, client = openai_client, model_name = \"gpt-4o\" , input = input , output = output\n)()\nanthropic_judge_score = partial(\nask_llm_judge_about_image_description,\nclient = anthropic_client,\nmodel_name = \"claude-3-5-sonnet-20240620\" ,\ninput = input ,\noutput = output,\n)()\ngrok_judge_score = partial(\nask_llm_judge_about_image_description,\nclient = wrapped_grok_client,\nmodel_name = \"grok-4-0709\" ,\ninput = input ,\noutput = output,\n)()\nreturn [\nScore( name = \"is_good_description_judge_oai\" , score = oai_judge_score),\nScore( name = \"is_good_description_judge_anthropic\" , score = anthropic_judge_score),\nScore( name = \"is_good_description_judge_grok\" , score = grok_judge_score),\nScore( name = \"is_good_description_jury\" , score = (oai_judge_score + anthropic_judge_score + grok_judge_score) / 3 ),\n]\nWhen we run that against our outputs from create_and_describe_image()\n, we'll get something like this to add to our traces:\nscore = is_good_description(\ninput = \"Create an SVG of a two cats riding a bicycle\" ,\noutput = rsp,\n)\nscore\n# [Score(name='is_good_description_judge_oai', score=1.0, metadata={}, error=None),\n# Score(name='is_good_description_judge_anthropic', score=0.75, metadata={}, error=None),\n# Score(name='is_good_description_judge_grok', score=1.0, metadata={}, error=None),\n# Score(name='is_good_description_jury', score=0.9166666666666666, metadata={}, error=None)]\nHere, we'll run a single eval with Grok 4, but this can also be extended to add more image descriptions and tests with different models.\nPython\ncurrent_date_str = datetime.now().strftime( \"%Y%m %d %H\" )\nprint (current_date_str)\n# This code was written to run in a Jupyter notebook\nawait bt.EvalAsync(\nname = \"YOUR_PROJECT_NAME\" ,\nexperiment_name = f \"reasoning-xai-grok4-0709- { current_date_str } \" ,\ndata =lambda : [bt.EvalCase( input = \"Generate an SVG of a pelican riding a bicycle\" )], # type: ignore\ntask = partial(\ncreate_and_describe_image,\nclient = wrapped_grok_client,\nmodel_name = \"grok-4-0709\" ,\ngeneration_kwargs = { \"max_tokens\" : 10000 },\n),\nscores = [is_good_description],\nmetadata = { \"vendor\" : \"xai\" , \"model\" : \"grok-4-0709\" },\n)\nRunning this returns this nice summary:\n=========================SUMMARY=========================\nreasoning-xai-grok4-0709-2025071115-ea1bd6d0 compared to reasoning-xai-grok4-0709-2025071115:\n75.00% 'is_good_description_judge_anthropic' score\n75.00% 'is_good_description_judge_grok' score\n100.00% 'is_good_description_judge_oai' score\n83.33% 'is_good_description_jury' score\n1752272157.79s start\n1752272241.95s end\n32.16s (-126.19%) 'duration' (1 improvements, 0 regressions)\n16.06s (-62.58%) 'llm_duration' (1 improvements, 0 regressions)\n326tok (-) 'prompt_tokens' (0 improvements, 0 regressions)\n1062tok (-5300.00%) 'completion_tokens' (1 improvements, 0 regressions)\n1388tok (-5300.00%) 'total_tokens' (1 improvements, 0 regressions)\n4tok (-) 'prompt_cached_tokens' (0 improvements, 0 regressions)\n0tok (-) 'prompt_cache_creation_tokens' (0 improvements, 0 regressions)\nSee results for reasoning-xai-grok4-0709-2025071115-ea1bd6d0 at https://www.braintrust.dev/app/braintrustdata.com/p/<your-project-name>/experiments/reasoning-xai-grok4-0709-2025071115-ea1bd6d0\nEvalResultWithSummary(summary=\"...\", results=[...])\nBased on the results, it looks like our jury thinks Grok 4 did well, with Anthropic giving it maximal praise.\nWith Braintrust, we can quickly view, aggregate, and add more experiments to better understand how well different models perform on this task.\nYou can select any experiment to see the individual eval trace:\nMore evals of course.\nIn addition to improving the scorers, you can add more image descriptions to test these models out, as well as test more models. Braintrust makes it easy to group\nand aggregate results by vendor or model family so that you can systematically measure the progress of these models over time.\nIf you have any interesting tests you run when a new model comes out, let us know !", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Latest news"}, {"href": "https://www.braintrust.dev/blog/grok-4", "anchor": "Building with Grok 4"}, {"href": "https://www.braintrust.dev/blog/grok-4", "anchor": "The pelican baseline"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "aligning the scorers to human judgments"}, {"href": "https://www.braintrust.dev/blog/grok-4", "anchor": "Setup"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "project"}, {"href": "https://www.braintrust.dev/blog/grok-4", "anchor": "Tasks"}, {"href": "https://www.braintrust.dev/blog/grok-4", "anchor": "Scoring"}, {"href": "https://www.braintrust.dev/blog/grok-4", "anchor": "Run evals"}, {"href": "https://www.braintrust.dev/blog/grok-4", "anchor": "What\u2019s next?"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/interpret", "anchor": "group and aggregate results"}, {"href": "https://www.braintrust.dev/blog/grok-4", "anchor": "Building with Grok 4"}, {"href": "https://www.braintrust.dev/blog/grok-4", "anchor": "The pelican baseline"}, {"href": "https://www.braintrust.dev/blog/grok-4", "anchor": "Setup"}, {"href": "https://www.braintrust.dev/blog/grok-4", "anchor": "Tasks"}, {"href": "https://www.braintrust.dev/blog/grok-4", "anchor": "Scoring"}, {"href": "https://www.braintrust.dev/blog/grok-4", "anchor": "Run evals"}, {"href": "https://www.braintrust.dev/blog/grok-4", "anchor": "What\u2019s next?"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 1}, "https://www.braintrust.dev/blog/faster-experiments": {"url": "https://www.braintrust.dev/blog/faster-experiments", "title": "Experiments UI: Now 10x faster - Blog - Braintrust", "text": "Experiments UI: Now 10x faster\nExperiments in Braintrust are now significantly faster, powered by Brainstore. Here's how we solved a critical performance challenge.\nThe challenge\nExperiments are core to the evaluation feedback loop in Braintrust. They're where you analyze results over time through rich visualizations and detailed metrics. You can instantly identify score regressions by specific fields and compare megabytes of JSON using side-by-side diffs, all while monitoring key metrics like cost, token counts, and error rates.\nBehind the scenes, we initially leveraged DuckDB for client-side computation in the browser. This required loading all spans for an experiment into the browser and running queries to aggregate and calculate summary scores and metrics for each test case. This worked well at first, and enabled us to quickly iterate on features like custom filters, metric and score comparisons, and multi-experiment views. We could also ship updates without requiring customers using our hybrid architecture to redeploy.\nHowever, experiments have grown exponentially over the past year. What began as 10\u201320 test cases per experiment has expanded to hundreds of thousands. The test cases themselves have also become more complex, incorporating multi-step agents and extensive external context.\nAs the volume of data grew, we began to hit the limits of browser-based processing. Memory constraints led to errors when DuckDB tried to process larger amounts of data and execute calculations with complex joins and subqueries.\nThe solution\nMoving these calculations to the backend wouldn't solve the problem. The core issue was that SQL databases have trouble handling complex JSON structures and large text volumes. This was the same challenge that originally motivated us to create Brainstore for logs.\nBrainstore is our high-performance analytics engine purpose-built for querying massive volumes of semi-structured LLM data. We initially supported spans\nand traces\nquery shapes, which provide access to raw, minimally processed data. To support fast, aggregated views, we\u2019ve now introduced a new summary\nquery shape.\nThe summary\nshape enables row-per-input aggregation over trace and span data, computing scores, applying filters, and projecting custom fields in a single request. Unlike SQL queries that would require complex GROUP BY\nor JOIN\noperations, Brainstore lets us control exactly how data is computed. For example, we can efficiently fetch all spans within a trace in one request.\nReal-time visibility\nOur customers want to view experiment results immediately after running them. This is at odds with the traditional architecture of a columnstore which has high latency consumption.\nBrainstore merges realtime data from its write-ahead log (WAL) with historical data from its object storage-backed columnstore in a single pass. This makes recent test results instantly available in queries without waiting for background indexing, while still allowing for fast access to older experiment data. Some experiment logic is implemented across both systems to ensure consistent, accurate results, no matter where the data is coming from.\nShowing realtime data was more straightforward when we loaded the raw experiment data directly in the browser, because we could merge incoming rows into the base data and recompute the summary directly in the UI. We know how important viewing live data is to our customers though, so figuring this out in the new method was non-negotiable.\nWe still ended up using DuckDB to help us merge rows (mostly so that we did not have to refactor downstream UIs that still rely on it), but reworked realtime to limit the amount of data the browser receives. When an experiment loads, we do two steps to enable the realtime flow:\nCREATE TABLE <exp_summary_table> ... ADD PRIMARY KEY (id)\nwith the rows from the initialsummary\nquery executed on page load.- Open a channel that receives an event every time a span is updated on that experiment.\nThe summary for each row now happens on the backend, so when an event arrives the root_span_id\nis added to a Set\n. A throttle\nfunction then batch refetches rows via a summary\nquery, filtering for all collected root_span_id\nvalues. With the result of that query, we do some magic to parse the scores and metrics to ensure any new columns appear in the table, and then\nFinally, the UI is informed of this data refresh and the new summaries appear in the table, ensuring all the latest data is available to you.\nEfficient preview aggregation\nThe final summary result returns a smaller payload to the browser by truncating user-defined fields (like input\n, output\n, metadata\n) based on the preview_length\nparameter in the query, as these fields can contain deeply nested JSON.\nIn a SQL database, it\u2019s only possible to truncate string previews after fully serializing a document. Believe it or not, this is quite slow. Brainstore avoids this by utilizing a custom streaming JSON serializer that is around 8x faster for large objects with many small strings (2MB+). In the case of one customer, this optimization reduced load time by six seconds for a single large experiment.\nAggregate scores and custom columns\nWhen customers define weighted scores and custom columns in the UI, the values need to be pulled from deeply nested structures. Summary queries in Brainstore support rich custom logic by incorporating aggregate scores and custom columns directly into the query plan. These expressions are compiled during planning and evaluated during execution.\nUnlike traditional warehouses that force JSON into rigid schemas, Brainstore processes the raw semi-structured data directly: computing weighted scores, adding comparison keys, and resolving custom columns during query execution. These fields are applied only after retrieving the relevant spans, ensuring projections happen exclusively on the necessary data.\nDynamic filtering across all fields\nWe also need to enable customers to filter across everything from basic fields like input\nor metadata.tags\nto computed metrics like duration\nor prompt_tokens\n, and even custom-defined fields.\nThe basic fields like input\nare present in the raw data and can be filtered before any aggregation. Other fields like duration\nare computed across multiple spans in a trace. We take into account the duration for each span to get the duration for the full test case, so filters on these fields must be handled post-aggregation.\nIn traditional SQL systems, filtering can be extremely slow and resource-intensive because you cannot stream or limit the number of rows you are computing on. Brainstore has custom logic to efficiently analyze the query and determine which filters can be executed pre-aggregation and which must be post-aggregation. This split-planning lets Brainstore efficiently compute post-aggregation filters without sacrificing performance. It also enables something no other provider offers: the ability to define custom fields and filter on arbitrary, computed metrics.\nBy bringing summary computation server-side and pairing it with Brainstore\u2019s real-time, schema-flexible architecture, we\u2019ve made it possible to analyze millions of test cases in seconds with no trade-offs between speed, flexibility, or correctness.\nBenchmark comparison\nThe table below shows a few examples of the impact this optimization has made for some of our customers. The yellow columns are the numbers before the optimization, and the green columns are after.\nNote: The notable difference in the yellow section between /btql response time\nand loaded in UI\nreflects the summary computations performed in the UI.\nWe've significantly enhanced overall performance and customer experience. Still, we're actively working to further reduce table loading times following the completion of the /btql\nrequest. Stay tuned for additional updates and improvements.\nFaster experiences across Braintrust\nThese performance upgrades extend beyond experiments. They are also available in log and dataset tables, for a consistently fast experience across Braintrust.\nIf your team needs scalable, high-performance LLM observability, reach out.", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Latest news"}, {"href": "https://www.braintrust.dev/blog/faster-experiments", "anchor": "Experiments UI: Now 10x faster"}, {"href": "https://www.braintrust.dev/blog/faster-experiments", "anchor": "The challenge"}, {"href": "https://www.braintrust.dev/blog/faster-experiments", "anchor": "The solution"}, {"href": "https://www.braintrust.dev/blog/brainstore", "anchor": "Brainstore"}, {"href": "https://www.braintrust.dev/blog/faster-experiments", "anchor": "Real-time visibility"}, {"href": "https://www.braintrust.dev/blog/faster-experiments", "anchor": "Efficient preview aggregation"}, {"href": "https://www.braintrust.dev/blog/faster-experiments", "anchor": "Aggregate scores and custom columns"}, {"href": "https://www.braintrust.dev/blog/faster-experiments", "anchor": "Dynamic filtering across all fields"}, {"href": "https://www.braintrust.dev/blog/faster-experiments", "anchor": "Benchmark comparison"}, {"href": "https://www.braintrust.dev/blog/faster-experiments", "anchor": "Faster experiences across Braintrust"}, {"href": "https://www.braintrust.dev/contact", "anchor": "reach out"}, {"href": "https://www.braintrust.dev/blog/faster-experiments", "anchor": "Experiments UI: Now 10x faster"}, {"href": "https://www.braintrust.dev/blog/faster-experiments", "anchor": "The challenge"}, {"href": "https://www.braintrust.dev/blog/faster-experiments", "anchor": "The solution"}, {"href": "https://www.braintrust.dev/blog/faster-experiments", "anchor": "Real-time visibility"}, {"href": "https://www.braintrust.dev/blog/faster-experiments", "anchor": "Efficient preview aggregation"}, {"href": "https://www.braintrust.dev/blog/faster-experiments", "anchor": "Aggregate scores and custom columns"}, {"href": "https://www.braintrust.dev/blog/faster-experiments", "anchor": "Dynamic filtering across all fields"}, {"href": "https://www.braintrust.dev/blog/faster-experiments", "anchor": "Benchmark comparison"}, {"href": "https://www.braintrust.dev/blog/faster-experiments", "anchor": "Faster experiences across Braintrust"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 1}, "https://www.braintrust.dev/blog/eval-playgrounds": {"url": "https://www.braintrust.dev/blog/eval-playgrounds", "title": "Eval playgrounds for faster, focused iteration - Blog - Braintrust", "text": "Eval playgrounds for faster, focused iteration\nEvaluations, or evals, are the secret ingredient for exceptional AI products. Crafting great prompts and choosing powerful models is important, but evals are what shape the reliability, usability, and ultimately, the success of AI systems. An eval is made up of clearly defined tasks (the scenarios your AI model needs to perform), scorers (success criteria), and datasets (the curated inputs that test your task\u2019s capability).\nIf you think of experiments in Braintrust like making a pull request to your repository, eval playgrounds are like editing and refining code in your IDE. We built eval playgrounds to complement experiments and provide an environment that significantly accelerates the iteration loop by letting you run full evaluations directly in a powerful editor UI.\nA UX-first approach to evals\nPlaygrounds embed tasks, scorers, and datasets directly into a single intuitive UI. You can:\n- Define and refine tasks to explore model and prompt performance\n- Adjust scoring functions on the fly to immediately gauge the impact of changes\n- Curate and expand datasets to explore more edge cases\nPlaygrounds maintain state and allow you to run the same underlying Eval\ncapabilities as formal experiments. You can quickly iterate, refine, and optimize parameters before committing to a formal snapshot. This approach means you can experiment more freely, collaborate more effectively, and achieve higher productivity across your AI teams.\nWhy UX matters\nCustomers using eval playgrounds in production consider it to be a critical part of their eval workflow. To support their expanding needs, the Clinical AI team at Ambience Healthcare transitioned from earlier methods involving manual data handling and separate prompt repositories to a more cohesive and advanced toolkit with Braintrust. With playgrounds, they've:\n- Reduced evaluation time by 50% with instant custom scorer editing.\n- Tripled their dataset size capabilities, uncovering deeper insights.\n- Leveraged collaborative real-time prompts, latency tracking, and trace comparisons, for clearer, faster, and more confident decision making.\nPlaygrounds fundamentally changed how we iterate\u2014we move twice as quickly without sacrificing precision.\nDifferentiated by design\nEvaluating an AI system has multiple parts: assessing nuanced decision-making, responsiveness to changing conditions, and safe outcomes, even in unpredictable environments. Eval playgrounds let you do all of this in one place by combining quick iteration, side-by-side trace comparisons, and scalable large dataset runs. This design enables AI teams to:\n- Handle larger datasets\n- Replace subjective assessments with objective, measurable metrics\n- Integrate evaluation results into organizational workflows and decisions\nEval playgrounds provide the design-driven approach and rapid iteration necessary to turn good ideas into great business outcomes. Try playgrounds today to rapidly iterate towards better AI products.", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Latest news"}, {"href": "https://www.braintrust.dev/blog/eval-playgrounds", "anchor": "Eval playgrounds for faster, focused iteration"}, {"href": "https://www.braintrust.dev/blog/eval-playgrounds", "anchor": "A UX-first approach to evals"}, {"href": "https://www.braintrust.dev/blog/eval-playgrounds", "anchor": "Why UX matters"}, {"href": "https://www.braintrust.dev/blog/eval-playgrounds", "anchor": "Differentiated by design"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Try playgrounds today"}, {"href": "https://www.braintrust.dev/blog/eval-playgrounds", "anchor": "Eval playgrounds for faster, focused iteration"}, {"href": "https://www.braintrust.dev/blog/eval-playgrounds", "anchor": "A UX-first approach to evals"}, {"href": "https://www.braintrust.dev/blog/eval-playgrounds", "anchor": "Why UX matters"}, {"href": "https://www.braintrust.dev/blog/eval-playgrounds", "anchor": "Differentiated by design"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 1}, "https://www.braintrust.dev/blog/coursera": {"url": "https://www.braintrust.dev/blog/coursera", "title": "How Coursera builds next-generation learning tools - Blog - Braintrust", "text": "How Coursera builds next-generation learning tools\nCoursera is a global online learning platform serving millions of learners and enterprise customers. As they began adopting large language models (LLMs) to enhance their user experience, particularly with their Coursera Coach chatbot and AI-assisted grading tools, they quickly realized the need for a better evaluation workflow. In this case study, we'll share how Coursera built a structured evaluation process to quickly ship reliable AI features that customers love.\nScaling AI evaluation\nBefore establishing a formal evaluation framework, Coursera relied on fragmented offline jobs in spreadsheets and human labeling processes. Their teams used manual data reviews for error detection. It was also difficult for teams to collaborate on evaluations, because each group wrote their own scripts. This made it difficult to quickly validate AI features and confidently push them to production.\nThe business impact of AI features\nTo emphasize just how important it was for Coursera to get these AI features right, let's dig into the significant business impact and metrics that demonstrate their value. These aren't just experimental technologies, they're core features delivering measurable results for learners and the company.\nThe Coursera Coach serves as a 24/7 learning assistant and psychological support system for students, maintaining an impressive 90% learner satisfaction rating1. The impact extends beyond satisfaction metrics\u2014users engaging with Coach complete courses faster and finish more courses overall. By providing judgment-free assistance at any hour, Coach has become an integral part of the learning experience.\nAutomated grading addresses a critical scaling challenge in Coursera's educational model. Before implementing AI, grading was done manually by teaching assistants and peers, with both approaches facing limitations to scale. Teaching assistants were capable of evaluating learners' skills, but at high cost. Peer grading helped with scale but often resulted in variable feedback quality. The automated system now provides consistent, fair assessment with actionable feedback, significantly reducing grading time while maintaining educational quality. Learners now receive grades within 1 minute of submission and benefit from approximately 45\u00d7 more feedback, driving a 16.7% increase in course completions within a day of peer review2.\nEvaluating AI features with Braintrust\nThe teams at Coursera use a four-step approach to evaluating their AI features.\n1. Define clear evaluation criteria upfront\nThey begin by establishing exactly what \"good enough\" looks like before development begins. For each AI feature, they identify specific output characteristics that matter most to their users and business goals.\nFor the Coach chatbot, they evaluate various quality metrics including response appropriateness, formatting consistency, content relevance, and performance standards for natural conversation flow. Their automated grading system is measured on alignment with human evaluation benchmarks, feedback effectiveness, clarity of assessment criteria, and equitable evaluation across diverse submissions.\nKey practice: Define what success looks like before building, not after.\n2. Curate targeted datasets\nDataset quality drives evaluation quality, which is why Coursera invests in creating comprehensive test data. Their team manually reviews anonymized chatbot transcripts and human-graded assignments, paying special attention to interactions with explicit user feedback (like thumbs up/down ratings). They supplement this example data with synthetic datasets generated by LLMs to test edge cases and extract challenging real-world examples that might expose weaknesses.\nThis balanced approach ensures their evaluation covers both typical use cases and the edge scenarios where AI typically struggles, giving them confidence that new features will perform well across all situations.\nKey practice: Balance real-world examples with synthetic data to test both common scenarios and edge cases.\n3. Implement both heuristic and model-based scorers\nCoursera's evaluation approach combines the precision of code-based checks with the nuance of AI-based judgments. Their heuristic checks provide deterministic evaluation of objective criteria, like format and response structure. For more subjective assessment, they employ LLM-as-a-judge evaluations to assess quality across multiple dimensions, including response accuracy and alignment with core teaching principles.\nThey round out evaluation with performance metrics that monitor latency, response time, and resource utilization to make sure AI features maintain operational excellence in addition to output quality.\nKey practice: Create a mix of deterministic checks and AI-based evaluations to balance strict requirements with nuanced quality assessment.\n4. Run evaluations and iterate rapidly\nWith evaluation infrastructure in place through Braintrust, Coursera maintains continuous quality awareness through three tracks. Their online monitoring logs production traffic through evaluation scorers, tracking real-time performance against established metrics and alerting on significant deviations. Offline testing runs comprehensive evaluations on curated datasets, comparing performance across different model parameters and detecting potential regressions before deployment.\nFor new features, their rapid prototyping process creates sample use cases in Braintrust's playground, comparing different models and testing feasibility before committing to full development. This approach allows them to catch issues early, communicate findings clearly across teams, and iterate quickly based on concrete data.\nKey practice: Establish both real-time monitoring and batch testing processes to continuously validate AI performance.\nResults: Better AI features, faster development\nCoursera's structured evaluation framework has transformed their AI development process with benefits across their organization. Teams now validate changes with objective measures, significantly increasing development confidence. The data-driven approach moves ideas from concept to release faster, with clear metrics supporting go/no-go decisions. Perhaps most importantly, standardized evaluation metrics have created a common language for discussing AI quality across teams and roles, while enabling more comprehensive and thorough testing than was previously possible.\nAs a more concrete example, early automated grading prototypes focused on valid submissions. Through their structured evaluation process, the team found that providing vague answers would still result in a high score. They were able to go back and evaluate more examples of negative test cases, resulting in overall better quality.\nPractical lessons for organizations adopting AI evaluation\nBased on Coursera's experience, here are the key takeaways for implementing your own AI evaluation system:\n- Start with clear success criteria: Define what \"good\" looks like before building, not after.\n- Balance evaluation methods: Use both deterministic checks for non-negotiable requirements and AI-based evaluation for more subjective quality aspects.\n- Build realistic test data: Invest in dataset curation that reflects actual use cases, including edge cases where AI typically struggles.\n- Consider the full spectrum of metrics: Evaluate not just output quality but also operational aspects like latency and resource usage.\n- Integrate evaluation throughout development: Make testing a continuous process, not just a final validation step.\nBy establishing a robust evaluation foundation, Coursera has positioned itself to confidently expand AI features while maintaining quality and user trust. If you\u2019re looking to do the same, get in touch.\nLearn more about Coursera and Braintrust.\nThank you to Winnie and Sophie for sharing these insights!\n1 Coursera Coach Learner Survey, Q1 2025", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Latest news"}, {"href": "https://www.braintrust.dev/blog/coursera", "anchor": "How Coursera builds next-generation learning tools"}, {"href": "https://www.braintrust.dev/blog/coursera", "anchor": "Scaling AI evaluation"}, {"href": "https://www.braintrust.dev/blog/coursera", "anchor": "The business impact of AI features"}, {"href": "https://www.braintrust.dev/blog/coursera", "anchor": "Evaluating AI features with Braintrust"}, {"href": "https://www.braintrust.dev/blog/coursera", "anchor": "1. Define clear evaluation criteria upfront"}, {"href": "https://www.braintrust.dev/blog/coursera", "anchor": "2. Curate targeted datasets"}, {"href": "https://www.braintrust.dev/blog/coursera", "anchor": "3. Implement both heuristic and model-based scorers"}, {"href": "https://www.braintrust.dev/blog/coursera", "anchor": "4. Run evaluations and iterate rapidly"}, {"href": "https://www.braintrust.dev/blog/coursera", "anchor": "Results: Better AI features, faster development"}, {"href": "https://www.braintrust.dev/blog/coursera", "anchor": "Practical lessons for organizations adopting AI evaluation"}, {"href": "https://www.braintrust.dev/contact", "anchor": "get in touch"}, {"href": "https://www.braintrust.dev/", "anchor": "Braintrust"}, {"href": "https://www.braintrust.dev/blog/coursera", "anchor": "How Coursera builds next-generation learning tools"}, {"href": "https://www.braintrust.dev/blog/coursera", "anchor": "Scaling AI evaluation"}, {"href": "https://www.braintrust.dev/blog/coursera", "anchor": "The business impact of AI features"}, {"href": "https://www.braintrust.dev/blog/coursera", "anchor": "Evaluating AI features with Braintrust"}, {"href": "https://www.braintrust.dev/blog/coursera", "anchor": "1. Define clear evaluation criteria upfront"}, {"href": "https://www.braintrust.dev/blog/coursera", "anchor": "2. Curate targeted datasets"}, {"href": "https://www.braintrust.dev/blog/coursera", "anchor": "3. Implement both heuristic and model-based scorers"}, {"href": "https://www.braintrust.dev/blog/coursera", "anchor": "4. Run evaluations and iterate rapidly"}, {"href": "https://www.braintrust.dev/blog/coursera", "anchor": "Results: Better AI features, faster development"}, {"href": "https://www.braintrust.dev/blog/coursera", "anchor": "Practical lessons for organizations adopting AI evaluation"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 1}, "https://www.braintrust.dev/blog/best-practices": {"url": "https://www.braintrust.dev/blog/best-practices", "title": "Webinar recap: Eval best practices - Blog - Braintrust", "text": "Webinar recap: Eval best practices\nThanks to everyone who joined our webinar, In the Loop: Technical Q&A. Bryan Cox, our VP of Sales and Ankur Goyal, founder and CEO, hosted a technical session focused on evals, agents, and LLM observability.\nIf you missed it or want to revisit the details, you can watch the full webinar recording below.\nQ: How do I get started building effective evals?\nKeep it simple. Start with just about 10 examples and build a feedback loop. Don't worry about creating the perfect dataset from the start, just use real user feedback to iterate quickly.\nQ: What are some common scoring functions teams start with?\n-\nLevenshtein distance: Just a simple string comparison metric.\n-\nFactuality: A popular OpenAI prompt-based evaluation that checks factual correctness.\n-\nClosed QA: Scores outputs against defined criteria without needing an exact correct answer.\nTry these simpler methods first and adjust them based on discrepancies you see between automated and manual scores.\nQ: What is Braintrust\u2019s approach to multi-step prompt chaining (agents)?\nWe just added an agents feature in our playground UI, which makes chaining multiple prompts straightforward. You can already use loops, parallel branches, and external tool calls through our API, with SDK support available too. We're also working on an intuitive UI with visual annotations to make complex workflows easier.\nQ: How do customers integrate evals into continuous integration (CI)?\nSome of our advanced customers integrate Braintrust\u2019s GitHub action into their CI. It smartly caches results, so only evals affected by recent code changes get rerun, which saves a ton of time and cost.\nQ: How does Braintrust handle user feedback and PII in evals?\nWe're working on anonymization features to strip out personally identifiable information (PII), letting you safely incorporate real user feedback into your evals.\nQ: Can Braintrust evaluate multimodal data (images, audio, video)?\nWe recently rolled out support for multimodal attachments in the playground, so you can directly upload and evaluate those datasets.\nQ: How should teams balance automated scoring with human review?\nAutomated scoring helps flag interesting or tricky cases. Humans should focus on reviewing those flagged results. Plus, the playground lets non-technical users and subject matter experts directly refine prompts and scores, greatly improving your evaluation quality.\nQ: What is Brainstore and why was it developed?\nBrainstore is our logging database built specifically for large-scale LLM workloads. It solves issues like handling massive data volumes, large JSON logs, and rapid data growth. It scales quickly on object storage, offers instant search, and drastically improves log management and observability.\nQ: Have customers successfully used LLMs to automate scoring guidance?\nYou can, but usually as an initial step. Teams typically use LLMs to draft initial scoring criteria and then manually refine these criteria. This approach significantly speeds up the scoring process.\nQ: What\u2019s your advice on using synthetic data in evals?\nSynthetic data can be useful, especially when real data is scarce or unavailable due to privacy or regulations. But you should always use synthetic data to complement, not replace, real user data.\nThe future of evals\nIn the coming years, evals will automate more tasks, better aligning AI outputs with human expectations. They'll become more sophisticated and involve more team members beyond just engineers, shifting from manual efforts toward continuous automated evaluation and improvement.\nStay connected with Braintrust for future events and insights:", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Latest news"}, {"href": "https://www.braintrust.dev/blog/best-practices", "anchor": "Webinar recap: Eval best practices"}, {"href": "https://www.braintrust.dev/blog/best-practices", "anchor": "Q: How do I get started building effective evals?"}, {"href": "https://www.braintrust.dev/blog/best-practices", "anchor": "Q: What are some common scoring functions teams start with?"}, {"href": "https://www.braintrust.dev/blog/best-practices", "anchor": "Q: What is Braintrust\u2019s approach to multi-step prompt chaining (agents)?"}, {"href": "https://www.braintrust.dev/docs/guides/functions/agents", "anchor": "agents"}, {"href": "https://www.braintrust.dev/blog/best-practices", "anchor": "Q: How do customers integrate evals into continuous integration (CI)?"}, {"href": "https://www.braintrust.dev/blog/best-practices", "anchor": "Q: How does Braintrust handle user feedback and PII in evals?"}, {"href": "https://www.braintrust.dev/blog/best-practices", "anchor": "Q: Can Braintrust evaluate multimodal data (images, audio, video)?"}, {"href": "https://www.braintrust.dev/blog/best-practices", "anchor": "Q: How should teams balance automated scoring with human review?"}, {"href": "https://www.braintrust.dev/blog/best-practices", "anchor": "Q: What is Brainstore and why was it developed?"}, {"href": "https://www.braintrust.dev/blog/best-practices", "anchor": "Q: Have customers successfully used LLMs to automate scoring guidance?"}, {"href": "https://www.braintrust.dev/blog/best-practices", "anchor": "Q: What\u2019s your advice on using synthetic data in evals?"}, {"href": "https://www.braintrust.dev/blog/best-practices", "anchor": "The future of evals"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Get in touch"}, {"href": "https://www.braintrust.dev/blog/best-practices", "anchor": "Webinar recap: Eval best practices"}, {"href": "https://www.braintrust.dev/blog/best-practices", "anchor": "Q: How do I get started building effective evals?"}, {"href": "https://www.braintrust.dev/blog/best-practices", "anchor": "Q: What are some common scoring functions teams start with?"}, {"href": "https://www.braintrust.dev/blog/best-practices", "anchor": "Q: What is Braintrust\u2019s approach to multi-step prompt chaining (agents)?"}, {"href": "https://www.braintrust.dev/blog/best-practices", "anchor": "Q: How do customers integrate evals into continuous integration (CI)?"}, {"href": "https://www.braintrust.dev/blog/best-practices", "anchor": "Q: How does Braintrust handle user feedback and PII in evals?"}, {"href": "https://www.braintrust.dev/blog/best-practices", "anchor": "Q: Can Braintrust evaluate multimodal data (images, audio, video)?"}, {"href": "https://www.braintrust.dev/blog/best-practices", "anchor": "Q: How should teams balance automated scoring with human review?"}, {"href": "https://www.braintrust.dev/blog/best-practices", "anchor": "Q: What is Brainstore and why was it developed?"}, {"href": "https://www.braintrust.dev/blog/best-practices", "anchor": "Q: Have customers successfully used LLMs to automate scoring guidance?"}, {"href": "https://www.braintrust.dev/blog/best-practices", "anchor": "Q: What\u2019s your advice on using synthetic data in evals?"}, {"href": "https://www.braintrust.dev/blog/best-practices", "anchor": "The future of evals"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 1}, "https://www.braintrust.dev/blog/resilient-design": {"url": "https://www.braintrust.dev/blog/resilient-design", "title": "Resilient observability by design - Blog - Braintrust", "text": "Resilient observability by design\nGreat AI observability enhances your LLM infrastructure without compromising stability. That's why Braintrust is designed as a non-blocking, optional layer for your AI workloads. Your application's core logic runs uninterrupted while we quietly log data in the background. This means even if Braintrust experiences downtime or network issues, your product stays up and stable.\nOur inline features follow the same principle. They're designed to operate independently, even within your app's critical path, so your system can function reliably without direct dependence on Braintrust services.\nNon-blocking logging by default\nOur SDK logger is initialized to asyncFlush: true\nby default, meaning log data is sent in the background without blocking your running code. When you log an AI request or result, our SDK quickly queues that log and immediately returns control to your application. The logs are then transmitted to our backend asynchronously, often in batches to minimize overhead.\nEven if Braintrust experiences downtime, your application continues running normally. Your API calls, user requests, or batch jobs proceed as usual, while the logging system handles network issues separately. If Braintrust is unavailable, your app will continue running, and the worst case scenario is that some logged data will not make it. If you set BRAINTRUST_FAILED_PUBLISH_PAYLOADS_DIR, then you can further ensure those payloads are saved locally, and you can then upload them later.\nBuilt for serverless environments\nServerless functions and edge runtimes have unique constraints since they may terminate immediately after returning a response. We account for this by making sure logging remains non-intrusive even in ephemeral environments. By default, the async logging works on platforms like Vercel and Cloudflare Workers, which provide a mechanism (such as waitUntil\n) to finish background tasks after sending a response. In these environments, you can keep asyncFlush: true\nand we'll automatically use the platform's background task APIs, so your serverless function doesn't have to wait for logs to send.\nFor other serverless platforms that don't support background completion, you can disable async flushing (asyncFlush: false\n) and flush logs at the end of each function call. This prevents log loss while keeping overhead minimal.\nThe SDK also provides tunable parameters (like queue sizes and drop policies) to prevent log operations from holding up your function. In practice, many teams have found our defaults safe for serverless use. We've engineered Braintrust so that logging \"just works\" without interfering with execution in both long-running servers and cloud functions.\nReliable proxy service\nThe Braintrust AI Proxy provides a standardized interface for executing LLM workloads across multiple providers. It's lightweight, stateless, and runs globally on Cloudflare Workers, so it will only go down if Cloudflare itself experiences an outage.\nWhen accessing via https://api.braintrust.dev/v1/proxy\n, requests are routed through AWS CloudFront. For applications requiring maximum resilience, we recommend implementing a tiered fallback strategy:\n- Primary endpoint:\nhttps://api.braintrust.dev/v1/proxy\n(via CloudFront) - Secondary endpoint:\nhttps://braintrustproxy.com/v1/\n(direct Cloudflare access) - Tertiary fallback: Direct provider API calls (for example, OpenAI)\nThis helps make sure your LLM operations continue even in the unlikely event of infrastructure disruptions at both CloudFront and Cloudflare.\nResilient prompt management\nBraintrust Prompts enable versioning and iteration of your LLM workloads while maintaining production reliability. Our client-side implementation ensures prompts remain available even during service disruptions.\nThe loadPrompt\nfunction implements a sophisticated strategy:\n- Initial prompts are fetched from Braintrust's servers\n- A two-level caching system stores prompts in both memory and on disk\n- Subsequent calls retrieve prompts from cache, eliminating network dependencies\n- Disk caching persists across application restarts, providing continuity\nFor mission-critical applications requiring absolute reliability, we offer a complete offline solution through our prompt pulling mechanism. By running npx braintrust pull\n, you can download prompt definitions as standalone code files that can be directly imported into your application. These files contain complete Prompt object definitions that function independently of the Braintrust service, ensuring your application remains operational under any circumstances.\nConfidence through design\nWe believe that observability should enhance, never compromise, your application's stability. By operating asynchronously and separating logging from critical execution paths, we can make sure that your system's core functionality remains uninterrupted even if logs fail or connections drop. We strongly recommend that you stress test your application \u2014 a quick way to do this is to set BRAINTRUST_API_URL\nto a bogus endpoint (or a local proxy) and confirm your app continues to function normally. This simple test provides confidence that your observability layer operates as designed, enhancing your system without introducing new points of failure.\nEffective observability isn't just about collecting data, it's about building reliability and resilience into your applications.", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Latest news"}, {"href": "https://www.braintrust.dev/blog/resilient-design", "anchor": "Resilient observability by design"}, {"href": "https://www.braintrust.dev/blog/resilient-design", "anchor": "Non-blocking logging by default"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "BRAINTRUST_FAILED_PUBLISH_PAYLOADS_DIR"}, {"href": "https://www.braintrust.dev/blog/resilient-design", "anchor": "Built for serverless environments"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "tunable parameters"}, {"href": "https://www.braintrust.dev/blog/resilient-design", "anchor": "Reliable proxy service"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI Proxy"}, {"href": "https://www.braintrust.dev/blog/resilient-design", "anchor": "Resilient prompt management"}, {"href": "https://www.braintrust.dev/docs/guides/functions/prompts", "anchor": "Prompts"}, {"href": "https://www.braintrust.dev/blog/resilient-design", "anchor": "Confidence through design"}, {"href": "https://www.braintrust.dev/blog/resilient-design", "anchor": "Resilient observability by design"}, {"href": "https://www.braintrust.dev/blog/resilient-design", "anchor": "Non-blocking logging by default"}, {"href": "https://www.braintrust.dev/blog/resilient-design", "anchor": "Built for serverless environments"}, {"href": "https://www.braintrust.dev/blog/resilient-design", "anchor": "Reliable proxy service"}, {"href": "https://www.braintrust.dev/blog/resilient-design", "anchor": "Resilient prompt management"}, {"href": "https://www.braintrust.dev/blog/resilient-design", "anchor": "Confidence through design"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 1}, "https://www.braintrust.dev/blog/brainstore-default": {"url": "https://www.braintrust.dev/blog/brainstore-default", "title": "Brainstore is now on by default - Blog - Braintrust", "text": "Brainstore is now the default\nEarlier this month, we publicly announced Brainstore, a database built for high-scale AI workloads. It's 80x faster on real-world benchmarks\nwith median query times under one second, even across terabytes of data. We've been running Brainstore in production for several months now, allowing\ncustomers to opt-in via a feature flag in the UI and via the use_brainstore\nparameter in the API.\nAt this point, almost every customer with a significant volume of logs has moved to Brainstore. The response has been overwhelmingly positive:\nI flipped it on and then loaded the UI \u2014 ~5s load time difference. Individual logs also load a lot faster.\nRunning with Brainstore now. Wow that was ... instant.\nBrainstore is awesome, well done team!\nBased on this response, we're excited to share that Brainstore is now the default in both our UI and API. Let's walk through what that means and a bit about what's coming next.\nChanges to expect\nBraintrust's API exposes a special /btql\nendpoint which is the work-horse behind all of the REST API endpoints as well as the\nUI. This endpoint now defaults to using Brainstore. If you'd like to fall back to querying via Postgres (the previous default), you\ncan explicitly set use_brainstore=false\nin your request.\nThe Brainstore feature flag in the UI will now also default to on. If you'd like to disable Brainstore, you can do so by toggling the feature flag off in the UI.\nA breaking API change\nPrior to Brainstore, BTQL queries against logs and experiments would return every span from traces that matched the search. Although this is a powerful feature, our users have consistently found it to be confusing, so Brainstore allows you to explicitly specify whether you'd like to see spans or traces.\nIn Brainstore, the default is spans\n, not traces\n, but since this is a breaking change, for the next 30 days, if you do not\nspecify use_brainstore\nexplicitly, we will fall back to the old behavior of returning traces by default.\nSelf-hosted users\nIf you are self-hosting Braintrust, then these changes to the defaults have no effect on you. Whenever you're ready, you can\nenable Brainstore as the default for your users by setting the BRAINSTORE_DEFAULT\nenvironment variable to true\n. Your support\ncontact at Braintrust can help you navigate this when you're ready.\nOnce you set BRAINSTORE_DEFAULT\nto true\n, your team will experience exactly the same behavior as outlined above.\nLooking ahead\nRolling out a large infrastructure change like Brainstore is not an overnight task, but we're excited about the progress we've made and what's ahead. Here's what you can expect:\nUsers of our hosted service\n- As early as April 28, 2025, we will make\nspans\nthe default in the API for all users.\nSelf-hosted users\n- As of today, no new self-hosted deployments should install ClickHouse. We will still support customers running ClickHouse for the foreseeable future, but we expect you to enable Brainstore as soon as feasible. Note that Brainstore is both cheaper and faster in the context of Braintrust's product.\n- Sometime this year, we will make Brainstore a required component of the Braintrust stack. If you haven't planned enabling it yet, please reach out to your Braintrust support contact so we can help you plan the transition.\nWe're grateful to all of our customers who have gave us early feedback to help us develop the best-in-class database for AI workloads. If you're interested in getting your team on Braintrust, please reach out, and if you want to help build the future of LLM observability software, we're hiring.", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Latest news"}, {"href": "https://www.braintrust.dev/blog/brainstore-default", "anchor": "Brainstore is now the default"}, {"href": "https://www.braintrust.dev/blog/brainstore", "anchor": "Brainstore"}, {"href": "https://www.braintrust.dev/blog/brainstore-default", "anchor": "Changes to expect"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "/btql endpoint"}, {"href": "https://www.braintrust.dev/blog/brainstore-default", "anchor": "A breaking API change"}, {"href": "https://www.braintrust.dev/blog/brainstore-default", "anchor": "Self-hosted users"}, {"href": "https://www.braintrust.dev/blog/brainstore-default", "anchor": "Looking ahead"}, {"href": "mailto:info@braintrust.dev", "anchor": "reach out"}, {"href": "https://www.braintrust.dev/careers?ashby_jid=8b9cfa26-627f-442c-a358-783b0e4ef930", "anchor": "we're hiring"}, {"href": "https://www.braintrust.dev/blog/brainstore-default", "anchor": "Brainstore is now the default"}, {"href": "https://www.braintrust.dev/blog/brainstore-default", "anchor": "Changes to expect"}, {"href": "https://www.braintrust.dev/blog/brainstore-default", "anchor": "A breaking API change"}, {"href": "https://www.braintrust.dev/blog/brainstore-default", "anchor": "Self-hosted users"}, {"href": "https://www.braintrust.dev/blog/brainstore-default", "anchor": "Looking ahead"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 1}, "https://www.braintrust.dev/blog/brainstore": {"url": "https://www.braintrust.dev/blog/brainstore", "title": "Brainstore: the database designed for the AI engineering era - Blog - Braintrust", "text": "Brainstore: the purpose-built database for the AI engineering era\nAI engineering has completely changed how we build software. Organizations are processing more tokens than ever, running agent-based systems with thousands of intermediate steps, and collecting massive amounts of structured and unstructured data. But traditional observability tools just aren\u2019t keeping up.\nThat\u2019s why we built Brainstore, a database built from the ground up for high-scale AI workloads. It\u2019s 80x faster on real-world benchmarks, with median query times under one second, even across terabytes of data. That means AI teams can debug, gain insights, and iterate on products faster.\nThe breakpoint for classic observability\nObservability tools built for microservices or web applications often struggle when it comes to AI data. Here\u2019s why:\nHuge data volumes\nAI workloads generate an overwhelming amount of data, and we've seen p95 log size soar from 500KB to nearly 3MB over the course of a few months, as shown in the chart below. Traces can easily reach several megabytes, and individual spans often exceed 1MB as teams push token limits and build more complex agents (1 MB is roughly 260,000 tokens). That\u2019s orders of magnitude more than traditional observability workloads were designed to handle.\nComplex queries (full-text search)\nAI workflows often require advanced, flexible queries, like finding recent prompts containing a specific phrase such as \"over the rainbow.\" Another query might be filtering by dynamic application-defined fields (like output.steps[1] = 'router'\n) that don\u2019t fit into a well-defined schema. With traditional tools, these more sophisticated searches either run too slowly or fail altogether, making it difficult to get the insights AI teams need.\nSecurity and privacy constraints\nAI logs often contain proprietary data, and sometimes even PII/PHI, which means companies need to keep them in-house. That makes self-hosting a must, but deploying, scaling, and managing these systems is a huge challenge. The few self-hosted search/warehouse solutions out there are complex to operate, especially for teams that want to focus on AI engineering, not database maintenance.\nIntroducing Brainstore\nThis is a widely felt problem in the industry, as we\u2019ve heard from many of our customers. To date, most approaches have tried to push traditional analytic databases into services, but they\u2019re just not built for these workloads and are many times slower than what is needed. Instead of working around these limitations, we decided to do it right and build our own database. At a glance, Brainstore is:\n- A single Rust binary that\u2019s trivial to set up. Just point it to an object storage (e.g. S3) bucket (data), Postgres (metadata), and Redis (distributed locks). It re-uses your existing Postgres and Redis and doesn\u2019t require you to manage anything new.\n- Stateless, elastic scaling. No local disk state. Scale down to zero or up to hundreds of containers. Separate read-optimized nodes from write-optimized nodes.\n- Highly optimized full-text search. We utilize an inverted index and columnstore designed specifically for object storage, handling real-time search in under 50ms (hot) and under 500ms (cold).\n- Incredible write throughput and low-latency commits. A single thread can write 40 million 50kb documents per day. Data is visible immediately after flush with strong consistency thanks to our write-ahead log.\nBenchmarks\nWe're incredibly happy with the results. To keep ourselves honest, we benchmarked Brainstore against a popular open-source data warehouse solution (\u201dPopular data warehouse\") and another LLM observability provider that uses that same warehouse. As you can see, Brainstore outperforms them significantly:\n*Both Brainstore and the data warehouse are deployed on identical hardware with NVME disks (c7gd.8xlarge on AWS). Brainstore caches locally and persists data to object storage, while the data warehouse just stores data locally, but we wanted to give them an equal playing field for read performance.\n**This flush latency is low because the data warehouse supports asynchronous inserts and is only writing to NVME. However, it takes an additional 2.3 seconds for the data to be visible.\n***Writes in Brainstore are strongly consistent and visible immediately because it directly reads and merges the write-ahead-log. The data warehouse used by Competitor is eventually consistent.\nKey takeaways\n- Brainstore is optimized specifically to be good at full-text search. Traditional data warehouses often rely on bloom filters or partitioned inverted indices to eliminate (or \u201cskip\u201d) batches of rows that do not contain terms. This is fine for slicing/dicing 10% of a dataset, but agonizingly slow for targeted text searches.\n- Brainstore commits as soon as it writes to the write-ahead log. Write ahead logs are a simple object storage write, so are both low latency and scale very well. This also makes Brainstore real-time and strongly consistent.\n- Object storage is incredibly cheap, so Brainstore is much less expensive to run than a traditional data warehouse.\nArchitecture\nTo achieve lightning-fast reads/writes at a massive scale, Brainstore\u2019s architecture has three key principles, each inspired by real pain from trying to run Braintrust on a traditional data warehouse.\n- All data lives on object storage. It\u2019s infinitely scalable, resilient, and far simpler to operate than local disks. Popular commercial solutions partially use object storage, but still rely on persistent disks for metadata, statistics, and consensus, making them a pain to run yourself.\n- Each customer\u2019s data lives in its own distinct partition. In contrast, building on top of a data warehouse requires storing everyone\u2019s data in one massive table, which slows down queries as the table grows.\n- First-class support for semi-structured data. Data warehouses often \u201cshred\u201d JSON into columns, which works fine for relatively shallow, static JSON schemas but falls apart as you add more fields.\nUnder the hood, Brainstore leverages:\n- Tantivy: a popular open-source library that has a built-in inverted index and columnstore. It\u2019s extensible and well-suited for object storage, and supports semi-structured data efficiently.\n- A custom write-ahead log (WAL) implementation that is designed for efficient real-time writes and reads on object storage. As soon as a WAL entry is written, reads can access it (alongside the indexed data), making Brainstore strongly consistent. We compact the write-ahead log into a highly optimized Tantivy index in the background.\n- A lot of object-storage-specific optimizations to make cold starts and the common query patterns we serve extremely fast. For example, we make sure to store all of the spans of a trace within the same physical index.\nWe'll follow up with a technical deep dive of Brainstore in a future blog post. I also want to say thank you to friends from Turbopuffer, Neon, and Warpstream who are each building object-storage native database systems and helped us work out the design.\nWhy is it so much faster?\nBraintrust is built for observability. Most solutions today have much broader mandates, and thus have to support features that are not needed for LLM development, from joins to schema migrations to ANSI SQL compatibility. This makes them complex and slow. Brainstore zeroes in on one goal: lightning-fast search and analytics for AI-shaped logs in object storage. By specializing, we cut out the bloat and optimize for exactly what AI engineers need.\nHaving experienced systems engineers also helps. I\u2019ve worked on databases for 15+ years, from Microsoft Cosmos to SingleStore to Impira. Manu, our lead engineer, cut his teeth at BigQuery, Dropbox\u2019s storage team, and Nuro\u2019s infra group. Austin, a physics PhD dropout, was one of the first engineers at Impira, where he optimized models to run on JIT-compiled bytecode and worked on incremental query processing. Deeks (Mike), previously a Braintrust user at Instacart, knows firsthand the pain of running cumbersome databases and has shaped Brainstore\u2019s operational experience to be exactly what he wished he had as a user.\nCustomer testimonials\nBrainstore was inspired by some of our largest customers' evolving needs. After sitting down with them and watching how they search through logs, we knew exactly what to build. We're proud that after putting our heads down and shipping Brainstore, it's already making a big difference:\nBrainstore opens a whole new world for working with LLMs. My team spends up to two hours a day looking at data. Not only do we get to see logs in crisp real-time, but we can also search through and understand them way faster.\n\u2013 Vitor Balocco, Staff Applied AI Engineer @ Zapier\nBrainstore has completely changed how our team interacts with logs. We've been able to discover insights by running searches in seconds that would previously take hours.\n\u2013 Sarah Sachs, Engineering Lead, AI Modeling @ Notion\nBraintrust customers often have complex security requirements, so we built Brainstore to be simple to run in your own infrastructure:\nI was able to get Brainstore up and running in less than a day in my own AWS account. The performance difference is insane.\n\u2013 Erik Munson, Founding Engineer @ Day.ai\nRollout and next steps\nBrainstore is already available for our SaaS users. To turn it on, navigate to Feature flags in your organization settings, and toggle the switch. We'll make this the default soon.\nIf you are self-hosting and want to try it out, reach out, and we'll help you get set up.\nLooking ahead\nThis is just the beginning of what we plan to do with Brainstore. Among other things, we're working on:\n- Using Brainstore to make more parts of Braintrust's UI insanely fast.\n- Providing support for more complex queries.\n- Integrating seamlessly with enterprise data lake houses.\nLast but not least, if building a specialized log processing database for AI engineering (in Rust!) sounds like your idea of fun, we're hiring.", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Latest news"}, {"href": "https://www.braintrust.dev/blog/brainstore", "anchor": "Brainstore: the purpose-built database for the AI engineering era"}, {"href": "https://www.braintrust.dev/blog/brainstore", "anchor": "The breakpoint for classic observability"}, {"href": "https://www.braintrust.dev/blog/brainstore", "anchor": "Huge data volumes"}, {"href": "https://www.braintrust.dev/blog/brainstore", "anchor": "Complex queries (full-text search)"}, {"href": "https://www.braintrust.dev/blog/meta/brainstore/full-text-search.mp4", "anchor": "Video"}, {"href": "https://www.braintrust.dev/blog/brainstore", "anchor": "Security and privacy constraints"}, {"href": "https://www.braintrust.dev/blog/brainstore", "anchor": "Introducing Brainstore"}, {"href": "https://www.braintrust.dev/blog/brainstore", "anchor": "Benchmarks"}, {"href": "https://www.braintrust.dev/blog/brainstore", "anchor": "Architecture"}, {"href": "https://www.braintrust.dev/blog/brainstore", "anchor": "Why is it so much faster?"}, {"href": "https://www.braintrust.dev/blog/brainstore", "anchor": "Customer testimonials"}, {"href": "https://www.braintrust.dev/blog/brainstore", "anchor": "Rollout and next steps"}, {"href": "mailto:info@braintrust.dev", "anchor": "reach out"}, {"href": "https://www.braintrust.dev/blog/brainstore", "anchor": "Looking ahead"}, {"href": "https://www.braintrust.dev/careers?ashby_jid=8b9cfa26-627f-442c-a358-783b0e4ef930", "anchor": "we're hiring"}, {"href": "https://www.braintrust.dev/blog/brainstore", "anchor": "Brainstore: the purpose-built database for the AI engineering era"}, {"href": "https://www.braintrust.dev/blog/brainstore", "anchor": "The breakpoint for classic observability"}, {"href": "https://www.braintrust.dev/blog/brainstore", "anchor": "Huge data volumes"}, {"href": "https://www.braintrust.dev/blog/brainstore", "anchor": "Complex queries (full-text search)"}, {"href": "https://www.braintrust.dev/blog/brainstore", "anchor": "Security and privacy constraints"}, {"href": "https://www.braintrust.dev/blog/brainstore", "anchor": "Introducing Brainstore"}, {"href": "https://www.braintrust.dev/blog/brainstore", "anchor": "Benchmarks"}, {"href": "https://www.braintrust.dev/blog/brainstore", "anchor": "Architecture"}, {"href": "https://www.braintrust.dev/blog/brainstore", "anchor": "Why is it so much faster?"}, {"href": "https://www.braintrust.dev/blog/brainstore", "anchor": "Customer testimonials"}, {"href": "https://www.braintrust.dev/blog/brainstore", "anchor": "Rollout and next steps"}, {"href": "https://www.braintrust.dev/blog/brainstore", "anchor": "Looking ahead"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 1}, "https://www.braintrust.dev/blog/model-updates": {"url": "https://www.braintrust.dev/blog/model-updates", "title": "Bedrock, Vertex AI, and universal structured outputs - Blog - Braintrust", "text": "Bedrock, Vertex AI, and universal structured outputs support\nBraintrust now fully supports Amazon Bedrock and Google Vertex AI, giving developers access to more models through a unified interface. Structured output handling has also been improved, ensuring reliable JSON responses across most providers, including those that don't natively support structured outputs.\nExpanded Bedrock and Vertex AI support\nYou can now use Amazon Bedrock and Vertex AI models in both the playground and AI proxy. This includes support for system prompts, tool calls, and multimodal inputs. Braintrust handles these integrations, so models from these platforms work without extra configuration.\nIn the Vertex AI integration, we also support authentication as a principal or service account using an OAuth 2.0 token or a service account key.\nConsistent structured outputs\nStructured output handling is now extended across Anthopic, Bedrock, and any OpenAI-flavored models, like Llama on Fireworks, that support tool calls, ensuring consistent JSON responses. Models that don\u2019t natively support structured outputs now return predictable, machine-readable responses.\nThis eliminates the need for additional prompt engineering or post-processing, and makes it simple to test different models for your use case. Additionally, it improves the fidelity and recall of your model invocations by preventing wasted inputs when the generated response fails to adhere to your specific classification schema.\nSimplified model selection in the playground\nThe playground model dropdown now groups models by provider and family. This makes it easier to find, compare, and test different models.\nAdditional updates\nWe've launched a few more updates to make sure the developer experience across AI providers is more reliable:\n- We now keep custom models from overriding default configurations, so provider settings remain stable.\n- Streaming JSON responses from non-OpenAI providers work correctly, allowing models to return data as expected.\n- You can now add templated custom headers for custom AI providers.\nWe want Braintrust to be the best place to build with leading AI models and providers. If you have any feedback, please let us know. To try out structured outputs with Anthropic models, check out our new cookbook on spam classification!", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Latest news"}, {"href": "https://www.braintrust.dev/blog/model-updates", "anchor": "Bedrock, Vertex AI, and universal structured outputs support"}, {"href": "https://www.braintrust.dev/blog/model-updates", "anchor": "Expanded Bedrock and Vertex AI support"}, {"href": "https://www.braintrust.dev/blog/model-updates", "anchor": "Consistent structured outputs"}, {"href": "https://www.braintrust.dev/blog/model-updates", "anchor": "Simplified model selection in the playground"}, {"href": "https://www.braintrust.dev/blog/model-updates", "anchor": "Additional updates"}, {"href": "mailto:support@braintrust.dev", "anchor": "please let us know"}, {"href": "https://www.braintrust.dev/docs/cookbook/recipes/SpamClassifier", "anchor": "spam classification"}, {"href": "https://www.braintrust.dev/blog/model-updates", "anchor": "Bedrock, Vertex AI, and universal structured outputs support"}, {"href": "https://www.braintrust.dev/blog/model-updates", "anchor": "Expanded Bedrock and Vertex AI support"}, {"href": "https://www.braintrust.dev/blog/model-updates", "anchor": "Consistent structured outputs"}, {"href": "https://www.braintrust.dev/blog/model-updates", "anchor": "Simplified model selection in the playground"}, {"href": "https://www.braintrust.dev/blog/model-updates", "anchor": "Additional updates"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 1}, "https://www.braintrust.dev/blog/fintool": {"url": "https://www.braintrust.dev/blog/fintool", "title": "How Fintool generates millions of financial insights - Blog - Braintrust", "text": "How Fintool generates millions of financial insights\nFintool is an AI equity research assistant that helps investors make better decisions by processing large volumes of unstructured financial data, from SEC filings to earnings call transcripts. They serve leading institutional investors such as Kennedy Capital and First Manhattan, as well as companies like PricewaterhouseCoopers.\nFor institutional investors, trust is paramount, and a single overlooked disclosure can have serious consequences. However, the sheer volume of daily regulatory filings makes it impossible for humans to review every document. Fintool addressed this problem by developing Fintool Feed, a Twitter-like interface where they summarize key sections of documents based on user prompts. Investors select the companies they want to monitor and configure alerts by specifying what type of information they want to be summarized.\nHowever, the team soon realized the need for real-time monitoring to maintain quality and user confidence. They faced a few key challenges:\n- Managing over 1.5 billion tokens across 70 million document chunks while processing gigabytes of data daily.\n- User prompts ranging from broad compliance monitoring to particular disclosures, like board membership changes.\n- The need for superior accuracy and reliability.\nIn this case study, we'll share how Fintool used Braintrust to develop a repeatable evaluation workflow that scales to massive amounts of data while maintaining trust in high-stakes financial contexts.\nFintool's continuous evaluation workflow\n1. Define quality standards and format rules\nFintool makes sure every insight includes a reliable source, like an SEC document ID, and automatically flags anything that\u2019s missing or doesn\u2019t look right. This is a big deal in finance, where trust comes down to having data you can verify.\nThey don\u2019t just check that sources are included. They also make sure they\u2019re valid, properly formatted, and tied directly to the insights. The team set up custom rules in Braintrust, like requiring SEC IDs and double-checking quoted text, and real-time monitoring catches anything that doesn\u2019t meet the standards.\nFintool also uses span iframes to show citations within trace spans, so expert reviewers can quickly validate the content.\n2. Curate golden datasets\nFintool leverages Braintrust\u2019s tools to benchmark the quality of LLM outputs in real time. The engineering team crafts golden datasets tailored to specific industries and document types, like healthcare compliance or tech KPIs.\nThe golden datasets are built by combining production logs with handpicked examples that reflect real-world scenarios, which helps the datasets stay fresh as Fintool processes over 1.5 billion tokens across 70 million data chunks daily.\n3. Automate evals with LLM-as-a-judge\nEach generated insight is evaluated using LLM-as-a-judge scorers on key metrics like accuracy, relevance, and completeness. Braintrust automatically updates whenever Fintool adjusts prompts or ingests new data, preventing surprise regressions and saving valuable engineering resources.\nUsing automated scoring functions frees up bandwidth for human reviewers to focus on the toughest cases.\n4. Add human in the loop oversight\nWhen content gets a low score or is downvoted, a human expert is immediately notified to step in. They can approve, reject, or edit the Markdown to fix issues like poor formatting. Since the Fintool database is linked directly to Braintrust, the expert can update the live content right from the Braintrust UI.\nThis quick response means that any problems are addressed and improved as soon as possible.\nResults\nThis evaluation workflow has helped Fintool manage millions of LLM-generated insights, improving accuracy, consistency, and efficiency at scale. By streamlining their eval process, Fintool is able to make sure their financial summaries and alerts meet the highest standards of trust and reliability. Key successes include:\n- Scalability: Fintool now processes millions of datapoints daily, delivering reliable financial insights at scale without compromising quality. Automating evals allows human reviewers to focus on the most challenging cases.\n- Efficiency: Automated, real-time evals makes detecting and resolving quality issues faster\n- Accuracy: Enforcing rigorous citation and format validation rules improved the precision of Fintool insights to make sure they meet the specific needs of institutional investors\n- Convenient human review: Human reviewers can intervene quickly and manage edits and updates right from the Braintrust UI\nConclusion\nFintool has set a new standard for financial AI, delivering timely and actionable insights with accuracy and efficiency.\nIf your AI product team needs to conduct scalable, industry-specific evaluations, try Braintrust today.\nLearn more about Fintool and Braintrust.\nThank you to Nicolas for sharing these insights!", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Latest news"}, {"href": "https://www.braintrust.dev/blog/fintool", "anchor": "How Fintool generates millions of financial insights"}, {"href": "https://www.braintrust.dev/blog/fintool", "anchor": "Fintool's continuous evaluation workflow"}, {"href": "https://www.braintrust.dev/blog/fintool", "anchor": "1. Define quality standards and format rules"}, {"href": "https://www.braintrust.dev/blog/fintool", "anchor": "2. Curate golden datasets"}, {"href": "https://www.braintrust.dev/blog/fintool", "anchor": "3. Automate evals with LLM-as-a-judge"}, {"href": "https://www.braintrust.dev/blog/fintool", "anchor": "4. Add human in the loop oversight"}, {"href": "https://www.braintrust.dev/blog/fintool", "anchor": "Results"}, {"href": "https://www.braintrust.dev/blog/fintool", "anchor": "Conclusion"}, {"href": "https://www.braintrust.dev/contact", "anchor": "try Braintrust today"}, {"href": "https://www.braintrust.dev/", "anchor": "Braintrust"}, {"href": "https://www.braintrust.dev/blog/fintool", "anchor": "How Fintool generates millions of financial insights"}, {"href": "https://www.braintrust.dev/blog/fintool", "anchor": "Fintool's continuous evaluation workflow"}, {"href": "https://www.braintrust.dev/blog/fintool", "anchor": "1. Define quality standards and format rules"}, {"href": "https://www.braintrust.dev/blog/fintool", "anchor": "2. Curate golden datasets"}, {"href": "https://www.braintrust.dev/blog/fintool", "anchor": "3. Automate evals with LLM-as-a-judge"}, {"href": "https://www.braintrust.dev/blog/fintool", "anchor": "4. Add human in the loop oversight"}, {"href": "https://www.braintrust.dev/blog/fintool", "anchor": "Results"}, {"href": "https://www.braintrust.dev/blog/fintool", "anchor": "Conclusion"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 1}, "https://www.braintrust.dev/blog/loom": {"url": "https://www.braintrust.dev/blog/loom", "title": "How Loom auto-generates video titles - Blog - Braintrust", "text": "How Loom auto-generates video titles\nLoom is a video communication platform used by millions of professionals worldwide. With the rise of generative AI, the Loom team saw an opportunity to make video communication more accessible and efficient with new features powered by LLMs, like automatically generated video titles.\nBehind the scenes, developing these AI features came with its own set of challenges. The team at Loom recognized that building great AI features requires a robust method for evaluating quality. How do they know if their auto-generated titles are any good? To answer that question, they started running evals on Braintrust with their own custom scoring functions.\nIn this post, we'll share how Loom's software engineering team conceptualizes and builds scoring functions. By defining clear, actionable criteria and iterating quickly, Loom has found a sweet spot between innovation and reliability.\nHow do I figure out what scorers to create?\nLoom\u2019s philosophy is that all AI-driven features should be built around how real users evaluate the generated output: to write great scorers, you need to understand the feature first. Instead of asking, \u201cIs the model doing what I want?,\u201d which optimizes for the model following instructions, Loom focuses on the feature itself by evaluating whether the output is ideal, regardless of how it\u2019s generated. This approach allows for higher-level improvements to the feature. By replacing a generic notion of \u201cquality\u201d with a structured, four-step process, Loom iterates on scoring functions, making sure that each AI feature meets its intended goals and delights users.\n1. Identify the traits of great video titles\nWhen kicking off a new feature, their team looks at:\nThe input: what data or prompt is the model receiving?\nThe output: what is the model supposed to generate?\nThey then ask, \u201cHow do humans evaluate the quality of this output?\u201d and \u201cWhat do great examples of this output look like?\u201d\nFor example, in the case of auto-generating video titles, they first identify what makes a great video title:\n- Conveys the main idea\n- Concise yet descriptive\n- Engaging to readers\n- Readable (no grammar or spelling issues)\n- Doesn\u2019t contain hallucinations\n- Ends with a single relevant emoji (for a bit of fun!)\nBy identifying these traits, the team can begin to outline how they\u2019ll measure each aspect of a great output, even before writing any actual scoring code or prompts.\n2. Check for common measures of quality\nThere are a handful of quality measures that apply across a broad range of LLM use cases. They won\u2019t necessarily be useful for every use case, but it\u2019s worth doing a quick check to see if any of them apply:\n- Relevance (also known as faithfulness or hallucinations): does the output accurately reflect the source material (for example, video transcripts)?\n- Readability: is it written in clear, understandable language?\n- Structure/formatting: does it follow a desired format, like a JSON schema or specific text layout, like a title with multiple subheadings?\n- Factuality: if the output is supposed to be factual, like when drawing from a RAG system, is it accurate?\n- Bias/toxicity/brand safety: is it safe to show the output to users, without offensive or biased language?\n- Correct language: is the output in the requested language?\nThese general checks are powerful, but Loom also customizes them to the specific feature by building small, custom scorers. By tailoring each scorer to the task, they remove unnecessary noise in prompts and get more reliable results.\n3. Implement objective measures with code\nWhenever possible, Loom automates these quality checks with deterministic, code-based scorers. Objective checks like \u201cDoes the output text contain exactly one emoji at the end?\u201d or \u201cDoes the JSON response contain all required keys?\u201d can be validated without an LLM.\nThis approach saves time and money, as code-based scorers are low-cost and fast, even at scale, and eliminate the variablilty of LLM responses.\n4. Create initial scorers and iterate\nUsing their feature-specific criteria and any relevant common measures as a starting point, the team at Loom sets up an initial round of scorers in Braintrust. Here are some examples of scorers they ended up building for auto-generated video titles:\n- Relevance, focusing on the main topic of the video\n- Conciseness\n- Engagement potential\n- Clarity\n- Correct language\nFrom there, they continue iterating by feeding in around 10-15 test examples to get a feel for how the scorers are performing, inspecting the results, and refining as needed. After they're satisfied that the scorers are dialed in, they begin running evals at scale by configuring online evaluations.\nHere are a few best practices they've picked up along the way:\n- When using LLM-as-a-judge scorers, it's crucial to enable the chain-of-thought option. This gives you back a \u201crationale\u201d in the score where the LLM explains why it gave the score it did. Use this to calibrate the scorer.\n- Each scorer should focus on a distinct aspect of the output (e.g., factual correctness vs. style vs. presence of an emoji).\n- When some aspects matter more (e.g., factual accuracy over style), they configure weighted averages to combine scores.\n- Sometimes a binary (yes/no) score is enough. Other times, a 3- or 5-point scale is necessary for more nuance.\nThrough multiple iterations of prompt tuning, scorer adjustment, and dataset honing, Loom tailors each feature's scoring process unil they're sure it captures all the essential qualities.\nWhy it works\nThis cycle of define \u2192 implement \u2192 evaluate \u2192 refine gives Loom a strategic advantage. By integrating both objective and subjective measures into their scorers, the team can quickly identify what\u2019s working and where improvements are needed. They also avoid \u201canalysis paralysis\u201d by starting small and iterating quickly.\nThe results: faster, better AI features\nAt Loom, they've established a repeatable, reliable system for shipping features faster and more confidently using Braintrust evals. By systematically scoring AI outputs, they can run large-scale evaluations more quickly, make sure features reliably meet users' needs, and ship improvements with the knowledge they've been thoroughly tested.\nWhether you're building auto-generated titles, chat-based assistants, or something entirely different, Loom's guide to scoring functions is an excellent blueprint to follow.\nLearn more about Loom and Braintrust.\nThank you to Matt for sharing these insights!", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Latest news"}, {"href": "https://www.braintrust.dev/blog/loom", "anchor": "How Loom auto-generates video titles"}, {"href": "https://www.braintrust.dev/blog/loom", "anchor": "How do I figure out what scorers to create?"}, {"href": "https://www.braintrust.dev/blog/loom", "anchor": "1. Identify the traits of great video titles"}, {"href": "https://www.braintrust.dev/blog/loom", "anchor": "2. Check for common measures of quality"}, {"href": "https://www.braintrust.dev/docs/guides/functions/scorers", "anchor": "custom scorers"}, {"href": "https://www.braintrust.dev/blog/loom", "anchor": "3. Implement objective measures with code"}, {"href": "https://www.braintrust.dev/docs/guides/functions/scorers", "anchor": "code-based scorers"}, {"href": "https://www.braintrust.dev/blog/loom", "anchor": "4. Create initial scorers and iterate"}, {"href": "https://www.braintrust.dev/docs/guides/evals/write", "anchor": "online evaluations"}, {"href": "https://www.braintrust.dev/docs/guides/functions/scorers", "anchor": "LLM-as-a-judge scorers"}, {"href": "https://www.braintrust.dev/docs/guides/evals/interpret", "anchor": "weighted averages"}, {"href": "https://www.braintrust.dev/blog/loom", "anchor": "Why it works"}, {"href": "https://www.braintrust.dev/blog/loom", "anchor": "The results: faster, better AI features"}, {"href": "https://www.braintrust.dev/", "anchor": "Braintrust"}, {"href": "https://www.braintrust.dev/blog/loom", "anchor": "How Loom auto-generates video titles"}, {"href": "https://www.braintrust.dev/blog/loom", "anchor": "How do I figure out what scorers to create?"}, {"href": "https://www.braintrust.dev/blog/loom", "anchor": "1. Identify the traits of great video titles"}, {"href": "https://www.braintrust.dev/blog/loom", "anchor": "2. Check for common measures of quality"}, {"href": "https://www.braintrust.dev/blog/loom", "anchor": "3. Implement objective measures with code"}, {"href": "https://www.braintrust.dev/blog/loom", "anchor": "4. Create initial scorers and iterate"}, {"href": "https://www.braintrust.dev/blog/loom", "anchor": "Why it works"}, {"href": "https://www.braintrust.dev/blog/loom", "anchor": "The results: faster, better AI features"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 1}, "https://www.braintrust.dev/blog/evaluating-agents": {"url": "https://www.braintrust.dev/blog/evaluating-agents", "title": "Evaluating agents - Blog - Braintrust", "text": "Evaluating agents\nThis blog post is a companion guide to Anthropic's research on building effective agents. It requires prior knowledge of agentic systems.\nBuilding an agentic system, whether it\u2019s a simple augmented large language model (LLM) or a fully autonomous agent, involves many moving parts. Your design might make sense on paper, but without measurement and iterative improvement, unexpected issues can pop up in production. Inspired by Anthropic\u2019s guide to building effective agents, in this post, we\u2019ll walk through practical strategies for evaluating the quality and accuracy of agentic systems.\nWhy run evaluations?\nLLM-based agents:\n- Can have unpredictable error modes (hallucinations, repetitive loops, etc). For example, a financial advisory chatbot might hallucinate stock market predictions based on fictional data, potentially misleading users.\n- Can rely on context, memory, or retrieval tools that may have their own failures. For example, a legal research assistant might misinterpret a poorly retrieved document and provide incorrect case law summaries.\n- Iteratively refine their own outputs, which can hide the root cause of an error. For example, a content generation tool might produce a misleading statement in an early draft, then repeatedly refine and expand on it, amplifying the inaccuracy in the final version.\nEvals help you detect and debug these issues before they impact your users. They can also help you decide how you might improve your application.\nChoosing eval metrics\nBecause agentic systems can be extremely complex, there\u2019s no one-size-fits-all set of metrics, but we\u2019ll explore an example of an agent and a potential set of scorers for each pattern defined in the guide to building effective agents. Metrics refer to the quantitative or qualitative measures used to evaluate how well the agent performs against its intended objectives, while scorers are the code-based or LLM-as-a-judge functions used to calculate these metrics. Like the guide, we\u2019ll start simple and increase in complexity. Think of each set of scorers as a menu to pick and choose from depending on your specific system\u2019s goals and constraints.\nQuantitative vs. qualitative metrics\nQuantitative metrics help track performance over time and compare different implementations. Some examples include accuracy against a test dataset, average cost per request, or average response latency. These metrics measure LLM behaviors with standardized checks. Scorers for these metrics are logical and deterministic. Qualitative metrics measure whether the agent is achieving the desired experience for the end user. These usually come from user feedback or support tickets, and require some sort of human review or LLM-as-a-judge analysis. These qualitative checks capture nuances that raw metrics can miss. Scorers for these metrics can measure things like trends, \"vibes\", or lagging indicators.\nIn many cases, you\u2019ll want both. For example, you might track how often your agent successfully handles a customer request and measure user satisfaction with the agent\u2019s helpfulness.\nIterative evaluation process\nGenerally, a reasonable first pass at evaluating any agent is to start by looking at the final output. This gives a high-level view of the agent\u2019s performance. If you notice failure points, you can go deeper on a particular intermediate step and add more evaluations, until you build a comprehensive eval system. You might find that you need more granular evals for things like:\n- Behavioral patterns - step-by-step decisions\n- Tool effectiveness - how well the agent uses specific tools or APIs\n- Cost efficiency - monitoring resource usage over multiple iterations\nBuilding block: the augmented LLM\n- First, the LLM decides whether it needs data from an external knowledge base or a particular tool.\n- If retrieval or a tool call is needed, we append the results to our prompt so the next LLM call can incorporate that new info.\n- Finally, we ask the LLM to generate the \u201creal\u201d output for the user or system, updating our memory with the final result in case future steps rely on it.\nFor example, a user asks for a recipe for grilled chicken. The question is embedded and an agent retrieves recipes from NYT Cooking related to the embedded terms. The retrieved context is fed to the LLM as context, and the LLM provides an output to the user.\nContextInclusion\n- check if the final LLM output string contains a required keyword or phrase, indicating that it successfully incorporated the retrieved context. Since the point of an augmented LLM is to show that it used external information effectively, the augmentation step is wasted if the final output ignores the external information.\nFactuality\n- use the Factuality scorer from autoevals, or use it as a starting point and add your own custom logic. Augmentation is supposed to reduce hallucinations by leveraging real data, soFactuality\nmakes sure that the retrieval actually improved correctness.\nIn this code snippet, we also show how to run an evaluation on Braintrust via the SDK using the Factuality\nscorer. For more information, check out the documentation.\nRelevanceJudge\n- qualitatively judge if the solution is relevant and using the retrieved info correctly. Sometimes, an LLM might parrot key words without actually understanding the user\u2019s input.\nPrompt chaining\nA fixed series of LLM calls:\n- Summarize\n- Draft\n- Refine\n- And so on\nFor example, an agent first summarizes a user\u2019s text, then writes a short draft, and finally refines it.\nExactMatch\n- compare the output to an \u201cexpected\u201d output. If each chain step is very structured or requires exact text, a simple \u201cexact match\u201d or \u201cstring similarity\u201d at each step can be quite effective.\nYou could also use ExactMatch from autoevals.\nStepByStepAccuracy\n\u2014 instrument metadata such as a boolean for each step that checks the accuracy, then check the accuracy of the final output. In prompt chaining, upstream errors cascade. Checking each step ensures you can pinpoint exactly where the chain fails.\nFlowCoherenceJudge\n\u2014 read each step and decide if the chain logically flows and improves by the end. Some tasks require a more subjective measure. This is a good first step if you aren\u2019t ready to incorporate human review.\nRouting\n- The system classifies user inputs into distinct routes\n- Each route has its own specialized logic or sub-prompt\n- If the agent picks the wrong route, the final output is likely incorrect\nFor example, an agent reads a customer request like \u201cI want my money back\u201d and decides whether to use the Refund flow or the General Inquiry flow.\nRouteAccuracy\n- checks if the agent\u2019s chosen route (found inoutput\n) matches theexpected\nroute label. If the wrong route is chosen at the start, the final answer will be wrong.\nDownstreamTaskQuality\n- qualitatively checks whether, after picking a route, the final response actually solves the user\u2019s problem. If you have a super nuanced set of routes, like multiple categories of customer support, an LLM might be better at determining accuracy than code.\nParallelization\nMultiple LLM calls happen at once, either:\n- Each doing a different section of a task\n- Producing multiple candidate outputs and then \u201cvoting\u201d on the best\nThe final step merges or selects from these parallel results.\nFor example, an agent splits a long text into two halves, processes each half with a separate LLM call, then merges them into a single summary.\nMergeCoherenceJudge\n- uses an LLM to see if the combined sections produce a cohesive final text. Merging partial outputs can lead to repetition or writing styles that aren't combined cohesively.\nVotingConsensusCheck\n- if the system collects multiple candidates, check how many of them match the final chosen answer. If the final answer was \u201cvoted in,\u201d it\u2019s important to measure how strongly the candidates agreed.\nParallelCostCheck\n- returns 1 if the total token usage for parallel calls did not exceed a threshold. Parallelization can balloon your token usage. This lets you keep an eye on cost-performance tradeoffs.\nOrchestrator-workers\n- An \u201corchestrator\u201d LLM breaks a large or ambiguous request into subtasks\n- \u201cWorker\u201d LLMs each handle a subtask, returning partial outputs\n- The orchestrator merges them into a final result\nFor example, an agent breaks a coding request into multiple file edits, each file handled by a different worker, and then merges them into a single pull request.\nSubtaskCoverage\n- checks if the final result includes all required subtasks (listed inexpected.subtasks\n). If the orchestrator is supposed to handle[\"Implement function A\",\"Implement function B\"]\nbut the final text doesn\u2019t mention function B, we fail.\nPartialAccuracy\n- scores the correctness of each \u201cworker\u201d subtask, stored inmetadata.workerOutputs\n. Each subtask might have a known correct snippet. This helps you pinpoint whether specific workers performed well.\nFinalMergeCoherence\n- checks if the orchestrator\u2019s final merge step produced a cohesive, non-redundant result. Even if each worker subtask is correct, the final step might accidentally produce a contradictory result.\nEvaluator-optimizer\n- One LLM (\u201coptimizer\u201d) attempts a solution\n- Another LLM (\u201cevaluator\u201d) critiques it and provides feedback\n- The optimizer refines the answer, repeating until it meets certain criteria or hits an iteration limit\nFor example, an agent tries to write a short poem. The evaluator LLM says \u201cNeeds more vivid imagery.\u201d The agent modifies it, and so on.\nImprovementCheck\n- compares the initial draft (stored in metadata) and the final output, deciding if the final is \u201csignificantly improved.\u201d If the final text is basically the same as the first, the evaluator\u2019s feedback wasn\u2019t used effectively.\nIterationCount\n- logs how many times we looped (used iterations). Scores 1 if it\u2019s less than or equal to some max. This makes sure the agent doesn\u2019t iterate forever.\nFeedbackSpecificity\n- rates the evaluator\u2019s feedback (stored in metadata) for clarity. For example, is it more than 20 characters, or does it mention specific improvements? If the evaluator always says \u201cLooks good!\u201d the \u201coptimization\u201d loop won\u2019t help.\nFully autonomous agent\n- The agent decides each step on its own, calling tools, asking the user for more info, or finishing the task.\n- It can run for many steps or until it hits a maximum iteration/cost guardrail.\nFor example, consider an agent that books travel. It calls an AirlineAPI\ntool to find flights, then a PaymentAPI\ntool to complete the booking. It continues working step-by-step until it completes the task or encounters a guardrail.\nAutonomous agents are difficult to evaluate because they can contain any number of the above agentic systems within them. As you choose scorers for your autonomous agents, consider each step the agent takes and what scorers from earlier examples might be useful for that step.\nStepLimitCheck\n- simple scorer to ensure the agent\u2019s step count is less than or equal to 5. Autonomy can lead to runaway loops.\nComplianceCheck\n- another LLM checks the final log or output for policy violations (harassment, disallowed content, etc.). Fully autonomous agents can easily do the wrong thing without direct guardrails.\nTaskSuccessRate\n- checks whether the agent claims success (like \u201cBooking confirmed\u201d) or ifmetadata.successClaimed\nis true. At the end, we need to check if the agent actually finished the job.\nBest practices\nUltimately, choosing the right set of scorers will depend on the exact setup of your agentic system. In addition to the specific examples for the types of agents above, here\u2019s some general guidance for how to choose the right evaluation metrics.\nCode-based scorers are great for:\n- Exact or binary conditions\n- Did the system pick the \u201ccustomer support\u201d route?\n- Did it stay under 5 steps?\n- Numeric comparisons\n- Numeric difference from the expected output\n- Structured or factual checks\n- Is the final code snippet error-free?\nLLM-as-a-judge scorers are best when:\n- You need subjective or contextual feedback\n- Did the agent output a coherent paragraph?\n- Human-like interpretation is needed to decide if the agent responded politely or thoroughly\n- You want to check improvement across multiple drafts\nAutoevals are useful for:\n- Basic correctness (Factuality)\n- QA tasks (ClosedQA)\n- Similarity checks (EmbeddingSimilarity)\nCustom scorers let you:\n- Incorporate domain-specific knowledge\n- Checking that a generated invoice meets certain business rules\n- Evaluate multi-step flows\n- Partial checks, iteration loops\n- Implement your own specialized logic\n- Analyzing a chain-of-thought or verifying references in a research doc\nOver time, you can refine or replace scorers as you learn more about the real-world behaviors of your agent at scale.\nNext steps\nWhen you\u2019re happy with your scorers, you can deploy them at scale on production logs by configuring online evaluation. Online evaluation runs your scoring functions asynchronously as you upload logs.\nIf your agentic system is extremely complex, you may want to incorporate human review. This can take the form of incorporating user feedback, or having your product team or subject matter experts manually evaluate your LLM outputs. You can use human review to evaluate/compare experiments, assess the efficacy of your automated scoring methods, and curate log events to use in your evals.", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Latest news"}, {"href": "https://www.braintrust.dev/blog/evaluating-agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/blog/evaluating-agents", "anchor": "Why run evaluations?"}, {"href": "https://www.braintrust.dev/blog/after-evals", "anchor": "how you might improve your application"}, {"href": "https://www.braintrust.dev/blog/evaluating-agents", "anchor": "Choosing eval metrics"}, {"href": "https://www.braintrust.dev/blog/evaluating-agents", "anchor": "Quantitative vs. qualitative metrics"}, {"href": "https://www.braintrust.dev/blog/evaluating-agents", "anchor": "Iterative evaluation process"}, {"href": "https://www.braintrust.dev/blog/evaluating-agents", "anchor": "Building block: the augmented LLM"}, {"href": "https://www.braintrust.dev/docs/guides/evals/write", "anchor": "documentation"}, {"href": "https://www.braintrust.dev/blog/evaluating-agents", "anchor": "Prompt chaining"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "human review"}, {"href": "https://www.braintrust.dev/blog/evaluating-agents", "anchor": "Routing"}, {"href": "https://www.braintrust.dev/blog/evaluating-agents", "anchor": "Parallelization"}, {"href": "https://www.braintrust.dev/blog/evaluating-agents", "anchor": "Orchestrator-workers"}, {"href": "https://www.braintrust.dev/blog/evaluating-agents", "anchor": "Evaluator-optimizer"}, {"href": "https://www.braintrust.dev/blog/evaluating-agents", "anchor": "Fully autonomous agent"}, {"href": "https://www.braintrust.dev/blog/evaluating-agents", "anchor": "Best practices"}, {"href": "https://www.braintrust.dev/blog/evaluating-agents", "anchor": "Next steps"}, {"href": "https://www.braintrust.dev/docs/guides/evals/write", "anchor": "online evaluation"}, {"href": "https://www.braintrust.dev/docs/guides/logs/write", "anchor": "user feedback"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "manually evaluate"}, {"href": "https://www.braintrust.dev/blog/evaluating-agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/blog/evaluating-agents", "anchor": "Why run evaluations?"}, {"href": "https://www.braintrust.dev/blog/evaluating-agents", "anchor": "Choosing eval metrics"}, {"href": "https://www.braintrust.dev/blog/evaluating-agents", "anchor": "Quantitative vs. qualitative metrics"}, {"href": "https://www.braintrust.dev/blog/evaluating-agents", "anchor": "Iterative evaluation process"}, {"href": "https://www.braintrust.dev/blog/evaluating-agents", "anchor": "Building block: the augmented LLM"}, {"href": "https://www.braintrust.dev/blog/evaluating-agents", "anchor": "Prompt chaining"}, {"href": "https://www.braintrust.dev/blog/evaluating-agents", "anchor": "Routing"}, {"href": "https://www.braintrust.dev/blog/evaluating-agents", "anchor": "Parallelization"}, {"href": "https://www.braintrust.dev/blog/evaluating-agents", "anchor": "Orchestrator-workers"}, {"href": "https://www.braintrust.dev/blog/evaluating-agents", "anchor": "Evaluator-optimizer"}, {"href": "https://www.braintrust.dev/blog/evaluating-agents", "anchor": "Fully autonomous agent"}, {"href": "https://www.braintrust.dev/blog/evaluating-agents", "anchor": "Best practices"}, {"href": "https://www.braintrust.dev/blog/evaluating-agents", "anchor": "Next steps"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 1}, "https://www.braintrust.dev/blog/hybrid-deployment": {"url": "https://www.braintrust.dev/blog/hybrid-deployment", "title": "Our approach to hybrid deployment - Blog - Braintrust", "text": "Our approach to hybrid deployment\nWhen it comes to using Braintrust as part of your LLM development workflow, we want to make sure you have full flexibility and control over your data. To accomplish this, we designed our architecture with two main components: the data plane and the control plane. The data plane is the component that handles the actual data, while the control plane serves the UI along with metadata. When you deploy Braintrust in hybrid mode, you host the data plane (experiment logs, dataset records, etc.) in your own environment, while the control plane (web app and metadata) is hosted by Braintrust. Customers like Notion and Ramp use our hybrid deployment model to take advantage of our newest UI and platform features while simultaneously keeping sensitive data secure.\nThis model gives you the best of both worlds: security and compliance and UI updates. All your data, like experiment inputs/outputs, logs, and sensitive customer information, lives securely in your own environment. You can make sure it\u2019s behind a firewall or VPN and meets your organization\u2019s compliance requirements. And since the UI and metadata are hosted by Braintrust, you can visit our site to see the latest features and improvements, just like any other SaaS product. Our product engineering team can continuously polish every little detail and fix bugs instantly, and you don\u2019t have to manually update anything.\nKeeping your data secure\nBecause the data plane runs entirely in your environment, Braintrust's servers and employees do not require access to it. All data requests from the Braintrust UI go directly from your browser to your self-hosted data plane (via CORS), bypassing our servers. The data plane sends only metrics and status telemetry back to the control plane, not logs, traces, or customer data.\nData storage\nThe data plane contains your data:\n- Experiment records (input, output, expected, scores, metadata, traces, spans)\n- Log records\n- Dataset records\n- Prompt playground prompts and completions\n- Human review scores\nThe control plane stores metadata:\n- Experiment and dataset names\n- Project names and settings\n- Organization info\n- API keys (hashed)\n- Encrypted LLM provider secrets\nAuth credentials are managed through our external authentication service (Clerk).\nConstraining the SDK\nIn most setups, the Braintrust SDK will communicate with both your data plane and the control plane to retrieve various metadata, but it\u2019s also possible to constrain all SDK communication solely to your data plane. You can configure it so that the data plane acts as a proxy to the control plane, eliminating any need for outbound connections from the SDK to Braintrust\u2019s servers.\nAdvantages of hybrid deployment\n- You get access to the latest Braintrust features automatically.\n- You decide where to host the data plane and what data to purge, so you can meet any data residency requirements like GDPR.\n- You can configure rate limits, custom URLs, and domain proxies to ensure the deployment works within your IT environment and security policies.\nGetting started\nWe provide a Docker Compose configuration file that you can use as-is or adapt to your infrastructure. After starting the containers, you\u2019ll point to your newly hosted API in your Braintrust settings. From there, any time you load Braintrust via braintrust.dev, your browser will connect to your self-hosted data plane to retrieve and store data.\nYou can also self-host on AWS via Terraform.\nFor more information, check out the self-hosting guide. We\u2019re also happy to help set up advanced deployment scenarios\u2014 just reach out to support@braintrust.dev.", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Latest news"}, {"href": "https://www.braintrust.dev/blog/hybrid-deployment", "anchor": "Our approach to hybrid deployment"}, {"href": "https://www.braintrust.dev/blog/hybrid-deployment", "anchor": "Keeping your data secure"}, {"href": "https://www.braintrust.dev/blog/hybrid-deployment", "anchor": "Data storage"}, {"href": "https://www.braintrust.dev/blog/hybrid-deployment", "anchor": "Constraining the SDK"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting/advanced", "anchor": "configure it"}, {"href": "https://www.braintrust.dev/blog/hybrid-deployment", "anchor": "Advantages of hybrid deployment"}, {"href": "https://www.braintrust.dev/blog/hybrid-deployment", "anchor": "Getting started"}, {"href": "https://braintrust.dev", "anchor": "braintrust.dev"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting/aws", "anchor": "self-host on AWS"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "self-hosting guide"}, {"href": "mailto:support@braintrust.dev", "anchor": "support@braintrust.dev"}, {"href": "https://www.braintrust.dev/blog/hybrid-deployment", "anchor": "Our approach to hybrid deployment"}, {"href": "https://www.braintrust.dev/blog/hybrid-deployment", "anchor": "Keeping your data secure"}, {"href": "https://www.braintrust.dev/blog/hybrid-deployment", "anchor": "Data storage"}, {"href": "https://www.braintrust.dev/blog/hybrid-deployment", "anchor": "Constraining the SDK"}, {"href": "https://www.braintrust.dev/blog/hybrid-deployment", "anchor": "Advantages of hybrid deployment"}, {"href": "https://www.braintrust.dev/blog/hybrid-deployment", "anchor": "Getting started"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 1}, "https://www.braintrust.dev/blog/2024": {"url": "https://www.braintrust.dev/blog/2024", "title": "The top 10 most loved features of 2024 - Blog - Braintrust", "text": "The top 10 most loved features of 2024\nThis year was all about making Braintrust simpler, faster, and more powerful for you. Here are the top 10 features of 2024:\n1. Custom scorer, tool, and prompt functions\nCreate and manage your own scorers, tools, and prompts, all within the SDK.\n2. AI proxy and hybrid self-hosting\nWe bundled the AI proxy with the main API, introduced temp credentials for front-end requests, and broadened provider support (Cerebras, Fireworks, Lepton, etc.). Self-hosters also got more straightforward Docker deployments and better config options.\n3. Structured outputs in the playground\nWe added structured outputs to the playground, plus improved the UX so you can quickly parse and analyze raw data. No more wrestling with JSON\u2014just clean, organized feedback.\n4. Monitoring improvements\nSparkline charts on project home, flexible resizable charts, and a dedicated monitor page for performance metrics. Keeping an eye on system health has never been easier (or prettier).\n5. Faster experiment and log loading\nMajor speed boosts for experiments and logs, even at scale. We also added new grouping options and CSV/JSON exports, so you can comb through data at whatever level of detail you need.\n6. Human review\nSometimes you just need a real human\u2019s opinion on LLM responses. Define custom human review scores in the project configuration, quickly label data in \u201creview mode,\u201d and feed those insights back into automated scoring for richer, more accurate results.\n7. Custom provider configuration\nYou can now configure endpoints to stream results even if the model doesn\u2019t natively support it. Think \u201cinstant streaming\u201d from your custom providers to the Braintrust playground.\n8. Improved logs and search\nThe logs viewer now supports exporting rows as CSV or JSON, plus advanced BTQL search and UI filters across metadata fields.\n9. Attachment uploads and previews, plus Realtime API support\nLog, preview, and analyze file attachments directly in Braintrust. Use the OpenAI Realtime API securely in serverless environments. Whether it\u2019s data dumps, audio, images, or debug logs, everything can live under one roof.\n10. Flexible visualizations\nWe added a grid view for experiment comparisons, an expanded timeline and tree view for traces, and built-in side-by-side diff modes. You can also render custom iframe URLs as fields in trace spans, add tags, and define custom columns. Visualizing performance differences is quick and intuitive.\nThank you\nTo everyone who used Braintrust, shared feedback, or reported issues\u2014thank you. Your input guided every improvement we made this year. We can\u2019t wait to share even more with you soon!", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Latest news"}, {"href": "https://www.braintrust.dev/blog/2024", "anchor": "The top 10 most loved features of 2024"}, {"href": "https://www.braintrust.dev/blog/2024", "anchor": "1. Custom scorer, tool, and prompt functions"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "scorers, tools, and prompts"}, {"href": "https://www.braintrust.dev/blog/2024", "anchor": "2. AI proxy and hybrid self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosters"}, {"href": "https://www.braintrust.dev/blog/2024", "anchor": "3. Structured outputs in the playground"}, {"href": "https://www.braintrust.dev/docs/guides/functions/prompts", "anchor": "structured outputs"}, {"href": "https://www.braintrust.dev/blog/2024", "anchor": "4. Monitoring improvements"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "monitor page"}, {"href": "https://www.braintrust.dev/blog/2024", "anchor": "5. Faster experiment and log loading"}, {"href": "https://www.braintrust.dev/blog/2024", "anchor": "6. Human review"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "human review"}, {"href": "https://www.braintrust.dev/blog/2024", "anchor": "7. Custom provider configuration"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "configure endpoints"}, {"href": "https://www.braintrust.dev/blog/2024", "anchor": "8. Improved logs and search"}, {"href": "https://www.braintrust.dev/blog/2024", "anchor": "9. Attachment uploads and previews, plus Realtime API support"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "attachments"}, {"href": "https://www.braintrust.dev/docs/cookbook/recipes/Realtime", "anchor": "OpenAI Realtime API"}, {"href": "https://www.braintrust.dev/blog/2024", "anchor": "10. Flexible visualizations"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "custom iframe URLs"}, {"href": "https://www.braintrust.dev/blog/2024", "anchor": "Thank you"}, {"href": "https://www.braintrust.dev/blog/2024", "anchor": "The top 10 most loved features of 2024"}, {"href": "https://www.braintrust.dev/blog/2024", "anchor": "1. Custom scorer, tool, and prompt functions"}, {"href": "https://www.braintrust.dev/blog/2024", "anchor": "2. AI proxy and hybrid self-hosting"}, {"href": "https://www.braintrust.dev/blog/2024", "anchor": "3. Structured outputs in the playground"}, {"href": "https://www.braintrust.dev/blog/2024", "anchor": "4. Monitoring improvements"}, {"href": "https://www.braintrust.dev/blog/2024", "anchor": "5. Faster experiment and log loading"}, {"href": "https://www.braintrust.dev/blog/2024", "anchor": "6. Human review"}, {"href": "https://www.braintrust.dev/blog/2024", "anchor": "7. Custom provider configuration"}, {"href": "https://www.braintrust.dev/blog/2024", "anchor": "8. Improved logs and search"}, {"href": "https://www.braintrust.dev/blog/2024", "anchor": "9. Attachment uploads and previews, plus Realtime API support"}, {"href": "https://www.braintrust.dev/blog/2024", "anchor": "10. Flexible visualizations"}, {"href": "https://www.braintrust.dev/blog/2024", "anchor": "Thank you"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 1}, "https://www.braintrust.dev/blog/monitor": {"url": "https://www.braintrust.dev/blog/monitor", "title": "New monitor page for easy analytics - Blog - Braintrust", "text": "New monitor page for easy analytics\nWe're launching a new monitor page that provides comprehensive performance and usage patterns across both logs and experiments. This tool is useful for:\n- Debugging performance issues by correlating different metrics\n- Optimizing costs by identifying expensive patterns\n- Tracking the impact of prompt changes on model performance\n- Monitoring production stability through response times\nThe monitor page aggregates the key metrics that matter for LLM applications:\n- Latency\n- Token counts\n- Time to first token\n- Cost\n- Request volume\n- Model performance scores\nWhat makes this particularly powerful is the ability to analyze these metrics across both your logs and experiments, giving you a complete picture of your application's behavior in both production and testing environments.\nFlexible data analysis\nUnderstanding that LLM applications often have complex metadata structures, we've built the monitor page with flexibility in mind. You can:\n- Group your data by any metadata field, including custom fields you've defined\n- Apply filters to individual charts to focus on specific segments of your data\n- Analyze data across different timeframes to understand trends and patterns\n- Drill down into specific datapoints to view the corresponding traces\nGetting started\nThe monitor page is now available in all projects. To begin using it, just navigate to Monitor in the top navigation of any project. If you have requests for other metrics you'd like to see, let us know!", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Latest news"}, {"href": "https://www.braintrust.dev/blog/monitor", "anchor": "New monitor page for easy analytics"}, {"href": "https://www.braintrust.dev/blog/monitor", "anchor": "Flexible data analysis"}, {"href": "https://www.braintrust.dev/blog/monitor", "anchor": "Getting started"}, {"href": "mailto:support@braintrust.dev", "anchor": "let us know"}, {"href": "https://www.braintrust.dev/blog/monitor", "anchor": "New monitor page for easy analytics"}, {"href": "https://www.braintrust.dev/blog/monitor", "anchor": "Flexible data analysis"}, {"href": "https://www.braintrust.dev/blog/monitor", "anchor": "Getting started"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 1}, "https://www.braintrust.dev/blog/new-model": {"url": "https://www.braintrust.dev/blog/new-model", "title": "What to do when a new AI model comes out - Blog - Braintrust", "text": "What to do when a new AI model comes out\nEvery week, it seems like another AI provider releases a state-of-the-art model. These announcements come with impressive benchmarks, but those benchmarks rarely reflect real-world use cases. So, how do you know if the new model is worth deploying in your app?\nDevelop a baseline\nTo gauge if a particular model will improve your application, it\u2019s first worth understanding how well your app is currently performing. In AI applications, performance is measured using evaluations that consider the accuracy or quality of the LLM outputs. Setting up a baseline is simple\u2014 the easiest way is to run an eval with a set of data, the AI function you want to test, and a scoring function.\nAfter you run your first evaluation, you can adapt it to run against more models.\nBenchmark against real data\nThe best way to evaluate a new AI model is by testing it against the actual data your app handles in production. Generic benchmarks might give a sense of performance, but only your data can reveal how well a model works in your product. To do this in Braintrust, start by pulling real logs from your app and organizing them into a dataset. Consider choosing a set of logs that are underperforming to see if the new model makes an impact on the scores.\nThen, use the dataset to run an evaluation using the new model and directly compare the performance (and other factors like cost, tokens, and more) against the one you\u2019re already using.\nImportantly, you should also closely monitor your overall scores to ensure you are not regressing in any areas. To do so, you can use the Group by menu in the Experiments pane to see the results of your evals sorted by model.\nYou can also run further evaluations on a more extensive set of logs and check out the eval summaries.\nSwap your model in production\nIf the results show that the new model outperforms your current one, update it in production. If you make your LLM calls in production using the AI proxy, you can do this with just a one-line code change.\nMonitor your application\nAfter shipping the new model in production, you can keep tabs on its performance on the Monitor page. To focus on your model change, select Group by model and tighten the timeline to when you made the changes.\nWhat\u2019s next\nWhen the next AI model comes out, you won\u2019t need to guess if it\u2019s better for your app. By testing it with your actual data and swapping models quickly, you\u2019ll know for sure, and be able to make a confident decision every time.", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Latest news"}, {"href": "https://www.braintrust.dev/blog/new-model", "anchor": "What to do when a new AI model comes out"}, {"href": "https://www.braintrust.dev/blog/new-model", "anchor": "Develop a baseline"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "run your first evaluation"}, {"href": "https://www.braintrust.dev/blog/new-model", "anchor": "Benchmark against real data"}, {"href": "https://www.braintrust.dev/docs/guides/logs/write", "anchor": "real logs from your app"}, {"href": "https://www.braintrust.dev/blog/img/add-to-dataset.mp4", "anchor": "Video"}, {"href": "https://www.braintrust.dev/blog/new-model", "anchor": "Swap your model in production"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/blog/img/change-model.mp4", "anchor": "Video"}, {"href": "https://www.braintrust.dev/blog/new-model", "anchor": "Monitor your application"}, {"href": "https://www.braintrust.dev/blog/new-model", "anchor": "What\u2019s next"}, {"href": "https://www.braintrust.dev/blog/new-model", "anchor": "What to do when a new AI model comes out"}, {"href": "https://www.braintrust.dev/blog/new-model", "anchor": "Develop a baseline"}, {"href": "https://www.braintrust.dev/blog/new-model", "anchor": "Benchmark against real data"}, {"href": "https://www.braintrust.dev/blog/new-model", "anchor": "Swap your model in production"}, {"href": "https://www.braintrust.dev/blog/new-model", "anchor": "Monitor your application"}, {"href": "https://www.braintrust.dev/blog/new-model", "anchor": "What\u2019s next"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 1}, "https://www.braintrust.dev/blog/rag-mongodb": {"url": "https://www.braintrust.dev/blog/rag-mongodb", "title": "Building a RAG app with MongoDB Atlas - Blog - Braintrust", "text": "Building a RAG app with MongoDB Atlas\nRetrieval augmented generation (RAG) is a powerful technique for adding context to your LLM responses. However, retrieving the context typically involves API calls, and therefore, you can only iterate on RAG applications in your codebase. In Braintrust, you can simplify this workflow by instead pushing the retrieval tool from your codebase to our UI, then iterating with prompts and models in an intuitive interface until you get your desired results.\nOne of the most common RAG applications is an AI agent that answers questions about your documentation. Let's say you deploy one to your users, and you receive some feedback that they wish it included more code examples in its responses. Normally, you would have to jump into your codebase, tweak the prompt, and try out the changes. If you want to compare multiple versions side-by-side, you'd have to deploy each version separately.\nUsing Braintrust functions, a RAG agent can be defined as just two components:\n- A system prompt containing instructions for how to retrieve content and synthesize answers\n- A vector search tool, implemented in TypeScript, which embeds a query, searches for relevant documents, and returns them\nThis architecture enables you to experiment with different prompts together with retrieval logic, side-by-side, all within the playground UI.\nLet's jump into the details.\nGetting started\nTo follow along, you'll need a few accounts:\nand node\n, npm\n, and typescript\ninstalled locally.\nWhen you log into your Atlas account, you'll need to create a new cluster. Call it braintrust-docs\n, and configure your network access to allow the IP address 0.0.0.0/0\n.\nThis makes your cluster accessible to everyone that is a database user in your Atlas account. Do not use this IP address for enterprise clusters.\nFind the connection string for your cluster, and add it to a new .env.local\nfile along with your Braintrust API key:\nMake sure to set your OPENAI_API_KEY\nenvironment variable in the AI providers section of your Braintrust account.\nUpload vectors\nThe first thing we'll do is upload the files we want to give the LLM for context. First, download this docs-sample\ndirectory from GitHub\u2014 these documents are various parts of the Braintrust documentation and blog.\nThen, we'll upload them in the form of vectors to our database.\nTo upload the vectors, download the upload-vectors.ts\nfile onto your computer and run the script:\nThis script reads all the files from the docs-sample\ndirectory, breaks them into sections based on headings, and creates vector embeddings for each section using OpenAI's API. It then stores those embeddings along with the section's title and content in MongoDB Atlas.\nThat's it for setup! Now let's try to retrieve the vectors using Braintrust.\nCreating a RAG tool\nBraintrust allows you to create RAG tools and then run them in the UI, API, and via prompts, enabling you to quickly iterate on assistant-style agents.\nThe retrieval tool is defined in retrieval-and-prompt.ts\n. You'll need to download the file onto your computer to be able to run it.\nThis code takes a search query, converts it into a numerical vector using OpenAI's embedding model, and then sends that vector to Atlas to find the most similar items stored in the database. It retrieves the top results based on similarity and returns key information (title and content) from the matching items.\nTo push the tool to Braintrust, run:\nThe output should be:\nTry out the tool\nTo try out the tool, visit the project in Braintrust, and navigate to the Tools section of your Library.\nHere, you can test different searches and refine the logic. For example, you could try playing with various\ntop_k\nvalues, or adding a prefix to the query to guide the results. If you change the code, run\nnpx braintrust push retrieval-and-prompt.ts\nagain to update the tool.\nWriting a prompt\nWhen we pushed retrieval-and-prompt.ts\n, we also pushed an initial definition of the prompt to Braintrust:\nYou can run the prompt in the UI and even try it out on some examples:\nIf you visit the Logs tab, you can check out detailed logs for each call:\nWe recommend using code-based prompts to initialize projects, but we'll show how convenient it is to tweak your prompts in the UI in a moment.\nImport a dataset\nTo get a better sense of how well this prompt and tool work, let's upload a dataset with a few questions and assertions. The assertions allow us to test specific characteristics about the answers, without spelling out the exact answer itself.\nThe dataset is defined in questions-dataset.ts\n. You'll first need to download it to your computer from GitHub. Then, upload it to Braintrust by running:\nOnce you create it, if you visit the Datasets tab, you'll be able to explore it:\nCreate a playground\nTo try out the prompt together with the dataset, we'll create a playground. Go to your prompt, and scroll down to Create playground with prompt.\nOnce you create the playground, hit Run to run the prompt and tool on the questions in the dataset.\nDefine a scorer\nNow that we have an interactive environment to test out our prompt and tool call, let's define a scorer that helps us evaluate the results.\nSelect the Scorers dropdown menu, then Create custom scorer. Choose the LLM-as-a-judge tab, and enter\nFor the choice scores, configure (a) as 1, (b) as 0.5, and (c) as 0.\nOnce you define the scorer, hit Run to run it on the questions in the dataset.\nTweak the prompt\nNow, let's tweak the prompt to see if we can improve the results. Hit the copy icon to duplicate your prompt and start tweaking. You can also tweak the original prompt and save your changes there if you'd like. For example, you can try instructing the model to always include a Python and TypeScript code snippet.\nOnce you're satisfied with the prompt, hit Update to save the changes. Each time you save the prompt, you create a new version. To learn more about how to use a prompt in your code, check out the prompts guide.\nRun full experiments\nThe playground is very interactive, but if you'd like to create a more detailed evaluation, where you can:\n- See every step, including the tool calls and scoring prompts\n- Compare side-by-side diffs, improvements, and regressions\n- Share a permanent snapshot of results with others on your team\nthen you can run a full experiment by selecting +Experiments. Once you run the experiments, you can dig in further to the full analysis:\nNext steps\nNow that you've built a RAG app in Braintrust, you can:\n- Deploy the prompt in your app\n- Conduct more detailed evaluations\n- Learn about logging LLM calls to create a data flywheel", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Latest news"}, {"href": "https://www.braintrust.dev/blog/rag-mongodb", "anchor": "Building a RAG app with MongoDB Atlas"}, {"href": "https://www.braintrust.dev/blog/img/mongo/Side-by-side.mp4", "anchor": "Video"}, {"href": "https://www.braintrust.dev/blog/rag-mongodb", "anchor": "Getting started"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Braintrust"}, {"href": "https://www.braintrust.dev/app/braintrustdata.com/settings/secrets", "anchor": "AI providers"}, {"href": "https://www.braintrust.dev/blog/rag-mongodb", "anchor": "Upload vectors"}, {"href": "https://www.braintrust.dev/blog/rag-mongodb", "anchor": "Creating a RAG tool"}, {"href": "https://www.braintrust.dev/blog/rag-mongodb", "anchor": "Try out the tool"}, {"href": "https://www.braintrust.dev/blog/img/mongo/Test-tool.mp4", "anchor": "Video"}, {"href": "https://www.braintrust.dev/blog/rag-mongodb", "anchor": "Writing a prompt"}, {"href": "https://www.braintrust.dev/blog/img/mongo/Test-prompt.mp4", "anchor": "Video"}, {"href": "https://www.braintrust.dev/blog/rag-mongodb", "anchor": "Import a dataset"}, {"href": "https://www.braintrust.dev/blog/rag-mongodb", "anchor": "Create a playground"}, {"href": "https://www.braintrust.dev/blog/img/mongo/Run-playground.mp4", "anchor": "Video"}, {"href": "https://www.braintrust.dev/blog/rag-mongodb", "anchor": "Define a scorer"}, {"href": "https://www.braintrust.dev/blog/rag-mongodb", "anchor": "Tweak the prompt"}, {"href": "https://www.braintrust.dev/docs/guides/functions/prompts", "anchor": "prompts guide"}, {"href": "https://www.braintrust.dev/blog/rag-mongodb", "anchor": "Run full experiments"}, {"href": "https://www.braintrust.dev/blog/rag-mongodb", "anchor": "Next steps"}, {"href": "https://www.braintrust.dev/docs/guides/prompts", "anchor": "Deploy the prompt in your app"}, {"href": "https://www.braintrust.dev/docs/guides/evals", "anchor": "Conduct more detailed evaluations"}, {"href": "https://www.braintrust.dev/docs/guides/logging", "anchor": "logging LLM calls"}, {"href": "https://www.braintrust.dev/blog/rag-mongodb", "anchor": "Building a RAG app with MongoDB Atlas"}, {"href": "https://www.braintrust.dev/blog/rag-mongodb", "anchor": "Getting started"}, {"href": "https://www.braintrust.dev/blog/rag-mongodb", "anchor": "Upload vectors"}, {"href": "https://www.braintrust.dev/blog/rag-mongodb", "anchor": "Creating a RAG tool"}, {"href": "https://www.braintrust.dev/blog/rag-mongodb", "anchor": "Try out the tool"}, {"href": "https://www.braintrust.dev/blog/rag-mongodb", "anchor": "Writing a prompt"}, {"href": "https://www.braintrust.dev/blog/rag-mongodb", "anchor": "Import a dataset"}, {"href": "https://www.braintrust.dev/blog/rag-mongodb", "anchor": "Create a playground"}, {"href": "https://www.braintrust.dev/blog/rag-mongodb", "anchor": "Define a scorer"}, {"href": "https://www.braintrust.dev/blog/rag-mongodb", "anchor": "Tweak the prompt"}, {"href": "https://www.braintrust.dev/blog/rag-mongodb", "anchor": "Run full experiments"}, {"href": "https://www.braintrust.dev/blog/rag-mongodb", "anchor": "Next steps"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 1}, "https://www.braintrust.dev/blog/gemini": {"url": "https://www.braintrust.dev/blog/gemini", "title": "Evaluating Gemini models for vision - Blog - Braintrust", "text": "Evaluating Gemini models for vision\nWith the AI proxy, evaluating the performance of leading image models like Gemini Pro 1.5 and Flash 1.5 is as simple as a one-line model change. Gemini can be used for any multimodal use cases, like document extraction and image captioning. To see how these models stack up, we put them to the test\u2014 read on to find out which vision model came out on top.\nGemini for document extraction\nTo understand Gemini's capabilities, we compared them to other state-of-the-art multimodal models using the receipt extraction cookbook. Besides swapping to Gemini, we used the same setup: the input is an image of a receipt, and the model extracts values for keys.\nWe used Gemini models (gemini-1.5-pro-002\nand gemini-1.5-flash-002\n, specifically), alongside GPT-4o, Llama 3.2, and Pixtral 12B to assess accuracy, efficiency, and latency. Here's what we found:\nKey takeaways\n- Gemini models use significantly fewer tokens per image compared to the GPT models. For this particular eval, GPT-4o uses 3.5x the number of tokens per image of Gemini Pro and Flash 1.5, and GPT-4o mini uses 111x the number of tokens of the Gemini models.\n- Levenshtein distance scores, a heuristic metric, are largely comparable between the Gemini 1.5 and GPT-4o models. Factuality, which is LLM based (using GPT-4o as the autorater), diverges more between the models, with Gemini 1.5 Pro and Flash 002 performing marginally better than GPT-4o and GPT-4o mini. We observed a maximum of 3% variance (89%-94% with Pro 1.5 and 84%-89% with Flash) when re-running the same evals with the Gemini models.\n- The Gemini models process inputs faster than the GPT-4o models (due to more efficient tokenization) for this document extraction task.\n- The number of completion tokens for this task is 20-25% higher with the Gemini models.\nIn summary, Gemini models were more token-efficient, faster at processing inputs, and slightly more accurate in factuality, though they generate more completion tokens compared to GPT-4o models.\nGetting started with Gemini\nGiven that the example above is based on a specific task and image domain, we encourage you to try Gemini through the proxy for your own use cases or other vision prompts. The proxy is standardized to OpenAI's interface, but you can query and run evals with the Gemini models using just a single-line model parameter change:\nWant to see Gemini in action? You can use the playground to compare different models and prompts side by side.\nWorking with multimodal models\nEvery request through the AI proxy is logged to Braintrust for deeper insights into your performance and usage. Whether you\u2019re exploring document extraction, image captioning, or other multimodal use cases, the proxy lets you integrate Gemini into your applications with just a single-line code change.\nThe playground also provides a way to experiment with Gemini\u2019s multimodal capabilities, compare outputs across models, and fine-tune prompts to meet your specific needs. For a deeper dive into working with multimodal models and understanding traces, check out our detailed guide.", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Latest news"}, {"href": "https://www.braintrust.dev/blog/gemini", "anchor": "Evaluating Gemini models for vision"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/blog/gemini", "anchor": "Gemini for document extraction"}, {"href": "https://www.braintrust.dev/docs/cookbook/recipes/ReceiptExtraction", "anchor": "cookbook"}, {"href": "https://www.braintrust.dev/blog/gemini", "anchor": "Key takeaways"}, {"href": "https://www.braintrust.dev/blog/gemini", "anchor": "Getting started with Gemini"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "playground"}, {"href": "https://www.braintrust.dev/blog/gemini", "anchor": "Working with multimodal models"}, {"href": "https://www.braintrust.dev/docs/guides/tracing", "anchor": "detailed guide"}, {"href": "https://www.braintrust.dev/blog/gemini", "anchor": "Evaluating Gemini models for vision"}, {"href": "https://www.braintrust.dev/blog/gemini", "anchor": "Gemini for document extraction"}, {"href": "https://www.braintrust.dev/blog/gemini", "anchor": "Key takeaways"}, {"href": "https://www.braintrust.dev/blog/gemini", "anchor": "Getting started with Gemini"}, {"href": "https://www.braintrust.dev/blog/gemini", "anchor": "Working with multimodal models"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 1}, "https://www.braintrust.dev/blog/python-tools-uv": {"url": "https://www.braintrust.dev/blog/python-tools-uv", "title": "Python tool functions: powered by uv - Blog - Braintrust", "text": "Support for Python tool functions\nToday, we\u2019re announcing support for Python-based tools: custom general-purpose code blocks that LLMs can invoke to add complex logic or external operations. Tools are defined in code and sync with the UI using braintrust push\n. If you import libraries or dependencies in your codebase, braintrust push\nalso bundles and uploads them alongside your code.\nInitially, we only supported defining tools in TypeScript because tools like esbuild\nand the JavaScript bundling ecosystem made it fairly straightforward. As we built out support for Python-based tools, it was difficult to compile dependencies into one artifact, and many dependencies (like numpy\n, NLTK\n, and scikit-learn\n) ship as platform-specific binaries due to their use of native code.\nTo address this challenge, we leveraged uv, the Python package manager from Astral. uv supports cross-bundling virtual environments by resolving and installing Python dependencies based on a specified target platform and Python version. In Python, braintrust push\nnow leverages uv to automatically create a Linux-compatible virtual environment with your choice of dependencies and syncs it to Braintrust with your tool code. Braintrust takes care of the rest\u2014 sandboxing, auth, and invoking it from within the platform.\nTools are reusable and composable, making it straightforward to iterate on assistant-style agents and more advanced applications. For example, you can use tools to create simple and composable agents that perform tasks like web-scraping, retrieval augmented generation (RAG), or API execution. Once you build a tool, you can use prompts to deploy it across the UI and API. And thanks to uv, we now support creating tools in both TypeScript and Python.\nTo learn more about tools, read the tools guide and try building one today.", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Latest news"}, {"href": "https://www.braintrust.dev/blog/python-tools-uv", "anchor": "Support for Python tool functions"}, {"href": "https://www.braintrust.dev/docs/guides/functions/tools", "anchor": "tools"}, {"href": "https://www.braintrust.dev/docs/guides/functions/tools", "anchor": "tools guide"}, {"href": "https://www.braintrust.dev/blog/python-tools-uv", "anchor": "Support for Python tool functions"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 1}, "https://www.braintrust.dev/blog/realtime-api": {"url": "https://www.braintrust.dev/blog/realtime-api", "title": "Building serverless apps with the OpenAI Realtime API - Blog - Braintrust", "text": "Building secure and scalable production apps with OpenAI\u2019s Realtime API\nIn early October, OpenAI released the Realtime API, designed for building advanced multimodal conversational experiences. This API enables rich AI applications with features like speech-to-speech interaction and simultaneous multimodal output. However, there are three key pain points that need to be solved before you can use the API to build secure and scalable production applications:\n- User-facing credentials\n- Logging\n- Evaluations\nAt Braintrust, we want building AI applications with the most cutting-edge models to be a simple, wonderful developer experience. Today, we\u2019re excited to announce support for the Realtime API via the Braintrust AI proxy, and solutions to these specific pain points.\nInfrastructure challenges\nThe OpenAI Realtime API is built on WebSockets to enable a responsive user experience. However, if you\u2019re using a serverless backend like Vercel or AWS Lambda, which do not support WebSockets, it\u2019s impossible to connect to the API without hosting a separate server somewhere else.\nThe API also currently lacks client-side authentication, making it insecure to connect to the API directly from the user\u2019s browser.\nThe architecture requires developers to solve these problems by setting up a separate, long-running Node.js relay server. The relay server runs the provided relay.js\ncode and holds an OpenAI API key to handle Realtime API connections. Running a separate server complicates your architecture, but the only alternative\u2014storing your API key in the frontend\u2014isn\u2019t secure for production.\nBecause all Realtime API calls need to pass through the relay, it also has to scale up quickly to handle your app\u2019s traffic without impacting responsiveness. We believe that developers would rather focus on the features that make their app unique, rather than infrastructure scaling.\nUsing the AI proxy\nTo address this operational complexity, we rearchitected a solution using our existing AI proxy. This way, you won\u2019t need to embed your OpenAI API key directly into your backend, and you can continue using whichever serverless platform you\u2019re used to using for building AI applications.\nThe AI proxy securely manages your OpenAI API key, issuing temporary credentials to your backend and frontend. The frontend sends any voice data from your app to the proxy, which handles secure communication with OpenAI\u2019s Realtime API. This offloads the infrastructure burden to us, and allows you to focus on building your app.\nConfiguring your app\nTo access the Realtime API through the Braintrust proxy, change the proxy URL when instantiating the RealtimeClient\nto https://braintrustproxy.com/v1/realtime\n.\nAs an example, we forked the sample app from OpenAI and hooked it up to our proxy with just a couple of lines of code:\nNext, generate a Braintrust temporary credential, to be used instead of the OpenAI API key. This means your OpenAI API key will not be exposed to the client.\nYou can also use our proxy with an AI provider\u2019s API key, but you will not have access to other Braintrust features, like logging.\nLogging\nIn addition to client-side authentication, you\u2019ll also get the other benefits of building with Braintrust, like logging, built in. We support logging audio, as well as text, structured data, and images. When you connect to the Realtime API, a log will begin generating, and when your session is closed, the log will be ready to view. Each LLM and tool call will be contained in its own span inside of the trace. And most importantly, all multimodal content is now able to be uploaded and viewed as an attachment in your trace. This means that you won\u2019t have to exit the UI to double-click on an LLM call and view the input and output, no matter what type of format its in.\nWhat\u2019s next\nYou can use this open-source repo as a starting point for any projects using the Realtime API. Because your app will automatically generate logs in Braintrust, you will have data in exactly the right format to run evaluations.\nTry building with the OpenAI Realtime API today and let us know what you create!", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Latest news"}, {"href": "https://www.braintrust.dev/blog/realtime-api", "anchor": "Building secure and scalable production apps with OpenAI\u2019s Realtime API"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/blog/img/realtime-api/realtime-demo.mp4", "anchor": "Video"}, {"href": "https://www.braintrust.dev/blog/realtime-api", "anchor": "Infrastructure challenges"}, {"href": "https://www.braintrust.dev/blog/realtime-api", "anchor": "Using the AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "temporary credentials"}, {"href": "https://www.braintrust.dev/blog/realtime-api", "anchor": "Configuring your app"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "Braintrust temporary credential"}, {"href": "https://www.braintrust.dev/blog/realtime-api", "anchor": "Logging"}, {"href": "https://www.braintrust.dev/blog/attachments", "anchor": "attachment"}, {"href": "https://www.braintrust.dev/blog/realtime-api", "anchor": "What\u2019s next"}, {"href": "https://www.braintrust.dev/docs/guides/evals", "anchor": "evaluations"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "Try building with the OpenAI Realtime API"}, {"href": "https://www.braintrust.dev/contact", "anchor": "let us know"}, {"href": "https://www.braintrust.dev/blog/realtime-api", "anchor": "Building secure and scalable production apps with OpenAI\u2019s Realtime API"}, {"href": "https://www.braintrust.dev/blog/realtime-api", "anchor": "Infrastructure challenges"}, {"href": "https://www.braintrust.dev/blog/realtime-api", "anchor": "Using the AI proxy"}, {"href": "https://www.braintrust.dev/blog/realtime-api", "anchor": "Configuring your app"}, {"href": "https://www.braintrust.dev/blog/realtime-api", "anchor": "Logging"}, {"href": "https://www.braintrust.dev/blog/realtime-api", "anchor": "What\u2019s next"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 1}, "https://www.braintrust.dev/blog/attachments": {"url": "https://www.braintrust.dev/blog/attachments", "title": "Logging with attachments - Blog - Braintrust", "text": "Logging with attachments\nAI applications are increasingly expanding beyond text input and output to accept and generate images, audio, video, and more. At Braintrust, we are committed to supporting these advanced capabilities. Today, we\u2019re excited to share that you can now upload, download, and contextually view attachments in Braintrust.\nUntil today, you could log text, structured data, and images (base64 or URLs) to Braintrust. Now, you can upload any type of binary attachment. Once the upload is complete, you can view images, audio, video, and PDF files right inside your traces.\nUploading an attachment\nTo upload an attachment, create a new Attachment\nobject to represent the file path or in-memory buffer that you want to upload:\nYou can place the Attachment\nanywhere in a log, dataset, or feedback log.\nBehind the scenes, the Braintrust SDK automatically detects and uploads attachments in the background, in parallel to the original logs. This ensures that the latency of your logs isn\u2019t affected by any additional processing.\nYour files are securely stored in an object store and associated with the uploading user\u2019s organization. Only you can access your attachments.\nViewing an attachment in a trace\nIf your trace has any attachments, they show up as an additional list under the data viewer in the trace pane. You can preview most images, audio files, videos, or PDFs in the Braintrust UI. You can also download any file to view it locally.\nWhat\u2019s next\nUploading, downloading, and viewing attachments not only simplifies debugging, but also provides observability for multimodal and complex applications. You can use it to log:\n- The user\u2019s input images to multimodal models like GPT-4o and Claude 3.\n- Input and output audio from a speech-to-speech model such as GPT-4o Audio Preview.\n- Raw documents, including PDFs, text files, Word documents, and spreadsheets, before your application processes them into a model input.\nTry out attachments today and let us know what you think. And if you have strong opinions on how complex evals should be implemented, come work with us.", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Latest news"}, {"href": "https://www.braintrust.dev/blog/attachments", "anchor": "Logging with attachments"}, {"href": "https://www.braintrust.dev/blog/img/attachments/attachments-demo.mp4", "anchor": "Video"}, {"href": "https://www.braintrust.dev/blog/attachments", "anchor": "Uploading an attachment"}, {"href": "https://www.braintrust.dev/docs/guides/tracing", "anchor": "upload an attachment"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/classes/Attachment", "anchor": "Braintrust SDK"}, {"href": "https://www.braintrust.dev/blog/attachments", "anchor": "Viewing an attachment in a trace"}, {"href": "https://www.braintrust.dev/blog/attachments", "anchor": "What\u2019s next"}, {"href": "https://www.braintrust.dev/docs/guides/tracing", "anchor": "Try out attachments today"}, {"href": "https://www.braintrust.dev/contact", "anchor": "let us know what you think"}, {"href": "https://www.braintrust.dev/careers", "anchor": "come work with us"}, {"href": "https://www.braintrust.dev/blog/attachments", "anchor": "Logging with attachments"}, {"href": "https://www.braintrust.dev/blog/attachments", "anchor": "Uploading an attachment"}, {"href": "https://www.braintrust.dev/blog/attachments", "anchor": "Viewing an attachment in a trace"}, {"href": "https://www.braintrust.dev/blog/attachments", "anchor": "What\u2019s next"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 1}, "https://www.braintrust.dev/blog/after-evals": {"url": "https://www.braintrust.dev/blog/after-evals", "title": "I ran an eval. Now what? - Blog - Braintrust", "text": "I ran an eval. Now what?\nSo you built an AI application, curated an initial set of test examples, picked a scoring function, and ran an eval. You got a score\u2014great!\nNow what?\nWe'll walk you through how to think about that score, and more importantly, what steps you should take next to continuously improve both your AI application and your evaluation process, including:\n- How to determine what to focus on next\n- How to iteratively improve your evals and AI application\n- Best practices for creating a rapid development loop\nNote: this guide assumes you already have a basic evaluation infrastructure in place. If not, we recommend starting with these two guides first: Structuring evals and Getting started with automated evals.\nHow to choose your focus\nAfter running an eval, your two goals should be to:\n- Improve the fidelity and coverage of your evaluations\n- Improve your AI application directly\nTo figure out where to focus first, start by reviewing 5-10 actual examples from your evals. For each row, inspect the trace\u2013the input, output, and how your evaluation function scored the output.\nAs you review these examples, ask yourself:\n- Is the output good or bad?\n- Did the scoring function correctly identify it as good or bad?\nEvery example will fall into one of these four categories:\nOnce you've analyzed these examples, you'll start to notice patterns around how your application is performing and whether your scoring is accurate. From there, you can decide whether to refine your evals or make changes to your application.\nThe key here is iteration. Don\u2019t overthink it! In almost all cases, it\u2019s worth spending time improving both your evals and your AI application. Move fast, try things out, inspect the results, and roll back updates if necessary.\nHow to improve your evals\nThere are three main ways to improve your evals:\n- Refine the scoring function to ensure it accurately reflects the success criteria.\n- Add new scoring functions to capture different performance aspects (for example, correctness or efficiency).\n- Expand your dataset with more diverse or challenging test cases.\nFor an in-depth guide, check out the full blog post.\nHow to improve your AI application\nIf your evaluations are robust, you can confidently iterate on your AI application, since any changes are tested for performance impact and potential regressions each time you re-run your evals.\nHere's the general workflow for making improvements:\n- Decide on an improvement\n- Think through how to make that improvement\n- Make the update and re-run your evals to verify results\nDecide on an improvement\nBefore you make an update, it's important to define what you're improving and why. This clarity is important because it makes your iteration targeted and measurable. Start by examining your eval results: overall scores, individual examples, and feedback.\n- Low scores: these often highlight underperforming areas or bugs, making them prime targets for improvement\n- Random and high scores: potential edge cases or unexpected issues\n- Patterns: indications of where systemic improvements can be made\nOnce you've identified a potential improvement, frame it in one of two ways:\nIncrease a score\nWell-defined scores are tied to specific attributes (e.g. factuality, length of output) or capabilities (e.g. selecting the right tool, writing executable code). By increasing a score, you're clearly indicating that you are improving your application along a particular axis. Start with end-to-end scores\u2014these give you a broad view of how well your system is performing. Then, as needed, zoom in on specific sections where smaller, more focused changes can make a significant impact.\nFix a specific set of bad outputs\nFixing examples of bad outputs is essential when dealing with customer complaints or catching critical failures while inspecting examples. These fixes typically address very visible or user-facing problems, so they're usually high leverage.\nThink through how to make that improvement\nOnce you've decided what to improve, the next step is to think through what update would result in the improvement you decided on. This part of the process is critical because the range of potential tweaks is broad, but choosing the right one can be both fun and challenging.\nHere are some common areas we see AI teams change:\n- Prompts (e.g. wording, in-context examples, OpenAI guide)\n- Model weights (fine-tuning)\n- Retrieval pipeline (embedding model, chunking, re-ranking, # of documents retrieved)\n- Non-LLM code (e.g. pre/post processing, routing)\n- API call parameters (e.g. function-calling, temperature, max tokens)\n- Tool usage\n- Base model\nMake the update and re-run your evals to verify results\nNow, it's time to make the update to your AI application. Whether you\u2019re tweaking a prompt, adjusting model weights, or modifying pipeline logic, the goal is to introduce changes that directly target the performance or behavior you\u2019re trying to improve.\nAfter implementing the change, re-run your evals to see if your change to verify the impact.\n- If your goal was to improve a specific score/attribute, you should check that score\n- If your goal was to fix a specific customer problem, you should inspect examples relevant to that problem\nImportantly, you should also closely monitor your overall scores to ensure you are not regressing in other areas. Avoiding unintended regressions is critical to continuous forward progress.\nRepeat!\nDid the change move the needle? Great\u2014build on that momentum. If not, you now have more data to make better decisions and try again. The key is in the iteration\u2014every cycle moves your application and evals closer to optimal performance.\nIdeal workflow\nWe've shared how critical a rapid development loop is to delivering high-quality AI features. After each improvement, re-run your evaluations and go through the workflow again.\nMany of our customers cycle through this loop 50+ times a day. This fast-paced iteration is what enables AI teams to stay ahead, ship robust AI features, and quickly react to feedback and issues. To see an example of the workflow in action, check out how Notion develops world-class AI features.\nAt Braintrust, our goal is to make AI development as iterative and robust as possible. If you're looking to optimize your workflow or scale your AI product development efforts, get in touch!", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Latest news"}, {"href": "https://www.braintrust.dev/blog/after-evals", "anchor": "I ran an eval. Now what?"}, {"href": "https://www.braintrust.dev/blog/eval-feedback-loops", "anchor": "Structuring evals"}, {"href": "https://www.braintrust.dev/blog/getting-started-evals", "anchor": "Getting started with automated evals"}, {"href": "https://www.braintrust.dev/blog/after-evals", "anchor": "How to choose your focus"}, {"href": "https://www.braintrust.dev/blog/after-evals", "anchor": "How to improve your evals"}, {"href": "https://www.braintrust.dev/blog/improve-evals", "anchor": "check out the full blog post"}, {"href": "https://www.braintrust.dev/blog/after-evals", "anchor": "How to improve your AI application"}, {"href": "https://www.braintrust.dev/blog/after-evals", "anchor": "Decide on an improvement"}, {"href": "https://www.braintrust.dev/blog/after-evals", "anchor": "Increase a score"}, {"href": "https://www.braintrust.dev/blog/after-evals", "anchor": "Fix a specific set of bad outputs"}, {"href": "https://www.braintrust.dev/blog/after-evals", "anchor": "Think through how to make that improvement"}, {"href": "https://www.braintrust.dev/blog/after-evals", "anchor": "Make the update and re-run your evals to verify results"}, {"href": "https://www.braintrust.dev/blog/after-evals", "anchor": "Repeat!"}, {"href": "https://www.braintrust.dev/blog/after-evals", "anchor": "Ideal workflow"}, {"href": "https://www.braintrust.dev/blog/notion", "anchor": "how Notion develops world-class AI features"}, {"href": "https://www.braintrust.dev/contact", "anchor": "get in touch"}, {"href": "https://www.braintrust.dev/blog/after-evals", "anchor": "I ran an eval. Now what?"}, {"href": "https://www.braintrust.dev/blog/after-evals", "anchor": "How to choose your focus"}, {"href": "https://www.braintrust.dev/blog/after-evals", "anchor": "How to improve your evals"}, {"href": "https://www.braintrust.dev/blog/after-evals", "anchor": "How to improve your AI application"}, {"href": "https://www.braintrust.dev/blog/after-evals", "anchor": "Decide on an improvement"}, {"href": "https://www.braintrust.dev/blog/after-evals", "anchor": "Increase a score"}, {"href": "https://www.braintrust.dev/blog/after-evals", "anchor": "Fix a specific set of bad outputs"}, {"href": "https://www.braintrust.dev/blog/after-evals", "anchor": "Think through how to make that improvement"}, {"href": "https://www.braintrust.dev/blog/after-evals", "anchor": "Make the update and re-run your evals to verify results"}, {"href": "https://www.braintrust.dev/blog/after-evals", "anchor": "Repeat!"}, {"href": "https://www.braintrust.dev/blog/after-evals", "anchor": "Ideal workflow"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 1}, "https://www.braintrust.dev/blog/notion": {"url": "https://www.braintrust.dev/blog/notion", "title": "How Notion develops world-class AI features  - Blog - Braintrust", "text": "How Notion develops world-class AI features\nNotion, a connected workspace for documents, knowledge bases, and project management systems, is the go-to platform for millions of users, from agile startups to global enterprises. Notion\u2019s commitment to customer experience has always set them apart \u2014 but with the rise of generative AI, they saw an opportunity to push their platform even further.\nCo-founders Ivan Zhao and Simon Last were early in recognizing the potential of generative AI and started experimenting with large language models (LLMs) soon after GPT-2 launched in 2019. Given that context, it\u2019s no surprise that Notion was one of the first teams to integrate GPT-4 into their product:\n- In November 2022, they introduced Notion AI, a writing assistant (2 weeks before ChatGPT was released)\n- In June 2023, they introduced AI Autofill, a tool to generate summaries and run custom prompts across an entire workspace\n- And in November 2023, they introduced Notion Q&A for chatting with your entire Notion workspace\nToday, those features (and much more!) make up Notion AI, which powers four essential capabilities: searching workspaces, generating and editing tailored documents, analyzing PDFs and images, and answering questions using information from both your workspace and the web. However, as the team rolled out these features, they realized their existing workflows for evaluating AI products weren\u2019t up to the challenge.\nIn this case study, we\u2019ll explore how the team at Notion evolved their evaluation workflow\u2014 dramatically improving their AI development speed and accuracy\u2014 by partnering with Braintrust.\nEvaluating Notion Q&A\nOne of Notion\u2019s earliest AI innovations was its Q&A feature. With Q&A, users could ask complex questions, and AI would pull data directly from their Notion pages to provide insightful responses. It was a magical customer experience \u2014 despite the broad and unstructured input and output space, it almost always returned a helpful response.\nBehind this magic was an immense engineering effort. Notion\u2019s AI team did a ton of work to ensure Q&A could understand diverse user queries and generate a helpful output. However, the complexity of the product meant that their evaluation process\u2014 how they tested and improved the AI\u2019s performance\u2014 needed an upgrade.\nInitially, Notion\u2019s evaluation workflow was manual and intensive:\n- Large and diverse datasets were stored as\nJSONL\nfiles in a git repo, making them difficult to manage, version, and collaborate on. - Human evaluators scored individual responses, resulting in an expensive and time-consuming feedback loop.\nWhile this process helped Notion successfully launch Q&A in beta in late 2023, it was clear they needed a more scalable, efficient approach to take their AI products to the next level. That\u2019s where Braintrust came in.\nNotion\u2019s evals today\nPartnering with Braintrust transformed Notion\u2019s eval workflow, enabling faster iterations and ultimately higher quality features in production. Here\u2019s an overview of the new workflow:\n1. Decide on an improvement\nThe first step is to decide on an improvement. It could be adding a new feature, fixing an issue based on user feedback, or enhancing existing capabilities. In the newest Notion AI product, users can ask questions and chat about anything \u2014 from the work context that lives in their Notion pages, to unrelated ideas and concepts across a wide variety of topics, powered by LLMs.\n2. Curate targeted datasets\nInstead of manually creating JSONL\nfiles, Notion now leverages Braintrust to curate, version, and manage datasets seamlessly. They typically start with 10-20 examples. To curate their datasets, Notion uses examples from real-world usage that are automatically logged in Braintrust, and writes examples by hand. To give you a sense of how important this step is, Notion has hundreds of eval datasets - and the number grows every week.\n3. Tie these datasets to specific scoring functions\nNotion\u2019s approach is to clearly, often narrowly, define the scope of each dataset and scoring function. For more complex tasks, Notion will use a variety of well-defined evals, each testing for specific criteria. Typically, this is a mix of heuristic and LLM-as-a-judge scorers, along with a healthy dose of human review. Because Braintrust allows users to define their own custom scoring functions, Notion can flexibly test anything \u2014 tool usage, factual accuracy, hallucinations, recall, and more.\n4. Run evals & inspect results\nAfter making an update, Notion leverages Braintrust to immediately understand how performance changed overall and drill down into specific improvements and regressions. The team follows this rough process:\n- Hone in on specific scorers & test cases to determine if the update led to the targeted improvement\n- Inspect all scores holistically to ensure there were no unintended regressions\n- Dig into failures and regressions (typically indicated by low scores) to understand where the application is still failing\n- Diff outputs from multiple experiments side-by-side\n- Optional: curate additional test examples to add to datasets\n5. Iterate quickly\nThe team continues the cycle of make an update -> run evals and inspect results\nuntil they\u2019re happy with the improvements and are ready to ship \u2014 on to the next!\nThe results\nThe team at Notion is now able to triage and fix 30 issues per day, compared to just 3 per day using the old workflow\u2014 allowing them to deliver even better products, like their new suite of incredible AI features. By redefining their workflow, Notion has set a new standard for how AI product teams can evaluate and improve their generative AI products.\nConclusion\nGenerative AI is redefining what\u2019s possible in software, and it\u2019s been inspiring to watch Notion leverage AI to push the boundaries of excellent user experience. With Braintrust, Notion has created an evaluation workflow that empowers its AI team to ship faster and build with confidence.\nIf you\u2019re an AI product team looking to scale your evals, take a page from Notion\u2019s playbook. With Braintrust, you can transform how you iterate on and test your AI products\u2014and build the future of software.\nLearn more about Notion and Braintrust.\nThank you to Simon for sharing these insights!", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Latest news"}, {"href": "https://www.braintrust.dev/blog/notion", "anchor": "How Notion develops world-class AI features"}, {"href": "https://www.braintrust.dev/blog/notion", "anchor": "Evaluating Notion Q&A"}, {"href": "https://www.braintrust.dev/blog/notion", "anchor": "Notion\u2019s evals today"}, {"href": "https://www.braintrust.dev/blog/notion", "anchor": "1. Decide on an improvement"}, {"href": "https://www.braintrust.dev/blog/notion", "anchor": "2. Curate targeted datasets"}, {"href": "https://www.braintrust.dev/blog/notion", "anchor": "3. Tie these datasets to specific scoring functions"}, {"href": "https://www.braintrust.dev/blog/notion", "anchor": "4. Run evals & inspect results"}, {"href": "https://www.braintrust.dev/blog/notion", "anchor": "5. Iterate quickly"}, {"href": "https://www.braintrust.dev/blog/notion", "anchor": "The results"}, {"href": "https://www.braintrust.dev/blog/notion", "anchor": "Conclusion"}, {"href": "https://www.braintrust.dev/contact", "anchor": "take a page from Notion\u2019s playbook"}, {"href": "https://www.braintrust.dev/", "anchor": "Braintrust"}, {"href": "https://www.braintrust.dev/blog/notion", "anchor": "How Notion develops world-class AI features"}, {"href": "https://www.braintrust.dev/blog/notion", "anchor": "Evaluating Notion Q&A"}, {"href": "https://www.braintrust.dev/blog/notion", "anchor": "Notion\u2019s evals today"}, {"href": "https://www.braintrust.dev/blog/notion", "anchor": "1. Decide on an improvement"}, {"href": "https://www.braintrust.dev/blog/notion", "anchor": "2. Curate targeted datasets"}, {"href": "https://www.braintrust.dev/blog/notion", "anchor": "3. Tie these datasets to specific scoring functions"}, {"href": "https://www.braintrust.dev/blog/notion", "anchor": "4. Run evals & inspect results"}, {"href": "https://www.braintrust.dev/blog/notion", "anchor": "5. Iterate quickly"}, {"href": "https://www.braintrust.dev/blog/notion", "anchor": "The results"}, {"href": "https://www.braintrust.dev/blog/notion", "anchor": "Conclusion"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 1}, "https://www.braintrust.dev/blog/functions": {"url": "https://www.braintrust.dev/blog/functions", "title": "Functions: flexible AI engineering primitives - Blog - Braintrust", "text": "Functions: flexible AI engineering primitives\nOver the past year, the field of software engineering has been inundated with AI. There are tons of tools for improving developer productivity with the help of large language models (LLMs), but very few make it straightforward to build and scale AI-powered products. That's because introducing AI models fundamentally changes the software development lifecycle. Instead of measuring performance and reliability, developers need to assess accuracy and quality, which are much harder to pin down. Work happens in two distinct environments when iterating and making improvements to a product that uses an LLM: interactive playgrounds designed for testing prompts and data, and code editors, where we integrate those prompts with programming logic, evaluations, and deployments. Neither environment feels purpose-built for developing AI-powered software.\nWe believe this development experience\u2014where you can fluidly compose, edit, and run prompts, data, and business logic\u2014represents the future of software engineering with LLMs. Today, we're excited to unveil the first major step towards this vision: functions.\nFunctions in Braintrust\nFunctions allow you to define atomic, reusable building blocks for executing AI-related logic. There are currently three types of functions in Braintrust:\n- Prompts, templated messages to send to an LLM\n- Tools, general purpose code that can be invoked by LLMs\n- Custom scorers, functions for scoring the quality of LLM outputs\nYou can create functions in your codebase or in the UI. You can push them from your codebase to the UI, and pull them back into your codebase as you edit them. You can also link them together; for example, a prompt can call a tool, and Braintrust will automatically feed its results back to the prompt and continue execution. Functions can also be invoked through the API, with built-in support for streaming and structured outputs.\nThis means you can:\n- Create a custom tool that browses the internet and invoke it through a prompt in the UI\n- Run custom TypeScript and Python scorers in the playground\n- Run scoring functions on a sample of your logs\nFunctions are the fastest way to prototype and deploy agentic applications. Braintrust takes care of the heavy lifting of securely and scalably executing, versioning, and logging each function independently, enabling you to iterate on your prompts and code much faster.\nLast, but not least, functions are open and embrace standards \u2014 this is not a framework. For example, prompts work with any OpenAI-compatible model (our proxy extends this to non-OpenAI models as well including Gemini, Anthropic, LLaMa, and others). Tools are TypeScript or Python functions with a well-defined input type, invoked through the standard tool calling protocol. And scorers fit the popular, open-source autoevals framework.\nPrompts are functions\nThe simplest type of function is a prompt. Any prompts you've created in Braintrust's prompt editor and playgrounds are already accessible as functions.\nFor example, a prompt like:\ncan now be invoked as a function that takes an argument called question\n:\nWhen you invoke prompts as functions, it unlocks even more capabilities. To stream results in an easy-to-parse format, set stream: true\n. Behind the scenes, Braintrust automatically caches and optimizes the prompt through our proxy and logs it to your project so you can debug completions later. This allows you to quickly change the model in the Braintrust UI and automatically deploy it to any environment that invokes it.\nCustom scoring functions\nYou can now create both prompt and code-based scorers from the Braintrust UI or your codebase, and seamlessly sync them between the UI and your app logic using the API. Once you create a custom function, you can use it in the playground and your code, and run it automatically on a sample of logs. For more information on scoring functions, check out the full blog post.\nFunction composition\nFunctions can also be composed using tool calls to produce sophisticated applications that would otherwise require lots of brittle orchestration logic. Any function can be used as a tool, which can be called, and its output added to the chat history. For example, a RAG agent can be defined as just two components:\n- A system prompt containing instructions for how to retrieve content and synthesize answers\n- A vector search tool, implemented in TypeScript or Python, which embeds a query, searches for relevant documents, and returns them\nTo learn more about building a RAG agent with functions, check out this cookbook.\nThe new software engineering lifecycle\nFunctions are not only the future of AI engineering, but they also have deep roots in the history of AI. At its core, a machine learning model itself is a black-box function, optimized through a loss function to minimize error and improve predictions. This concept has always been at the heart of AI \u2014 using functions to transform data into meaningful insights.\nIn Braintrust, we\u2019ve taken this foundational idea and expanded it, bringing together evals, logging, datasets, prompts, and the playground into one powerful workflow that redefines the software engineering lifecycle. Try building with functions for free today, and let us know what you think.", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Latest news"}, {"href": "https://www.braintrust.dev/blog/functions", "anchor": "Functions: flexible AI engineering primitives"}, {"href": "https://www.braintrust.dev/blog/functions", "anchor": "Functions in Braintrust"}, {"href": "https://www.braintrust.dev/docs/guides/prompts", "anchor": "Prompts"}, {"href": "https://www.braintrust.dev/docs/guides/prompts", "anchor": "Tools"}, {"href": "https://www.braintrust.dev/docs/guides/evals/write", "anchor": "Custom scorers"}, {"href": "https://www.braintrust.dev/docs/reference/api/Functions", "anchor": "API"}, {"href": "https://www.braintrust.dev/docs/reference/streaming", "anchor": "streaming"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "proxy"}, {"href": "https://www.braintrust.dev/blog/functions", "anchor": "Prompts are functions"}, {"href": "https://www.braintrust.dev/blog/functions", "anchor": "Custom scoring functions"}, {"href": "https://www.braintrust.dev/blog/custom-scorers", "anchor": "full blog post"}, {"href": "https://www.braintrust.dev/blog/functions", "anchor": "Function composition"}, {"href": "https://www.braintrust.dev/docs/cookbook/recipes/ToolRAG", "anchor": "cookbook"}, {"href": "https://www.braintrust.dev/blog/functions", "anchor": "The new software engineering lifecycle"}, {"href": "https://www.braintrust.dev/docs/cookbook/recipes/ToolRAG", "anchor": "Try building with functions"}, {"href": "https://www.braintrust.dev/contact", "anchor": "let us know what you think"}, {"href": "https://www.braintrust.dev/blog/functions", "anchor": "Functions: flexible AI engineering primitives"}, {"href": "https://www.braintrust.dev/blog/functions", "anchor": "Functions in Braintrust"}, {"href": "https://www.braintrust.dev/blog/functions", "anchor": "Prompts are functions"}, {"href": "https://www.braintrust.dev/blog/functions", "anchor": "Custom scoring functions"}, {"href": "https://www.braintrust.dev/blog/functions", "anchor": "Function composition"}, {"href": "https://www.braintrust.dev/blog/functions", "anchor": "The new software engineering lifecycle"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 1}, "https://www.braintrust.dev/blog/custom-scorers": {"url": "https://www.braintrust.dev/blog/custom-scorers", "title": "Custom scoring functions in the Braintrust Playground - Blog - Braintrust", "text": "Custom scoring functions in the Braintrust Playground\nTesting and iterating on prompts in a traditional IDE is difficult because AI evaluation is an inherently dynamic and collaborative process. As a result, developers and non-technical builders tend to spend a lot of time using tools like the Braintrust Playground to move faster, run more experiments, and ultimately build better AI products.\nToday, I\u2019m excited to announce that you can now create custom scorers and access them via the Braintrust UI and API. Scoring is the foundation of AI evaluation \u2013 a scoring function allows you to assess the output of an LLM, usually against an expected output, and assign a score of 0 to 100%. In the Braintrust Playground, we have long provided several scorers that work out of the box through our open-source autoevals library. But once you begin running evaluations, you\u2019ll often need custom scorers for your specific use cases to get a well-rounded view of your application\u2019s performance. Until today, that required jumping back to your codebase to run full evals.\nThe new custom scoring functionality allows you to run sophisticated, multi-model comparisons across multiple prompts and scoring functions, defined as LLM-as-a-judge, TypeScript, Python, or HTTP endpoints. After you create a custom scoring function, you can even use Braintrust for server-side online evaluations that run asynchronously.\nLet\u2019s dig into how to use this feature, and what new capabilities it unlocks.\nHow to create custom scorers\nScorers tend to be a combination of heuristics (best expressed as code) and LLM-as-a-judge (best expressed as a prompt).\nTo create a prompt-based scorer, define a prompt that classifies the output and a mapping from choices to scores. You can also specify whether or not to use chain-of-thought (CoT). This is the same technique used by the LLMClassifier\nclass in autoevals.\nTo create a code-based scorer, write a TypeScript or Python handler function that returns a score between 0 and 1.\nYou can also use any of the evaluators in autoevals as a starting point for your custom scorers.\nUploading custom scorers\nIn addition to creating custom scorers from the UI, you can also upload task and scorer functions to Braintrust from the command line, and use them in the Playground. To bundle and upload the functions to Braintrust, and update the bundled scorers in your project, run:\nnpx braintrust eval \u2014-push\nWhat's next\nCreating custom scorers unlocks several workflows in Braintrust. You can:\n- Use custom scorers in the playground as you iterate on your prompts, and kick off experiments\n- Access custom scorers through the API\n- Run server-side online evaluations asynchronously on specific logs\nOnce you compute scores on your logs, you can monitor performance over time, find anomalous cases, and add logs to a dataset to use in additional evaluations.\nIf you already have a Braintrust account, you can get started with custom scorers today. If you don\u2019t, sign up for free and give it a try. For more information, check out our docs or chat with us.", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Latest news"}, {"href": "https://www.braintrust.dev/blog/custom-scorers", "anchor": "Custom scoring functions in the Braintrust Playground"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Braintrust Playground"}, {"href": "https://www.braintrust.dev/blog/custom-scorers", "anchor": "How to create custom scorers"}, {"href": "https://www.braintrust.dev/blog/custom-scorers", "anchor": "Uploading custom scorers"}, {"href": "https://www.braintrust.dev/blog/custom-scorers", "anchor": "What's next"}, {"href": "https://www.braintrust.dev/signup", "anchor": "sign up for free"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "our docs"}, {"href": "https://www.braintrust.dev/contact", "anchor": "chat with us"}, {"href": "https://www.braintrust.dev/blog/custom-scorers", "anchor": "Custom scoring functions in the Braintrust Playground"}, {"href": "https://www.braintrust.dev/blog/custom-scorers", "anchor": "How to create custom scorers"}, {"href": "https://www.braintrust.dev/blog/custom-scorers", "anchor": "Uploading custom scorers"}, {"href": "https://www.braintrust.dev/blog/custom-scorers", "anchor": "What's next"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 1}, "https://www.braintrust.dev/blog/soc2": {"url": "https://www.braintrust.dev/blog/soc2", "title": "Braintrust achieves SOC 2 Type II compliance - Blog - Braintrust", "text": "Latest news\nBraintrust achieves SOC 2 Type II compliance\n15 July 2024Ankur Goyal\nWe are excited to announce that Braintrust has achieved SOC 2 Type II compliance!\nBraintrust is the end-to-end developer platform for building AI products. We recognized from the start how critical data privacy and security are to AI teams, and have always treated governance, risk, and compliance as first class citizens. That\u2019s why Braintrust was architected from the ground up to support multiple deployment models for security-conscious enterprises. SOC 2 compliance is another important milestone on our journey.\nIf you want to learn more, check out our docs or chat with us.", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Latest news"}, {"href": "https://www.braintrust.dev/blog/soc2", "anchor": "Braintrust achieves SOC 2 Type II compliance"}, {"href": "https://www.braintrust.dev/docs/reference/platform/architecture", "anchor": "architected"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "our docs"}, {"href": "https://www.braintrust.dev/contact", "anchor": "chat with us"}, {"href": "https://www.braintrust.dev/blog/soc2", "anchor": "Braintrust achieves SOC 2 Type II compliance"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 1}, "https://www.braintrust.dev/blog/improve-evals": {"url": "https://www.braintrust.dev/blog/improve-evals", "title": "How to improve your evaluations - Blog - Braintrust", "text": "How to improve your evaluations\nThis post will cover what evaluations are, what you should work towards, and some actionable approaches to improving your evals.\nWhat are evaluations?\nTo effectively build production-grade AI products, you need great evaluations. Evals measure the performance of your AI application and help establish an effective feedback loop for you and your team. Evals consist of 3 parts:\n- Data: a test set of examples\n- Task: the AI function you want to test\n- Scores: a set of scoring functions that take an\ninput\n,output\n, and optionalexpected\nvalue and compute a score\nWith these 3 pieces, you can run your task on your test data and compute scores for each example. These scores measure how well your application performed and can be aggregated or inspected individually.\nIf you are new to evals, check out our overview of evals and our guide to getting started with automated evals.\nWhat you should work towards\nWhile evals are quick to get started with, they take time and iteration to get right. In particular, there are two key goals to work toward:\n- Taking your understanding of what a good response looks like and codifying it into a set of scoring functions\n- Gathering a set of test examples that is 1. broad and 2. representative of real-world usage\nWorking towards these goals is time well-spent, as you will also be crystallizing your own understanding of the application you\u2019re building.\nHow to improve your evals\nHere are 3 great approaches you can leverage to improve your evals:\n- Identify new and useful evaluators\n- Improve your existing scorers\n- Add new test cases to your dataset\nIdentify new and useful evaluators\nFor you (or any human) to determine whether a subjective LLM response is good or bad, you will intuitively evaluate it on multiple, often unrelated axes. This same principle should apply to your evaluation functions. While you can sometimes rely on a single scorer, it\u2019s almost always worthwhile to leverage multiple. This is especially true for more complex or unstructured use cases.\nFor example, many of our customers use factuality to identify hallucinations successfully. However, hallucinations are only part of the puzzle (i.e. no hallucinations != great response). Your outputs might still be too wordy, or unhelpful, or include the word \u201csorry\u201d (which users hate). In these cases, you can improve your evals by adding additional scorers to measure these attributes; for example, a scorer that penalizes long responses.\nImprove your existing scorers\nTo improve your existing scoring functions, dig into specific examples and inspect the inputs, outputs, and scores. As you go through, you should think through whether the scores are accurate and how they could be more informative. You can then take action with one of these two approaches:\n- Add more company- or use-case-specific context - you have a uniquely deep level of context on your users, the product you\u2019re building, and what good/bad looks like. The more of this context you can codify into your scoring functions, the more useful they will be\n- Be more precise about what you\u2019re testing for - great scoring functions provide high-fidelity signal on a clearly defined axis. For each of your scoring functions, think through what exactly you want to test for, and then update your scorer to test for that as precisely as possible. It\u2019s almost always better to have multiple tightly defined scoring functions vs. a single vaguely defined one, so don\u2019t be afraid to split scoring functions up\nFor example, closedQA is a very open-ended scorer. If you set the criteria to something vague like \u201cOutput should be helpful\u201d, you may find yourself frequently disagreeing with how helpfulness is being scored. In these situations, you will have a lot more success if you:\n- Define what helpful means more clearly\n- Narrow the scope of your scorer, e.g. specifically test whether outputs respond to all parts of their respective inputs\nclosedQA scoring function:\nAdd new test cases to your dataset\nBecause the input/output space of AI applications is so wide, adding more test cases will almost always increase the coverage of your evals and provide additional useful signal. As a result, great AI teams are constantly adding to and updating their set of test cases.\nFor example, you may find that your application is performing well on your existing test set. However, when you ship your product to internal users, you see that your application is failing in a specific way; e.g. it fails to use the calculator tool when it should have. This is an important failure, but only 1 of your current test cases requires calculator use. In this situation, you should improve your test case coverage by sourcing 5-10 targeted examples that require calculator use and adding them to your datasets going forward. You might then add an evaluator to test tool usage (or even specifically to test calculator usage).\nThis pattern is especially powerful once you start logging real user interactions. Learn more about how to establish a logging -> evals feedback loop here!\nConclusion\nIf you can\u2019t trust your evals (or don\u2019t have them at all), it often feels like you are developing in the dark. Great evals solve this problem by helping you understand the impact of the changes you make. As a result, spending time iterating on your evals is a critical step towards building great AI products.\nWe hope these methods are helpful as you work towards improving your evals. Happy eval\u2019ing \ud83d\ude42.\nCheck out our docs or get in touch to learn more about Braintrust.", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Latest news"}, {"href": "https://www.braintrust.dev/blog/improve-evals", "anchor": "How to improve your evaluations"}, {"href": "https://www.braintrust.dev/blog/improve-evals", "anchor": "What are evaluations?"}, {"href": "https://www.braintrust.dev/docs/guides/evals", "anchor": "overview of evals"}, {"href": "https://www.braintrust.dev/blog/getting-started-evals", "anchor": "guide to getting started with automated evals"}, {"href": "https://www.braintrust.dev/blog/improve-evals", "anchor": "What you should work towards"}, {"href": "https://www.braintrust.dev/blog/improve-evals", "anchor": "How to improve your evals"}, {"href": "https://www.braintrust.dev/blog/improve-evals", "anchor": "Identify new and useful evaluators"}, {"href": "https://www.braintrust.dev/blog/improve-evals", "anchor": "Improve your existing scorers"}, {"href": "https://www.braintrust.dev/blog/improve-evals", "anchor": "Add new test cases to your dataset"}, {"href": "https://www.braintrust.dev/docs/guides/logging", "anchor": "logging"}, {"href": "https://www.braintrust.dev/blog/eval-feedback-loops", "anchor": "here"}, {"href": "https://www.braintrust.dev/blog/improve-evals", "anchor": "Conclusion"}, {"href": "https://www.braintrust.dev/docs/guides/evals", "anchor": "docs"}, {"href": "https://www.braintrust.dev/contact", "anchor": "get in touch"}, {"href": "https://www.braintrust.dev/blog/improve-evals", "anchor": "How to improve your evaluations"}, {"href": "https://www.braintrust.dev/blog/improve-evals", "anchor": "What are evaluations?"}, {"href": "https://www.braintrust.dev/blog/improve-evals", "anchor": "What you should work towards"}, {"href": "https://www.braintrust.dev/blog/improve-evals", "anchor": "How to improve your evals"}, {"href": "https://www.braintrust.dev/blog/improve-evals", "anchor": "Identify new and useful evaluators"}, {"href": "https://www.braintrust.dev/blog/improve-evals", "anchor": "Improve your existing scorers"}, {"href": "https://www.braintrust.dev/blog/improve-evals", "anchor": "Add new test cases to your dataset"}, {"href": "https://www.braintrust.dev/blog/improve-evals", "anchor": "Conclusion"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 1}, "https://www.braintrust.dev/blog/zapier-ai": {"url": "https://www.braintrust.dev/blog/zapier-ai", "title": "How Zapier builds production-ready AI products - Blog - Braintrust", "text": "How Zapier builds production-ready AI products\nZapier is the world\u2019s leading workflow automation platform, directly connecting to over 6,000 apps and supporting developers and non-developers at over 2 million companies.\nZapier was also one of the first companies to embrace GenAI:\n- Shipped AI by Zapier with support for 4,000+ app integrations in early 2022\n- Mike and Bryan gave up their exec team roles to be ICs and go all-in on AI summer 2022\n- Launched OpenAI on Zapier in December of 2022, less than 2 weeks after ChatGPT was released\n- One of 11 launch partners for ChatGPT plugins\n- Rolled out a suite of AI features in May 2023, including text-to-zap and semantic search\n- Shipped custom AI chatbots in June 2023\nSince then, the Zapier team has continued to ship amazing features like Zapier Central (AI bots), AI Actions, and many more internal and external use cases.\nZapier's AI development process\nAt Braintrust, we\u2019ve had the opportunity to support Zapier since August of 2023. In that time, we've learned a lot by watching their AI development process evolve.\nIn this post, we\u2019re excited to share some of Mike\u2019s insights on how Zapier goes from an initial idea to a production-ready AI product:\n- Prototype & build v0\n- Ship v1\n- Collect user feedback\n- Establish a set of evals\n- Iterate to improve product quality\n- Take a moment to be proud\n- Optimize\n1. Prototype & build v0\nStep 1 is all about quickly validating whether your AI feature idea is feasible with existing models. To do this effectively, you should very quickly cycle through different prompts and examples to get a sense for what works and what doesn\u2019t. In this initial phase, Zapier only uses the smartest (and therefore the most expensive/slow) models - GPT-4 Turbo and Claude Opus.\nBraintrust\u2019s playground is a great place to do this. Here's an example testing out newly released GPT-4o:\n2. Ship v1\nAfter validating your idea, you should build and ship v1. Sub 50% accuracy is okay! In this stage of the development process, \u201cvibes\u201d (i.e. having your team sanity check outputs one by one) is sufficient to make progress.\nThe goal of this step is to rapidly improve and then get your feature into the hands of users as quickly as possible. Having real users try your product is the best way to understand usage patterns and collect a diverse set of inputs, both of which are foundational to making future improvements.\nWe find that many teams get stuck here. To mitigate the risks of shipping, you can:\n- Label the feature as \u201cbeta\u201d\n- Ship to internal users\n- Ship to a very small subsegment of external users\n- Make the feature opt-in\n- Keep humans in the loop (e.g. suggestions)\n3. Collect user feedback\nAfter shipping v1, you should obsessively collect every piece of feedback you can. This includes both explicit feedback (e.g. thumbs up/down or stars) and implicit feedback (e.g. errors, whether the user accepted a change or asked a follow-up). Zapier generally weights explicit feedback more.\nYou will now have a growing collection of examples with user feedback. Let negative feedback guide you towards areas to improve, and tinker with your prompts/code to attempt to fix those issues.\nHowever, as you go to commit your new update, you will likely realize that while you may have fixed one set of issues, it\u2019s hard to know how your update will impact performance overall. From here, you have 3 options:\n- Ship the change, hope for the best, and see what customers think\n- Do a manual vibe-checking exercise every time you want to make an update\n- Establish a set of evals\n4. Establish a set of evals\nA well-defined set of evaluations accurately scores your application on a broad set of examples, similar to a natural-language test suite. Evals are particularly effective when your test set is both diverse and reflects real customer usage patterns.\nTo bootstrap a great test set, Zapier leverages customer examples from step 3. Both positive and negative feedback are helpful:\n- Positive customer feedback: use the inputs/outputs as examples of what good looks like\n- Negative customer feedback: correct the outputs and then include them as test cases. This is worth the effort as these examples represent areas where your product struggles\nOver time, you can start to construct golden datasets out of these examples to benchmark performance and prevent regressions.\nBraintrust is purpose-built for this workflow. The Zapier team uses Braintrust to log user interactions, dig into their logs, track customer feedback, filter on that customer feedback, and directly add interesting logs to their test sets. Braintrust\u2019s datasets feature also abstracts away the pain of managing and versioning test sets.\nAfter you construct a test set and run an evaluation, Braintrust also helps you understand high-level performance, dig into specific examples where your model performs poorly/well, and filter by which examples got better or worse. This gives you and your team the signal to confidently decide whether or not to ship an update into production!\n5. Iterate to improve product quality\nNow that you've established a feedback loop, you should continue iterating to improve product quality. The ability to immediately test whether an update moved you in the right direction gives you the freedom and confidence to absorb customer feedback, make a change, test it, and ship your change. This enables you to rapidly improve performance.\nWith this feedback loop in place, Zapier has improved many of their AI products from sub-50% accuracy to 90%+ within 2-3 months.\nAs accuracy increases, you can start expanding your product\u2019s availability (e.g. shipping to more users) and capabilities (e.g. allowing it to take more actions independently).\n6. Take a moment to be proud\nYou\u2019ve shipped an amazing AI product if:\n- Quality is sufficiently high\n- The product is available to a good portion (or all) of your user base\n- Product usage is high\nCongrats!\n7. Optimize\nAt this point, the Zapier team will begin thinking through how to optimize cost and latency. By the time you reach this step, you should have a robust set of evals to test your AI product on, making it straightforward to benchmark how swapping in cheaper/smaller models impacts your product\u2019s accuracy.\nNote: this step can come earlier if your product is prohibitively expensive to ship with frontier models or requires super low latency.\nConclusion\nLLMs are incredibly powerful, and we love seeing companies release great AI features. It\u2019s been a lot of fun working with Mike and the Zapier team as they continue to push the limits of leveraging LLMs to amaze their customers.\nWe hope this guide will serve as inspiration for AI teams struggling to overcome the initial inertia of shipping AI features. We can\u2019t wait to see what you will ship :).\nLearn more about Zapier and Braintrust.", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Latest news"}, {"href": "https://www.braintrust.dev/blog/zapier-ai", "anchor": "How Zapier builds production-ready AI products"}, {"href": "https://www.braintrust.dev/blog/zapier-ai", "anchor": "Zapier's AI development process"}, {"href": "https://www.braintrust.dev/blog/zapier-ai", "anchor": "1. Prototype & build v0"}, {"href": "https://www.braintrust.dev/blog/zapier-ai", "anchor": "2. Ship v1"}, {"href": "https://www.braintrust.dev/blog/zapier-ai", "anchor": "3. Collect user feedback"}, {"href": "https://www.braintrust.dev/blog/zapier-ai", "anchor": "4. Establish a set of evals"}, {"href": "https://www.braintrust.dev/blog/zapier-ai", "anchor": "5. Iterate to improve product quality"}, {"href": "https://www.braintrust.dev/blog/zapier-ai", "anchor": "6. Take a moment to be proud"}, {"href": "https://www.braintrust.dev/blog/zapier-ai", "anchor": "7. Optimize"}, {"href": "https://www.braintrust.dev/blog/zapier-ai", "anchor": "Conclusion"}, {"href": "https://www.braintrust.dev/", "anchor": "Braintrust"}, {"href": "https://www.braintrust.dev/blog/zapier-ai", "anchor": "How Zapier builds production-ready AI products"}, {"href": "https://www.braintrust.dev/blog/zapier-ai", "anchor": "Zapier's AI development process"}, {"href": "https://www.braintrust.dev/blog/zapier-ai", "anchor": "1. Prototype & build v0"}, {"href": "https://www.braintrust.dev/blog/zapier-ai", "anchor": "2. Ship v1"}, {"href": "https://www.braintrust.dev/blog/zapier-ai", "anchor": "3. Collect user feedback"}, {"href": "https://www.braintrust.dev/blog/zapier-ai", "anchor": "4. Establish a set of evals"}, {"href": "https://www.braintrust.dev/blog/zapier-ai", "anchor": "5. Iterate to improve product quality"}, {"href": "https://www.braintrust.dev/blog/zapier-ai", "anchor": "6. Take a moment to be proud"}, {"href": "https://www.braintrust.dev/blog/zapier-ai", "anchor": "7. Optimize"}, {"href": "https://www.braintrust.dev/blog/zapier-ai", "anchor": "Conclusion"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 1}, "https://www.braintrust.dev/blog/ai-development-loops": {"url": "https://www.braintrust.dev/blog/ai-development-loops", "title": "AI development loops - Blog - Braintrust", "text": "AI development loops\nWhether you are hot-gluing pasta art or developing world-class software, the archetypal development loop is:\n- Try something\n- See what happens\n- Repeat\nTo the extent that this loop is fast and provides clear signal, it frees you to try things and incrementally improve them.\nWith AI development, speed and signal are complicated by a number of factors:\n- Response times are slow\n- Inputs are often unstructured and generated by humans. As a result, there are an unbounded number of possible inputs with minute variations. So it\u2019s hard to select a realistic scenario (or set of scenarios) to \u201csee what happens\u201d\n- Outputs are unstructured and non-deterministic, making it difficult to assess the result\nTo enable a useful development loop in these circumstances, it is necessary to add a couple of supporting activities that address these complications. These form the three core loops of AI development:\n- Exploration \u2013 A quicker feedback loop for trying things. (This provides speed.)\n- Evaluation \u2013 A testing approach that automatically applies a set of test inputs and assesses the results with a set of scorers. (This provides signal.)\n- Data Collection \u2013 A process for finding interesting data from production to use as test inputs. (This ensures the signal is reliable.)\nBraintrust provides tools that support all three of these loops as well as the handoffs between them. This tightens the feedback loop and makes the testing and results easier to work with.\nLoop 1: Exploration\nThe goal of this loop is to play around\u2014to quickly try out changes in a low-stakes environment and get a sense of whether a direction seems good before investing in it further.\nStrategically, you try a change, do a simple test, and manually assess the result. If things look promising, you apply the change to a branch of your codebase and run an evaluation.\nSteps\n- Develop/make a change (For example, tweak the prompt)\n- Manually test on one or more inputs and see instant results\n- Do something with them. (For example, save and trigger a new evaluation)\nBraintrust supports this through our Playgrounds, which are built for rapid iteration. You can test prompts, tweak them, try out different models, and even access test inputs from your datasets.\nLoop 2: Evaluation\nThe goal of this loop is to get high certainty about the impact of a change. It allows you to ensure that a change really results in the desired improvement and that nothing will break when you merge it.\nStrategically, you make a change, test it on a suite of test cases and scorers, and assess the results to determine next steps.\nSteps\n- Make a change in:\n- Prompts, model params, and app code,\n- Fine tuning,\n- Scorers, or\n- Test cases\n- Evaluate. (Run each test case and apply the scorers to the results.)\n- View the results to:\n- Determine whether the change had the intended effect\n- Find a pattern to investigate\n- Analyze outputs and traces in detail, and decide the next action\n- Repeat\nThis is the main experience in Braintrust. It supports robust, methodical testing with high convenience. Braintrust makes it accessible for any software engineer to define and run evals on golden datasets and to do it every time you make a change or send a PR. We also allow you to view the results in detail, including an entire trace, with comparisons to past results.\nLoop 3: Data Collection\nThe goal of this loop is to build high-quality datasets for testing. Strategically, identify patterns and interesting results in your production logs, capture the inputs, and write good expected outputs. You then use these input+expected tuples to generate signal in the other two loops.\nSteps\n- Search for patterns or problems in production logs\n- Analyze the inputs, outputs, and traces in detail and select inputs to add as test cases\n- Repeat\nBraintrust helps you do this in a bunch of ways. The first is our logs. They provide robust filtering and visualizations to help you find noteworthy production user interactions. When you\u2019ve found an interesting interaction, our logs give you access to full details of it, including input, output, trace details with all spans in context, and scores. The second is our datasets. They hold your test cases, allowing you to store interesting real-world examples and to access them to use in evals and exploration.\nConclusion\nThe ability to enact AI development is gated by the ability to flow through each of these loops and to connect them to each other. This can be hard to do well, but Braintrust can help. One thing Braintrust can particularly help with is the flow between the three loops. Because evals, logs, and datasets have the same data structure, Braintrust makes it really straightforward to move data from a log row to a dataset row to an eval row, ensuring your loops can flow into each other.", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Latest news"}, {"href": "https://www.braintrust.dev/blog/ai-development-loops", "anchor": "AI development loops"}, {"href": "https://www.braintrust.dev/blog/ai-development-loops", "anchor": "Loop 1: Exploration"}, {"href": "https://www.braintrust.dev/blog/ai-development-loops", "anchor": "Loop 2: Evaluation"}, {"href": "https://www.braintrust.dev/blog/ai-development-loops", "anchor": "Loop 3: Data Collection"}, {"href": "https://www.braintrust.dev/blog/ai-development-loops", "anchor": "Conclusion"}, {"href": "https://www.braintrust.dev/blog/ai-development-loops", "anchor": "AI development loops"}, {"href": "https://www.braintrust.dev/blog/ai-development-loops", "anchor": "Loop 1: Exploration"}, {"href": "https://www.braintrust.dev/blog/ai-development-loops", "anchor": "Loop 2: Evaluation"}, {"href": "https://www.braintrust.dev/blog/ai-development-loops", "anchor": "Loop 3: Data Collection"}, {"href": "https://www.braintrust.dev/blog/ai-development-loops", "anchor": "Conclusion"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 1}, "https://www.braintrust.dev/blog/getting-started-evals": {"url": "https://www.braintrust.dev/blog/getting-started-evals", "title": "Getting started with automated evaluations - Blog - Braintrust", "text": "Getting started with automated evaluations\nAt Braintrust, when we chat with engineers building AI applications, one of the most common questions we hear is \u201cHow do we get started with automated evaluations?\u201d\nIn this post, we will discuss the state of evals today and lay out some high-leverage ways to quickly get started with automated evaluations.\nThe state of evals today\nPrior to Braintrust, we see AI teams leverage a few common approaches to evals:\n- Vibes-based: engineers and PMs remember some interesting test cases and eyeball the results\n- Benchmarks and/or black box: MMLU for general tasks, HellaSwag for common sense, TruthfulQA for truthfulness, HumanEval for code generation, many more\n- Stitched together manual review: a combination of examples saved in spreadsheets, a script to run through test cases, and humans (engineers/PMs/SMEs) manually checking examples\nWhile the above approaches are all helpful, we find that all three fall short in important ways. Vibes and manual review do not scale, and general benchmarks are not sufficiently application-specific and are hard to customize. This means engineering teams struggle to understand product performance, resulting in a very slow dev loop and frustrating behavior like:\n- Making updates without having a good sense of how they impact end users\n- Playing whack-a-mole to identify regressions\n- Manually scoring responses one by one\n- Manually tracking experiments and examples over time (or worse, not tracking them)\nAutomated evaluations\nAutomated evaluations are straightforward to set up and can make an immediate impact on AI development speed. In this section, we will walk through 3 great approaches: LLM evaluators, heuristics, and comparative evals.\nLLM evaluators\nLLMs are incredibly useful for evaluating responses out-of-the-box, even with minimal prompting. Anything you can ask a human to evaluate, you can (at least partially) encode into an LLM evaluator. Here are some examples:\n- Comparing a generated output vs. an expected output - instead of having an engineer scroll through an Excel spreadsheet and manually compare generated responses vs expected responses, you can use a factuality prompt to compare the two. Many of our customers use this type of test to detect and prevent hallucinations\n- Checking whether an output fully addresses a question - if you provide a task and a response, LLMs do a great job of scoring whether the response is relevant and addresses all parts of the task\nThe above two methods are great places to start, and we\u2019ve seen customers successfully configure LLMs to score many other subjective characteristics - conciseness, tone, helpfulness, writing quality, and many more.\nHeuristics\nHeuristics are a valuable objective way to score responses. We\u2019ve found that the best heuristics fall into one of two buckets:\n- Functional - ensuring the output fulfills a specific functional criteria\n- Examples: testing if an output is valid markdown, if generated code is executable, if the model selected a valid option from a list, Levenshtein distance\n- Subjective - using objective heuristics as a proxy for subjective factors\n- Examples: checking if an output exceeds a certain number of words (conciseness), checking if an output contains the word \u201csorry\u201d (usefulness/tone)\nImportantly - to make heuristic scoring as valuable as possible, engineering teams should be able to see updated scores after every change, quickly drill down into interesting examples, and add/tweak heuristics.\nComparative evals\nComparative evals compare an updated set of responses vs. a previous iteration. This is particularly helpful in understanding whether your application is improving as you make changes. Comparative evals also do not require expected responses, so they can be a great option for very subjective tasks. Here are a few examples:\n- Testing whether summarization is improving (example)\n- Comparing cost, token usage, duration (especially when switching between models)\n- Starting with a standard template like battle and tweaking the questions and scores over time to be use-case specific\nBraintrust natively supports hill climbing, which allows you to iteratively compare new outputs to previous ones.\nContinuous iteration\nWhile there is no replacement for human review, setting up basic structure around automated evals unlocks the ability for developers to start iterating quickly. The ideal AI dev loop enables teams to immediately understand performance, track experiments over time, identify and drill down into interesting examples, and codify what \u201cgood\u201d looks like. This also makes human review time much higher leverage as you can point reviewers to useful examples and continuously utilize their scores.\nGetting this foundation in place does not require a big time investment up front. A single scoring function with 10-30 examples is enough to enable teams to start iterating. We\u2019ve seen teams start from that foundation and very quickly scale into making 50+ updates per day across their AI applications, evaluation methods, and test data.\nAt Braintrust, we obsess over making the AI development process as smooth and iterative as possible. Setting up evaluations in Braintrust takes less than 1 hour and makes a huge difference. If you want to learn more, sign up, check out our docs or get in touch!", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Latest news"}, {"href": "https://www.braintrust.dev/blog/getting-started-evals", "anchor": "Getting started with automated evaluations"}, {"href": "https://www.braintrust.dev/blog/getting-started-evals", "anchor": "The state of evals today"}, {"href": "https://www.braintrust.dev/blog/getting-started-evals", "anchor": "Automated evaluations"}, {"href": "https://www.braintrust.dev/blog/getting-started-evals", "anchor": "LLM evaluators"}, {"href": "https://www.braintrust.dev/blog/getting-started-evals", "anchor": "Heuristics"}, {"href": "https://www.braintrust.dev/blog/getting-started-evals", "anchor": "Comparative evals"}, {"href": "https://www.braintrust.dev/blog/getting-started-evals", "anchor": "Continuous iteration"}, {"href": "https://www.braintrust.dev/docs", "anchor": "check out our docs"}, {"href": "https://www.braintrust.dev/contact", "anchor": "get in touch"}, {"href": "https://www.braintrust.dev/blog/getting-started-evals", "anchor": "Getting started with automated evaluations"}, {"href": "https://www.braintrust.dev/blog/getting-started-evals", "anchor": "The state of evals today"}, {"href": "https://www.braintrust.dev/blog/getting-started-evals", "anchor": "Automated evaluations"}, {"href": "https://www.braintrust.dev/blog/getting-started-evals", "anchor": "LLM evaluators"}, {"href": "https://www.braintrust.dev/blog/getting-started-evals", "anchor": "Heuristics"}, {"href": "https://www.braintrust.dev/blog/getting-started-evals", "anchor": "Comparative evals"}, {"href": "https://www.braintrust.dev/blog/getting-started-evals", "anchor": "Continuous iteration"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 1}, "https://www.braintrust.dev/blog/eval-feedback-loops": {"url": "https://www.braintrust.dev/blog/eval-feedback-loops", "title": "Eval feedback loops - Blog - Braintrust", "text": "Eval feedback loops\nIn AI engineering, it can be tough to understand how a change will impact end users. The solution is to test AI products on a set of real-world examples, aka \u201cevals\u201d.\nOnce you establish an initial set of evals, however, a few problems typically emerge:\n- It\u2019s hard to find great eval data\n- It\u2019s hard to identify interesting cases / failures in production\n- It\u2019s hard to tie user feedback to your evals\nThe solution to these problems is to connect your real-world log data to your evals, so that as you encounter new and interesting cases in the wild, you can eval them, improve, and avoid regressing in the future. In this post, we\u2019ll work through how to set this up in the ideal way.\nSpecifically, we\u2019ll cover:\n- How to structure your evals and when to run them\n- How to flow production logs into your eval data\n- How Braintrust makes this seamless\nStructuring your evals\nWell-implemented evals serve as the foundation of the AI engineering dev loop. If you can run them \"offline\" from your production application, evals free you to experiment with changes to code, prompts, data, etc. without affecting your users.\nFundamentally, an eval is a function of some (a) data, (b) prompts/code (we\u2019ll call this a task), and (c) scoring functions. Updating the (b) task impacts product behavior, while updating the (a) data or (c) scorers improves the fidelity of the evaluation. Consequently, each time any of these change, you should run a new eval and assess how your performance has changed.\nBraintrust\u2019s Eval\nfunction makes this workflow very clear, by literally having 3 inputs:\nIn the next section of this post, we\u2019re going to focus in on the \u201cdata\u201d part. And in particular, how to build and continually improve your datasets, by capturing real-world data the right way.\nCapturing and utilizing logs\nThe key to evaluating with good data is to use real-world examples. When we\u2019re playing with AI products, we often discover good test cases while interacting with them. For example, someone may struggle to get your chatbot to produce a markdown table. When this happens, it\u2019s the perfect moment to capture the context into a dataset you evaluate on.\nBecause logs and evals function similarly, the data from logs can quickly be used to power your evals.\nInitial steps\nWe highly recommend tracing and logging your projects from inception. In the early days of developing a new product or feature, each time someone encounters a bug, you can immediately find the trace corresponding to their interaction in the UI. You can also scan through most or all traces to look for patterns. This is a benefit of small scale \ud83d\ude42.\nBraintrust makes this seamless. To log user interactions, you instrument your application\u2019s code to trace relevant bits of context. Traces appear on the logs page, where you can view them in detail.\nAs you find interesting examples in your logs, you can quickly add them to a dataset. You can also use tags to organize different kinds of issues, e.g. toxic-input\n, but you can also place different categories of logs into separate datasets.\nOnce you set up a dataset, you can run evals on it by referencing it as the data\nparameter in the Eval\nfunction:\nAs you add new cases to your dataset, your Eval\nwill automatically test them. You can also pin an Eval\nto a particular version of a dataset, and then explicitly change the version when you\u2019re ready to.\nScaling\nAs you scale, it becomes critical to filter the logs to only consider the interesting ones. There are three ways to accomplish this:\n- Use filters, e.g. on\nmetadata\nortags\n, to help you track down interesting cases. For example, you might know that your evals are missing a certain type of request, filter down to them, scan a few, and add 1-2 to your eval dataset. - Track user feedback, and review rated examples. For example, you may routinely capture \ud83d\udc4d\ud83c\udffd cases as positive cases not to regress, or carefully review recent \ud83d\udc4e\ud83c\udffd cases, and add them to a dataset for further improvement.\n- Run online scores, ideally the same scores you do offline evals on, and find low scoring examples. This is a fantastic and very direct way to uncover test cases that you know need improvement by your own metrics.\nPutting this to practice\nImplementing a good eval feedback loop is incredibly rewarding, but can get unwieldy, even at relatively small scale. Many teams we meet start with json data in their git repo or source code. But they quickly find that it\u2019s a lot of manual effort to keep these files up-to-date with real-world examples, collaborate on them, and visualize the data.\nBraintrust is specifically built and designed for this purpose:\n- You can instrument your code once and reuse it across both logging and evaluation. For example, if you implement a score while evaluating (e.g. ensuring the length of an output is at most 2 paragraphs), you can automatically compute the score in production, discover examples where it\u2019s low, and add them to a dataset.\n- The shared code across logging and evals also naturally unifies the UI. This is one of the most powerful ideas in Braintrust \u2014 you can use exactly the same code and UI to explore your logs and your evals. The trace is an incredibly powerful data structure that contains information about every LLM call plus additional data you log. You can even reproduce the LLM calls your users saw, tweak the prompts, and test them directly in the UI.\n- All of your datasets, logs, evals, and user data can be stored in your cloud environment. We know how sensitive and valuable your AI data is, so we\u2019ve supported self-hosting from day one.\nIf you think Braintrust can help, then feel free to sign up or get in touch. We\u2019re also hiring!", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Latest news"}, {"href": "https://www.braintrust.dev/blog/eval-feedback-loops", "anchor": "Eval feedback loops"}, {"href": "https://www.braintrust.dev/blog/eval-feedback-loops", "anchor": "Structuring your evals"}, {"href": "https://www.braintrust.dev/blog/eval-feedback-loops", "anchor": "Capturing and utilizing logs"}, {"href": "https://www.braintrust.dev/blog/eval-feedback-loops", "anchor": "Initial steps"}, {"href": "https://www.braintrust.dev/blog/eval-feedback-loops", "anchor": "Scaling"}, {"href": "https://www.braintrust.dev/blog/eval-feedback-loops", "anchor": "Putting this to practice"}, {"href": "https://www.braintrust.dev/blog/eval-feedback-loops", "anchor": "Eval feedback loops"}, {"href": "https://www.braintrust.dev/blog/eval-feedback-loops", "anchor": "Structuring your evals"}, {"href": "https://www.braintrust.dev/blog/eval-feedback-loops", "anchor": "Capturing and utilizing logs"}, {"href": "https://www.braintrust.dev/blog/eval-feedback-loops", "anchor": "Initial steps"}, {"href": "https://www.braintrust.dev/blog/eval-feedback-loops", "anchor": "Scaling"}, {"href": "https://www.braintrust.dev/blog/eval-feedback-loops", "anchor": "Putting this to practice"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 1}, "https://www.braintrust.dev/blog/wing-30": {"url": "https://www.braintrust.dev/blog/wing-30", "title": "Braintrust selected to be in the Enterprise Tech 30 - Blog - Braintrust", "text": "Latest news\nBraintrust selected to be in the Enterprise Tech 30\n9 April 2024Ankur Goyal\nWe\u2019re excited to announce that Braintrust was selected to be in the Enterprise Tech 30!\nThe Enterprise Tech 30 by Wing Venture Capital names the private companies in enterprise technology with the most potential to meaningfully shift how tech enterprises operate for the better.\nAt Braintrust, we are lucky to support a lot of incredible AI teams at companies like Zapier, Coda, Notion, Instacart, Airtable, Brex, Retool, Replit, and more. If shipping AI features is a priority for you, sign up for free or get in touch. We\u2019re also hiring for a number of roles.\nCheck out the full Enterprise Tech 30 list here.", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Latest news"}, {"href": "https://www.braintrust.dev/blog/wing-30", "anchor": "Braintrust selected to be in the Enterprise Tech 30"}, {"href": "https://www.braintrust.dev/contact", "anchor": "get in touch"}, {"href": "https://www.braintrust.dev/careers", "anchor": "roles"}, {"href": "https://www.braintrust.dev/blog/wing-30", "anchor": "Braintrust selected to be in the Enterprise Tech 30"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 1}, "https://www.braintrust.dev/blog/hostinger-evals": {"url": "https://www.braintrust.dev/blog/hostinger-evals", "title": "How Hostinger evaluates AI applications with Braintrust - Blog - Braintrust", "text": "How Hostinger evaluates AI applications with Braintrust\nHostinger is a leading provider of web hosting solutions, serving over a million creators from 150+ countries. At Braintrust, we've had the opportunity to work closely with Hostinger as they work on exciting AI applications, like an AI Customer Support chatbot that now handles over 40% of support chat conversations.\nIn the video below, Liucija (Senior Data Scientist on the AI team @ Hostinger) explains how she approaches AI evaluations.\nHostinger's Approach to Evaluations (0:10)\nAs Liucija explains in the video, the Hostinger team has 3 main goals when they run LLM evaluations:\n- Evaluate how offline changes impact application behavior during development, including running evaluations as part of the CI/CD process\n- Continually assess the performance of live AI applications\n- Identify and prioritize emerging issues to further improve the application\nWhat Evaluations does Hostinger Run? (1:47)\nThe Hostinger team runs a variety of evaluations. Some examples include:\n- Component checks (URLs, correct doc)\n- Automated comparison vs. expected answers (factuality, truthfulness)\n- Automated evaluations without expected answers (sentiment analysis, safety)\n- Internal human review (thumbs up/down, comments)\n- Live user feedback (thumbs up/down, comments)\nHow Braintrust Supports Hostinger (2:40)\nAfter a quick setup process (Liucija got started by herself!), the Hostinger team now leverages Braintrust to run evaluations, track progress using a visual dashboard, drill down into specific improvements / regressions, manage / version datasets, and log production data. Braintrust saves the Hostinger team hours of manual evaluation a day, allowing Hostinger to work on 3x more AI features with high quality assurance.\nIf you are building AI applications and want to see how Braintrust can help, check out our docs or get in touch!", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Latest news"}, {"href": "https://www.braintrust.dev/blog/hostinger-evals", "anchor": "How Hostinger evaluates AI applications with Braintrust"}, {"href": "https://www.braintrust.dev/blog/hostinger-evals", "anchor": "Hostinger's Approach to Evaluations (0:10)"}, {"href": "https://www.braintrust.dev/blog/hostinger-evals", "anchor": "What Evaluations does Hostinger Run? (1:47)"}, {"href": "https://www.braintrust.dev/blog/hostinger-evals", "anchor": "How Braintrust Supports Hostinger (2:40)"}, {"href": "https://www.braintrust.dev/blog/hostinger-evals", "anchor": "How Hostinger evaluates AI applications with Braintrust"}, {"href": "https://www.braintrust.dev/blog/hostinger-evals", "anchor": "Hostinger's Approach to Evaluations (0:10)"}, {"href": "https://www.braintrust.dev/blog/hostinger-evals", "anchor": "What Evaluations does Hostinger Run? (1:47)"}, {"href": "https://www.braintrust.dev/blog/hostinger-evals", "anchor": "How Braintrust Supports Hostinger (2:40)"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 1}, "https://www.braintrust.dev/blog/2023-summary": {"url": "https://www.braintrust.dev/blog/2023-summary", "title": "2023, a year in review - Blog - Braintrust", "text": "Latest news\n2023, a year in review\n21 December 2023Ankur Goyal\n2023 was an exciting year for the Braintrust community. Many of our users started running evals for the first time ever, and saw massive improvements in the quality, performance, and iteration velocity of their AI products and features. In aggregate, Braintrust users evaluated\n- 100m+ tokens\n- 15m+ seconds (~6 months) of model compute time\n- 3m+ datapoints\nTo celebrate the end of the year, we're excited to share your 2023 year in review dashboard. If you are an existing Braintrust user, visit your dashboard to see your year in review.\nHere's mine. Cheers and happy new year!", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Latest news"}, {"href": "https://www.braintrust.dev/blog/2023-summary", "anchor": "2023, a year in review"}, {"href": "https://www.braintrust.dev/app/year-in-review", "anchor": "your dashboard"}, {"href": "https://www.braintrust.dev/blog/2023-summary", "anchor": "2023, a year in review"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 1}, "https://www.braintrust.dev/blog/seed-round": {"url": "https://www.braintrust.dev/blog/seed-round", "title": "Braintrust's seed round: $5m to build infrastructure for AI products - Blog - Braintrust", "text": "Braintrust's seed round: $5m to build infrastructure for AI products\nI'm excited to announce that Saam Motamedi from Greylock is leading our $5.1m seed round, bringing our total capital raised to $8.3m! Braintrust has become critical infrastructure that powers AI at leading technology companies like Zapier, Coda, Airtable, and Instacart. Since we announced our last round just 6 weeks ago, we've doubled both our team and customer base. This round builds on our momentum and allows us to accelerate our vision of building world-class infrastructure for AI products.\nOver the past year, building on top of the new AI paradigm has been a continuous rollercoaster of progress, innovation, occasional drama, and frankly the most fun I've had in my career. Despite how nascent the practice is, within a short time, many of the world's leading companies have started shipping products featuring generative AI. At the same time, if you talk to engineers building with AI, you'll consistently hear one painful challenge they all face: \u201cevals\u201d, the practice of systematically and predictably measuring how well an AI system performs.\nEvaluation has become an essential component of the development loop in AI software. AI problems are by their nature difficult to define, and it is usually impossible to produce an exact logical specification of their solution. The AI engineer's task is instead to measure correctness using context-specific data and metrics, and then rapidly experiment with various models, prompts, fine-tuning, and other techniques to achieve the desired results. In AI engineering, measurement and evaluation are of equal, if not greater, importance than the code used to achieve their ends.\nBy partnering with customers like Zapier, Coda, Airtable, and Instacart we've built Braintrust into a toolkit for instrumenting code and running evaluations, enabling teams to assess, log, refine, and enhance their AI-enabled products as a core part of their dev loop. I'm inspired every day by stories from users about how Braintrust has helped them improve the accuracy of their systems by 30%+ in just a few weeks of use, leading to faster ship cycles, increased product engagement, and better team collaboration. We can do all of this within the customer's cloud environment, so Braintrust is safe to use for even the most data-sensitive tasks.\nI've spent my entire career helping companies innovate with data, and it's very clear to me that enabling developers to build with AI is the highest potential opportunity in my lifetime to do that. The team at Greylock shares this vision, and today I'm excited to announce that Saam Motamedi is leading our $5.1m seed round, bringing our total capital raised to $8.3m. I've known Saam and Greylock for almost a decade, and I've consistently admired how they partner with founders at their earliest stages and together, build enduring businesses. After our first conversation about Braintrust, Saam instantly realized that evals are fundamental in the shift to non-deterministic software, introduced me to a number of potential customers, and started brainstorming what Braintrust could become a decade from now.\nI'm thrilled to add Greylock to our existing set of investors, which includes Elad Gil, Basecase Capital, SV Angel, Box Group, and a rockstar group of angel investors including Clem Delangue, CEO of HuggingFace; Greg Brockman, Co-founder and President of OpenAI; Howie Liu, CEO of Airtable; Jack Altman, Founder of Lattice; Guillermo Rauch, CEO of Vercel; Bryan Helmig, CTO of Zapier; Simon Last, CTO of Notion; Vipul Ved Prakash, CEO of Together; Tuhin Srivastava, CEO of Baseten; Olivier Pomel, CEO of Datadog; Qasar Younis, CEO of Applied Intuition; and Gautum Kedia, ML Leader at Stripe.\nI feel incredibly fortunate to work with a once-in-a-lifetime group of colleagues, investors, and customers. Each and every one of them is someone I would want to work for, and to be honest a lot of startup building is just that. To learn more about Braintrust or sign up for the product, check out our website, docs, or reach out. We are still building our core team, so please check out our careers page if you are interested in joining us.", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Latest news"}, {"href": "https://www.braintrust.dev/blog/seed-round", "anchor": "Braintrust's seed round: $5m to build infrastructure for AI products"}, {"href": "https://www.braintrust.dev/", "anchor": "website"}, {"href": "https://www.braintrust.dev/docs", "anchor": "docs"}, {"href": "https://www.braintrust.dev/contact", "anchor": "reach out"}, {"href": "https://www.braintrust.dev/careers", "anchor": "careers page"}, {"href": "https://www.braintrust.dev/blog/seed-round", "anchor": "Braintrust's seed round: $5m to build infrastructure for AI products"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 1}, "https://www.braintrust.dev/blog/open-sourcing-proxy": {"url": "https://www.braintrust.dev/blog/open-sourcing-proxy", "title": "Open sourcing the AI proxy - Blog - Braintrust", "text": "Open sourcing the AI proxy\nLast week, we released the Braintrust AI Proxy, a new, free way to access LLaMa2, Mistral, OpenAI, Anthropic, and many other models behind the OpenAI protocol with built-in caching and API key management.\nFolks immediately started reaching out about running the proxy in production. We firmly believe that code on the critical path to production should be open source, so we're excited to announce that the proxy's source code is now available on GitHub under the MIT license.\nDeployment options\nYou can continue to access the proxy, for free, by using the hosted version at https://braintrustproxy.com\n. It's hosted\non Cloudflare workers and end-to-end encrypts cached data using 256-bit AES-GCM encryption.\nFor more details, see the documentation or source code.\nThe repository also contains instructions for deploying the proxy to Vercel Edge Functions, Cloudflare workers, AWS Lambda, or as a plain-old Express server.\nBenchmarks\nI did some quick benchmarks, from my in-laws' place in California and an EC2 machine (US East N. Virginia) to compare performance across options (code).\nThe AWS Lambda functions are deployed in us-east-1\n. aws-pc\nis AWS Lambda with provisioned concurrency.\nIn-laws (CA)\nEC2 (US East N. Virginia)\nAs you can see, Cloudflare and Vercel are consistently very fast, and AWS Lambda in US East suffers (as expected) when measured from CA. I was surprised that AWS Lambda with provisioned concurrency was slower than without. Perhaps I misconfigured something...\nAdditional features\nAlong with the open source release, the proxy contains a number of useful built-in features.\nCaching\nThe proxy automatically caches responses from the model provider if you set a seed\nvalue or temperature=0\n.\nSeeds are a new feature in the OpenAI API that allows you to create reproduceable results, but most model providers\ndo not yet support them. The proxy automatically handles that for you.\nAPI key management\nYou can add API keys across providers as secrets in Braintrust, and use a single API key to access all of them. This is a great way to manage your API keys in one place, and share them with your team.\nLoad balancing\nYou can now add multiple keys and organizations as secrets in Braintrust, and the proxy will automatically load balance across them for you. This is a simple way to add resiliency across OpenAI accounts or providers (e.g. OpenAI and Azure).\nAzure OpenAI\nYou can access Azure's OpenAI endpoints through the proxy, with vanilla OpenAI drivers, by configuring Azure endpoints in Braintrust. If you configure both OpenAI and Azure endpoints, the proxy will automatically load balance between them.\nWhat's next\nWe have an exciting roadmap ahead for the proxy, including more advanced load balancing/resiliency features, support for more models/providers, and deeper integrations into Braintrust.\nIf you have any feedback or want to collaborate, send us an email or join our Discord.", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Latest news"}, {"href": "https://www.braintrust.dev/blog/open-sourcing-proxy", "anchor": "Open sourcing the AI proxy"}, {"href": "https://www.braintrust.dev/blog/ai-proxy", "anchor": "Braintrust AI Proxy"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "other models"}, {"href": "https://www.braintrust.dev/blog/open-sourcing-proxy", "anchor": "Deployment options"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "documentation"}, {"href": "https://www.braintrust.dev/blog/open-sourcing-proxy", "anchor": "Benchmarks"}, {"href": "https://www.braintrust.dev/blog/open-sourcing-proxy", "anchor": "Additional features"}, {"href": "https://www.braintrust.dev/blog/open-sourcing-proxy", "anchor": "Caching"}, {"href": "https://www.braintrust.dev/blog/open-sourcing-proxy", "anchor": "API key management"}, {"href": "https://www.braintrust.dev/app/settings?subroute=secrets", "anchor": "secrets in Braintrust"}, {"href": "https://www.braintrust.dev/blog/open-sourcing-proxy", "anchor": "Load balancing"}, {"href": "https://www.braintrust.dev/app/settings?subroute=secrets", "anchor": "secrets in Braintrust"}, {"href": "https://www.braintrust.dev/blog/open-sourcing-proxy", "anchor": "Azure OpenAI"}, {"href": "https://www.braintrust.dev/app/settings?subroute=secrets", "anchor": "Braintrust"}, {"href": "https://www.braintrust.dev/blog/open-sourcing-proxy", "anchor": "What's next"}, {"href": "https://www.braintrust.dev/contact", "anchor": "send us an email"}, {"href": "https://www.braintrust.dev/blog/open-sourcing-proxy", "anchor": "Open sourcing the AI proxy"}, {"href": "https://www.braintrust.dev/blog/open-sourcing-proxy", "anchor": "Deployment options"}, {"href": "https://www.braintrust.dev/blog/open-sourcing-proxy", "anchor": "Benchmarks"}, {"href": "https://www.braintrust.dev/blog/open-sourcing-proxy", "anchor": "Additional features"}, {"href": "https://www.braintrust.dev/blog/open-sourcing-proxy", "anchor": "Caching"}, {"href": "https://www.braintrust.dev/blog/open-sourcing-proxy", "anchor": "API key management"}, {"href": "https://www.braintrust.dev/blog/open-sourcing-proxy", "anchor": "Load balancing"}, {"href": "https://www.braintrust.dev/blog/open-sourcing-proxy", "anchor": "Azure OpenAI"}, {"href": "https://www.braintrust.dev/blog/open-sourcing-proxy", "anchor": "What's next"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 1}, "https://www.braintrust.dev/blog/ai-proxy": {"url": "https://www.braintrust.dev/blog/ai-proxy", "title": "AI proxy: fostering a more open ecosystem - Blog - Braintrust", "text": "AI proxy: fostering a more open ecosystem\nLike most of us, I spent last weekend thinking about the past, present, and future of AI. It's hard to imagine the industry without OpenAI \u2014 an institution we all look to, respect, and rely on \u2014 at its forefront. I for one am rooting for the company and brilliant folks who work there to continue leading the way. However, I also realized that the AI ecosystem benefits from being interoperable and default open and that we at Braintrust have an important role to play in that.\nFor months, a few key challenges in AI development have been nagging me:\n- The prevailing LLM toolset is the OpenAI SDK. However, because the libraries are specific to OpenAI, you end up writing code that is not interoperable with other providers, making it harder to evaluate alternatives.\n- While developing LLM apps, I frequently rewrite small pieces of code, and re-run the same AI calls over and over again. Each time, I implement a slightly different cache, because of the proliferation of languages and platforms.\n- It's a pain to manage API keys within and across providers, e.g. to load balance and dynamically route workloads. I wish I could just manage a single API key, specify the model, and let a system automatically call the right system for me.\nThis weekend felt like the perfect time to address these challenges. I'm very excited to announce the newest feature of Braintrust: an AI proxy. The proxy addresses the above pain points by embracing OpenAI's interface as the lingua franca for LLMs, and adding caching, logging, and API key management behind the scenes. It also supports popular open source models like LLaMa 2 and Mistral via Perplexity and all of OpenAI's and Anthropic's models.\nIf you have something built on GPT-4 or another model, you can now try it out on LLaMa2, Mistral, Anthropic, or others \u2014 and vice versa \u2014 without changing any code. We believe this is just the start and that the AI proxy will enable our current and future customers to build robust, low latency systems that work across a thriving and open ecosystem of model providers.\nJust let me try it!\nBefore we get into the details, here's a quick demo + instructions to try it out. You can use your favorite OpenAI drivers, and\nset the base url to https://api.braintrust.dev/v1/proxy\n. Try running the following script in your favorite language, twice.\nIf you have access to Perplexity or Anthropic, feel free to use their API keys with mistral-7b-instruct\nor\nclaude-instant-1.2\ninstead. Under the hood, we're proxying the requests, caching the results with end-to-end\nencryption, and streaming back the results.\nRead on to learn more technical details, or check out the docs.\nArchitecture\nThe AI proxy is optimized for a few key objectives: low latency, security, and portability:\n- Latency was the toughest to achieve \u2014 although there are a variety of techniques to run code close to users, we settled on Cloudflare Workers which offer a unique combination of low latency, streaming support, and low cost. In our tests, we're able to get end-to-end latencies consistently under 100ms and often around 50ms, when results are cached.\n- Our customers tend to be very security conscious, so we wanted to make sure our cache cannot (even theoretically) leak data across users. We achieve this by encrypting model responses using your API key (which we do not store or log) and 256-bit AES-GCM encryption. We are also in talks with the Cloudflare team to support additional security measures for enterprise customers.\n- We're betting on the OpenAI API protocol as the lingua franca for LLMs, and therefore mapping each supported\nprovider into an OpenAI compatible format. As far as we know, this is the first, and if not, certainly the\neasiest, way to access Anthropic's models by swapping in\nclaude-2\nforgpt-4\n. We'll figure out a way to make this scale \u2014 via open source or other forms of open collaboration.\nCaching\nThe feature I'm personally most excited by is the cache. When I'm writing code, I like to tinker and re-run things very often. For example, I was recently working on an LLM-based list-of-strings comparator and tweaked the threshold at which I use an LLM to compare strings. Each time I tweaked this threshold, a majority of the string comparisons were the same, but I'd waste minutes (!) waiting for GPT-4 to recompute the same results.\nThe AI proxy solves this problem by caching model calls for you, both ordinary and streaming. By default,\nrequests with temperature=0\nor the new seed\nparameter,\nare cached. You can also set the x-bt-use-cache\nto always\nor never\nto more directly control this behavior.\nBecause the cached values are encrypted in terms of your API key, the cache is not shared across users. Braintrust customers can choose to share cached values across users in their organization.\nAPI keys\nYou can use your OpenAI, Anthropic, and Perplexity API keys to access their respective models. However, if you're a Braintrust user, you can create a single API key that will work across services (even on the free plan). Create a Braintrust account and enter a value for each service you'd like to use:\nThen, pass your Braintrust API key into the SDK instead of a provider-specific one. When you issue a request, the proxy will translate your Braintrust API key into the appropriate secret behind the scenes. We believe this pattern is very powerful \u2014 you can configure how you want the proxy to behave behind the scenes without changing a single line of code \u2014 and are already working on features like deeper integration within Braintrust's evaluation and logging tools, load balancing, and model routing.\nAvailability\nThe AI proxy is available for all to use, for free, as a beta. You get a common interface across providers and caching out of the box, and if you create a Braintrust account, you can configure a single API key to work across OpenAI, Anthropic, and Perplexity, as well as a number of other powerful features like evaluations, logging, and more.\nWe hope it's straightforward enough to use that you'll always just use https://api.braintrust.dev/v1/proxy\nas a default base\nurl. We are also interested in expanding the proxy's features, providers, and of course fixing bugs and\nimproving performance. Let us know your thoughts by email or on\ndiscord.", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Latest news"}, {"href": "https://www.braintrust.dev/blog/ai-proxy", "anchor": "AI proxy: fostering a more open ecosystem"}, {"href": "https://www.braintrust.dev/blog/ai-proxy", "anchor": "Just let me try it!"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "the docs"}, {"href": "https://www.braintrust.dev/blog/ai-proxy", "anchor": "Architecture"}, {"href": "https://www.braintrust.dev/blog/ai-proxy", "anchor": "Caching"}, {"href": "https://www.braintrust.dev/blog/ai-proxy", "anchor": "API keys"}, {"href": "https://www.braintrust.dev/blog/ai-proxy", "anchor": "Availability"}, {"href": "https://www.braintrust.dev/contact", "anchor": "email"}, {"href": "https://www.braintrust.dev/blog/ai-proxy", "anchor": "AI proxy: fostering a more open ecosystem"}, {"href": "https://www.braintrust.dev/blog/ai-proxy", "anchor": "Just let me try it!"}, {"href": "https://www.braintrust.dev/blog/ai-proxy", "anchor": "Architecture"}, {"href": "https://www.braintrust.dev/blog/ai-proxy", "anchor": "Caching"}, {"href": "https://www.braintrust.dev/blog/ai-proxy", "anchor": "API keys"}, {"href": "https://www.braintrust.dev/blog/ai-proxy", "anchor": "Availability"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 1}, "https://www.braintrust.dev/blog/state-of-ai": {"url": "https://www.braintrust.dev/blog/state-of-ai", "title": "State of AI development 2023 - Blog - Braintrust", "text": "State of AI development 2023\nRetool recently surveyed over 1,500 workers and how their companies are adopting AI in their State of AI 2023 report.\nIt seems that most companies are building AI features and putting them into production!\n- 66.2% said they have at least one internal AI use case live\n- 43.1% said they have at least one external use case live\nThis is extremely exciting as workers get to save time with better AI internal tools and users get better user experiences with AI.\nHowever, building AI apps is tough. Companies are running into challenges with building and productionizing their AI apps. Some of their top challenges include model output accuracy, hallucinations, and prompt engineering.\nEveryone's trying to make AI apps they can count on. They're asking:\n- \"How do I evaluate my AI app?\"\n- \"How do I integrate evaluations with CICD?\"\n- \"How to use GPT-4 to score AI outputs instead of manually grading everything?\"\n- \"How do I build my own internal evaluation system?\"\nThese are tough questions. Companies are trying to find the best ways to do things, making their own tools, and learning from mistakes. Right now, most companies either check their AI's work by hand or don't check it much at all. Others are making their own tools for checking.\nYou don't need to worry about making your own tools. With the Braintrust platform, you can add evals to your AI app in < 10 minutes. Braintrust helps you run evaluations, visualize and inspect your results, and experiment with prompts quickly. You don't have to spend time and effort building your own internal tools.", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Latest news"}, {"href": "https://www.braintrust.dev/blog/state-of-ai", "anchor": "State of AI development 2023"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up"}, {"href": "https://www.braintrust.dev/docs", "anchor": "docs"}, {"href": "https://www.braintrust.dev/blog/state-of-ai", "anchor": "State of AI development 2023"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 1}, "https://www.braintrust.dev/blog/journey": {"url": "https://www.braintrust.dev/blog/journey", "title": "The AI product development journey - Blog - Braintrust", "text": "The AI product development journey\nBuilding reliable AI apps is hard. It\u2019s easy to build a cool demo but hard to build an AI app that works in production for real users. In traditional software development, there\u2019s a set of best practices like setting up CI/CD and writing tests to make your software robust and easy to build on. But, with LLM apps it\u2019s not obvious how to create these tests or processes.\nEveryone is still figuring out their evaluation process for AI apps. Once you get it set up, you realize how useful it is. A good evaluation system saves so much time and effort testing new changes, fixing bugs, and deciding when your app is ready to launch.\nLet\u2019s walk through the AI app development journey and see how evaluations are useful. Imagine you are building a new AI app that answers user questions based on your product\u2019s documentation. This is what the journey from prototype to production could look like:\n1. Start with a prototype\nYou write a quick TypeScript program to scrape and embed your docs into a vector db using OpenAI\u2019s assistant API. Then, you create a simple backend endpoint with an OpenAI generation function that you can hit with POST requests.\n2. Manually testing\nYou test the endpoint with a bunch of example queries that you can think of: \u201cHow do I get started?\u201d, \u201cWhat libraries does your project have?\u201d, etc. The endpoint seems to work ok and some of your answers are correct and some are wrong.\n3. Get some friends to test\nYou make a simple frontend and then ask some teammates to test it out. They start asking for more features or reporting bugs like: \u201cIt gives me the wrong answer when I ask it about the company\u201d, or \u201cIt gives answers that are way too long\u201d.\n4. Fix bugs and add features\nYou get to work building the new features and fixes by changing the prompt and experimenting with different ways to embed your data. After every change, you have to rerun the app manually on different inputs to see if things got better.\nAt this point, you are tired of copying pasting inputs in, reviewing outputs, and manually testing things all the time to see what changed. It\u2019s a constant battle and tedious process to answer questions like:\n- Did my latest prompt change make things better overall? Or did it get worse?\n- On what types of inputs did things get better and worse at?\n- How can I actually measure how good my LLM app is?\n- How can I prove to my manager that this is ready for production and ready to announce for our users to use?\n- Is this answer actually correct? Is it partially correct?\n5. Evaluations enlightenment\nYou realize it\u2019s time to build an evaluation workflow for your app to automate this testing and validation process. You already have a set of questions you ask your app every time to test if new changes worked or caused regressions. So it makes sense to automate this using an evaluation script and use a preset list of answers to compare against. It will be tedious to write a good scoring function for the answers and to run all these test cases concurrently. It\u2019s probably going to take a lot of work to write and maintain this code. You might need to make a Retool dashboard to make the test cases easy to create for anyone on the team (including PMs and designers). But, a good evaluation script would save so much time and improve developer iteration speed immensely for your team. It helps you figure out what you need to fix and how to improve your AI app.\nLuckily, Braintrust streamlines this process and handles all the boring work for you so you don\u2019t need to maintain your own eval internal tools. Braintrust provides libraries in TypeScript and Python to run evaluations, score outputs using prebuilt scoring functions, and a nice web UI to visualize and inspect your eval results. We also help you manage your test cases with your team using a web UI and help log traces as well. You can set up your evaluation workflow with Braintrust in <10 minutes and focus on building the fun parts of AI apps.\nNow, every time you make a change, you just run the evaluation script and see in the Braintrust UI exactly which test cases improved and which got worse. When something gets worse, you can click into the test case and see the full trace and logs for how the output was generated. You can now share progress and collaborate on development just by sharing all the experiment data, test cases, and logs to your team using Braintrust.\n6. Launch \ud83d\ude80\nYou finish adding the new features and iteratively validate the results with Braintrust. Now, your team feels confident launching the new AI feature to your users. You launch the new feature to your users and continue to make new improvements quickly as you get feedback.\nConclusion\nThis is what the AI app development journey usually looks like. Setting up an evaluation system saves your team time and effort. You don\u2019t need to reinvent the wheel. Braintrust provides the enterprise-grade stack for building AI products including evaluation tools, model-graded evals, datasets, tracing, prompt playgrounds, and more.", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Latest news"}, {"href": "https://www.braintrust.dev/blog/journey", "anchor": "The AI product development journey"}, {"href": "https://www.braintrust.dev/blog/journey", "anchor": "1. Start with a prototype"}, {"href": "https://www.braintrust.dev/blog/journey", "anchor": "2. Manually testing"}, {"href": "https://www.braintrust.dev/blog/journey", "anchor": "3. Get some friends to test"}, {"href": "https://www.braintrust.dev/blog/journey", "anchor": "4. Fix bugs and add features"}, {"href": "https://www.braintrust.dev/blog/journey", "anchor": "5. Evaluations enlightenment"}, {"href": "https://www.braintrust.dev/blog/journey", "anchor": "6. Launch \ud83d\ude80"}, {"href": "https://www.braintrust.dev/blog/journey", "anchor": "Conclusion"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up"}, {"href": "https://www.braintrust.dev/docs", "anchor": "docs"}, {"href": "https://www.braintrust.dev/blog/journey", "anchor": "The AI product development journey"}, {"href": "https://www.braintrust.dev/blog/journey", "anchor": "1. Start with a prototype"}, {"href": "https://www.braintrust.dev/blog/journey", "anchor": "2. Manually testing"}, {"href": "https://www.braintrust.dev/blog/journey", "anchor": "3. Get some friends to test"}, {"href": "https://www.braintrust.dev/blog/journey", "anchor": "4. Fix bugs and add features"}, {"href": "https://www.braintrust.dev/blog/journey", "anchor": "5. Evaluations enlightenment"}, {"href": "https://www.braintrust.dev/blog/journey", "anchor": "6. Launch \ud83d\ude80"}, {"href": "https://www.braintrust.dev/blog/journey", "anchor": "Conclusion"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 1}, "https://www.braintrust.dev/blog/update-6": {"url": "https://www.braintrust.dev/blog/update-6", "title": "Weekly update 11/13/23 - Blog - Braintrust", "text": "Latest news\nBraintrust Weekly Update\n13 November 2023David Song\nIt\u2019s been another busy week at Braintrust. We\u2019ve shipped some user experience improvements and fixes this week to make it easier for you to build reliable AI apps:\nWe added function calling to our playground\nMany developers have been asking us to support OpenAI\u2019s function calling in our prompt playground. We added it this week. Just click \u201cAdd tools\u201d in your prompt to get started experimenting with OpenAI functions.\nRelease notes:\n- Made experiment column resized widths persistent\n- Fixed our libraries including Autoevals to work with OpenAI\u2019s new libraries\n- Added support for function calling and tools in our prompt playground\n- Added tabs on a project page for Datasets, Experiments, etc.\nFun links and mentions:\n- State of AI Report from Retool. How are companies adopting AI?\n- The AI app development journey: how do evaluations save you time?\nBraintrust is the enterprise-grade stack for building AI products. From evaluations, to prompt playground, to data management, we take uncertainty and tedium out of incorporating AI into your business.\nSign up now, or check out our pricing page for more details.", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Latest news"}, {"href": "https://www.braintrust.dev/blog/update-6", "anchor": "Braintrust Weekly Update"}, {"href": "https://www.braintrust.dev/blog/update-6", "anchor": "We added function calling to our playground"}, {"href": "https://www.braintrust.dev/blog/update-6", "anchor": "Release notes :"}, {"href": "https://www.braintrust.dev/blog/update-6", "anchor": "Fun links and mentions:"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "pricing page"}, {"href": "https://www.braintrust.dev/blog/update-6", "anchor": "Braintrust Weekly Update"}, {"href": "https://www.braintrust.dev/blog/update-6", "anchor": "We added function calling to our playground"}, {"href": "https://www.braintrust.dev/blog/update-6", "anchor": "Release notes :"}, {"href": "https://www.braintrust.dev/blog/update-6", "anchor": "Fun links and mentions:"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 1}, "https://www.braintrust.dev/blog/update-5": {"url": "https://www.braintrust.dev/blog/update-5", "title": "Weekly update 11/06/23 - Blog - Braintrust", "text": "Latest news\nBraintrust Weekly Update\n06 November 2023David Song\nWe\u2019ve been heads down at Braintrust working on an exciting new feature. We\u2019ll announce it soon! In the meantime, we\u2019ve shipped some user experience improvements and fixes this week:\nNew OpenAI and Open Source models in playground\nWe wanted to try out open source models like Mistral, Codellama, Llama2, etc. and compare them to OpenAI's new models. So, we added them to the playground and also used Perplexity's API for the OS models. They are so fast!\nRelease notes:\n- Improved selectors for diffing and comparison modes on experiment view\n- Added support for new OpenAI models (GPT4 preview, 3.5turbo-1106) in playground\n- Added support for open source models (Mistral, Codellama, Llama2,etc.) in the playground using Perplexity's APIs\nFun links and mentions:\n- OpenAI's reproducible outputs makes evals and testing 10x better\n- Have fun building AI apps with your team\nBraintrust is the enterprise-grade stack for building AI products. From evaluations, to prompt playground, to data management, we take uncertainty and tedium out of incorporating AI into your business.\nSign up now, or check out our pricing page for more details.", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Latest news"}, {"href": "https://www.braintrust.dev/blog/update-5", "anchor": "Braintrust Weekly Update"}, {"href": "https://www.braintrust.dev/blog/update-5", "anchor": "New OpenAI and Open Source models in playground"}, {"href": "https://www.braintrust.dev/blog/update-5", "anchor": "Release notes :"}, {"href": "https://www.braintrust.dev/blog/update-5", "anchor": "Fun links and mentions:"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "pricing page"}, {"href": "https://www.braintrust.dev/blog/update-5", "anchor": "Braintrust Weekly Update"}, {"href": "https://www.braintrust.dev/blog/update-5", "anchor": "New OpenAI and Open Source models in playground"}, {"href": "https://www.braintrust.dev/blog/update-5", "anchor": "Release notes :"}, {"href": "https://www.braintrust.dev/blog/update-5", "anchor": "Fun links and mentions:"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 1}, "https://www.braintrust.dev/blog/update-4": {"url": "https://www.braintrust.dev/blog/update-4", "title": "Weekly update 10/30/23 - Blog - Braintrust", "text": "Latest news\nBraintrust Weekly Update\n30 October 2023Ankur Goyal\nWe\u2019ve been heads down at Braintrust working on an exciting new feature. We\u2019ll announce it soon! In the meantime, we\u2019ve shipped some user experience improvements and fixes this week:\nExperiment sidebar is now resizable\nWe realized the sidebar was sometimes too small to view the logged data in a trace. Now, you can just click to resize the sidebar in the experiment view. The sidebar width is also saved across page loads.\nRelease notes:\n- Improved experiment sidebar to be fully responsive and resizable\n- Improved tooltips within the web UI\n- Multiple performance optimizations and bug fixes\nFun links and mentions:\n- We made it onto replit.com\n- We sparked some debate on Twitter about supporting on prem for software\n- VoyageAI announced new embedding model to rival ada002\nBraintrust is the enterprise-grade stack for building AI products. From evaluations, to prompt playground, to data management, we take uncertainty and tedium out of incorporating AI into your business.\nSign up now, or check out our pricing page for more details.", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Latest news"}, {"href": "https://www.braintrust.dev/blog/update-4", "anchor": "Braintrust Weekly Update"}, {"href": "https://www.braintrust.dev/blog/update-4", "anchor": "Experiment sidebar is now resizable"}, {"href": "https://www.braintrust.dev/blog/update-4", "anchor": "Release notes :"}, {"href": "https://www.braintrust.dev/blog/update-4", "anchor": "Fun links and mentions:"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "pricing page"}, {"href": "https://www.braintrust.dev/blog/update-4", "anchor": "Braintrust Weekly Update"}, {"href": "https://www.braintrust.dev/blog/update-4", "anchor": "Experiment sidebar is now resizable"}, {"href": "https://www.braintrust.dev/blog/update-4", "anchor": "Release notes :"}, {"href": "https://www.braintrust.dev/blog/update-4", "anchor": "Fun links and mentions:"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 1}, "https://www.braintrust.dev/blog/update-3": {"url": "https://www.braintrust.dev/blog/update-3", "title": "Weekly update 10/23/23 - Blog - Braintrust", "text": "Braintrust Weekly Update\nIt\u2019s been another busy week for us at Braintrust. It\u2019s been exciting to see many users start to ship their LLM apps into production and evaluate them with Braintrust. Here\u2019s some of the new features we shipped this week:\nPrompt playground variable improvements\nManaging and writing evaluation test cases is tedious because they are usually in a JSON format. This means manually editing JSON strings all the time. This week we improved the playground so you can quickly create input variables and edit them for your prompts and test cases without needing to edit raw JSON. Everything is visualized and accessible through our UI. The input names also autofill when you are typing out the prompt.\nTime duration summary metrics for experiments\nWhen iterating on LLM apps there is a tradeoff between time to execute and accuracy. For example, running more context retrievals can take longer but can improve the quality of the answer for a RAG app. We added in time duration metrics to Braintrust experiments so that you can compare how long each test case changes between experiments and compare it to overall accuracy.\nRelease notes:\n- Improved prompt playground variable handling and visualization\n- Added time duration statistics per row to experiment summaries\n- Multiple performance optimizations and bug fixes\nFun links and mentions:\n- Madrona wrote an excellent report on the GenAI development stack including Braintrust\n- Ironclad updated their prompt engineering tool: Rivet launched plugins including a Braintrust plugin\n- Ankur spoke on the Bits and Bots podcast with Parcha about AI agents in production\n- We received our first community contribution to our autoevals library (our model graded evals library). Huge thank you to Edward Atkins @ecatkins for authoring the PR.\nBraintrust is the enterprise-grade stack for building AI products. From evaluations, to prompt playground, to data management, we take uncertainty and tedium out of incorporating AI into your business.\nSign up now, or check out our pricing page for more details.", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Latest news"}, {"href": "https://www.braintrust.dev/blog/update-3", "anchor": "Braintrust Weekly Update"}, {"href": "https://www.braintrust.dev/blog/update-3", "anchor": "Prompt playground variable improvements"}, {"href": "https://www.braintrust.dev/blog/update-3", "anchor": "Time duration summary metrics for experiments"}, {"href": "https://www.braintrust.dev/blog/update-3", "anchor": "Release notes :"}, {"href": "https://www.braintrust.dev/blog/update-3", "anchor": "Fun links and mentions:"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "pricing page"}, {"href": "https://www.braintrust.dev/blog/update-3", "anchor": "Braintrust Weekly Update"}, {"href": "https://www.braintrust.dev/blog/update-3", "anchor": "Prompt playground variable improvements"}, {"href": "https://www.braintrust.dev/blog/update-3", "anchor": "Time duration summary metrics for experiments"}, {"href": "https://www.braintrust.dev/blog/update-3", "anchor": "Release notes :"}, {"href": "https://www.braintrust.dev/blog/update-3", "anchor": "Fun links and mentions:"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 1}, "https://www.braintrust.dev/blog/update-2": {"url": "https://www.braintrust.dev/blog/update-2", "title": "Weekly update 10/16/23 - Blog - Braintrust", "text": "Braintrust Weekly Update\nIt\u2019s been a busy week for us at Braintrust. Here\u2019s some of the new features we shipped this week:\nTracing: log and visualize complex LLM chains and executions.\nWe built a new feature to help you debug and observe your LLM app. Add two lines of code to log a function call and get useful traces in the Braintrust UI. Investigate each step of your LLM pipeline, see how long it took, and debug the context. You can also log metadata like token usage, errors, and more. Try it out and learn more on our Tracing docs .\nExperiment dashboard customization\nWe made the experiment dashboard customizable. Click on the settings button next to the summary title to select which charts you\u2019d like to see for the experiment.\nRelease notes:\n- Added a new \u201ctext-block\u201d prompt type in the playground that just returns a string or variable back without a LLM call (useful for chaining prompts and debugging)\n- Increased default # of rows per page from 10 to 100 for experiments\n- Updated evals documentation\n- UI fixes and improvements for the side panel and tooltips\n- Launched new tracing feature: log and visualize complex LLM chains and executions.\n- The experiment dashboard can be customized to show the most relevant charts\nBraintrust is the enterprise-grade stack for building AI products. From evaluations, to prompt playground, to data management, we take uncertainty and tedium out of incorporating AI into your business.\nSign up now, or check out our pricing page for more details.", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Latest news"}, {"href": "https://www.braintrust.dev/blog/update-2", "anchor": "Braintrust Weekly Update"}, {"href": "https://www.braintrust.dev/blog/update-2", "anchor": "Tracing: log and visualize complex LLM chains and executions."}, {"href": "https://www.braintrust.dev/blog/update-2", "anchor": "Experiment dashboard customization"}, {"href": "https://www.braintrust.dev/blog/update-2", "anchor": "Release notes:"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "pricing page"}, {"href": "https://www.braintrust.dev/blog/update-2", "anchor": "Braintrust Weekly Update"}, {"href": "https://www.braintrust.dev/blog/update-2", "anchor": "Tracing: log and visualize complex LLM chains and executions."}, {"href": "https://www.braintrust.dev/blog/update-2", "anchor": "Experiment dashboard customization"}, {"href": "https://www.braintrust.dev/blog/update-2", "anchor": "Release notes:"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 1}, "https://www.braintrust.dev/blog/update-1": {"url": "https://www.braintrust.dev/blog/update-1", "title": "Weekly update 10/09/23 - Blog - Braintrust", "text": "Braintrust Weekly Update\nIt\u2019s been a busy week for us at Braintrust. Here\u2019s some of the new features we shipped this week:\n-\nAll experiment loading HTTP requests are 100-200ms faster\n- We released a new tutorial: finetune GPT3.5 to write SQL queries\nYou can quickly finetune GPT3.5 to generate SQL queries using OpenAI and then evaluate how the fine tuned model compares to the base model using Braintrust. Check out the Jupyter Notebook example here to get started.\nWe evaluated the Alpaca evals leaderboard in Braintrust\nThe Alpaca evals use Claude and GPT4 to rank how different LLMs perform on a variety of tasks. You can see the aggregated rankings and also dig into individual models and better understand their strengths and weaknesses. Check out the Alpaca Evals braintrust project on Braintrust to dig in further\u2014no login required.\nWe improved Datasets. See when they were last edited and the version number from the UI.\nEasily see when a dataset was last changed from the UI by hovering over the ID. We also provide example code so you can quickly use the current dataset version in your project. Learn more on our datasets guide.\nRelease notes\n- All experiment loading HTTP requests are 100-200ms faster\n- The prompt playground now supports autocomplete\n- Dataset versions are now displayed on the datasets page\n- Projects in the summary page are now sorted alphabetically\n- Long text fields in logged data can be expanded into scrollable blocks\nBraintrust is the enterprise-grade stack for building AI products. From evaluations, to prompt playground, to data management, we take uncertainty and tedium out of incorporating AI into your business.\nSign up now, or check out our pricing page for more details.", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Latest news"}, {"href": "https://www.braintrust.dev/blog/update-1", "anchor": "Braintrust Weekly Update"}, {"href": "https://www.braintrust.dev/blog/update-1", "anchor": "We evaluated the Alpaca evals leaderboard in Braintrust"}, {"href": "https://www.braintrust.dev/blog/update-1", "anchor": "We improved Datasets. See when they were last edited and the version number from the UI."}, {"href": "https://www.braintrust.dev/blog/update-1", "anchor": "Release notes"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "pricing page"}, {"href": "https://www.braintrust.dev/blog/update-1", "anchor": "Braintrust Weekly Update"}, {"href": "https://www.braintrust.dev/blog/update-1", "anchor": "We evaluated the Alpaca evals leaderboard in Braintrust"}, {"href": "https://www.braintrust.dev/blog/update-1", "anchor": "We improved Datasets. See when they were last edited and the version number from the UI."}, {"href": "https://www.braintrust.dev/blog/update-1", "anchor": "Release notes"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 1}, "https://www.braintrust.dev/blog/reliable-ai": {"url": "https://www.braintrust.dev/blog/reliable-ai", "title": "It's time to build reliable AI - Blog - Braintrust", "text": "It's time to build reliable AI\nToday, we're introducing Braintrust: the enterprise-grade stack for rapidly building and shipping AI products, without guesswork.\nOver the last year, the AI space has rapidly evolved. Building anything from chatbots to question-answering or content generation systems used to require months of work. Today, these and more advanced tasks can be solved with a handful of API calls. At the same time, market demand for AI to be a core component of every business has skyrocketed\u2014thanks in large part to ChatGPT, which demonstrated AI's potential to the world when it was released just 8 months ago.\nThe end of predictable software\nIn the mad dash to build with AI, it's become increasingly difficult to deliver high quality, reliable software products. Going from idea to production can be challenging (something I experienced first-hand building AI products at Impira and Figma). For example, how do you define what it means to be \"100% correct\" when summarizing text? And it only gets harder once you ship. The smallest change in code can have significant downstream effects\u2014how can you be sure that you won't break things for your customers?\nI've long had a personal connection to this problem. I started my career building a relational database system that powers critical processes at banks, telcos, and governments. I'll never forget a managing director at a bank warning me that his team would lose their jobs if our database lost data. Prioritizing quality and reliability has always been essential in software workflows. The same is true for AI, despite its inherently non-deterministic behavior. AI's spontaneous and unpredictable nature is what makes it such a great writing partner, programming assistant, and photo generator\u2014but translating its behavior into meaningful quality metrics is incredibly challenging.\nThis is where Braintrust comes in. The faster you can test your AI software on real world examples, the faster you can iterate and improve it. Braintrust is like an operating system for engineers building AI software. It allows you to evaluate and improve your product, every single time you use it. Simply put, Braintrust helps you ship higher quality products, faster.\nAutomated evaluations you can trust\nRobust evaluations enable teams to iterate much, much faster. We make it simple to score outputs, log evaluations, visualize them, interrogate failures, and track performance over time with a simple yet powerful UI. For example, you can instantly answer questions like \"Which specific examples regressed on my change?\", \"Why did the model pick option A instead of B?\", and \"What happens if I try this new model?\"\nHaving this information readily available frees you to try out more changes and make informed decisions. While the premise is simple, implementing evaluations is no easy task. Our infrastructure eliminates the friction of logging, measuring, tracking, visualizing, and sharing evaluations, leaving space for you to focus on building and iterating your product.\nDataset management\nIt can be a lot of work to manage datasets. However, they're key to measuring how your application is performing, and they provide a shared source of truth that can drive experimentation. Braintrust makes it simple to capture new user examples from staging and production, evaluate them, and incorporate them into \"golden\" datasets. Datasets are automatically versioned, so you can make changes without risk of breaking evaluations that depend on them.\nAdditionally, we believe you should own your data. Braintrust can run within your cloud environment, on top of your existing data stack, so our servers never see your data. This architecture enables our customers to comfortably use Braintrust on their most important and IP-sensitive tasks, like leveraging datasets to fine-tune and evaluate custom models.\nCollaborative playground for rapid testing\nPrompt playgrounds have become a critical part of the AI development process, bringing together technical and non-technical users to experiment with and compare prompts. Our early users asked us for a native playground that would allow them to riff on their evaluations and datasets. And we built exactly that\u2014in one click you can explore prompts over any dataset or prior experiment, try out new ideas, and even evaluate them.\nWe believe that a new product development stack is forming around AI, and that by creating tools that integrate seamlessly together, we enable our customers to consistently stay at the forefront of this rapidly changing field. Braintrust's playground is the first of many features we'll introduce that natively integrate with other pieces of the platform and can run on-premises.\nTomorrow, together\nBraintrust is already partnering with several enterprise customers at the forefront of AI.\n- \"Braintrust fills the missing (and critical!) gap of evaluating non-deterministic AI systems. We've used it to successfully measure and improve our AI-first products.\" - Mike Knoop, Co-founder & Head of AI at Zapier\n- \"We're now using a tool called Braintrust to monitor prompt quality over time and to evaluate whether one prompt or model is better than another. It's made it easy to turn iteration and optimization into a science.\" - David Kossnick, Head of AI Product at Coda\n- \"Testing in production is painfully familiar to many AI engineers developing with LLMs. Braintrust finally brings end-to-end testing to AI products, helping companies produce meaningful quality metrics.\" - Michele Catasta, VP of AI at Replit\nFinally, I'm thrilled to be building Braintrust with an incredible group of people. I started the company with assistance from Elad Gil, who helped incubate the initial eval product and team with me. He backed Impira, my previous startup, and over the past six years we've developed a strong working relationship. Joining us on this exciting journey is Coleen Baik as founding designer, and Manu Goyal as founding engineer. David Song, who built the evaluation solutions at mem.ai and is a part of Elad's team, is helping us as well.\nElad and Alana of Base Case Capital are leading our initial funding round, along with incredible founders & executives including:\nGet started for free\nEveryone can get started with Braintrust for free. We recognize how important it is to be part of the development process from day one, so we are making these tools immediately accessible.\nAdditionally, Braintrust is free for open source projects and academic research. AI is largely a community-driven effort, and we want to support these efforts with tools that can help improve AI quality and reliability for everyone.\nSign up now, or check out our pricing page for more details.", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Latest news"}, {"href": "https://www.braintrust.dev/blog/reliable-ai", "anchor": "It's time to build reliable AI"}, {"href": "https://www.braintrust.dev/blog/reliable-ai", "anchor": "The end of predictable software"}, {"href": "https://www.braintrust.dev/blog/reliable-ai", "anchor": "Automated evaluations you can trust"}, {"href": "https://www.braintrust.dev/blog/reliable-ai", "anchor": "Dataset management"}, {"href": "https://www.braintrust.dev/blog/reliable-ai", "anchor": "Collaborative playground for rapid testing"}, {"href": "https://www.braintrust.dev/blog/reliable-ai", "anchor": "Tomorrow, together"}, {"href": "https://www.braintrust.dev/blog/reliable-ai", "anchor": "Get started for free"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "pricing page"}, {"href": "https://www.braintrust.dev/blog/reliable-ai", "anchor": "It's time to build reliable AI"}, {"href": "https://www.braintrust.dev/blog/reliable-ai", "anchor": "The end of predictable software"}, {"href": "https://www.braintrust.dev/blog/reliable-ai", "anchor": "Automated evaluations you can trust"}, {"href": "https://www.braintrust.dev/blog/reliable-ai", "anchor": "Dataset management"}, {"href": "https://www.braintrust.dev/blog/reliable-ai", "anchor": "Collaborative playground for rapid testing"}, {"href": "https://www.braintrust.dev/blog/reliable-ai", "anchor": "Tomorrow, together"}, {"href": "https://www.braintrust.dev/blog/reliable-ai", "anchor": "Get started for free"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 1}, "https://www.braintrust.dev/docs/start/eval-ui": {"url": "https://www.braintrust.dev/docs/start/eval-ui", "title": "Eval via UI - Docs - Start - Braintrust", "text": "Evaluate via UI\nThe following steps require access to a Braintrust organization, which represents a company or a team. Sign up to create an organization for free.\nConfigure your API keys\nNavigate to the AI providers page in your settings and configure at least one API key. For this quickstart, be sure to add your OpenAI API key. After completing this initial setup, you can access models from many providers through a single, unified API.\nFor more advanced use cases where you want to use custom models or avoid plugging your API key into Braintrust, you may want to check out the SDK quickstart.\nCreate a new project\nFor every AI feature your organization is building, the first thing you'll do is create a project.\nCreate a new prompt\nNavigate to Library in the top menu bar, then select Prompts. Create a new prompt in your project called \"movie matcher\". A prompt is the input you provide to the model to generate a response. Choose GPT 4o\nfor your model, and type this for your system prompt:\nSelect the + Message button below the system prompt, and enter a user message:\nPrompts can use mustache templating syntax to refer to variables. In this case, the input corresponds to the movie description given by the user.\nSelect Save as custom prompt to save your prompt.\nExplore the prompt playground\nScroll to the bottom of the prompt viewer, and select Create playground with prompt. This will open the prompt you just created in the prompt playground, a tool for exploring, comparing, and evaluating prompts. In the prompt playground, you can evaluate prompts with data from your datasets.\nImporting a dataset\nOpen this sample dataset, and right-click to select Save as... and download it. It is a .csv\nfile with two columns, Movie Title and Original Description. Inside your playground, select Dataset, then Upload dataset, and upload the CSV file. Using drag and drop, assign the CSV columns to dataset fields. The input column corresponds to Original Description, and the expected column should be Movie Title. Then, select Import.\nChoosing a scorer\nA scoring function allows you to compare the expected output of a task to the actual output and produce a score between 0 and 1. Inside your playground, select Scorers to choose from several types of scoring functions. There are two main types of scoring functions: heuristics are great for well-defined criteria, while LLM-as-a-judge is better for handling more complex, subjective evaluations. You can also create a custom scorer. For this example, since there is a clear correct answer, we can choose ExactMatch.\nRunning your first evaluation\nFrom within the playground, select + Experiment to set up your first evaluation. To run an eval, you need three things:\n- Data: a set of examples to test your application on\n- Task: the AI function you want to test (any function that takes in an input and returns an output)\n- Scores: a set of scoring functions that take an input, output, and optional expected value and compute a score\nIn this example, the Data is the dataset you uploaded, the Task is the prompt you created, and Scores is the scoring function we selected.\nCreating an experiment from the playground will automatically log your results to Braintrust.\nInterpreting your results\nNavigate to the Experiments page to view your evaluation. Examine the exact match scores and other feedback generated by your evals. If you notice that some of your outputs did not match what was expected, you can tweak your prompt directly in the UI until it consistently produces high-quality outputs. If changing the prompt doesn't yield the desired results, consider experimenting with different models.\nAs you iterate on your prompt, you can run more experiments and compare results.\nNext steps\n- Now that you've run your first evaluation, learn how to write your own eval script.\n- Check out more examples and sample projects in the Braintrust Cookbook.\n- Explore the guides to read more about evals, logging, and datasets.", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Evaluate via UI"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Configure your API keys"}, {"href": "https://www.braintrust.dev/app/settings?subroute=secrets", "anchor": "AI providers"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "SDK"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Create a new project"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Create a new prompt"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Explore the prompt playground"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "prompt playground"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "datasets"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Importing a dataset"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Choosing a scorer"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Running your first evaluation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Interpreting your results"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Next steps"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "write your own eval script"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Braintrust Cookbook"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "guides"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Evaluate via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Configure your API keys"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Create a new project"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Create a new prompt"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Explore the prompt playground"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Importing a dataset"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Choosing a scorer"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Running your first evaluation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Interpreting your results"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Next steps"}], "depth": 1}, "https://www.braintrust.dev/docs/start/eval-sdk": {"url": "https://www.braintrust.dev/docs/start/eval-sdk", "title": "Eval via SDK - Docs - Start - Braintrust", "text": "Evaluate via SDK\nThis guide walks through the steps to set up and run an Experiment using the Braintrust SDK. Wrappers are available for TypeScript, Python, and other languages.\nInstall Braintrust libraries\nInstall the Braintrust SDK for your language. For TypeScript and Python, use the following commands:\npnpm add braintrust autoevals\nCreate a simple evaluation script\nThe eval framework allows you to declaratively define evaluations in your code. Inspired by tools like Jest, you can define a set of evaluations in files named _.eval.ts or _.eval.js (Node.js) or eval_*.py (Python).\nCreate a file named tutorial.eval.ts\nor eval_tutorial.py\nwith the following code.\nThis script sets up the basic scaffolding of an evaluation:\ndata\nis an array or iterator of data you'll evaluatetask\nis a function that takes in an input and returns an outputscores\nis an array of scoring functions that will be used to score the tasks's output\nIn addition to adding each data point inline when you call the Eval()\nfunction, you can also pass an existing or new dataset directly.\n(You can also write your own code. Make sure to follow the naming conventions for your language. TypeScript\nfiles should be named *.eval.ts\nand Python files should be named eval_*.py\n.)\nCreate an API key\nNext, create an API key to authenticate your evaluation script. You can create an API key in the settings page.\nRun this command to add your API key to your environment:\nRun your evaluation script\nRun your evaluation script with the following command:\nThis will create an experiment in Braintrust. Once the command runs, you'll see a link to your experiment.\nTip: To test your evaluation locally without sending results to Braintrust, add the --no-send-logs\nflag:\nView your results\nCongrats, you just ran an eval! You should see a dashboard like this when you load your experiment. This view is called the experiment view, and as you use Braintrust, we hope it becomes your trusty companion each time you change your code and want to run an eval.\nThe experiment view allows you to look at high level metrics for performance, dig into individual examples, and compare your LLM app's performance over time.\nRun another experiment\nAfter running your first evaluation, you\u2019ll see that we achieved a 77.8% score. Can you adjust the evaluation to improve this score? Make your changes and re-run the evaluation to track your progress.\nNext steps\n- Dig into our evals guide to learn more about how to run evals.\n- Look at our cookbook to learn how to evaluate RAG, summarization, text-to-sql, and other popular use cases.\n- Learn how to log traces to Braintrust.\n- Read about Braintrust's platform and architecture.", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Evaluate via SDK"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiment"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "TypeScript"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "Python"}, {"href": "https://www.braintrust.dev/docs/reference/api", "anchor": "other languages"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Install Braintrust libraries"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Create a simple evaluation script"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "pass an existing or new dataset directly"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Create an API key"}, {"href": "https://www.braintrust.dev/app/settings?subroute=api-keys", "anchor": "settings page"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Run your evaluation script"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "View your results"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Run another experiment"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Next steps"}, {"href": "https://www.braintrust.dev/docs/guides/evals", "anchor": "evals guide"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "cookbook"}, {"href": "https://www.braintrust.dev/docs/guides/logging", "anchor": "log traces"}, {"href": "https://www.braintrust.dev/docs/platform/architecture", "anchor": "platform and architecture"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Evaluate via SDK"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Install Braintrust libraries"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Create a simple evaluation script"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Create an API key"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Run your evaluation script"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "View your results"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Run another experiment"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Next steps"}], "depth": 1}, "https://www.braintrust.dev/docs/guides": {"url": "https://www.braintrust.dev/docs/guides", "title": "Guides - Docs - Guides - Braintrust", "text": "Guides\nGuides are step-by-step walkthroughs to help you accomplish a specific goal in Braintrust.\nCore functionality\nEvals\nTest AI features and changes before shipping\nLogs\nInstrument your code to monitor its performance\nFeatures\nDatasets\nManage test cases and use log data for evals\nPrompts\nManage and version your prompts and sync them between your live code, playgrounds, and evals\nPlayground\nAn IDE for AI: make changes and see results quickly\nHuman review\nAllow humans to grade AI outputs inside Braintrust\nProxy\nMake it easier to work with API calls to AI providers", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Core functionality"}, {"href": "https://www.braintrust.dev/docs/guides/evals", "anchor": "Evals Test AI features and changes before shipping"}, {"href": "https://www.braintrust.dev/docs/guides/logging", "anchor": "Logs Instrument your code to monitor its performance"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Features"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets Manage test cases and use log data for evals"}, {"href": "https://www.braintrust.dev/docs/guides/prompts", "anchor": "Prompts Manage and version your prompts and sync them between your live code, playgrounds, and evals"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playground An IDE for AI: make changes and see results quickly"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review Allow humans to grade AI outputs inside Braintrust"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "Proxy Make it easier to work with API calls to AI providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Advanced usecases"}, {"href": "https://www.braintrust.dev/docs/guides/tracing", "anchor": "Tracing Control exactly what gets logged in logs and evals"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting Run Braintrust on-prem"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}], "depth": 1}, "https://www.braintrust.dev/docs/cookbook": {"url": "https://www.braintrust.dev/docs/cookbook", "title": "Cookbook - Docs - Braintrust", "text": "Cookbook\nThis cookbook, inspired by OpenAI's cookbook, is a collection of recipes for common use cases of Braintrust. Each recipe is an open source self-contained example, hosted on GitHub. We welcome community contributions and aspire for the cookbook to be a collaborative, living, breathing collection of best practices for building high quality AI products.\nLoading cookbook filter...", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/cookbook/recipes/AgentWhileLoop", "anchor": "Building reliable AI agents"}, {"href": "https://www.braintrust.dev/docs/cookbook/recipes/AISearch", "anchor": "AI Search Bar"}, {"href": "https://www.braintrust.dev/docs/cookbook/recipes/APIAgent-Py", "anchor": "An agent that runs OpenAPI commands"}, {"href": "https://www.braintrust.dev/docs/cookbook/recipes/Assertions", "anchor": "How Zapier uses assertions to evaluate tool usage in chatbots"}, {"href": "https://www.braintrust.dev/docs/cookbook/recipes/ClassifyingNewsArticles", "anchor": "Classifying news articles"}, {"href": "https://www.braintrust.dev/docs/cookbook/recipes/CodaHelpDesk", "anchor": "Coda's Help Desk with and without RAG"}, {"href": "https://www.braintrust.dev/docs/cookbook/recipes/EvaluatingChatAssistant", "anchor": "Evaluating a chat assistant"}, {"href": "https://www.braintrust.dev/docs/cookbook/recipes/Github-Issues", "anchor": "Improving Github issue titles using their contents"}, {"href": "https://www.braintrust.dev/docs/cookbook/recipes/HTMLGenerator", "anchor": "Generating beautiful HTML components"}, {"href": "https://www.braintrust.dev/docs/cookbook/recipes/LLaMa-3_1-Tools", "anchor": "Tool calls in LLaMa 3.1"}, {"href": "https://www.braintrust.dev/docs/cookbook/recipes/ModelComparison", "anchor": "Comparing evals across multiple AI models"}, {"href": "https://www.braintrust.dev/docs/cookbook/recipes/OTEL-logging", "anchor": "Using OpenTelemetry for LLM observability"}, {"href": "https://www.braintrust.dev/docs/cookbook/recipes/PDFPlayground", "anchor": "Using PDF attachments in playgrounds"}, {"href": "https://www.braintrust.dev/docs/cookbook/recipes/PrecisionRecall", "anchor": "Evaluating the precision and recall of an emotion classifier"}, {"href": "https://www.braintrust.dev/docs/cookbook/recipes/PromptChaining", "anchor": "Evaluating a prompt chaining agent"}, {"href": "https://www.braintrust.dev/docs/cookbook/recipes/PromptInjectionDetector", "anchor": "Detecting Prompt Injections"}, {"href": "https://www.braintrust.dev/docs/cookbook/recipes/PromptVersioning", "anchor": "Prompt versioning and deployment"}, {"href": "https://www.braintrust.dev/docs/cookbook/recipes/ProviderBenchmark", "anchor": "Benchmarking inference providers"}, {"href": "https://www.braintrust.dev/docs/cookbook/recipes/Realtime", "anchor": "Evaluating audio with the OpenAI Realtime API"}, {"href": "https://www.braintrust.dev/docs/cookbook/recipes/ReceiptExtraction", "anchor": "Evaluating multimodal receipt extraction"}, {"href": "https://www.braintrust.dev/docs/cookbook/recipes/ReleaseNotes", "anchor": "Generating release notes and hill-climbing to improve them"}, {"href": "https://www.braintrust.dev/docs/cookbook/recipes/SimpleQA", "anchor": "Evaluating SimpleQA"}, {"href": "https://www.braintrust.dev/docs/cookbook/recipes/SimpleRagas", "anchor": "Optimizing Ragas to evaluate a RAG pipeline"}, {"href": "https://www.braintrust.dev/docs/cookbook/recipes/SpamClassifier", "anchor": "Classifying spam using structured outputs"}, {"href": "https://www.braintrust.dev/docs/cookbook/recipes/Text2SQL", "anchor": "Text-to-SQL"}, {"href": "https://www.braintrust.dev/docs/cookbook/recipes/Text2SQL-Data", "anchor": "LLM Eval For Text2SQL"}, {"href": "https://www.braintrust.dev/docs/cookbook/recipes/ToolOCR", "anchor": "Using Python functions to extract text from images"}, {"href": "https://www.braintrust.dev/docs/cookbook/recipes/ToolRAG", "anchor": "Using functions to build a RAG agent"}, {"href": "https://www.braintrust.dev/docs/cookbook/recipes/UnreleasedAI", "anchor": "Unreleased AI: A full stack Next.js app for generating changelogs"}, {"href": "https://www.braintrust.dev/docs/cookbook/recipes/VercelAISDKTracing", "anchor": "Tracing Vercel AI SDK applications"}, {"href": "https://www.braintrust.dev/docs/cookbook/recipes/VideoQA", "anchor": "Evaluating video QA"}, {"href": "https://www.braintrust.dev/docs/cookbook/recipes/VideoQATwelveLabs", "anchor": "Evaluating video QA with Twelve Labs"}, {"href": "https://www.braintrust.dev/docs/cookbook/recipes/VoiceAgent", "anchor": "Evaluating a voice agent"}, {"href": "https://www.braintrust.dev/docs/cookbook/recipes/WebAgent", "anchor": "Evaluating a web agent"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/", "anchor": "Braintrust"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}], "depth": 1}, "https://www.braintrust.dev/docs/changelog": {"url": "https://www.braintrust.dev/docs/changelog", "title": "Changelog - Docs - Braintrust", "text": "Changelog\nWeek of 2025-09-08\n- Trace tree is now visible in human review mode.\n- BTQL sandbox improvements\n- Loop is now on the page and can write queries, debug errors and answer syntax questions\n- Tabs\n- Simple charts\n- Improved auto-complete\n- Updated UI color palette\n- Custom charts added to the monitor page (requires data plane 1.1.22)\n- View state changes for non-saved views\n- Before: We would attempt to restore any previous edited view state to the URL\n- After: With a few exceptions, edited view state for non-saved views is only represented in the URL\nSDK Integrations: LangChain (JS) (version 0.0.7)\n- Fixed dependency issue that prevented the integration from using the latest braintrust SDK.\nPython SDK version 0.2.7 (upcoming)\n- Fixed an OpenAI Agents concurrency bug that incorrectly handled root propagation of input/output.\n- Fixed parent span precedence issues for better trace hierarchy\n- Support locking down remote evals via\n--dev-org-name\nto only accept users from your org. - Added\nupdate-stack-url\nCLI option to explicitly change the URL of the data plane.\nTypeScript SDK version 0.3.8 (upcoming)\n- Prevents logging Braintrust API keys when logging Span objects.\n- Improved error messages when we fail to find evaluators or code definitions.\nData plane (1.1.22)\n- Added ability to create and edit custom charts in the monitor dashboard\n- Added support for more Grok models and improved model refresh handling in\n/invoke\nendpoint - Added support for\nIN\nclause in BTQL queries - Improved processing of pydantic-ai OpenTelemetry spans with tool names in span names and proper input/output field mapping\n- Added OpenAI Agents logs formatter for better span rendering in the UI\n- Added retention support for Postgres WAL and object WAL (write-ahead logs)\n- Add S3 lifecycle policies to reclaim additional space from bucket\n- Added authentication support for remote evaluation endpoints\n- Improved ability to fetch all datasets efficiently\n- New\nMAX_LIMIT_FOR_QUERIES\nparameter to set the maximum allowable limit for BTQL queries. Larger result sets can still be queried through pagination\nAutoevals PY (version 0.0.130)\n- Fold the\nbraintrust_core\nexternal package into theautoevals\npackage, since it is the only user ofbraintrust_core\n. Future braintrust packages will not depend on thebraintrust_core\npy package.\nWeek of 2025-09-01\n- Loop can search through Braintrust's docs and blog posts to help you answer questions about how to use Braintrust, including generating sample code.\nWeek of 2025-08-25\n- Traces in the trace viewer on the logs page can now show all associated traces based on a metadata field or tag.\nTypeScript SDK version 0.3.7\n- Support locking down remote evals via\n--dev-org-name\nto only accept users from your org. - Fixed parent span precedence issues for better trace hierarchy\n- Improved propagation of parentSpanId into parentSpanContext for OpenTelemetry JS v2 compatibility\n- Fold the\n@braintrust/core\npackage intobraintrust\n. This package consists of a small set of utility functions that is more easily-managed as part of the mainbraintrust\npackage. After version0.3.7\n, you should no longer need a dependency on@braintrust/core\n.\nPython SDK version 0.2.6\n- Python SDK now correctly nests spans logged from inside tool calls in OpenAI Agents\nTypeScript SDK version 0.3.6\n- OpenAI responses wrapper no longer filters out span data fields when logging\n- Fixed\nwithResponse\nandwrapOpenAI\ninteraction to not hide response data\nData plane (1.1.21)\n- Process pydantic-ai OTel spans\n- AI proxy now supports temperature > 1 for models which allow it\n- Preview of data retention on logs, datasets, and experiments\nWeek of 2025-08-18\n- Monitor page layout changed to be more responsive to screen size\n- Various UX improvements to prompt dialog\n- Improved onboarding experience\n- Trace timeline layout improvements\nData plane (1.1.20)\n- Brainstore vacuum is enabled by default. This will reclaim space from object storage. As a bonus, vacuum also cleans up more data (segment-level write-ahead logs)\n- AI proxy now dynamically fetches updates to the model registry\n- Performance improvements to summary,\nIS NOT NULL\n, and!= NULL\nqueries - Handle cancelled BTQL queries earlier and optimize schema inference queries\n- Added a REST API for managing service tokens. See docs\n- Support custom columns on the experiments page\n- Aggregate custom metrics and include more built-in agent metrics in experiments and logs\n- Preview of data retention on logs. You can define a per-project policy on logs which will be deleted on a schedule and no longer available in the UI and API\nWeek of 2025-08-18\nPython SDK version 0.2.5\n- Support data masking (see docs)\n- Remote evals in Python SDK\n- Support tags in Eval hooks\n- Validate attachment file readability at creation time\nTypeScript SDK version 0.2.5\n- Support data masking (see docs)\n- Support tags in Eval hooks\n- Validate attachment file readability at creation time\nSDK Integrations: Google ADK (Python) (version 0.1.1)\n- Added integration with Google Agent Development Kit (ADK).\nPython SDK version 0.2.4\n- Allow non-batch span processors in\nBraintrustSpanProcessor\n.\nWeek of 2025-08-11\n- Pro plan organizations can now downgrade to the Free plan via the settings page without contacting support\n- Prevent read-only users from downloading data from the UI\nPython SDK version 0.2.3\n- Fix openai-agents to inherit the right tracing context\nTypeScript SDK version 0.2.4\n- Support OpenAI Agents SDK\nSDK Integrations: OpenAI Agents (TS) (version 0.0.2)\n- Fix openai-agents to inherit the right tracing context\nData plane (1.1.19)\n- Add support for GPT-5 models\n- OTel tracing support for Google Agent Development Kit\n- OTel support for deleting fields\n- Fix binder error handling for malformed BTQL queries\n- Enable environment tags on prompt versions\nWeek of 2025-08-04\n- @mention team members in comments to notify them via email. To mention someone, type \"@\" and a team member's name or email in any comment input.\n- You can now assign users to rows in experiments, logs, and datasets. Once assigned, you can filter rows by a specific user or a group of users.\n- View configuration has been changed to no longer auto-save changes. It now shows a dirty state and you have the option of saving or resetting those changes back to the base view.\nPython SDK version 0.2.2\n- Added\nenvironment\nparameter toload_prompt\n- The Otel SpanProcessor now keeps\ntraceloop.*\nspans by default. - Experiments can now be run without sending results to the server.\n- Span creation is significantly faster in Python.\nTypeScript SDK version 0.2.3\n- Added\nenvironment\nparameter toload_prompt\n- The Otel SpanProcessor now keeps\ntraceloop.*\nspans by default. - Experiments can now be run without sending results to the server.\n- Fix\nnpx braintrust pull\nfor large prompts\nTypeScript SDK version 0.2.2\n- Fix ai-sdk tool call formatting in output\n- Log OpenAI Agents input and output to root span\n- Wrap OpenAI responses.parse\n- Add wrapTraced support for generator functions\nPython SDK version 0.2.1\n- Fix langchain-py integration tracing when users use a @traced method\n- Wrap OpenAI responses.parse\n- Add @traced support for generator functions\nWeek of 2025-07-28\n- New improved UI for trace tree.\n- Token and cost metrics are computed per sub-tree in the trace viewer.\n- Download BTQL sandbox results as JSON or CSV\nData plane (1.1.18)\nThis is our largest data plane release in a while, and it includes several significant performance improvements, bug fixes, and new features:\n- Improve performance for non-selective searches. Eg make\nfoo != 'bar'\nfaster. - Improve performance for score filters. Eg make\nscores.correctness = 0\nfaster. - Improve group by performance. This should make the monitor page and project summary page significantly faster.\n- Add syntax for explicit casting. You can now use explicit casting functions to cast data to any datatype. e.g.\nto_number(input.foo)\n,to_datetime(input.foo)\n, etc. - Fix ILIKE queries on nested json: ILIKE queries previously returned incorrect results on nested json objects. ILIKE now works as expected for all json objects.\n- Improve backfill performance. New objects should get picked up faster.\n- Improve compaction latency. Indexing should kick in much faster, and in particular, this means data gets indexed a lot faster.\n- Improved support for OTel mappings, including the new GenAI Agent conventions and strands framework.\n- Add Gemini 2.5 Flash-Lite GA, GPT-OSS models on several providers, and Claude Opus 4.1.\nWeek of 2025-07-21\n- Moved monitor chart legends to the bottom and increased chart heights.\n- Fixed a monitor chart issue where the series toggle selector would filter the incorrect series.\n- Improved monitor fullscreen experience: charts now open faster and retain their series filter state.\n- Loop is now available in the experiments page and has a new ability to render interactive components inside the chat that will help you find the UI element that Loop is referencing.\n- You can now use remote evals with the \"+Experiment\" button to create a new experiment. Previously, they were only available in the playground.\nTypeScript SDK version 0.2.1\n- Fix support for the\nopenai.chat.completions.parse\nmethod when used withwrapOpenAI\n. - Added support for ai-sdk@beta with new\nBraintrustMiddleware\n- Support running remote evals as full experiments.\nTypeScript SDK version 0.2.0\n- When running multiple trials per input (\ntrial_count > 1\n), you can now access the current trial index (0-based) viahooks.trialIndex\nin your task function. - Added\nBraintrustExporter\nin addition toBraintrustSpanProcessor\n. - Bound max ancestors in git to 1,000.\nPython SDK version 0.2.0\n- When running multiple trials per input (\ntrial_count > 1\n), you can now access the current trial index (0-based) viahooks.trial_index\nin your task function. - New LiteLLM\nwrap_litellm\nwrapper. - Increase max ancestors in git to 1,000.\nData plane (1.1.15)\n- Add ability to run scorers as tasks in the playground\n- You can now use object storage, instead of Redis, as a locks manager.\n- Support async python in inline code functions\n- Don't re-trigger online scoring on existing traces if only metadata fields like\ntags\nchange.\nWeek of 2025-07-14\n- Add monitor page UTC timezone toggle\n- Improved trace view loading performance for large traces.\nPython SDK version 0.1.8\n- Added\nBraintrustSpanProcessor\nto simplify Braintrust's integration with OpenTelemetry.\nTypeScript SDK version 0.1.1\n- Added\nBraintrustSpanProcessor\nto simplify integration with OpenTelemetry.\nData plane (1.1.14)\n- Switch the default query shape from\ntraces\ntospans\nin the API. This means that btql queries will now return 1 row per span, rather than per trace. This change also applies to the REST API. - Service tokens with scoped, user-independent credentials for system integrations.\n- Fix a bug where very large experiments (run through the API) would drop spans if they could not flush data fast enough.\n- Support built-in OTel metrics (contact your account team for more details)\n- New parallel backfiller improves performance of loading data into Brainstore across many projects.\nPython SDK version 0.1.7\n- Added support for loading prompts by ID via the\nload_prompt\nfunction. You can now load prompts directly by their unique identifier:\nTypeScript SDK version 0.1.0\n- Fix a bug where large experiments would drop spans if they could not flush data fast enough.\n- Fix bug in attachment uploading in evals executed with\nnpx braintrust eval\n. - Upgrading zod dependency from\n^3.22.4\nto^3.25.3\n- Added support for loading prompts by ID via the\nloadPrompt\nfunction. You can now load prompts directly by their unique identifier:\nWeek of 2025-07-07\n- Loop can now create custom code scorers in playgrounds\n- Schema builder UI for structured outputs\n- Sort datasets when the\nFaster tables\nfeature flag is enabled - Change LLM duration to be the sum, not average, of LLM duration across spans\n- Add support for Grok 4 and Mistral's Devstral Small Latest\nData plane (1.1.13)\n- Fix support for\nCOALESCE\nwith variadic arguments - Add option to select logs for online scoring with a BTQL filter\n- Add ability to test online scoring configuration on existing logs\n- Mmap based indexing optimization enabled by default for Brainstore\nData plane (1.1.12) [skipped]\nWeek of 2025-06-30\n- Time range filters on the logs page\nData plane (1.1.11)\n- Add support for LLaMa 4 Scout for Cerebras\n- Turn on index validation (which enables self-healing failed compactions) in the Cloudformation by default.\nWeek of 2025-06-23\n- Add support for multi-factor authentication\n- Fix a bug with Vertex AI calls when the request includes the anthropic-beta header\n- Add Zapier integration to trigger Zaps when there's a new automation event or a new project.\nData plane (1.1.7)\n- Improve performance of error count queries in Brainstore\n- Automatically heal segments that fail to compact\n- Add support for new models including o3 pro\n- Improve error messages for LLM-originated errors in the proxy\nAutoevals.js v0.0.130\n- Remove dependency on\n@braintrust/core\nTypeScript SDK version 0.0.209\n- Ensure SpanComponentsV3 encoding works in the browser.\nTypeScript SDK version 0.0.208\n- Ensure running remote evals (i.e.\nrunDevServer\n) works without the CLI wrapper. - Add span + parent ids to\nStartSpanArgs\nWeek of 2025-06-16\n- Add OpenAI's o3-pro model to the playground and AI proxy.\n- View parameters are now present in the url when viewing a default view\n- Experiments charting controls have been added into views\n- Experiment objects now support tags through the API and on the experiments view\n- Add support for Gemini 2.5 Pro, Gemini 2.5 Flash, and Gemini 2.5 Flash Lite\nPython SDK version 0.1.5\n- The SDK's under-the-hood log queue will not block when full\nand has a default size of 25000 logs. You can configure the max size by setting\nBRAINTRUST_LOG_QUEUE_MAX_SIZE\nin your environment. The environment variableBRAINTRUST_QUEUE_DROP_WHEN_FULL\nis no longer used. - Improvements to the logging of parallel tool calls.\n- Attachments are now converted to base64 data URLs, making it easier to work with image attachments in prompts.\nTypeScript SDK version 0.0.207\n- The SDK's under-the-hood queue for sending logs now has a default size of 5000 logs.\nYou can configure the max size by setting\nBRAINTRUST_LOG_QUEUE_MAX_SIZE\nin your environment. - Improvements to the logging of parallel tool calls.\n- Attachments are now converted to base64 data URLs, making it easier to work with image attachments in prompts.\nData plane (1.1.6)\n- Patch a bug in 1.1.5 related to the\nrealtime_state\nfield in the API response.\nData plane (1.1.5)\n- Default query timeout in Brainstore is now 32 seconds.\n- Auto-recompact segments which have been rendered unusable due to an S3-related issue.\n- Gemini 2.5 models\nData plane (1.1.4)\n- Optimize \"Activity\" (audit log) queries, which reduces the query workload on Postgres for large traces (even if you are using Brainstore).\n- Automatically convert base64 payloads to attachments in the data plane. This\nreduces the amount of data that needs to be stored in the data plane and\nimproves page load times. You can disable this by setting\nDISABLE_ATTACHMENT_OPTIMIZATION=true\norDisableAttachmentOptimization=true\nin your stack. - Improve AI proxy errors for status codes 401->409\n- Increase real-time query memory limit to 10GB in Brainstore\nWeek of 2025-06-09\n- Correctly propagate\nexpected\nandmetadata\nvalues to function calls when runninginvoke\n. This means that if you provideexpected\normetadata\n,input\nrefers to the top-level input argument. If you are passing in a value like{input: \"a\"}\n, then you must now use{{input.input}}\nto refer to the string \"a\", if you pass inexpected\normetadata\n. This should have no effect on the playground or scorers. - Chat-like thread layout that simplifies thread display to LLM and score data\n- Enable all agent nodes to access dataset variables with the mustache variable\n{{dataset}}\n. For example, to accessmetadata.foo\nin the third prompt in an agent, you can use{{dataset.metadata.foo}}\n. - Improve reliability of online scoring when logging high volumes of data to a project.\n- Tags can now be sorted in the project configuration page which will change their display order in other parts of the UI.\n- System-only messages are now supported in Anthropic and Bedrock models.\n- Logs page UI can now filter nested data fields in\nmetadata\n,input\n,output\n, andexpected\n.\nPython SDK version 0.1.4\n- Add\nproject.publish()\nto directlypush\nprompts to Braintrust (without runningbraintrust push\n). @traced\nnow works correctly with async generator functions.- The OpenAI and Anthropic wrappers set\nprovider\nmetadata.\nTypeScript SDK version 0.0.206\n- Add support for\nproject.publish()\nto directlypush\nprompts to Braintrust (without runningbraintrust push\n). - The OpenAI and Anthropic wrappers set\nprovider\nmetadata.\nWeek of 2025-06-02\n- Support reasoning params and reasoning tokens in streaming and non-streaming responses in the AI proxy and across the product (requires a stack update to 0.0.74).\n- New braintrust-proxy Python library to help developers integrate with their IDEs to support new reasoning input and output types.\n- New\n@braintrust/proxy/types\nmodule to augment OpenAI libraries with reasoning input and output types. - New streaming protocol between Brainstore and the API server speeds up queries.\n- Time brushing interaction enabled on Monitor page charts.\n- Can create user-defined views in the monitoring page.\n- Live updating time mode added to the monitoring page.\n- The\nanthropic\npackage is now included by default in Python functions. - Audit log queries must now specify an\nid\nfilter for the set of rows to fetch. These queries will only return the audit log for the specified rows, rather than the whole trace. - (Beta) continuously export logs, experiments, and datasets to S3.\n- Enable passing\nmetadata\nandexpected\nas arguments to the first agent prompt node.\nPython SDK version 0.1.3\n- Improve retry logic in the control plane connection (used to create new experiments and datasets).\nWeek of 2025-05-26\n- The \"Faster tables\" flag is now the default (you may need to update your data plane if you are self-hosted). You should notice experiments, datasets, and the logs page load much faster.\n- Add Claude 4 models in Bedrock and Vertex to the AI proxy and playground.\n- Braintrust now incorporates cached tokens into the cost calculations for experiments and logs. The monitor page also now includes separate lines so you can track costs and counts for uncached, cached, and cache creation tokens.\n- Native support for thinking parameters in the playground.\nPython SDK version 0.1.2\n- Added support for\nmetadata\nandtags\narguments toinvoke\n. - The SDK now gracefully handles OpenAI's\nNotGiven\nparameter. - Added\nspan.link()\nto synchronously generate permalinks.\nTypeScript SDK version 0.0.206 [upcoming]\n- Add support for\nmetadata\nandtags\narguments toinvoke\n.\nWeek of 2025-05-19\n- Improved playground prompt editor stability and performance.\n- Capture cached tokens from OpenAI and Anthropic models in a unified format and surface them in the UI.\n- Create experiments from the experiments list page using saved prompts/agents.\n- New BTQL sandbox page and editor with autocomplete\n- Fullscreen-able monitor charts\n- Added a 'Copy page' button to the top of every docs page.\n- Brainstore now supports vacuuming data from object storage to reclaim space. If you are self-hosted, please reach out to Braintrust support to learn more about the feature.\n- Organization owners can manage API keys for all users in their organization in the UI.\n- Add endpoint for admins to list all ACLs within an org.\nTypeScript SDK version 0.0.205\n- Make the\n_xact_id\nfield inorigin\noptional. - Added\nspan.link()\nas a synchronous means of generating permalinks.\nPython SDK version 0.1.1\n- Update cached token accounting in\nwrap_anthropic\nto correctly capture cached tokens. - Pull additional metadata in\nbraintrust pull\nfor prompts and functions to improve tracing.\nTypeScript SDK version 0.0.204\n- Update cached token accounting in\nwrapAnthropic\nto correctly capture cached tokens.\nWeek of 2025-05-12\n- Collapsible sidebar navigation\n- Command bar (CMD/CTRL+K) to quickly navigate and between pages and projects\n- View monitor page logs across all projects in an organization\nSDK (version 0.1.0)\n- Allow custom model descriptions in Braintrust.\n- Improve support for PDF attachments to multimodal OpenAI models.\nThe Python library no longer has a dependency on braintrust_core\n.\nbraintrust_core\nwill be deprecated in the near future, but the package will\nremain on PyPI. If you wrote code that directly imports from braintrust_core\n, you can\neither:\n- Change your imports to\nfrom braintrust.score import Score, Scorer\n(preferred) - or, add\nbraintrust_core\nto your project's dependencies.\nThe TypeScript SDK also includes a small packaging bugfix.\nSDK (version 0.0.203)\n- Add new reasoning to OpenAI messages\nWeek of 2025-05-05\n- Added Mistral Medium 3 and Gemini 2.5 Pro Preview to the AI proxy and playground.\n- Self-hosted builds now log in a structured JSON format that is easier to parse.\nSDK (version 0.0.202)\n- Gracefully handle experiment summarization failures in Eval()\n- Fix a bug where\nwrap_openai\nwas breakingpydantic_ai run_stream\nfunc. - Add tracing to the\nclient.beta.messages\ncalls in the TypeScript Anthropic library. - Fix some deprecation warnings in the Python SDK.\nWeek of 2025-04-28\n- Permission groups settings page now allows admins to set group-level permissions (i.e. which users can read, delete, and add/remove members from a particular group)\n- Automations alpha: trigger webhooks based on log events\nWeek of 2025-04-21\n- Preview attachments in playground input cells.\n- Playground now support list mode which includes score and metric summaries.\n- Handle structured outputs from OpenAI's responses API in the \"Try prompt\" experience.\nSDK (version 0.0.201)\n- Support OpenAI\nclient.beta.chat.completions.parse\nin the Python wrapper.\nSDK (version 0.0.200)\n- Ensure the prompt cache properly handles any manner of prompt names.\n- Ensure the output of\nanthropic.messages.create\nis properly traced when called withstream=True\nin an async program.\nWeek of 2025-04-14\n- Allow users to remove themselves from any organization they are part of using\nthe\n/v1/organization/members\nREST endpoint. - Group monitor page charts by metadata path.\n- Download playground contents as CSV.\n- Add pending and streaming state indicators to playground cells.\n- Distinguish per-row and global playground progress.\n- Added GPT-4.1, o4-mini and o3 to the AI proxy and playground.\n- On the monitor page, add aggregate values to chart legends.\n- Add Gemini 2.5 Flash Preview model to the AI proxy and playground.\n- Add support for audio and video inputs for Gemini models in the AI proxy and playground.\n- Add support for PDF files for OpenAI models.\n- Native tracing support in the proxy has finally arrived! Read more in the docs\n- Upload attachments directly in the UI in datasets, playgrounds, and prompts (requires a stack update to 0.0.67).\nSDK (version 0.0.199)\n- Fix a bug that broke async calls to the Python version of\nanthropic.messages.create\n. - Store detailed metrics from OpenAI's\nchat.completion\nTypeScript API.\nSDK (version 0.0.198)\n- Trace the\nopenai.responses\nendpoint in the Typescript SDK. - Store the\ntoken_details\nmetrics return by theopenai/responses\nAPI.\nWeek of 2025-04-07\n- Playground option to append messages from a dataset to the end of a prompt\n- A new toggle that lets you skip tracing scoring info for online scoring. This is useful when you are scoring old logs and don't want to hurt search performance as a result.\n- GIF and image support in comments\n- Add embedded view and download action for inline attachments of supported file types\nAPI (version 0.0.65)\n- Improve error messages when trying to insert invalid unicode\nSDK (version 0.0.197)\n- Fix a bug in\ninit_function\nin the Python SDK which prevented theinput\nargument from being passed to the function correctly when it was used as a scorer. - Support setting\ndescription\nandsummarizeScores\n/summarize_scores\ninEval(...)\n.\nAPI (version 0.0.65)\n- Backend support for appending messages.\nWeek of 2025-03-31\n- Many improvements to the playground experience:\n- Fixed many crashes and infinite loading spinner states\n- Improved performance across large datasets\n- Better support for running single rows for the first time\n- Fixed re-ordering prompts\n- Fixed adding and removing dataset rows\n- You can now re-run specific prompts for individual cells and columns\n- You can now do \"does not contain\" filters for tags in experiments and datasets. Coming soon to logs!\n- When you\ninvoke()\na function, inline base64 payloads will be automatically logged as attachments. - Add a strict mode to evals and functions which allows you to fail test cases when a variable is not present in a prompt. Without strict mode, prompts will always render (and sometimes miss variables). With strict mode on, these variables show clearly as errors in the playground and experiments.\n- Add Fireworks' DeepSeek V3 03-24 and DeepSeek R1 (Basic), along with Qwen QwQ 32B in Fireworks and Together.ai, to the playground and AI proxy.\n- Fix bug that prevented Databricks custom provider form from being submitted without toggling authentication types.\n- Unify Vertex AI, Azure, and Databricks custom provider authentication inputs.\n- Add Llama 4 Maverick and Llama 4 Scout models to Together.ai, Fireworks, and Groq providers in the playground and AI proxy.\n- Add Mistral Saba and Qwen QwQ 32B models to the Groq provider in the playground and AI proxy.\n- Add Gemini 2.5 Pro Experimental and Gemini 2.0 Flash Thinking Mode models to the Vertex provider in the playground and AI proxy.\nAPI (version 0.0.64)\n- Brainstore is now set as the default storage option\n- Improved backfilling performance and overall database load\n- Enabled relaxed search mode for ClickHouse to improve query flexibility\n- Added strict mode option to prompts that fails when required template arguments are missing\n- Enhanced error reporting for missing functions and eval failures\n- Fixed streaming errors that previously resulted in missing cells instead of visible error states\n- Abort evaluations on server when stopped from playground\n- Added support for external bucket attachments\n- Improved handling of large base64 images by converting them to attachments\n- Fixed proper handling of UTF-8 characters in attachment filenames\n- Added the ability to set telemetry URL through admin settings\nSDK (version 0.0.196)\n- Adding Anthropic tracing for our TypeScript SDK. See\nbraintrust.wrapAnthropic\n. - The SDK now paginates datasets and experiments, which should improve performance for large datasets and experiments.\n- Add\nstrict\nflag toinvoke\nwhich implements the strict mode described above. - Raise if a Python tool is pushed without without defined parameters, instead of silently not showing the tool in the UI.\n- Fix Python OpenAI wrapper to work for older versions of the OpenAI library without\nresponses\n. - Set time_to_first_token correctly from AI SDK wrapper\nWeek of 2025-03-24\n- Add OpenAI's o1-pro model to the playground and AI proxy.\n- Support OpenAI Responses API in the AI proxy.\n- Add support for the Gemini 2.5 Pro Experimental model in the playground and AI proxy.\n- Option to disable the experiment comparison auto-select behavior\n- Add support for Databricks custom provider as a default cloud provider in the playground and AI proxy.\n- Allow supplying a base API URL for Mistral custom providers in the playground and AI proxy.\n- Support pushed code bundles larger than 50MB.\nSDK (version 0.0.195)\n- Improve the metadata collected by the Anthropic client.\n- Anthropic client can now be run with\nbraintrust.wrap_anthropic\n- Fix a bug when\nmessages.create\nwas called withstream=True\nSDK (version 0.0.194)\n- Add Anthropic tracing to the Python SDK with\nwrap_anthropic_client\n- Fix a bug calling\nbraintrust.permalink\nwithNoopSpan\nSDK (version 0.0.193)\n- Fix retry bug when downloading large datasets/experiments from the SDK\n- Background logger will load environment variables upon first use rather than when module is imported.\nWeek of 2025-03-17\n- The OTEL endpoint now understands structured output calls from the Vercel AI\nSDK. Logging via\ngenerateObject\nandstreamObject\nwill populate the schema in Braintrust, allowing the full prompt to be run. - Added support for\nconcat\n,lower\n, andupper\nstring functions in BTQL. - Correctly propagate Bedrock streaming errors through the AI proxy and playground.\n- Online scoring supports sampling rates with decimal precision.\nSDK (version 0.0.192)\n- Improve default retry handler in the python SDK to cover more network-related exceptions.\nAutoevals (version 0.0.124)\n- Added\ninit\nto set a global default client for all evaluators (Python and Node.js). - Added\nclient\nargument to all evaluators to specify the client to use. - Improved the Autoevals docs with more examples, and Python reference docs now include moderation, ragas, and other evaluators that were missing from the initial release.\nWeek of 2025-03-10\n- Added support for OpenAI GPT-4o Search Preview and GPT-4o mini Search Preview in the playground and AI proxy.\n- Add support for making Anthropic and Google-format requests to corresponding models in the AI proxy.\n- Fix bug in model provider key modal that prevents submitting a Vertex provider with an empty base URL.\n- Add column menu in grid layout with sort and visibility options.\n- Enable logging the\norigin\nfield through the REST API\nAutoevals (version 0.0.123)\n- Swapped\npolyleven\nforlevenshtein\nfor faster string matching.\nSDK Integrations: LangChain (Python) (version 0.0.2)\n- Add a new\nbraintrust-langchain\nintegration with an improvedBraintrustCallbackHandler\nandset_global_handler\nto set the handler globally for all LangChain components.\nSDK Integrations: LangChain.js (version 0.0.6)\n- Small improvement to avoid logging unhelpful LangGraph spans.\n- Updated peer dependencies with LangChain core that fixes the global handler for LangGraph runs.\nSDK Integrations: Val Town\n- New\nval.town\nintegration with example vals to quickly get started with Braintrust.\nSDK (version 0.0.190)\n- Fix\nprompt pull\nfor long prompts. - Fix a bug in the Python SDK which would not retry requests that were severed after a connection timeout.\nSDK (version 0.0.189)\n- Added integration with OpenAI Agents SDK.\nSDK (version 0.0.188)\n- Deprecated\nbraintrust.wrapper.langchain\nin favor of the newbraintrust-langchain\npackage.\nWeek of 2025-03-03\n- Add support for \"image\" pdfs in the AI proxy. See the proxy docs for more details.\n- Fix issue in which code function executions could hang indefinitely.\n- Add support for custom base URLs for Vertex AI providers.\n- Add dataset column to experiments table.\n- Add python3.13 support to user-defined functions.\n- Fix bug that prevented calling Python functions from the new unified playground.\nSDK (version 0.0.187)\n- Always bundle default python packages when pushing code with\nbraintrust push\n. - Fix bug in the TypeScript SDK where\nasyncFlush\nwas not correctly defaulted to false. - Fix a bug where\nspan_attributes\nfailed to propagate to child spans through propagated events.\nWeek of 2025-02-24\n- Add support for removing all permissions for a group/user on an object with a single click.\n- Add support for Claude 3.7 Sonnet model.\n- Add llms.txt for docs content.\n- Enable spellcheck for prompt message editors.\n- Add support for Anthropic Claude models in Vertex AI.\n- Add support for Claude 3.7 Sonnet in Bedrock and Vertex AI.\n- Add support for Perplexity R1 1776, Mistral Saba, Gemini LearnLM, and more Groq models.\n- Support system instructions in Gemini models.\n- Add support for Gemini 2.0 Flash-Lite, and remove preview model, which no longer serves requests.\n- Add support for default Bedrock cross-region inference profiles in the playground and AI proxy.\n- Move score distribution charts to the experiment sidebar.\n- Add support for OpenAI GPT-4.5 model in the playground and AI proxy.\n- Add deprecation warning for\n_parent_id\nfield in the REST API (docs). This field will be removed in a future release.\nAPI (version 0.0.63)\n- Support for Claude 3.7 Sonnet, Gemini 2.0 Flash-Lite, and several other models in the proxy.\n- Stability and performance improvements for ETL processes.\n- A new\n/status\nendpoint to check the health of Braintrust services.\nSDK (version 0.0.187)\n- Added support for handling score values when an Eval has errored.\nWeek of 2025-02-17\n- Add support for stop sequences in Anthropic, Bedrock, and Google models.\n- Resolve JSON Schema references when translating structured outputs to Gemini format.\n- Add button to copy table cell contents to clipboard.\n- Add support for basic Cache-Control headers in the AI proxy.\n- Add support for selecting all or none in the categories of permission dialogs.\n- Respect Bedrock providers not supporting streaming in the AI proxy.\nSDK (version 0.0.187)\n- Improve support for binary packages in\nnpx braintrust eval\n. - Support templated structured outputs.\n- Fix dataset summary types in Typescript.\nWeek of 2025-02-10\n- Store table grouping, row height, and layout options in the view configuration.\n- Add the ability to set a default table view.\n- Add support for Google Cloud Vertex AI in the playground and proxy. Google Cloud auth is supported for principals and service accounts via either OAuth 2.0 token or service account key.\n- Add default cloud providers section to the organization AI providers page.\n- Support streaming responses from OpenAI o1 models in the playground and AI proxy.\nWeek of 2025-02-03\n- Add complete support for Bedrock models in the playground and AI proxy; this includes support for system prompts, tool calls, and multimodal inputs.\n- Fix model provider configuration issues in which custom models could clobber default models, and different providers of the same type could clobber each other.\n- Fix bug in streaming JSON responses from non-OpenAI providers.\n- Supported templated structured outputs in experiments run from the playground.\n- Support structured outputs in the playground and AI proxy for Anthropic models, Bedrock models, and any OpenAI-flavored models that support tool calls, e.g. LLaMa on Together.ai.\n- Support templated custom headers for custom AI providers. See the proxy docs for more details.\n- Added and updated models across all providers in the playground and AI proxy.\n- Support tool usage and structured outputs for Gemini models in the playground and AI proxy.\n- Simplify playground model dropdown by showing model variations in a nested dropdown.\nWeek of 2025-01-27\n- Add support for duplicating prompts, scorers, and tools.\n- Fix pagination for the\n/v1/prompt\nREST API endpoint. - \"Unreviewed\" default view on experiment and logs tables to filter out rows that have been human reviewed.\n- Add o3-mini to the AI proxy and playground.\n- Scorer dropdown now supports using custom scoring functions across projects.\nSDK Integrations: LangChain.js (version 0.0.5)\n- Less noisy logging from the LangChain.js integration.\n- You can now pass a\nNOOP_SPAN\nto theBraintrustCallbackHandler\nto disable logging. - Fixes a bug where the LangChain.js integration could not handle null/undefined values in chain inputs/outputs.\nSDK (version 0.0.184)\nspan.export()\nwill no longer throw if braintrust is down- Improvement to the Python prompt rendering to correctly render formatted messages, LLM tool calls, and other structured outputs.\nWeek of 2025-01-20\n- Drag and drop to reorder span fields in experiment/log traces and dataset rows. On wider screens, fields can also be arranged side-by-side.\n- Small convenience improvement to the BTQL Sandbox to avoid having to add include\nfilter:\nto an advanced filter clause. - Add an attachments browser to view all attachments for a span in a sidebar. To open the attachments browser, expand the trace and click the arrow icon in the attachments section. It will only be visible when the trace panel is wide enough.\nSDK (version 0.0.183)\n- Fix a bug related to\ninitDataset()\nin the Typescript SDK creating links inEval()\ncalls. - Fix a few type checking issues in the Python SDK.\nWeek of 2025-01-13\n- Add support for setting a baseline experiment for experiment comparisons. If a baseline experiment is set, it will be chosen by default as the comparison when clicking on an experiment.\n- UI updates to experiment and log tables.\n- Trace audit log now displays granular changes to span data.\n- Start/end columns shown as dates/times.\n- Non-existent trace records display an error message instead of loading indefinitely.\nSDK Integrations: LangChain.js (version 0.0.4)\n- Support logging spans from inside evals in the LangChain.js integration.\nSDK (version 0.0.182)\n- Improved logging for moderation models from the SDK wrappers.\nWeek of 2025-01-06\n- Creating an experiment from a playground now correctly renders prompts with\ninput\n,metadata\n,expected\n, andoutput\nmapped fields. - Fixes small bug where\ninput.output\ndata could pollute the dataset'soutput\nwhen rendering the prompts. - The AI proxy now includes\nx-bt-used-endpoint\nas a response header. It specifies which of your configured AI providers was used to complete the request. - Add support for deeplinking to comments within spans, allowing users to easily copy and share links to comments.\n- In Human Review mode, display all scores in a form.\n- Experiment table rows can now be sorted based on score changes and regressions for each group, relative to a selected comparison experiment.\n- The OTEL endpoint now converts attributes under the\nbraintrust\nnamespace directly to the corresponding Braintrust fields. For example,braintrust.input\nwill appear asinput\nin Braintrust. See the tracing guide for more details. - New OTEL attributes that accept JSON-serialized values have been added for convenience:\ngen_ai.prompt_json\ngen_ai.completion_json\nbraintrust.input_json\nbraintrust.output_json\nFor more details, see the tracing guide.\n- Experiment tables and individual traces now support comparing trial data between experiments.\nSDK (version 0.0.181)\n- Add\nReadonlyAttachment.metadata\nhelper method to fetch a signed URL for downloading the attachment metadata.\nSDK (version 0.0.179)\n- New\nhook.expected\nfor reading and updating expected values in the Eval framework. - Small type improvements for\nhook\nobjects. - Fixed a bug to enable support for\ninit_function\nwith LLM scorers in Python. - Support nested attachments in Python.\nWeek of 2024-12-30\n- Add support for free-form human review scores (written to the\nmetadata\nfield).\nSDK (version 0.0.179) (unreleased)\n- Add support for imports in Python functions pushed to Braintrust via\nbraintrust push\n.\nSDK (version 0.0.178)\n- Cache prompts locally in a two-layered memory/disk cache, and attempt to use this cache if the prompt cannot be fetched from the Braintrust server.\n- Support for using custom functions that are stored in Braintrust in evals. See the docs for more details.\n- Add support for running traced functions in a\nThreadPoolExecutor\nin the Python SDK. See the customize traces guide for more information. - Improved formatting of spans logged from the Vercel AI SDK's\ngenerateObject\nmethod. The logged output now matches the format of OpenAI's structured outputs. - Default to\nasyncFlush: true\nin the TypeScript SDK. This is usually safe since Vercel and Cloudflare both havewaitUntil\n, and async flushes mean that clients will not be blocked if Braintrust is down.\nSDK integrations: LangChain.js (version 0.0.2)\n- Add support for initializing global LangChain callback handler to avoid manually passing the handler to each LangChain object.\nWeek of 2024-12-16\nAPI (version 0.0.61)\n- Upgraded to Node.js 22 in Docker containers.\nSDK (version 0.0.177)\n- Support for creating and pushing custom scorers from your codebase\nwith\nbraintrust push\n. Read the guides to scorers for more information.\nWeek of 2024-12-09\n- Add support for structured outputs in the playground.\n- Sparkline charts added to the project home page.\n- Better handling of missing data points in monitor charts.\n- Clicking on monitor charts now opens a link to traces filtered to the selected time range.\n- Add\nEndpoint supports streaming\nflag to custom provider configuration. The AI proxy will convert non-streaming endpoints to streaming format, allowing the provider's models to be used in the playground. - Experiments chart can be resized vertically by dragging the bottom of the chart.\n- BTQL sandbox to explore project data using Braintrust Query Language.\n- Add support for updating span data from custom span iframes.\nAutoevals (version 0.0.110)\n- Python Autoevals now support custom clients when calling evaluators. See docs for more details.\nSDK (version 0.0.176)\n- New\nhook.metadata\nfor reading and updating Eval metadata when using theEval\nframework. Previoushook.meta\nis now deprecated.\nSDK integrations: LangChain.js (version 0.0.1)\n- New LangChain.js integration to export traces from\nlangchainjs\nruns.\nSDK integrations: LangChain.js (version 0.0.1)\n- New LangChain.js integration to export traces from\nlangchainjs\nruns.\nWeek of 2024-12-02\n- Significantly speed up loading performance for experiments and logs, especially with lots of spans.\nThis speed up comes with a few changes in behavior:\n- Searches inside experiments will only work over content in the tabular view, rather than over the full trace.\n- While searching on the logs page, realtime updates are disabled.\n- Starring rows in experiment and dataset tables now supported.\n- \"Order by regression\" option in experiment column menu can now be toggled on and off without losing previous order.\n- Add expanded timeline view for traces.\n- Added a 'Request count' chart to the monitor page.\n- Add headers to custom provider configuration which the AI proxy will include in the request to the custom endpoint.\n- The logs viewer now supports exporting the currently loaded rows as a CSV or JSON file.\nAPI (version 0.0.60)\n- Make PG_URL configuration more uniform between nodeJS and python clients.\nSDK (version 0.0.175)\n- Fix bug with serializing ReadonlyAttachment in logs\nWeek of 2024-11-25\n- Experiment columns can now be reordered from the column menu.\n- You can now customize legends in monitor charts. Select a legend item to highlight its data, Shift (\u21e7) + Click to select multiple items, or Command (\u2318) / Ctrl (\u2303) + Click to deselect.\nSDK (version 0.0.174)\n- AI SDK fixes: support for image URLs and properly formatted tool calls so \"Try prompt\" works in the UI.\nSDK (version 0.0.173)\n- Attachments can now be loaded when iterating an experiment or dataset.\nSDK (version 0.0.172)\n- Fix a bug where\nbraintrust eval\ndid not respect certain configuration options, likebase_experiment_id\n. - Fix a bug where\ninvoke\nin the Python SDK did not properly stream responses.\nWeek of 2024-11-18\n- The Traceloop OTEL integration now uses the input and output attributes to populate the corresponding fields in Braintrust.\n- The monitor page now supports querying experiment metrics.\n- Removed the\nfilters\nparam from the REST API fetch endpoint. For complex queries, we recommend using the/btql\nendpoint (docs). - New experiment summary layout option, a url-friendly view for experiment summaries that respects all filters.\n- Add a default limit of 10 to all fetch and\n/btql\nrequests for project_logs. - You can now export your prompts from the playground as code snippets and run them through the AI proxy.\n- Add a fallback for the \"add prompt\" dropdown button in the playground, which will search for prompts within the current project if the cross-org prompts query fails.\nSDK (version 0.0.171)\n- Add a\n.data\nmethod to theAttachment\nclass, which lets you inspect the loaded attachment data.\nWeek of 2024-11-12\n- Support for creating and pushing custom Python tools and prompts from your codebase with\nbraintrust push\n. Read the guides to tools and prompts for more information. - You can now view grouped summary data for all experiments by selecting Include comparisons in group from the Group by dropdown inside an experiment.\n- The experiments page now supports downloading as CSV/JSON.\n- Downloading or duplicating a dataset in the UI now properly copies all dataset rows.\n- You can now view a score data as a bar chart for your experiments data by selecting Score comparison from the X axis selector.\n- Trials information is now shown as a separate column in diff mode in the experiment table.\n- Cmd/Ctrl + S hotkey to save from prompts in the playground and function dialogs.\nSDK (version 0.0.170)\n- Support uploading file attachments in the Python SDK.\n- Log, feedback, and dataset inputs to the Python SDK are now synchronously deep-copied for more consistent logging.\nSDK (version 0.0.169)\n- The Python SDK\nEval()\nfunction has been split intoEval()\nandEvalAsync()\nto make it clear which one should be called in an asynchronous context. The behavior ofEval()\nremains unchanged. However,Eval()\ncallers running in an asynchronous context are strongly recommended to switch toEvalAsync()\nto improve type safety. - Improved type annotations in the Python SDK.\nSDK (version 0.0.168)\n- A new\nSpan.permalink()\nmethod allows you to format a permalink for the current span. See TypeScript docs or Python docs for details. braintrust push\nsupport for Python tools and prompts.\nWeek of 2024-11-04\n- The Braintrust AI Proxy now supports the OpenAI Realtime API, providing observability for voice-to-voice model sessions and simplifying backend infrastructure.\n- Add \"Group by\" functionality to the monitor page.\n- The experiment table can now be visualized in a grid layout, where each column represents an experiment to compare long-form outputs side-by-side.\n- 'Select all' button in permission dialogs\n- Create custom columns on dataset, experiment and logs tables from\nJSON\nvalues ininput\n,output\n,expected\n, ormetadata\nfields.\nAPI (version 0.0.59)\n- Fix permissions bug with updating org-scoped env vars\nWeek of 2024-10-28\n- The Braintrust AI Proxy can now issue temporary credentials to access the proxy for a limited time. This can be used to make AI requests directly from frontends and mobile apps, minimizing latency without exposing your API keys.\n- Move experiment score summaries to the table column headers. To view improvements and regressions per metadata or input group, first group the table by the relevant field. Sooo much room for [table] activities!\n- You now receive a clear error message if you run out of free tier capacity while running an experiment from the playground.\n- Filters on JSON fields now support array indexing, e.g.\nmetadata.foo[0] = 'bar'\n. See docs.\nSDK (version 0.0.168)\ninitDataset()\n/init_dataset()\nused inEval()\nnow tracks the dataset ID and links to each row in the dataset properly.\nWeek of 2024-10-21\n- Preview file attachments in the trace view.\n- View and filter by comments in the experiment table.\n- Add table row numbers to experiments, logs, and datasets.\nSDK (version 0.0.167)\n- Support uploading file attachments in the TypeScript SDK.\n- Log, feedback, and dataset inputs to the TypeScript SDK are now synchronously deep-copied for more consistent logging.\n- Address an issue where the TypeScript SDK could not make connections when running in a Cloudflare Worker.\nAPI (version 0.0.59)\n- Support uploading file attachments.\n- You can now export OpenTelemetry (OTel) traces to Braintrust. See the tracing guide for more details.\nWeek of 2024-10-14\n- The Monitor page now shows an aggregate view of log scores over time.\n- Improvement/Regression filters between experiments are now saved to the URL.\n- Add\nmax_concurrency\nandtrial_count\nto the playground when kicking off evals.max_concurrency\nis useful to avoid hitting LLM rate limits, andtrial_count\nis useful for evaluating applications that have non-deterministic behavior. - Show a button to scroll to a single search result in a span field when using trace search.\n- Indicate spans with errors in the trace span list.\nSDK (version 0.0.166)\n- Allow explicitly specifying git metadata info in the Eval framework.\nSDK (version 0.0.165)\n- Support specifying dataset-level metadata in\ninitDataset/init_dataset\n.\nSDK (version 0.0.164)\n- Add\nbraintrust.permalink\nfunction to create deep links pointing to particular spans in the Braintrust UI.\nWeek of 2024-10-07\n- After using \"Copy to Dataset\" to create a new dataset row, the audit log of the new row now links back to the original experiment, log, or other dataset.\n- Tools now stream their\nstdout\nandstderr\nto the UI. This is helpful for debugging. - Fix prompt, scorer, and tool dropdowns to only show the correct function types.\nSDK (version 0.0.163)\n- Fix Python SDK compatibility with Python 3.8.\nSDK (version 0.0.162)\n- Fix Python SDK compatibility with Python 3.9 and older.\nSDK (version 0.0.161)\n- Add utility function\nspanComponentsToObjectId\nfor resolving the object ID from an exported span slug.\nWeek of 2024-09-30\n- The Github action now supports Python runtimes.\n- Add support for Cerebras models in the proxy, playground, and saved prompts.\n- You can now create span iframe viewers to visualize span data in a custom iframe. In this example, the \"Table\" section is a custom span iframe.\nNOT LIKE\n,NOT ILIKE\n,NOT INCLUDES\n, andNOT CONTAINS\nsupported in BTQL.- Add \"Upload Rows\" button to insert rows into an existing dataset from CSV or JSON.\n- Add \"Maximum\" aggregate score type.\n- The experiment table now supports grouping by input (for trials) or by a metadata field.\n- The Name and Input columns are now pinned\n- Gemini models now support multimodal inputs.\nWeek of 2024-09-23\n- Basic monitor page that shows aggregate values for latency, token count, time to first token, and cost for logs.\n- Create custom tools to use in your prompts and in the playground. See the docs for more details.\n- Set org-wide environment variables to use in these tools\n- Pull your prompts to your codebase using the\nbraintrust pull\ncommand. - Select and compare multiple experiments in the experiment view using the\ncompared with\ndropdown. - The playground now displays aggregate scores (avg/max/min) for each prompt and supports sorting rows by a score.\n- Compare span field values side-by-side in the trace viewer when fullscreen and diff mode is enabled.\nSDK (version 0.0.160)\n- Fix a bug with\nsetFetch()\nin the TypeScript SDK.\nSDK (version 0.0.159)\n- In Python, running the CLI with\n--verbose\nnow uses theINFO\nlog level, while still printing full stack traces. Pass the flag twice (-vv\n) to use theDEBUG\nlog level. - Create and push custom tools from your codebase with\nbraintrust push\n. See docs for more details. TypeScript only for now. - A long awaited feature: you can now pull prompts to your codebase using the\nbraintrust pull\ncommand. TypeScript only for now.\nAPI (version 0.0.56)\n- Hosted tools are now available in the API.\n- Environment variables are now supported in the API (not yet in the standard REST API). See the docker compose file for information on how to configure the secret used to encrypt them if you are using Docker.\n- Automatically backfill\nfunction_data\nfor prompts created via the API.\nWeek of 2024-09-16\n- The tag picker now includes tags that were added dynamically via API, in addition to the tags configured for your project.\n- Added a REST API for managing AI secrets. See docs.\nSDK (version 0.0.158)\n- A dedicated\nupdate\nmethod is now available for datasets. - Fixed a Python-specific error causing experiments to fail initializing when git diff --cached encounters invalid or inaccessible Git repositories.\n- Token counts have the correct units when printing\nExperimentSummary\nobjects. - In Python,\nMetricSummary.metric\ncould have anint\nvalue. The type annotation has been updated.\nWeek of 2024-09-09\n- You can now create server-side online evaluations for your logs. Online evals support both autoevals and custom scorers you define as LLM-as-a-judge, TypeScript, or Python functions. See docs for more details.\n- New member invitations now support being added to multiple permission groups.\n- Move datasets and prompts to a new Library navigation tab, and include a list of custom scorers.\n- Clean up tree view by truncating the root preview and showing a preview of a node only if collapsed.\n- Automatically save changes to table views.\nWeek of 2024-09-02\n- You can now upload typescript evals from the command line as functions, and then use them in the playground.\n- Click a span field line to highlight it and pin it to the URL.\n- Copilot tab autocomplete for prompts and data in the Braintrust UI.\nAPI (version 0.0.54)\n- Support for bundled eval uploads.\n- The\nPATCH\nendpoint for prompts now supports updating theslug\nfield.\nSDK (version 0.0.157)\n- Enable the\n--bundle\nflag forbraintrust eval\nin the TypeScript SDK.\nWeek of 2024-08-26\n- Basic filter UI (no BTQL necessary)\n- Add to dataset dropdown now supports adding to datasets across projects.\n- Add REST endpoint for batch-updating ACLs:\n/v1/acl/batch_update\n. - Cmd/Ctrl click on a table row to open it in a new tab.\n- Show the last 5 basic filters in the filter editor.\n- You can now explicitly set and edit prompt slugs.\nAutoevals (version 0.0.86)\n- Add support for Azure OpenAI in node.\nSDK (version 0.0.155)\n- The client wrappers\nwrapOpenAI()\n/wrap_openai()\nnow support Structured Outputs.\nAPI (version 0.0.54)\n- Don't fail insertion requests if realtime broadcast fails\nWeek of 2024-08-19\n- Fixed comment deletion.\n- You can now use\n%\nin BTQL queries to represent percent values. E.g.50%\nwill be interpreted as0.5\n.\nAPI (version 0.0.54)\n- Performance optimizations to filters on\nscores\n,metrics\n, andcreated\nfields. - Performance optimizations to filter subfields of\nmetadata\nandspan_attributes\n.\nWeek of 2024-08-12\n- You can now create custom LLM and code (TypeScript and Python) evaluators in the playground.\n- Fullscreen trace toggle\n- Datasets now accept JSON file uploads\n- When uploading a CSV/JSON file to a dataset, columns/fields named\ninput\n,expected\n, andmetadata\nare now auto-assigned to the corresponding dataset fields - Fix bug in logs/dataset viewer when changing the search params.\nAPI (version 0.0.53)\n- The API now supports running custom LLM and code (TypeScript and Python) functions. To enable this in the:\n- AWS Cloudformation stack: turn on the\nEnableQuarantine\nparameter - Docker deployment: set the\nALLOW_CODE_FUNCTION_EXECUTION\nenvironment variable totrue\n- AWS Cloudformation stack: turn on the\nWeek of 2024-08-05\n- Full text search UI for all span contents in a trace\n- New metrics in the UI and summary API: prompt tokens, completion tokens, total tokens, and LLM duration\n- These metrics, along with cost, now exclude LLM calls used in autoevals (as of 0.0.85)\n- Switching organizations via the header navigates to the same-named project in the selected organization\n- Added\nMarkAsyncWrapper\nto the Python SDK to allow explicitly marking functions which return awaitable objects as async\nAutoevals (version 0.0.85)\n- LLM calls used in autoevals are now marked with\nspan_attributes.purpose = \"scorer\"\nso they can be excluded from metric and cost calculations.\nAutoevals (version 0.0.84)\n- Fix a bug where\nrationale\nwas incorrectly formatted in Python. - Update the\nfull\ndocker deployment configuration to bundle the metadata DB (supabase) inside the main docker compose file. Thus no separate supabase cluster is required. See docs for details. If you are upgrading an existing full deployment, you will likely want to mark the supabase db volumesexternal\nto continue using your existing data (see comments in thedocker-compose.full.yml\nfile for more details).\nSDK (version 0.0.151)\nEval()\ncan now take a base experiment. Provide eitherbaseExperimentName\n/base_experiment_name\norbaseExperimentId\n/base_experiment_id\n.\nWeek of 2024-07-29\n- Errors now show up in the trace viewer.\n- New cookbook recipe on benchmarking LLM providers.\n- Viewer mode selections will no longer automatically switch to a non-editable view if the field is editable and persist across trace/span changes.\n- Show\n%\nin diffs instead ofpp\n. - Add rename, delete and copy current project id actions to the project dropdown.\n- Playgrounds can now be shared publicly.\n- Duration now reflects the \"task\" duration not the overall test case duration (which also includes scores).\n- Duration is now also displayed in the experiment overview table.\n- Add support for Fireworks and Lepton inference providers.\n- \"Jump to\" menu to quickly navigate between span sections.\n- Speed up queries involving metadata fields, e.g.\nmetadata.foo ILIKE '%bar%'\n, using the columnstore backend if it is available. - Added\nproject_id\nquery param to REST API queries which already acceptproject_name\n. E.g. GET experiments. - Update to include the latest Mistral models in the proxy/playground.\nSDK (version 0.0.148)\n- While tracing, if your code errors, the error will be logged to the span. You can also manually log the\nerror\nfield through the API or the logging SDK.\nSDK (version 0.0.147)\nproject_name\nis nowprojectName\n, etc. in theinvoke(...)\nfunction in TypeScriptEval()\nreturn values are printed in a nicer format (e.g. in Notebooks)updateSpan()\n/update_span()\nallows you to update a span's fields after it has been created.\nWeek of 2024-07-22\n- Categorical human review scores can now be re-ordered via Drag-n-Drop.\n- Human review row selection is now a free text field, enabling a quick jump to a specific row.\n- Added REST endpoint for managing org membership. See docs.\nAPI (version 0.0.51)\n- The proxy is now a first-class citizen in the API service, which simplifies deployment and sets the groundwork for some\nexciting new features. Here is what you need to know:\n- The updates are available as of API version 0.0.51.\n- The proxy is now accessible at\nhttps://api.braintrust.dev/v1/proxy\n. You can use this as a base URL in your OpenAI client, instead ofhttps://braintrustproxy.com/v1\n. [NOTE: The latter is still supported, but will be deprecated in the future.] - If you are self-hosting, the proxy is now bundled into the API service. That means you no longer need to deploy the proxy as a separate service.\n- If you have deployed through AWS, after updating the Cloudformation, you'll need to grab the \"Universal API URL\" from the \"Outputs\" tab.\n- Then, replace that in your settings page settings page\n- If you have a Docker-based deployment, you can just update your containers.\n- Once you see the \"Universal API\" indicator, you can remove the proxy URL from your settings page, if you have it set.\nSDK (version 0.0.146)\n- Add support for\nmax_concurrency\nin the Python SDK - Hill climbing evals that use a\nBaseExperiment\nas data will use that as the default base experiment.\nWeek of 2024-07-15\n- In preparation for auth changes, we are making a series of updates that may affect self-deployed instances:\n- Preview URLs will now be subdomains of\n*.preview.braintrust.dev\ninstead ofvercel.app\n. Please add this domain to your allow list. - To continue viewing preview URLs, you will need to update your stack (to update the allow list to include the new domain pattern).\n- The data plane may make requests back to\n*.preview.braintrust.dev\nURLs. This allows you to test previews that include control plane changes. You may need to whitelist traffic from the data plane to*.preview.braintrust.dev\ndomains. - Requests will optionally send an additional\nx-bt-auth-token\nheader. You may need to whitelist this header. - User impersonation through the\nx-bt-impersonate-user\nheader now accepts either the user's id or email. Previously only user id was accepted.\n- Preview URLs will now be subdomains of\nAutoevals (version 0.0.80)\n- New\nExactMatch\nscorer for comparing two values for exact equality.\nAutoevals (version 0.0.77)\n- Officially switch the default model to be\ngpt-4o\n. Our testing showed that it performed on average 10% more accurately thangpt-3.5-turbo\n! - Support claude models (e.g. claude-3-5-sonnet-20240620). You can use them by simply specifying the\nmodel\nparam in any LLM based evaluator.- Under the hood, this will use the proxy, so make sure to configure your Anthropic API keys in your settings.\nWeek of 2024-07-08\n- Human review scores are now sortable from the project configuration page.\n- Streaming support for tool calls in Anthropic models through the proxy and playground.\n- The playground now supports different \"parsing\" modes:\nauto\n: (same as before) the completion text and the first tool call arguments, if anyparallel\n: the completion text and a list of all tool callsraw\n: the completion in the OpenAI non-streaming formatraw_stream\n: the completion in the OpenAI streaming format\n- Cleaned up environment variables in the public docker deployment. Functionally, nothing has changed.\nAutoevals (version 0.0.76)\n- New\n.partial(...)\nsyntax to initialize a scorer with partial arguments likecriteria\ninClosedQA\n. - Allow messages to be inserted in the middle of a prompt.\nWeek of 2024-07-01\n- Table views can now be saved, persisting the BTQL filters, sorts, and column state.\n- Add support for the new\nwindow.ai\nmodel into the playground. - Use push history when navigating table rows to allow for back button navigation.\n- In the experiments list, grouping by a metadata field will group rows in the table as well.\n- Allow the trace tree panel to be resized.\n- Port the log summary query to BTQL. This should speed up the query, especially if you have clickhouse configured in your cloud environment. This functionality requires upgrading your data backend to version 0.0.50.\nSDK (version 0.0.140)\n- New\nwrapTraced\nfunction allows you to trace javascript functions in a more ergonomic way.\nSDK (version 0.0.138)\n- The TypeScript SDK's\nEval()\nfunction now takes amaxConcurrency\nparameter, which bounds the number of concurrent tasks that run. braintrust install api\nnow sets up your API and Proxy URL in your environment.- You can now specify a custom\nfetch\nimplementation in the TypeScript SDK.\nWeek of 2024-06-24\n- Update the experiment progress and experiment score distribution chart layouts\n- Format table column headers with icons\n- Move active filters to the table toolbar\n- Enable RBAC for all users. When inviting a new member, prompt to add that member to an RBAC Permission group.\n- Use btql to power the datasets list, making it significantly faster if you have multiple large datasets.\n- Experiments list chart supports click interactions. Left click to select an experiment, right click to add an annotation.\n- Jump into comparison view between 2 experiments by selecting them in the table an clicking \"Compare\"\nDeployment\n- The proxy service now supports more advanced functionality which requires setting the\nPG_URL\nandREDIS_URL\nparameters. If you do not set them, the proxy will still run without caching credentials or requests.\nWeek of 2024-06-17\n- Add support for labeling expected fields using human review.\n- Create and edit descriptions for datasets.\n- Create and edit metadata for prompts.\n- Click scores and attributes (tree view only) in the trace view to filter by them.\n- Highlight the experiments graph to filter down the set of experiments.\n- Add support for new models including Claude 3.5 Sonnet.\nWeek of 2024-06-10\n- Improved empty state and instructions for custom evaluators in the playground.\n- Show query examples when filtering/sorting.\n- Custom comparison keys for experiments.\n- New model dropdown in the playground/prompt editor that is organized by provider and model type.\nWeek of 2024-06-03\n- You can now collapse the trace tree. It's auto collapsed if you have a single span.\n- Improvements to the experiment chart including greyed out lines for inactive scores and improved legend.\n- Show diffs when you save a new prompt version.\nWeek of 2024-05-27\n- You can now see which users are viewing the same traces as you are in real-time.\n- Improve whitespace and presentation of diffs in the trace view.\n- Show markdown previews in score editor.\n- Show cost in spans and display the average cost on experiment summaries and diff views.\n- Published a new Text2SQL eval recipe\n- Add groups view for RBAC.\nWeek of 2024-05-20\n- Deprecate the legacy dataset format (\noutput\nin place ofexpected\n) in a new version of the SDK (0.0.130). For now, data can still be fetched in the legacy format by setting theuseOutput\n/use_output\nflag to false when usinginitDataset()\n/init_dataset()\n. We recommend updating your code to use datasets withexpected\ninstead ofoutput\nas soon as possible. - Improve the UX for saving and updating prompts from the playground.\n- New hide/show column controls on all tables.\n- New model comparison cookbook recipe.\n- Add support for model / metadata comparison on the experiments view.\n- New experiment picker dropdown.\n- Markdown support in the LLM message viewer.\nWeek of 2024-05-13\n- Support copying to clipboard from\ninput\n,output\n, etc. views - Improve the empty-state experience for datasets.\n- New multi-dimensional charts on the experiment page for comparing models and model parameters.\n- Support\nHTTPS_PROXY\n,HTTP_PROXY\n, andNO_PROXY\nenvironment variables in the API containers. - Support infinite scroll in the logs viewer and remove dataset size limitations.\nWeek of 2024-05-06\n- Denser trace view with span durations built in.\n- Rework pagination and fix scrolling across multiple pages in the logs viewer.\n- Make BTQL the default search method.\n- Add support for Bedrock models in the playground and the proxy.\n- Add \"copy code\" buttons throughout the docs.\n- Automatically overflow large objects (e.g. experiments) to S3 for faster loading and better performance.\nWeek of 2024-04-29\n- Show images in LLM view, adding the ability to display images in the LLM view in the trace viewer.\n- Send an invite email when you invite a new user to your organization.\n- Support selecting/deselecting scores in the experiment view.\n- Roll out Braintrust Query Language (BTQL) for querying logs and traces.\nWeek of 2024-04-22\n- Smart relative time labels for dates (\n1h ago\n,3d ago\n, etc.) - Added double quoted string literals support, e.g.,\ntags contains \"foo\"\n. - Jump to top button in trace details for easier navigation.\n- Fix a race condition in distributed tracing, in which subspans could hit the backend before their parent span, resulting in an inaccurate trace structure.\nAs part of this change, we removed the parent_id\nargument from the latest SDK,\nwhich was previously deprecated in favor of parent\n. parent_id\nis only able\nto use the race-condition-prone form of distributed tracing, so we felt it would\nbe best for folks to upgrade any of their usages from parent_id\nto parent\n.\nBefore upgrading your SDK, if you are currently using parent_id\n, you can port\nover to using parent\nby changing any exported IDs from span.id\nto\nspan.export()\nand then changing any instances of parent_id=[span_id]\nto\nparent=[exported_span]\n.\nFor example, if you had distributed tracing code like the following:\nIt would now look like this:\nWeek of 2024-04-15\n- Incremental support for roles-based access control (RBAC) logic within the API server backend.\nAs part of this change, we removed certain API endpoints which are no longer in\nuse. In particular, the /crud/{object_type}\nendpoint. For the handful of\nusages of these endpoints in old versions of the SDK libraries, we added\nbackwards-compatibility routes, but it is possible we may have missed a few.\nPlease let us know if your code is trying to use an endpoint that no longer\nexists and we can remediate.\n- Changed the semantics of experiment initialization with\nupdate=True\n. Previously, we would require the experiment to already exist, now we will create the experiment if it doesn't already exist otherwise return the existing one.\nThis change affects the semantics of the PUT /v1/experiment\noperation, so that\nit will not replace the contents of an existing experiment with a new one, but\ninstead just return the existing one, meaning it behaves the same as POST /v1/experiment\n. Eventually we plan to revise the update semantics for other\nobject types as well. Therefore, we have deprecated the PUT\nendpoint across\nthe board and plan to remove it in a future revision of the API.\nWeek of 2024-04-08\n- Added support for new multimodal models (\ngpt-4-turbo\n,gpt-4-vision-preview\n,gpt-4-1106-vision-preview\n,gpt-4-turbo-2024-04-09\n,claude-3-opus-20240229\n,claude-3-sonnet-20240229\n,claude-3-haiku-20240307\n). - Introduced REST API for RBAC (Role-Based Access Control) objects including CRUD operations on roles, groups, and permissions, and added a read-only API for users.\n- Improved AI search and added positive/negative tag filtering in AI search. To positively filter, prefix the tag with\n+\n, and to negatively filter, prefix the tag with-\n.\nWe are making some systematic changes to the search experience, and the search syntax is subject to change.\nWeek of 2024-04-01\n- Added functionality for distributed tracing. See the docs for more details.\nAs part of this change, we had to rework the core logging implementation in the\nSDKs to rely on some newer backend API features. Therefore, if you are hosting\nBraintrust on-prem, before upgrading your SDK to any version >= 0.0.115\n, make\nsure your API version is >= 0.0.35\n. You can query the version of the on-prem\nserver with curl [api-url]/version\n, where the API URL can be found on the settings page.\nWeek of 2024-03-25\n- Introduce multimodal support for OpenAI and Anthropic models in the prompt playground and proxy. You can now pass image URLs, base64-encoded image strings, or mustache template variables to models that support multimodal inputs.\n- The REST API now gzips responses.\n- You can now return dynamic arrays of scores in\nEval()\nfunctions (docs). - Launched Reporters, a way to summarize and report eval results in a custom format.\n- New coat of paint in the trace view.\n- Added support for Clickhouse as an additional storage backend, offering a more scalable solution for handling large datasets and performance improvements for certain query types. You can enable it by\nsetting the\nUseManagedClickhouse\nparameter totrue\nin the CloudFormation template or installing the docker container. - Implemented realtime checks using a WebSocket connection and updated proxy configurations to include CORS support.\n- Introduced an API version checker tool so you know when your API version is outdated.\nWeek of 2024-03-18\n- Add new database parameters for external databases in the CloudFormation template.\n- Faster optimistic updates for large writes in the UI.\n- \"Open in playground\" now opens a lighter weight modal instead of the full playground.\n- Can create a new prompt playground from the prompt viewer.\nWeek of 2024-03-11\n- Shipped support for prompt management.\n- Moved playground sessions to be within projects. All existing sessions are now in the \"Playground Sessions\" project.\n- Allowed customizing proxy and real-time URLs through the web application, adding flexibility for different deployment scenarios.\n- Improved documentation for Docker deployments.\n- Improved folding behavior in data editors.\nWeek of 2024-03-04\n- Support custom models and endpoint configuration for all providers.\n- New add team modal with support for multiple users.\n- New information architecture to enable faster project navigation.\n- Experiment metadata now visible in the experiments table.\n- Improve UI write performance with batching.\n- Log filters now apply to any span.\n- Share button for traces\n- Images now supported in the tree view (see tracing docs for more).\nWeek of 2024-02-26\n- Show auto scores before manual scores (matching trace) in the table\n- New logo is live!\n- Any span can now submit scores, which automatically average in the trace. This makes it easier to label scores in the spans where they originate.\n- Improve sidebar scrolling behavior.\n- Add AI search for datasets and logs.\n- Add tags to the SDK.\n- Support viewing and updating metadata on the experiment page.\nWeek of 2024-02-19\nWe rolled out a breaking change to the REST API that renames the\noutput\nfield to expected\non dataset records. This change brings\nthe API in line with last week's update to\nthe Braintrust SDK. For more information, refer to the REST API docs\nfor dataset records (insert\nand fetch).\n- Add support for tags.\n- Score fields are now sorted alphabetically.\n- Add support for Groq ModuleResolutionKind.\n- Improve tree viewer and XML parser.\n- New experiment page redesign\nWeek of 2024-02-12\nWe are rolling out a change to dataset records that renames the output\nfield to expected\n. If you are using the SDK, datasets will still fetch\nrecords using the old format for now, but we recommend future-proofing\nyour code by setting the useOutput\n/ use_output\nflag to false when\ncalling initDataset()\n/ init_dataset()\n, which will become the default\nin a future version of Braintrust.\nWhen you set useOutput\nto false, your dataset records will contain\nexpected\ninstead of output\n. This makes it easy to use them with\nEval(...)\nto provide expected outputs for scoring, since you'll\nno longer have to manually rename output\nto expected\nwhen passing\ndata to the evaluator:\nHere's an example of how to insert and fetch dataset records using the new format:\n- Support duplicate\nEval\nnames. - Fallback to\nBRAINTRUST_API_KEY\nifOPENAI_API_KEY\nis not set. - Throw an error if you use\nexperiment.log\nandexperiment.start_span\ntogether. - Add keyboard shortcuts (j/k/p/n) for navigation.\n- Increased tooltip size and delay for better usability.\n- Support more viewing modes: HTML, Markdown, and Text.\nWeek of 2024-02-05\n- Tons of improvements to the prompt playground:\n- A new \"compact\" view, that shows just one line per row, so you can quickly scan across rows. You can toggle between the two modes.\n- Loading indicators per cell\n- The run button transforms into a \"Stop\" button while you are streaming data\n- Prompt variables are now syntax highlighted in purple and use a monospace font\n- Tab now autocompletes\n- We no longer auto-create variables as you're typing (was causing more trouble than helping)\n- Slider params like\nmax_tokens\nare now optional\n- Cloudformation now supports more granular RDS configuration (instance type, storage, etc)\n- Support optional slider params\n- Made certain parameters like\nmax_tokens\noptional. - Accompanies pull request https://github.com/braintrustdata/braintrust-proxy/pull/23.\n- Made certain parameters like\n- Lots of style improvements for tables.\n- Fixed filter bar styles.\n- Rendered JSON cell values using monospace type.\n- Adjusted margins for horizontally scrollable tables.\n- Implemented a smaller size for avatars in tables.\n- Deleting a prompt takes you back to the prompts tab\nWeek of 2024-01-29\n- New REST API.\n- Cookbook of common use cases and examples.\n- Support for custom models in the playground.\n- Search now works across spans, not just top-level traces.\n- Show creator avatars in the prompt playground\n- Improved UI breadcrumbs and sticky table headers\nWeek of 2024-01-22\n- UI improvements to the playground.\n- Added an example of closed QA / extra fields.\n- New YAML parser and new syntax highlighting colors for data editor.\n- Added support for enabling/disabling certain git fields from collection (in org settings and the SDK).\n- Added new GPT-3.5 and 4 models to the playground.\n- Fixed scrolling jitter issue in the playground.\n- Made table fields in the prompt playground sticky.\nWeek of 2024-01-15\n- Added ability to download dataset as CSV\n- Added YAML support for logging and visualizing traces\n- Added JSON mode in the playground\n- Added span icons and improved readability\n- Enabled shift modifier for selecting multiple rows in Tables\n- Improved tables to allow editing expected fields and moved datasets to trace view\nWeek of 2024-01-08\n- Released new Docker deployment method for self hosting\n- Added ability to manually score results in the experiment UI\n- Added comments and audit log in the experiment UI\nWeek of 2024-01-01\n- Added ability to upload dataset CSV files in prompt playgrounds\n- Published new guide for tracing and logging your code\n- Added support to download experiment results as CSVs\nWeek of 2023-12-25\n- API keys are now scoped to organizations, so if you are part of multiple orgs, new API keys will only permit access to the org they belong to.\n- You can now search for experiments by any metadata, including their name, author, or even git metadata.\n- Filters are now saved in URL state so you can share a link to a filtered view of your experiments or logs.\n- Improve performance of project page by optimizing API calls.\nWe made several cleanups and improvements to the low-level typescript and python SDKs (0.0.86). If you use the Eval framework, nothing should change for you, but keep in mind the following differences if you use the manual logging functionality:\n- Simplified the low-level tracing API (updated docs coming soon!)\n- The current experiment and current logger are now maintained globally\nrather than as async-task-local variables. This makes it much simpler to\nstart tracing with minimal code modification. Note that creating\nexperiments/loggers with\nwithExperiment\n/withLogger\nwill now set the current experiment globally (visible across all async tasks) rather than local to a specific task. You may passsetCurrent: false/set_current=False\nto avoid setting the global current experiment/logger. - In python, the\n@traced\ndecorator now logs the function input/output by default. This might interfere with code that already logs input/output inside thetraced\nfunction. You may passnotrace_io=True\nas an argument to@traced\nto turn this logging off. - In typescript, the\ntraced\nmethod can start spans under the global logger, and is thus async by default. You may passasyncFlush: true\nto these functions to make the traced function synchronous. Note that if the function tries to trace under the global logger, it must also haveasyncFlush: true\n. - Removed the\nwithCurrent\n/with_current\nfunctions - In typescript, the\nSpan.traced\nmethod now acceptsname\nas an optional argument instead of a required positional param. This matches the behavior of all other instances oftraced\n.name\nis also now optional in python, but this doesn't change the function signature.\n- The current experiment and current logger are now maintained globally\nrather than as async-task-local variables. This makes it much simpler to\nstart tracing with minimal code modification. Note that creating\nexperiments/loggers with\nExperiments\nandDatasets\nare now lazily-initialized, similar toLoggers\n. This means all write operations are immediate and synchronous. But any metadata accessor methods ([Experiment|Logger].[id|name|project]\n) are now async.- Undo auto-inference of\nforce_login\niflogin\nis invoked with different params than last time. Nowlogin\nwill only re-login ifforceLogin: true/force_login=True\nis provided.\nWeek of 2023-12-18\n- Dropped the official 2023 Year-in-Review dashboard. Check out yours here!\n- Improved ergonomics for the Python SDK:\n- The\n@traced\ndecorator will automatically log inputs/outputs - You no longer need to use context managers to scope experiments or loggers.\n- The\n- Enable skew protection in frontend deploys, so hopefully no more hard refreshes.\n- Added syntax highlighting in the sidepanel to improve readability.\n- Add\njsonl\nmode to the eval CLI to log experiment summaries in an easy-to-parse format.\nWeek of 2023-12-11\n- Released new trials feature to rerun each input multiple times and collect aggregate results for a more robust score.\n- Added ability to run evals in the prompt playground. Use your existing dataset and the autoevals functions to score playground outputs.\n- Released new version of SDK (0.0.81) including a small breaking change. When setting the experiment name in the\nEval\nfunction, theexprimentName\nkey pair should be moved to a top level argument. before:\nafter:\n- Added support for Gemini and Mistral Platform in AI proxy and playground\nWeek of 2023-12-4\n- Enabled the prompt playground and datasets for free users\n- Added Together.ai models including Mixtral to AI Proxy\n- Turned prompts tab on organization view into a list\n- Removed data row limit for the prompt playground\n- Enabled configuration for dark mode and light mode in settings\n- Added automatic logging of a diff if an experiment is run on a repo with uncommitted changes\nWeek of 2023-11-27\n- Added experiment search on project view to filter by experiment name\n- Upgraded AI Proxy to support tracking Prometheus metrics\n- Modified Autoevals library to use the AI proxy\n- Upgraded Python braintrust library to parallelize evals\n- Optimized experiment diff view for performance improvements\nWeek of 2023-11-20\n- Added support for new Perplexity models (ex: pplx-7b-online) to playground\n- Released AI proxy: access many LLMs using one API w/ caching\n- Added load balancing endpoints to AI proxy\n- Updated org-level view to show projects and prompt playground sessions\n- Added ability to batch delete experiments\n- Added support for Claude 2.1 in playground\nWeek of 2023-11-13\n- Made experiment column resized widths persistent\n- Fixed our libraries including Autoevals to work with OpenAI\u2019s new libraries\n- Added support for function calling and tools in our prompt playground\n- Added tabs on a project page for datasets, experiments, etc.\nWeek of 2023-11-06\n- Improved selectors for diffing and comparison modes on experiment view\n- Added support for new OpenAI models (GPT4 preview, 3.5turbo-1106) in playground\n- Added support for OS models (Mistral, Codellama, Llama2, etc.) in playground using Perplexity's APIs\nWeek of 2023-10-30\n- Improved experiment sidebar to be fully responsive and resizable\n- Improved tooltips within the web UI\n- Multiple performance optimizations and bug fixes\nWeek of 2023-10-23\n-\nImproved prompt playground variable handling and visualization\n-\nAdded time duration statistics per row to experiment summaries\n- Multiple performance optimizations and bug fixes\nWeek of 2023-10-16\n- Launched new tracing feature: log and visualize complex LLM chains and executions.\n- Added a new \u201ctext-block\u201d prompt type in the playground that just returns a string or variable back without a LLM call (useful for chaining prompts and debugging)\n- Increased default # of rows per page from 10 to 100 for experiments\n- UI fixes and improvements for the side panel and tooltips\n- The experiment dashboard can be customized to show the most relevant charts\nWeek of 2023-10-09\n- Performance improvements related to user sessions\nWeek of 2023-10-02\n- All experiment loading HTTP requests are 100-200ms faster\n- The prompt playground now supports autocomplete\n- Dataset versions are now displayed on the datasets page\n- Projects in the summary page are now sorted alphabetically\n- Long text fields in logged data can be expanded into scrollable blocks\n- We evaluated the Alpaca evals leaderboard in Braintrust\n- New tutorial for finetuning GPT3.5 and evaluating with Braintrust\nWeek of 2023-09-18\n- The Eval framework is now supported in Python! See the updated evals guide for more information:\n- Onboarding and signup flow for new users\n- Switch product font to Inter\nWeek of 2023-09-11\n-\nBig performance improvements for registering experiments (down from ~5s to <1s). Update the SDK to take advantage of these improvements.\n-\nNew graph shows aggregate accuracy between experiments for each score.\n-\nThrow errors in the prompt playground if you reference an invalid variable.\n-\nA significant backend database change which significantly improves performance while reducing costs. Please contact us if you have not already heard from us about upgrading your deployment.\n-\nNo more record size constraints (previously, strings could be at most 64kb long).\n-\nNew autoevals for numeric diff and JSON diff\nWeek of 2023-09-05\n- You can duplicate prompt sessions, prompts, and dataset rows in the prompt playground.\n- You can download prompt sessions as JSON files (including the prompt templates, prompts, and completions).\n- You can adjust model parameters (e.g. temperature) in the prompt playground.\n- You can publicly share experiments (e.g. Alpaca Evals).\n- Datasets now support editing, deleting, adding, and copying rows in the UI.\n- There is no longer a 64KB limit on strings.\nWeek of 2023-08-28\n- The prompt playground is now live! We're excited to get your feedback as we continue to build this feature out. See the docs for more information.\nWeek of 2023-08-21\n- A new chart shows experiment progress per score over time.\n- The eval CLI now supports\n--watch\n, which will automatically re-run your evaluation when you make changes to your code. - You can now edit datasets in the UI.\nWeek of 2023-08-14\n- Introducing datasets! You can now upload datasets to Braintrust and use them in your experiments. Datasets are versioned, and you can use them in multiple experiments. You can also use datasets to compare your model's performance against a baseline. Learn more about how to create and use datasets in the docs.\n- Fix several performance issues in the SDK and UI.\nWeek of 2023-08-07\n- Complex data is now substantially more performant in the UI. Prior to this change, we ran schema\ninference over the entire\ninput\n,output\n,expected\n, andmetadata\nfields, which could result in complex structures that were slow and difficult to work with. Now, we simply treat these fields asJSON\ntypes. - The UI updates in real-time as new records are logged to experiments.\n- Ergonomic improvements to the SDK and CLI:\n- The JS library is now Isomorphic and supports both Node.js and the browser.\n- The Evals CLI warns you when no files match the\n.eval.[ts|js]\npattern.\nWeek of 2023-07-31\n- You can now break down scores by metadata fields:\n-\nImprove performance for experiment loading (especially complex experiments). Prior to this change, you may have seen experiments take 30s+ occasionally or even fail. To enable this, you'll need to update your CloudFormation.\n-\nSupport for renaming and deleting experiments:\n- When you expand a cell in detail view, the row is now highlighted:\nWeek of 2023-07-24\n- A new framework for expressing evaluations in a much simpler way:\nBesides being much easier than the logging SDK, this framework sets the foundation for evaluations that can be run automatically as your code changes, built and run in the cloud, and more. We are very excited about the use cases it will open up!\ninputs\nis nowinput\nin the SDK (>= 0.0.23) and UI. You do not need to make any code changes, although you should gradually start using theinput\nfield instead ofinputs\nin your SDK calls, asinputs\nis now deprecated and will eventually be removed.- Improved diffing behavior for nested arrays.\nWeek of 2023-07-17\n- A couple of SDK updates (>= v0.0.21) that allow you to update an existing experiment\ninit(..., update=True)\nand specify an id inlog(..., id='my-custom-id')\n. These tools are useful for running an experiment across multiple processes, tasks, or machines, and idempotently logging the same record (identified by itsid\n).- Note: If you have Braintrust installed in your own cloud environment, make sure to update the CloudFormation (available at https://braintrust-cf.s3.amazonaws.com/braintrust-latest.yaml).\n- Tables with lots and lots of columns are now visually more compact in the UI:\nBefore:\nAfter:\nWeek of 2023-07-10\n- A new Node.js SDK (npm) which mirrors the Python SDK. As this SDK is new, please let us know if you run into any issues or have any feedback.\nIf you have Braintrust installed in your own cloud environment, make sure to update the CloudFormation (available at https://braintrust-cf.s3.amazonaws.com/braintrust-latest.yaml) to include some functionality the Node.js SDK relies on.\nYou can do this in the AWS console, or by running the following command (with the braintrust\ncommand included in the Python SDK).\n- You can now swap the primary and comparison experiment with a single click.\n- You can now compare\noutput\nvs.expected\nwithin an experiment.\n- Version 0.0.19 is out for the SDK. It is an important update that throws an error if your payload is larger than 64KB in size.\nWeek of 2023-07-03\n-\nSupport for real-time updates, using Redis. Prior to this, Braintrust would wait for your data warehouse to sync up with Kafka before you could view an experiment, often leading to a minute or two of time before a page loads. Now, we cache experiment records as your experiment is running, making experiments load instantly. To enable this, you'll need to update your CloudFormation.\n-\nNew settings page that consolidates team, installation, and API key settings. You can now invite team members to your Braintrust account from the \"Team\" page.\n-\nThe experiment page now shows commit information for experiments run inside of a git repository.\nWeek of 2023-06-26\n- Experiments track their git metadata and automatically find a \"base\" experiment to compare against, using your repository's base branch.\n- The Python SDK's\nsummarize()\nmethod now returns anExperimentSummary\nobject with score differences against the base experiment (v0.0.10). - Organizations can now be \"multi-tenant\", i.e. you do not need to install in your cloud account. If you start with a multi-tenant account to try out Braintrust, and decide to move it into your own account, Braintrust can migrate it for you.\nWeek of 2023-06-19\n-\nNew scatter plot and histogram insights to quickly analyze scores and filter down examples.\n-\nAPI keys that can be set in the SDK (explicitly or through an environment variable) and do not require user login. Visit the settings page to create an API key.\n- Update the braintrust Python SDK to version 0.0.6 and the CloudFormation template (https://braintrust-cf.s3.amazonaws.com/braintrust-latest.yaml) to use the new API key feature.\nWeek of 2023-06-12\n- New\nbraintrust install\nCLI for installing the CloudFormation - Improved performance for event logging in the SDK\n- Auto-merge experiment fields with different types (e.g.\nnumber\nandstring\n)\nWeek of 2023-06-05\n-\nAutomatically refresh cognito tokens in the Python client\n-\nNew filter and sort operators on the experiments table:\n- Filter experiments by changes to scores (e.g. only examples with a lower score than another experiment)\n- Custom SQL filters\n- Filter and sort bubbles to visualize/clear current operations\n-\n[Alpha] SQL query explorer to run arbitrary queries against one or more experiments", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2025-09-08"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK Integrations: LangChain (JS) (version 0.0.7)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Python SDK version 0.2.7 (upcoming)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "TypeScript SDK version 0.3.8 (upcoming)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Data plane (1.1.22)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Autoevals PY (version 0.0.130)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2025-09-01"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2025-08-25"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "TypeScript SDK version 0.3.7"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Python SDK version 0.2.6"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "TypeScript SDK version 0.3.6"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Data plane (1.1.21)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2025-08-18"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Data plane (1.1.20)"}, {"href": "https://www.braintrust.dev/docs/reference/api/ServiceTokens", "anchor": "docs"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2025-08-18"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Python SDK version 0.2.5"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "docs"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "TypeScript SDK version 0.2.5"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "docs"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK Integrations: Google ADK (Python) (version 0.1.1)"}, {"href": "https://www.braintrust.dev/docs/guides/integrations", "anchor": "Google Agent Development Kit (ADK)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Python SDK version 0.2.4"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2025-08-11"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Python SDK version 0.2.3"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "TypeScript SDK version 0.2.4"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK Integrations: OpenAI Agents (TS) (version 0.0.2)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Data plane (1.1.19)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2025-08-04"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Python SDK version 0.2.2"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "TypeScript SDK version 0.2.3"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "TypeScript SDK version 0.2.2"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Python SDK version 0.2.1"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2025-07-28"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Data plane (1.1.18)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2025-07-21"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "TypeScript SDK version 0.2.1"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "TypeScript SDK version 0.2.0"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Python SDK version 0.2.0"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Data plane (1.1.15)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2025-07-14"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Python SDK version 0.1.8"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "TypeScript SDK version 0.1.1"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Data plane (1.1.14)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Python SDK version 0.1.7"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "TypeScript SDK version 0.1.0"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2025-07-07"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Data plane (1.1.13)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Data plane (1.1.12) [skipped]"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2025-06-30"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Data plane (1.1.11)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2025-06-23"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Data plane (1.1.7)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Autoevals.js v0.0.130"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "TypeScript SDK version 0.0.209"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "TypeScript SDK version 0.0.208"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2025-06-16"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Python SDK version 0.1.5"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "TypeScript SDK version 0.0.207"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Data plane (1.1.6)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Data plane (1.1.5)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Data plane (1.1.4)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2025-06-09"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Python SDK version 0.1.4"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "TypeScript SDK version 0.0.206"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2025-06-02"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Python SDK version 0.1.3"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2025-05-26"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Python SDK version 0.1.2"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "TypeScript SDK version 0.0.206 [upcoming]"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2025-05-19"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "TypeScript SDK version 0.0.205"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Python SDK version 0.1.1"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "TypeScript SDK version 0.0.204"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2025-05-12"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.1.0)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.203)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2025-05-05"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.202)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2025-04-28"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2025-04-21"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.201)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.200)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2025-04-14"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "the docs"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.199)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.198)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2025-04-07"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "API (version 0.0.65)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.197)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "API (version 0.0.65)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2025-03-31"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "API (version 0.0.64)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.196)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2025-03-24"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.195)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.194)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.193)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2025-03-17"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.192)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Autoevals (version 0.0.124)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2025-03-10"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Autoevals (version 0.0.123)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK Integrations: LangChain (Python) (version 0.0.2)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK Integrations: LangChain.js (version 0.0.6)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK Integrations: Val Town"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.190)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.189)"}, {"href": "https://www.braintrust.dev/docs/guides/traces/integrations", "anchor": "OpenAI Agents SDK"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.188)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2025-03-03"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "proxy docs"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.187)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2025-02-24"}, {"href": "https://www.braintrust.dev/docs/llms.txt", "anchor": "llms.txt"}, {"href": "https://www.braintrust.dev/docs/reference/api/Logs", "anchor": "docs"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "API (version 0.0.63)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.187)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2025-02-17"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.187)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2025-02-10"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2025-02-03"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "proxy docs"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2025-01-27"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK Integrations: LangChain.js (version 0.0.5)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.184)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2025-01-20"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.183)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2025-01-13"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK Integrations: LangChain.js (version 0.0.4)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.182)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2025-01-06"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/tracing/integrations", "anchor": "tracing guide"}, {"href": "https://www.braintrust.dev/docs/guides/tracing/integrations", "anchor": "tracing guide"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.181)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.179)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-12-30"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.179) (unreleased)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.178)"}, {"href": "https://www.braintrust.dev/docs/guides/evals/write", "anchor": "docs"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "customize traces guide"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK integrations: LangChain.js (version 0.0.2)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-12-16"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "API (version 0.0.61)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.177)"}, {"href": "https://www.braintrust.dev/docs/guides/functions/scorers", "anchor": "scorers"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-12-09"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "Braintrust Query Language"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Autoevals (version 0.0.110)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.176)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK integrations: LangChain.js (version 0.0.1)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK integrations: LangChain.js (version 0.0.1)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-12-02"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "API (version 0.0.60)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.175)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-11-25"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.174)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.173)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.172)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-11-18"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "docs"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.171)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-11-12"}, {"href": "https://www.braintrust.dev/docs/guides/functions/tools", "anchor": "tools"}, {"href": "https://www.braintrust.dev/docs/guides/functions/prompts", "anchor": "prompts"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.170)"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "file attachments in the Python SDK"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.169)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.168)"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/Span", "anchor": "TypeScript docs"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "Python docs"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-11-04"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI Proxy"}, {"href": "https://www.braintrust.dev/docs/guides/evals/interpret", "anchor": "grid layout"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "API (version 0.0.59)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-10-28"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI Proxy"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "issue temporary credentials"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "docs"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.168)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-10-21"}, {"href": "https://www.braintrust.dev/docs/guides/tracing", "anchor": "file attachments"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.167)"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/classes/Attachment", "anchor": "file attachments in the TypeScript SDK"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "API (version 0.0.59)"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/classes/Attachment", "anchor": "file attachments"}, {"href": "https://www.braintrust.dev/docs/guides/tracing/integrations", "anchor": "tracing guide"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-10-14"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.166)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.165)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.164)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-10-07"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.163)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.162)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.161)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-09-30"}, {"href": "https://www.braintrust.dev/docs/guides/evals/run", "anchor": "Github action"}, {"href": "https://www.braintrust.dev/docs/guides/tracing", "anchor": "span iframe viewers"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-09-23"}, {"href": "https://www.braintrust.dev/docs/guides/prompts", "anchor": "docs"}, {"href": "https://www.braintrust.dev/app/settings?subroute=env-vars", "anchor": "Set org-wide environment variables"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.160)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.159)"}, {"href": "https://www.braintrust.dev/docs/guides/prompts", "anchor": "docs"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "API (version 0.0.56)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-09-16"}, {"href": "https://www.braintrust.dev/docs/reference/api/AiSecrets", "anchor": "docs"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.158)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-09-09"}, {"href": "https://www.braintrust.dev/docs/reference/autoevals", "anchor": "autoevals"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "custom scorers"}, {"href": "https://www.braintrust.dev/docs/guides/evals/write", "anchor": "docs"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-09-02"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "API (version 0.0.54)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.157)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-08-26"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Autoevals (version 0.0.86)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.155)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "API (version 0.0.54)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-08-19"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "API (version 0.0.54)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-08-12"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "API (version 0.0.53)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-08-05"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Autoevals (version 0.0.85)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Autoevals (version 0.0.84)"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting/docker", "anchor": "docs"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.151)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-07-29"}, {"href": "https://www.braintrust.dev/docs/cookbook/recipes/ProviderBenchmark", "anchor": "benchmarking LLM providers"}, {"href": "https://www.braintrust.dev/docs/reference/api/Experiments", "anchor": "GET experiments"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.148)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.147)"}, {"href": "https://www.braintrust.dev/docs/guides/tracing", "anchor": "updateSpan() / update_span()"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-07-22"}, {"href": "https://www.braintrust.dev/docs/reference/api/Organizations", "anchor": "docs"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "API (version 0.0.51)"}, {"href": "https://www.braintrust.dev/app/settings?subroute=api-url", "anchor": "settings page"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.146)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-07-15"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Autoevals (version 0.0.80)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Autoevals (version 0.0.77)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-07-08"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Autoevals (version 0.0.76)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-07-01"}, {"href": "https://www.braintrust.dev/docs/reference/views", "anchor": "can now be saved"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.140)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.138)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-06-24"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Deployment"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-06-17"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "expected fields using human review"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-06-10"}, {"href": "https://www.braintrust.dev/docs/guides/evals/interpret", "anchor": "Custom comparison keys"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-06-03"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-05-27"}, {"href": "https://www.braintrust.dev/docs/cookbook/recipes/Text2SQL-Data", "anchor": "Text2SQL eval recipe"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-05-20"}, {"href": "https://www.braintrust.dev/docs/cookbook/recipes/ModelComparison", "anchor": "model comparison"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-05-13"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-05-06"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-04-29"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "Braintrust Query Language"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-04-22"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-04-15"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-04-08"}, {"href": "https://www.braintrust.dev/docs/api/spec", "anchor": "REST API for RBAC"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-04-01"}, {"href": "https://www.braintrust.dev/docs/guides/tracing", "anchor": "docs"}, {"href": "https://www.braintrust.dev/app/settings?subroute=api-url", "anchor": "settings page"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-03-25"}, {"href": "https://www.braintrust.dev/docs/guides/evals", "anchor": "docs"}, {"href": "https://www.braintrust.dev/docs/guides/evals", "anchor": "Reporters"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-03-18"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-03-11"}, {"href": "https://www.braintrust.dev/docs/guides/prompts", "anchor": "prompt management"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-03-04"}, {"href": "https://www.braintrust.dev/docs/guides/tracing", "anchor": "tracing docs"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-02-26"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-02-19"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "last week's update"}, {"href": "https://www.braintrust.dev/docs/api/spec", "anchor": "insert"}, {"href": "https://www.braintrust.dev/docs/api/spec", "anchor": "fetch"}, {"href": "https://www.braintrust.dev/docs/guides/logging", "anchor": "tags"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-02-12"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-02-05"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-01-29"}, {"href": "https://www.braintrust.dev/docs/api/spec", "anchor": "REST API"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "custom models"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-01-22"}, {"href": "https://www.braintrust.dev/docs/guides/evals", "anchor": "closed QA / extra fields"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-01-15"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-01-08"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-01-01"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2023-12-25"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2023-12-18"}, {"href": "https://www.braintrust.dev/app/year-in-review", "anchor": "here"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2023-12-11"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2023-12-4"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2023-11-27"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2023-11-20"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "load balancing endpoints"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2023-11-13"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2023-11-06"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2023-10-30"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2023-10-23"}, {"href": "https://www.braintrust.dev/docs/release-notes/ReleaseNotes-2023-10-PromptPlaygroundVar.mp4", "anchor": "Improved prompt playground variable handling and visualization"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2023-10-16"}, {"href": "https://www.braintrust.dev/docs/guides/evals", "anchor": "Launched new tracing feature: log and visualize complex LLM chains and executions."}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2023-10-09"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2023-10-02"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2023-09-18"}, {"href": "https://www.braintrust.dev/docs/guides/evals", "anchor": "evals guide"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2023-09-11"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2023-09-05"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2023-08-28"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "the docs"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2023-08-21"}, {"href": "https://www.braintrust.dev/docs/guides/evals", "anchor": "eval CLI"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2023-08-14"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "how to create and use datasets in the docs"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2023-08-07"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2023-07-31"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2023-07-24"}, {"href": "https://www.braintrust.dev/docs/guides/evals", "anchor": "framework"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2023-07-17"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2023-07-10"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Node.js SDK"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "Python SDK"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2023-07-03"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2023-06-26"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "summarize()"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "ExperimentSummary"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2023-06-19"}, {"href": "https://www.braintrust.dev/app/settings?subroute=api-keys", "anchor": "settings page"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2023-06-12"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2023-06-05"}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Tutorial guide + notebook"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2025-09-08"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK Integrations: LangChain (JS) (version 0.0.7)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Python SDK version 0.2.7 (upcoming)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "TypeScript SDK version 0.3.8 (upcoming)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Data plane (1.1.22)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Autoevals PY (version 0.0.130)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2025-09-01"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2025-08-25"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "TypeScript SDK version 0.3.7"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Python SDK version 0.2.6"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "TypeScript SDK version 0.3.6"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Data plane (1.1.21)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2025-08-18"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Data plane (1.1.20)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2025-08-18"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Python SDK version 0.2.5"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "TypeScript SDK version 0.2.5"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK Integrations: Google ADK (Python) (version 0.1.1)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Python SDK version 0.2.4"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2025-08-11"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Python SDK version 0.2.3"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "TypeScript SDK version 0.2.4"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK Integrations: OpenAI Agents (TS) (version 0.0.2)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Data plane (1.1.19)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2025-08-04"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Python SDK version 0.2.2"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "TypeScript SDK version 0.2.3"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "TypeScript SDK version 0.2.2"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Python SDK version 0.2.1"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2025-07-28"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Data plane (1.1.18)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2025-07-21"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "TypeScript SDK version 0.2.1"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "TypeScript SDK version 0.2.0"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Python SDK version 0.2.0"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Data plane (1.1.15)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2025-07-14"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Python SDK version 0.1.8"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "TypeScript SDK version 0.1.1"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Data plane (1.1.14)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Python SDK version 0.1.7"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "TypeScript SDK version 0.1.0"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2025-07-07"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Data plane (1.1.13)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Data plane (1.1.12) [skipped]"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2025-06-30"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Data plane (1.1.11)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2025-06-23"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Data plane (1.1.7)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Autoevals.js v0.0.130"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "TypeScript SDK version 0.0.209"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "TypeScript SDK version 0.0.208"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2025-06-16"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Python SDK version 0.1.5"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "TypeScript SDK version 0.0.207"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Data plane (1.1.6)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Data plane (1.1.5)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Data plane (1.1.4)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2025-06-09"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Python SDK version 0.1.4"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "TypeScript SDK version 0.0.206"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2025-06-02"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Python SDK version 0.1.3"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2025-05-26"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Python SDK version 0.1.2"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "TypeScript SDK version 0.0.206 [upcoming]"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2025-05-19"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "TypeScript SDK version 0.0.205"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Python SDK version 0.1.1"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "TypeScript SDK version 0.0.204"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2025-05-12"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.1.0)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.203)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2025-05-05"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.202)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2025-04-28"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2025-04-21"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.201)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.200)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2025-04-14"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.199)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.198)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2025-04-07"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "API (version 0.0.65)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.197)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "API (version 0.0.65)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2025-03-31"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "API (version 0.0.64)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.196)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2025-03-24"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.195)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.194)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.193)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2025-03-17"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.192)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Autoevals (version 0.0.124)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2025-03-10"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Autoevals (version 0.0.123)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK Integrations: LangChain (Python) (version 0.0.2)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK Integrations: LangChain.js (version 0.0.6)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK Integrations: Val Town"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.190)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.189)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.188)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2025-03-03"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.187)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2025-02-24"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "API (version 0.0.63)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.187)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2025-02-17"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.187)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2025-02-10"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2025-02-03"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2025-01-27"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK Integrations: LangChain.js (version 0.0.5)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.184)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2025-01-20"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.183)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2025-01-13"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK Integrations: LangChain.js (version 0.0.4)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.182)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2025-01-06"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.181)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.179)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-12-30"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.179) (unreleased)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.178)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK integrations: LangChain.js (version 0.0.2)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-12-16"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "API (version 0.0.61)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.177)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-12-09"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Autoevals (version 0.0.110)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.176)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK integrations: LangChain.js (version 0.0.1)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK integrations: LangChain.js (version 0.0.1)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-12-02"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "API (version 0.0.60)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.175)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-11-25"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.174)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.173)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.172)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-11-18"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.171)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-11-12"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.170)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.169)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.168)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-11-04"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "API (version 0.0.59)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-10-28"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.168)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-10-21"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.167)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "API (version 0.0.59)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-10-14"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.166)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.165)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.164)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-10-07"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.163)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.162)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.161)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-09-30"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-09-23"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.160)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.159)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "API (version 0.0.56)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-09-16"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.158)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-09-09"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-09-02"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "API (version 0.0.54)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.157)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-08-26"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Autoevals (version 0.0.86)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.155)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "API (version 0.0.54)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-08-19"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "API (version 0.0.54)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-08-12"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "API (version 0.0.53)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-08-05"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Autoevals (version 0.0.85)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Autoevals (version 0.0.84)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.151)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-07-29"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.148)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.147)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-07-22"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "API (version 0.0.51)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.146)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-07-15"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Autoevals (version 0.0.80)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Autoevals (version 0.0.77)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-07-08"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Autoevals (version 0.0.76)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-07-01"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.140)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "SDK (version 0.0.138)"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-06-24"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Deployment"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-06-17"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-06-10"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-06-03"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-05-27"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-05-20"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-05-13"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-05-06"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-04-29"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-04-22"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-04-15"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-04-08"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-04-01"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-03-25"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-03-18"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-03-11"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-03-04"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-02-26"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-02-19"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-02-12"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-02-05"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-01-29"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-01-22"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-01-15"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-01-08"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2024-01-01"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2023-12-25"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2023-12-18"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2023-12-11"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2023-12-4"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2023-11-27"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2023-11-20"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2023-11-13"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2023-11-06"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2023-10-30"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2023-10-23"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2023-10-16"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2023-10-09"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2023-10-02"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2023-09-18"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2023-09-11"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2023-09-05"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2023-08-28"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2023-08-21"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2023-08-14"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2023-08-07"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2023-07-31"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2023-07-24"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2023-07-17"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2023-07-10"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2023-07-03"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2023-06-26"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2023-06-19"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2023-06-12"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Week of 2023-06-05"}], "depth": 1}, "https://www.braintrust.dev/articles": {"url": "https://www.braintrust.dev/articles", "title": "Braintrust articles - Braintrust", "text": "Latest articles\nRead\nAI observability: Why traditional monitoring isn't enough\nBuild monitoring strategies designed for AI workloads beyond traditional uptime metrics.\n21 August 2025\nRead\nBest LLM evaluation platforms 2025\nCompare top LLM evaluation platforms: Braintrust, LangSmith, Langfuse, and Arize.\n21 August 2025\nRead\nAI testing and observability infrastructure\nSystematic evaluation and observability become critical infrastructure for reliable AI applications.\n21 August 2025\nRead\nProduction AI integration: From demo to reliable application\nBridge the gap between AI demos and production through architecture patterns.\n21 August 2025\nRead\nAI model testing: A systematic approach to evaluation loops\nBuild structured evaluation loops that turn model selection into data-driven decisions.\n21 August 2025\nRead\nPrompt engineering best practices: Data-driven optimization guide\nTransform prompt development from guesswork into systematic engineering with data-driven optimization.\n21 August 2025\nRead\nHow to test AI models and prompts: A complete guide\nSystematic workflow for testing model and prompt combinations at scale.\n21 August 2025", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/articles/atom", "anchor": ""}, {"href": "https://www.braintrust.dev/articles/ai-observability-monitoring", "anchor": "Read AI observability: Why traditional monitoring isn't enough Build monitoring strategies designed for AI workloads beyond traditional uptime metrics. 21 August 2025"}, {"href": "https://www.braintrust.dev/articles/best-llm-evaluation-platforms-2025", "anchor": "Read Best LLM evaluation platforms 2025 Compare top LLM evaluation platforms: Braintrust, LangSmith, Langfuse, and Arize. 21 August 2025"}, {"href": "https://www.braintrust.dev/articles/infrastructure-behind-ai-development", "anchor": "Read AI testing and observability infrastructure Systematic evaluation and observability become critical infrastructure for reliable AI applications. 21 August 2025"}, {"href": "https://www.braintrust.dev/articles/integrating-ai-into-production", "anchor": "Read Production AI integration: From demo to reliable application Bridge the gap between AI demos and production through architecture patterns. 21 August 2025"}, {"href": "https://www.braintrust.dev/articles/systematic-approach-ai-development", "anchor": "Read AI model testing: A systematic approach to evaluation loops Build structured evaluation loops that turn model selection into data-driven decisions. 21 August 2025"}, {"href": "https://www.braintrust.dev/articles/systematic-prompt-engineering", "anchor": "Read Prompt engineering best practices: Data-driven optimization guide Transform prompt development from guesswork into systematic engineering with data-driven optimization. 21 August 2025"}, {"href": "https://www.braintrust.dev/articles/testing-models-with-prompts-guide", "anchor": "Read How to test AI models and prompts: A complete guide Systematic workflow for testing model and prompt combinations at scale. 21 August 2025"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 1}, "https://www.braintrust.dev/legal/terms-of-service": {"url": "https://www.braintrust.dev/legal/terms-of-service", "title": "Terms of Service - Braintrust", "text": "Terms of Service\nLast Updated: September 21, 2023\nWelcome to the Terms of Service (these \" Terms\") for this website, and any affiliated websites, content features, functionality, tools, services, applications, products, software and other services, or any portion thereof (collectively, the \" Services\") owned or controlled by Braintrust Data, Inc. (\" Company\", \" we\" or \" us\").\nThese Terms govern your access to and use of the Services. Please read these Terms carefully, as they include important information about your legal rights. By accessing and/or using the Services, you are agreeing to these Terms. If you do not understand or agree to these Terms, please do not use the Services.\nFor purposes of these Terms, \" you\" and \" your\" means you as the user of the Services. If you use the Services on behalf of a company or other entity then \"you\" includes you and that entity, and you represent and warrant that (a) you are an authorized representative of the entity with the authority to bind the entity to these Terms, and (b) you agree to these Terms on the entity's behalf.\nSECTION 8 CONTAINS AN ARBITRATION CLAUSE AND CLASS ACTION WAIVER. BY AGREEING TO THESE TERMS, YOU AGREE (A) TO RESOLVE ALL DISPUTES (WITH LIMITED EXCEPTION) RELATED TO THE COMPANY'S SERVICES AND/OR PRODUCTS THROUGH BINDING INDIVIDUAL ARBITRATION, WHICH MEANS THAT YOU WAIVE ANY RIGHT TO HAVE THOSE DISPUTES DECIDED BY A JUDGE OR JURY, AND (B) TO WAIVE YOUR RIGHT TO PARTICIPATE IN CLASS ACTIONS, CLASS ARBITRATIONS, OR REPRESENTATIVE ACTIONS, AS SET FORTH BELOW. YOU HAVE THE RIGHT TO OPT-OUT OF THE ARBITRATION CLAUSE AND THE CLASS ACTION WAIVER AS EXPLAINED IN SECTION 8.\n1. Who May Use the Services\nYou must be 13 years of age or older and reside in the United States or any of its territories to use the Services.Minors under the age of majority in their jurisdiction but that are at least 13 years of age are only permitted to use the Services if the minor's parent or guardian accepts these Terms on the minor's behalf prior to use of the Services. Children under the age of 13 are not permitted to use the Services. By using the Services, you represent and warrant that you meet these requirements.\n2. User Accounts, SUBSCRIPTIONS and free trials\n2.1 Creating and Safeguarding your Account. To use the Services, you need to create an account or link another account, such as your Google account (\" Account\"). You agree to provide us with accurate, complete and updated information for your Account. You are solely responsible for any activity on your Account and for maintaining the confidentiality and security of your password. We are not liable for any acts or omissions by you in connection with your Account. You must immediately notify us at info@braintrustdata.com if you know or have any reason to suspect that your Account or password have been stolen, misappropriated or otherwise compromised, or in case of any actual or suspected unauthorized use of your Account. You agree not to create any Account if we have previously removed your, or we previously banned you from any of our Services, unless we provide written consent otherwise.\n2.2 Subscription Payment. If you buy or subscribe to any of our paid Services, you agree to pay us the applicable fees and taxes in U.S. Dollars. Failure to pay these fees and taxes will result in the termination of your access to the paid Services. You agree that (a) if you purchase a recurring subscription to any of the Services, we may store and continue billing your payment method (e.g. credit card) to avoid interruption of such Services, and (b) we may calculate taxes payable by you based on the billing information that you provide us at the time of purchase. We reserve the right to change our subscription plans or adjust pricing for the Services in any manner and at any time as we may determine in our sole and absolute discretion. Except as otherwise provided in these Terms, any price changes or changes to your subscription plan will take effect following reasonable notice to you. All subscriptions are payable in accordance with payment terms in effect at the time the subscription becomes payable. Payment can be made by credit card, debit card, or other means that we may make available. Subscriptions will not be processed until payment has been received in full, and any holds on your account by any other payment processor are solely your responsibility.\n2.3 Subscription Renewals and Cancellations. You agree that if you purchase a subscription, your subscription will automatically renew at the subscription period frequency referenced on your subscription page (or if not designated, then monthly) and at the then-current rates, and your payment method will automatically be charged at the start of each new subscription period for the fees and taxes applicable to that period. To avoid future subscription charges, you must cancel your subscription 30 days before the subscription period renewal date by doing the following: email support@braintrustdata.com.\n2.4 No Subscription Refunds. Except as expressly set forth in these Terms, payments for any subscriptions to the Services are nonrefundable and there are no credits for partially used periods. Following any cancellation by you, however, you will continue to have access to the paid Services through the end of the subscription period for which payment has already been made.\n2.5 Free Trials. You can sign up for a trial Account for the paid portion of the Services and your trial period starts on the day you create the trial Account and lasts for the duration indicated on your free trial confirmation email (or if not specified, then 30 days). If you are on a trial, you may cancel at any time until the last day of your trial by following the cancellation procedures outlined in Section 2.3 above. If you do not cancel your trial Account at the end of your free trial period, and we have notified you that your Account will be converted to a paid subscription at the end of the free trial period, you authorize us to charge your credit card or other designated billing method for continued use of the paid Services.** You may, however, then cancel your subscription in accordance with Section 2.3 of these Terms. **If you cancel your trial Account or decide not to purchase a paid version of the Services at the end of your trial period, your content or data associated with your trial Account will no longer be available to you, and the Company may delete or remove any such content or data.\n3. Location of Our Privacy Policy\n3.1 Privacy Policy. Our Privacy Policy describes how we handle the information you provide to us when you use the Services. For an explanation of our privacy practices, please visit our Privacy Policy located at https://braintrustdata.com/legal/privacy-policy.\n4. Rights We Grant You\n4.1. Right to Use Services. We hereby permit you to use the Services for your personal non-commercial use only, provided that you comply with these Terms in connection with all such use. If any software, content or other materials owned or controlled by us are distributed to you as part of your use of the Services, we hereby grant you, a personal, non-assignable, non-sublicensable, non-transferrable, and non-exclusive right and license to access and display such software, content and materials provided to you as part of the Services, in each case for the sole purpose of enabling you to use the Services as permitted by these Terms. Your access and use of the Services may be interrupted from time to time for any of several reasons, including, without limitation, the malfunction of equipment, periodic updating, maintenance or repair of the Service or other actions that Company, in its sole discretion, may elect to take.\n4.2 Restrictions On Your Use of the Services. You may not do any of the following in connection with your use of the Services, unless applicable laws or regulations prohibit these restrictions or you have our written permission to do so:\na. download, modify, copy, distribute, transmit, display, perform, reproduce, duplicate, publish, license, create derivative works from, or offer for sale any information contained on, or obtained from or through, the Services, except for temporary files that are automatically cached by your web browser for display purposes, or as otherwise expressly permitted in these Terms;\nb. duplicate, decompile, reverse engineer, disassemble or decode the Services (including any underlying idea or algorithm), or attempt to do any of the same;\nc. use, reproduce or remove any copyright, trademark, service mark, trade name, slogan, logo, image, or other proprietary notation displayed on or through the Services;\nd. access or use the Services in any manner that could disable, overburden, damage, disrupt or impair the Services or interfere with any other party's access to or use of the Services or use any device, software or routine that causes the same;\ne. attempt to gain unauthorized access to, interfere with, damage or disrupt the Services, or the computer systems or networks connected to the Services;\nf. circumvent, remove, alter, deactivate, degrade or thwart any technological measure or content protections of the Services;\ng. introduce any viruses, trojan horses, worms, logic bombs or other materials that are malicious or technologically harmful into our systems;\nh. submit, transmit, display, perform, post or store any content that is inaccurate, unlawful, defamatory, obscene, lewd, lascivious, filthy, excessively violent, pornographic, invasive of privacy or publicity rights, harassing, threatening, abusive, inflammatory, harmful, hateful, cruel or insensitive, deceptive, or otherwise objectionable, use the Services for illegal, harassing, bullying, unethical or disruptive purposes, or otherwise use the Services in a manner that is obscene, lewd, lascivious, filthy, excessively violent, harassing, harmful, hateful, cruel or insensitive, deceptive, threatening, abusive, inflammatory, pornographic, inciting, organizing, promoting or facilitating violence or criminal or harmful activities, defamatory, obscene or otherwise objectionable;\ni. violate any applicable law or regulation in connection with your access to or use of the Services;or\nj. access or use the Services in any way not expressly permitted by these Terms.\n** 4.3 Beta Offerings.** From time to time, we may, in our sole discretion, include certain test or beta features or products in the Services (\" Beta Offerings\") as we may designate from time to time. Your use of any Beta Offering is completely voluntary. The Beta Offerings are provided on an \"as is\" basis and may contain errors, defects, bugs, or inaccuracies that could cause failures, corruption or loss of data and information from any connected device. You acknowledge and agree that all use of any Beta Offering is at your sole risk. You agree that once you use a Beta Offering, your content or data may be affected such that you may be unable to revert back to a prior non-beta version of the same or similar feature. Additionally, if such reversion is possible, you may not be able to return or restore data created within the Beta Offering back to the prior non-beta version. If we provide you any Beta Offerings on a closed beta or confidential basis, we will notify you of such as part of your use of the Beta Offerings. For any such confidential Beta Offerings, you agree to not disclose, divulge, display, or otherwise make available any of the Beta Offerings without our prior written consent.\n5. Ownership and Content\n5.1 Ownership of the Services. The Services, including their \"look and feel\" (e.g., text, graphics, images, logos), proprietary content, information and other materials, are protected under copyright, trademark and other intellectual property laws. You agree that the Company and/or its licensors own all right, title and interest in and to the Services (including any and all intellectual property rights therein) and you agree not to take any action(s) inconsistent with such ownership interests. We and our licensors reserve all rights in connection with the Services and its content (other than Your Content), including, without limitation, the exclusive right to create derivative works.\n5.2 Ownership of Feedback. We welcome feedback, comments and suggestions for improvements to the Services (\" Feedback\"). You acknowledge and expressly agree that any contribution of Feedback does not and will not give or grant you any right, title or interest in the Services or in any such Feedback. All Feedback becomes the sole and exclusive property of the Company, and the Company may use and disclose Feedback in any manner and for any purpose whatsoever without further notice or compensation to you and without retention by you of any proprietary or other right or claim. You hereby assign to the Company any and all right, title and interest (including, but not limited to, any patent, copyright, trade secret, trademark, show-how, know-how, moral rights and any and all other intellectual property right) that you may have in and to any and all Feedback.\n5.3 Your Content License Grant. In connection with your use of the Services, you may be able to upload or submit content through the Services (\" Your Content\"). In order to operate the Service, we must obtain from you certain license rights in Your Content so that actions we take in operating the Service are not considered legal violations. Accordingly, by using the Service and uploading Your Content, you grant us a license to access, use, host, cache, store, reproduce, transmit, display, publish, distribute, and modify (for technical purposes, e.g., making sure content is viewable on smartphones as well as computers and other devices) Your Content but solely as required to be able to operate and provide the Services. By uploading or submitting Your Content through the Services, you represent and warrant that you have, or have obtained, all rights, licenses, consents, permissions, power and/or authority necessary to grant the rights granted herein for Your Content. You agree that Your Content will not contain material subject to copyright or other proprietary rights, unless you have the necessary permission or are otherwise legally entitled to post the material and to grant us the license described above.\n6. Third Party Services and Materials\n6.1 Use of Third Party Materials in the Services. Certain aspects Services may display, include or make available content, data, information, applications or materials from third parties (\" Third Party Materials\") or provide links to certain third party websites. By using the Services, you acknowledge and agree that the Company is not responsible for examining or evaluating the content, accuracy, completeness, availability, timeliness, validity, copyright compliance, legality, decency, quality or any other aspect of such Third Party Materials or websites. We do not warrant or endorse and do not assume and will not have any liability or responsibility to you or any other person for any third-party services, Third Party Materials or third-party websites, or for any other materials, products, or services of third parties. Third Party Materials and links to other websites are provided solely as a convenience to you.\n7. Disclaimers, Limitations of Liability and Indemnification\n7.1 Disclaimers. Your access to and use of the Services are at your own risk. You understand and agree that the Services are provided to you on an \"AS IS\" and \"AS AVAILABLE\" basis. Without limiting the foregoing, to the maximum extent permitted under applicable law, the Company, its parents, affiliates, related companies, officers, directors, employees, agents, representatives, partners and licensors (the \" the Company Entities\") DISCLAIM ALL WARRANTIES AND CONDITIONS, WHETHER EXPRESS OR IMPLIED, OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE OR NON-INFRINGEMENT. The Company Entities make no warranty or representation and disclaim all responsibility and liability for: (a) the completeness, accuracy, availability, timeliness, security or reliability of the Services; (b) any harm to your computer system, loss of data, or other harm that results from your access to or use of the Services; (c) the operation or compatibility with any other application or any particular system or device; (d) whether the Services will meet your requirements or be available on an uninterrupted, secure or error-free basis; and (e) the deletion of, or the failure to store or transmit, Your Content and other communications maintained by the Services. No advice or information, whether oral or written, obtained from the Company Entities or through the Services, will create any warranty or representation not expressly made herein.\n7.2 Limitations of Liability. TO THE EXTENT NOT PROHIBITED BY LAW, YOU AGREE THAT IN NO EVENT WILL THE COMPANY ENTITIES BE LIABLE (A) FOR INDIRECT SPECIAL, EXEMPLARY, INCIDENTAL, CONSEQUENTIAL OR PUNITIVE DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES, LOSS OF USE, DATA OR PROFITS, BUSINESS INTERRUPTION OR ANY OTHER DAMAGES OR LOSSES, ARISING OUT OF OR RELATED TO YOUR USE OR INABILITY TO USE THE SERVICES), HOWEVER CAUSED AND UNDER ANY THEORY OF LIABILITY, WHETHER UNDER THESE TERMS OR OTHERWISE ARISING IN ANY WAY IN CONNECTION WITH THE SERVICES OR THESE TERMS AND WHETHER IN CONTRACT, STRICT LIABILITY OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) EVEN IF THE COMPANY ENTITIES HAVE BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGE, OR (B) FOR ANY OTHER CLAIM, DEMAND OR DAMAGES WHATSOEVER RESULTING FROM OR ARISING OUT OF OR IN CONNECTION WITH THESE TERMS OR THE DELIVERY, USE OR PERFORMANCE OF THE SERVICES. THE COMPANY ENTITIES' TOTAL LIABILITY TO YOU FOR ANY DAMAGES FINALLY AWARDED SHALL NOT EXCEED THE GREATER OF ONE HUNDRED DOLLARS ($100.00), OR THE AMOUNT YOU PAID THE COMPANY ENTITIES, IF ANY, IN THE PAST SIX (6) MONTHS FOR THE SERVICES (OR OFFERINGS PURCHASED ON THE SERVICES) GIVING RISE TO THE CLAIM. THE FOREGOING LIMITATIONS WILL APPLY EVEN IF THE ABOVE STATED REMEDY FAILS OF ITS ESSENTIAL PURPOSE.\n7.3 Indemnification. By entering into these Terms and accessing or using the Services, you agree that you shall defend, indemnify and hold the Company Entities harmless from and against any and all claims, costs, damages, losses, liabilities and expenses (including attorneys' fees and costs) incurred by the Company Entities arising out of or in connection with: (a) your violation or breach of any term of these Terms or any applicable law or regulation; (b) your violation of any rights of any third party; (c) your misuse of the Services; (d) Your Content, or (e) your negligence or wilful misconduct. If you are obligated to indemnify any Company Entity hereunder, then you agree that Company (or, at its discretion, the applicable Company Entity) will have the right, in its sole discretion, to control any action or proceeding and to determine whether Company wishes to settle, and if so, on what terms, and you agree to fully cooperate with Company in the defense or settlement of such claim.\n8. ARBITRATION AND CLASS ACTION WAIVER\n8.1 Arbitration Agreement and Class Action Waiver. Any dispute, controversy, or claim (collectively, \" Claim\") relating in any way to the Company's services and/or products, including the Services, and any use or access or lack of access thereto, will be resolved by arbitration, including threshold questions of arbitrability of the Claim. You and the Company agree that any Claim will be settled by final and binding arbitration, using the English language, administered by JAMSunder its Comprehensive Arbitration Rules and Procedures (the \" JAMS Rules\") then in effect (those rules are deemed to be incorporated by reference into this section, and as of the date of these Terms). Because your contract with the Company, these Terms, and this Arbitration Agreement concern interstate commerce, the Federal Arbitration Act (\" FAA\") governs the arbitrability of all disputes. However, the arbitrator will apply applicable substantive law consistent with the FAA and the applicable statute of limitations or condition precedent to suit. Arbitration will be handled by a sole arbitrator in accordance with the JAMS Rules. Judgment on the arbitration award may be entered in any court that has jurisdiction. Any arbitration under these Terms will take place on an individual basis \u2013 class arbitrations and class actions are not permitted. You understand that by agreeing to these Terms, you and the Company are each waiving the right to trial by jury or to participate in a class action or class arbitration.\n8.2 Exceptions. Notwithstanding the foregoing, you and the Company agree that the following types of disputes will be resolved in a court of proper jurisdiction:\na. disputes or claims within the jurisdiction of a small claims court consistent with the jurisdictional and dollar limits that may apply, as long as it is brought and maintained as an individual dispute and not as a class, representative, or consolidated action or proceeding;\nb. disputes or claims where the sole form of relief sought is injunctive relief (including public injunctive relief); or\nc. intellectual property disputes.\n- Costs of Arbitration. Fees and costs may be awarded as provided pursuant to applicable law. If the arbitrator finds that either the substance of your claim or the relief sought in the demand is frivolous or brought for an improper purpose (as measured by the standards set forth in Federal Rule of Civil Procedure 11(b)), then the payment of all fees will be governed by the JAMS rules. In that case, you agree to reimburse the Company for all monies previously disbursed by it that are otherwise your obligation to pay under the applicable rules. If you prevail in the arbitration and are awarded an amount that is less than the last written settlement amount offered by the Company before the arbitrator was appointed, the Company will pay you the amount it offered in settlement. The arbitrator may make rulings and resolve disputes as to the payment and reimbursement of fees or expenses at any time during the proceeding and upon request from either party made within 14 days of the arbitrator's ruling on the merits\n**8.3 WAIVER OF RIGHT TO BRING CLASS ACTION AND REPRESENTATIVE CLAIMS. To the fullest extent permitted by applicable law, you and the Company each agree that any proceeding to resolve any dispute, claim, or controversy will be brought and conducted ONLY IN THE RESPECTIVE PARTY'S INDIVIDUAL CAPACITY AND NOT AS PART OF ANY CLASS (OR PURPORTED CLASS), CONSOLIDATED, MULTIPLE-PLAINTIFF, OR REPRESENTATIVE ACTION OR PROCEEDING (\"CLASS ACTION\"). You and the Company AGREE TO WAIVE THE RIGHT TO PARTICIPATE AS A PLAINTIFF OR CLASS MEMBER IN ANY CLASS ACTION. You and the Company EXPRESSLY WAIVE ANY ABILITY TO MAINTAIN A CLASS ACTION IN ANY FORUM. If the dispute is subject to arbitration, THE ARBITRATOR WILL NOT HAVE THE AUTHORITY TO COMBINE OR AGGREGATE CLAIMS, CONDUCT A CLASS ACTION, OR MAKE AN AWARD TO ANY PERSON OR ENTITY NOT A PARTY TO THE ARBITRATION. Further, you and the Company agree that the ARBITRATOR MAY NOT CONSOLIDATE PROCEEDINGS FOR MORE THAN ONE PERSON'S CLAIMS, AND IT MAY NOT OTHERWISE PRESIDE OVER ANY FORM OF A CLASS ACTION. For the avoidance of doubt, however, you can seek public injunctive relief to the extent authorized by law and consistent with the Exceptions clause above.\n- IF THIS CLASS ACTION WAIVER IS LIMITED, VOIDED, OR FOUND UNENFORCEABLE, THEN, UNLESS THE PARTIES MUTUALLY AGREE OTHERWISE, THE PARTIES' AGREEMENT TO ARBITRATE SHALL BE NULL AND VOID WITH RESPECT TO SUCH PROCEEDING SO LONG AS THE PROCEEDING IS PERMITTED TO PROCEED AS A CLASS ACTION. If a court decides that the limitations of this paragraph are deemed invalid or unenforceable, any putative class, private attorney general, or consolidated or representative action must be brought in a court of proper jurisdiction and not in arbitration.\n9. Additional Provisions\n9.1 Updating These Terms. We may modify these Terms from time to time in which case we will update the \"Last Revised\" date at the top of these Terms. If we make changes that are material, we will use reasonable efforts to attempt to notify you, such as by e-mail and/or by placing a prominent notice on the first page of our primary website. However, it is your sole responsibility to review these Terms from time to time to view any such changes. The updated Terms will be effective as of the time of posting, or such later date as may be specified in the updated Terms. Your continued access or use of the Services after the modifications have become effective will be deemed your acceptance of the modified Terms. No amendment shall apply to a dispute for which an arbitration has been initiated prior to the change in Terms\n9.2 Termination of License and Your Account. If you breach any of the provisions of these Terms, all licenses granted by the Company will terminate automatically. Additionally, the Company may suspend, disable, or delete your Account and/or the Services (or any part of the foregoing) with or without notice, for any or no reason. If the Company deletes your Account for any suspected breach of these Terms by you, you are prohibited from re-registering for the Services under a different name. In the event of Account deletion for any reason, the Company may, but is not obligated to, delete any of Your Content. the Company shall not be responsible for the failure to delete or deletion of Your Content. All sections which by their nature should survive the termination of these Terms shall continue in full force and effect subsequent to and notwithstanding any termination of these Terms by the Company or you. Termination will not limit any of the Company's other rights or remedies at law or in equity.\n9.3 Injunctive Relief. You agree that a breach of these Terms will cause irreparable injury to the Company for which monetary damages would not be an adequate remedy and the Company shall be entitled to equitable relief in addition to any remedies it may have hereunder or at law without a bond, other security or proof of damages.\n9.4 California Residents. If you are a California resident, in accordance with Cal. Civ. Code \u00a7 1789.3, you may report complaints to the Complaint Assistance Unit of the Division of Consumer Services of the California Department of Consumer Affairs by contacting them in writing at 1625 North Market Blvd., Suite N 112 Sacramento, CA 95834, or by telephone at (800) 952-5210.\n9.5 U.S. Government Restricted Rights. The Services and related documentation are \"Commercial Items\", as that term is defined at 48 C.F.R. \u00a72.101, consisting of \"Commercial Computer Software\" and \"Commercial Computer Software Documentation\", as such terms are used in 48 C.F.R. \u00a712.212 or 48 C.F.R. \u00a7227.7202, as applicable. Consistent with 48 C.F.R. \u00a712.212 or 48 C.F.R. \u00a7227.7202-1 through 227.7202-4, as applicable, the Commercial Computer Software and Commercial Computer Software Documentation are being licensed to U.S. Government end users (a) only as Commercial Items, and (b) with only those rights as are granted to all other end users pursuant to the terms and conditions herein.\n9.6 Export Laws. You agree that you will not export or re-export, directly or indirectly, the Services and/or other information or materials provided by the Company hereunder, to any country for which the United States or any other relevant jurisdiction requires any export license or other governmental approval at the time of export without first obtaining such license or approval. In particular, but without limitation, the Services may not be exported or re-exported (a) into any U.S. embargoed countries or any country that has been designated by the U.S. Government as a \"terrorist supporting\" country, or (b) to anyone listed on any U.S. Government list of prohibited or restricted parties, including the U.S. Treasury Department's list of Specially Designated Nationals or the U.S. Department of Commerce Denied Person's List or Entity List. By using the Services, you represent and warrant that you are not located in any such country or on any such list. You are responsible for and hereby agree to comply at your sole expense with all applicable United States export laws and regulations.\n9.7 Miscellaneous. If any provision of these Terms shall be unlawful, void or for any reason unenforceable, then that provision shall be deemed severable from these Terms and shall not affect the validity and enforceability of any remaining provisions. These Terms and the licenses granted hereunder may be assigned by the Company but may not be assigned by you without the prior express written consent of the Company. No waiver by either party of any breach or default hereunder shall be deemed to be a waiver of any preceding or subsequent breach or default. The section headings used herein are for reference only and shall not be read to have any legal effect. The Services are operated by us in the United States. Those who choose to access the Services from locations outside the United States do so at their own initiative and are responsible for compliance with applicable local laws. These Terms are governed by the laws of the State of California, without regard to conflict of laws rules, and the proper venue for any disputes arising out of or relating to any of the same will be the arbitration venue set forth in Section 8, or if arbitration does not apply, then the state and federal courts located in San Francisco, California.\n9.8 How to Contact Us. You may contact us regarding the Services or these Terms at: 548 Market St PMB 96611, San Francisco, California 94104-5401, by phone at (707) 682-7588 or by e-mail at info@braintrustdata.com.", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "1. Who May Use the Services"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "2. User Accounts, SUBSCRIPTIONS and free trials"}, {"href": "mailto:info@braintrustdata.com", "anchor": "info@braintrustdata.com"}, {"href": "mailto:support@braintrustdata.com", "anchor": "support@braintrustdata.com"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "3. Location of Our Privacy Policy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "4. Rights We Grant You"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "5. Ownership and Content"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "6. Third Party Services and Materials"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "7. Disclaimers, Limitations of Liability and Indemnification"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "8. ARBITRATION AND CLASS ACTION WAIVER"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "9. Additional Provisions"}, {"href": "mailto:info@braintrustdata.com", "anchor": "info@braintrustdata.com"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 1}, "https://www.braintrust.dev/legal/privacy-policy": {"url": "https://www.braintrust.dev/legal/privacy-policy", "title": "Privacy Policy - Braintrust", "text": "Braintrust Privacy Notice\nLast Updated: September 21, 2023\nThis Privacy Notice describes how Braintrust Data, Inc. (\" we\", \" us\", \" our\") collects, uses and discloses information about you when you use our website (braintrustdata.com), applications, services, tools and features, or otherwise interact with us (collectively, the \" Services\"). For the purposes of this Privacy Notice, \" you\" and \" your\" means you as the user of the Services. Please note that this Privacy Notice does not apply to information that our customers upload to or otherwise process using our Services. To learn more about how such information is collected, used, and disclosed, please contact the relevant BrainTrust customer.\nAt BrainTrust, we believe that you are in control of your information, and we are committed to privacy. Please read this Privacy Notice carefully. By using any of the Services, you agree to the collection, use, and disclosure of your information as described in this Privacy Notice. If you do not agree to this Privacy Notice, please do not use or access the Services.\n1. CHANGES TO THIS PRIVACY NOTICE\nWe may modify this Privacy Notice from time to time, in which case we will update the \"Last Updated\" date at the top of this Privacy Notice. If we make material changes to how we use or disclose information we collect, we will use reasonable efforts to notify you (such as by emailing you at the last email address you provided us, by posting notice of such changes on the Services, or by other means consistent with applicable law) and will take additional steps as required by applicable law. If you do not agree toany updates to this Privacy Notice, please do not continue using or accessing the Services.\n2. COLLECTION AND USE OF INFORMATION\nWhen you use or access the Services, we collect certain categories of information about you from a variety of sources.\nInformation You Provide to Us\nSome features of the Services may require you to directly provide certain information about yourself. You may elect not to provide this information, but doing so may prevent you from using or accessing these features. Information that you directly submit through our Services includes:\n- Basic contact and account details , such as name, organization name, email address and password. We use this information to create, maintain, and secure your account and provide the Services, and to communicate with you(including to tell you about products or services that may be of interest to you). If you choose to register an account, you are responsible for keeping your account credentials safe. We recommend you do not share your access details with anyone else. If you believe your account has been compromised, please contact us immediately.\n- Payment information , such as bank account, credit or debit card information, and billing address. We use this information to process your payment and provide the Services.\n- Applicant details , such as information included in your resume or CV, references, and job history. We use applicant details to process your application for employment and to evaluate your candidacy.\n- Any other information you choose to include in communications with us , for example, when sending a message through the Services.\nInformation Collected Automatically\nWe may also use cookies or other tracking technologies to automatically collect certain information about your interactions with the Services. We collect and use this information to tailor your experience with the Services, run analytics, better understand user interactions with the Services, etc. Such information includes:\n- Device information , such as device type, operating system, unique device identifier, and internet protocol (IP) address.\n- Other information regarding your interaction with the Services , such as browser type, log data, date and time stamps, and clickstream data.\nMost browsers accept cookies automatically, but you may be able to control the way in which your devices permit the use of cookies. If you so choose, you may block or delete certain of our cookies from your browser; however, blocking or deleting cookies may cause some of the Services, including any portal features and general functionality, to work incorrectly. Your browser settings may also allow you to transmit a \"Do Not Track\" signal when you visit various websites. Like many websites, our website is not designed to respond to \"Do Not Track\" signals received from browsers. To learn more about \"Do Not Track\" signals, you can visit http://www.allaboutdnt.com/.\nInformation Collected From Other Sources\nWe may obtain information about you from outside sources, including information that we collect directly from third parties and information from third parties that you choose to share with us. Such information includes:\n-\nAnalytics data we receive from analytics providers such as Twilio Segment which we use to improve our website, communications and the Services.\n-\nInformation we receive from career websites , such as LinkedIn, Monster, or Indeed, which we use to process your application for employment.\n-\nInformation we receive when you choose to link any social media platforms to your account or login to your account using any third party platforms , such as Google, which we use to maintain your account and login information.\nAny information we receive from outside sources will be treated in accordance with this Privacy Notice. We are not responsible for the accuracy of the information provided to us by third parties and are not responsible for any third party's policies or practices. For more information, see the section below, Third Party Websites and Links.\nIn addition to the specific uses described above, we may use any of the above information to provide you with the Services and to maintain our business relationship, including by enhancing the safety and security of our Services (e.g., troubleshooting, data analysis, testing, system maintenance, and reporting), providing customer support, sending service and other non-marketing communications, monitoring and analyzing trends, conducting internal research and development, complying with applicable legal obligations, enforcing any applicable terms of service, and protecting the Services, our rights, and the rights of our employees, users or other individuals.\nFinally, we may deidentify your information such that it cannot reasonably be used to infer information about you or otherwise be linked to you (or we may collect information that has already been deidentified), and we may use such deidentified information for any purpose.\n3. DISCLOSURE OF YOUR INFORMATION\nWe may disclose your information for legitimate purposes subject to this Privacy Notice, including the following categories of third parties:\n- Vendors or other service providers who help us provide the Services, including for system administration, cloud storage, security, customer relationship management, marketing communications, web analytics, payment networks, and payment processing.\n- Third parties to whom you request or direct us to disclose information, such as through your use of social media widgets or login integration.\n- Professional advisors, such as auditors, law firms, or accounting firms.\n- Third parties in connection with or anticipation of an asset sale, merger, bankruptcy, or other business transaction.\nWe may also disclose your information as needed to comply with applicable law or any obligations thereunder or to cooperate with law enforcement, judicial orders, and regulatory inquiries, to enforce any applicable terms of service, and to ensure the safety and security of our business, employees, and users.\n4. SOCIAL FEATURES\nCertain features of the Services allow you to initiate interactions between the Services and third-party services or platforms, such as social networks (\" Social Features\"). Social Features include features that allow you to access our pages on third-party platforms, and from there \"like\" or \"share\" our content. Use of Social Features may allow a third party to collect and/or use your information. If you use Social Features, information you post or make accessible may be publicly displayed by the third-party service. Both we and the third party may have access to information about you and your use of both the Services and the third-party service. For more information, see the section below, Third Party Websites and Links.\n5. THIRD PARTY WEBSITES AND LINKS\nWe may provide links to third-party websites or platforms. If you follow links to sites or platforms that we do not control and are not affiliated with us, you should review the applicable privacy notice, policies and other terms. We are not responsible for the privacy or security of, or information found on, these sites or platforms. Information you provide on public or semi-public venues, such as third-party social networking platforms, may also be viewable by other users of the Services and/or users of those third-party platforms without limitation as to its use. Our inclusion of such links does not, by itself, imply any endorsement of the content on such platforms or of their owners or operators.\n6. CHILDREN'S PRIVACY\nOur Services are not intended for children, and we do not seek or knowingly collect any personal information about children. If we become aware that we have unknowingly collected information about a child, in particular any child under 13 years of age, we will make commercially reasonable efforts to delete such information from our database. If you are the parent or guardian of a child under 13 years of age who has provided us with their personal information, you may contact us using the below information to request that it be deleted.\n7. DATA SECURITY AND RETENTION\nDespite our reasonable efforts to protect your information, no security measures are impenetrable, and we cannot guarantee \"perfect security.\" Any information you send to us electronically, while using the Services or otherwise interacting with us, may not be secure while in transit. We recommend that you do not use unsecure channels to send us sensitive or confidential information.\nWe retain your information for as long as is reasonably necessary for the purposes specified in this Privacy Notice. When determining the length of time to retain your information, we consider various criteria, including whether we need the information to continue to provide you the Services, resolve a dispute, enforce our contractual agreements, prevent harm, promote safety, security and integrity, or protect ourselves, including our rights, property or products.\n8. HOW TO CONTACT US\nShould you have any questions about our privacy practices or this Privacy Notice, please email us at info@braintrustdata.com or contact us at 548 Market St PMB 96611, San Francisco, California 94104-5401 or by phone at (707) 682-7588.", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Braintrust Privacy Notice"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "1. CHANGES TO THIS PRIVACY NOTICE"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "2. COLLECTION AND USE OF INFORMATION"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "3. DISCLOSURE OF YOUR INFORMATION"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "4. SOCIAL FEATURES"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "5. THIRD PARTY WEBSITES AND LINKS"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "6. CHILDREN'S PRIVACY"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "7. DATA SECURITY AND RETENTION"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "8. HOW TO CONTACT US"}, {"href": "mailto:info@braintrustdata.com", "anchor": "info@braintrustdata.com"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 1}, "https://www.braintrust.dev/playground": {"url": "https://www.braintrust.dev/playground", "title": "Playground - Braintrust", "text": "Sign in\nSign up for free\nPlayground\nDiff\nExperiments\nLoop\nRun", "links": [{"href": "https://www.braintrust.dev/signin?redirect_url=%2Fapp%2Fcreate-playground", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup?redirect_url=%2Fapp%2Fsetup%3Freferrer%3Dplayground", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}], "depth": 2}, "https://trust.braintrust.dev": {"url": "https://trust.braintrust.dev", "title": "Trust Center", "text": "", "links": [], "depth": 2}, "https://www.braintrust.dev/docs/start": {"url": "https://www.braintrust.dev/docs/start", "title": "Get started - Docs - Start - Braintrust", "text": "Get started with Braintrust\nBraintrust is an end-to-end platform for building AI applications. It makes software development with large language models (LLMs) robust and iterative.\nIterative experimentation\nRapidly prototype with different prompts\nand models in the playground\nPerformance insights\nBuilt-in tools to evaluate how models and prompts are performing in production, and dig into specific examples\nReal-time monitoring\nLog, monitor, and take action on real-world interactions with robust and flexible monitoring\nData management\nManage and review data to store and version\nyour test sets centrally\nWhat makes Braintrust powerful is how these tools work together. With Braintrust, developers can move faster, run more experiments, and ultimately build better AI products.", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started with Braintrust"}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Iterative experimentation"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "playground"}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Performance insights"}, {"href": "https://www.braintrust.dev/docs/guides/evals", "anchor": "evaluate"}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Real-time monitoring"}, {"href": "https://www.braintrust.dev/docs/guides/logging", "anchor": "Log"}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Data management"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Manage"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "review"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}], "depth": 2}, "https://www.braintrust.dev/docs/start/frameworks": {"url": "https://www.braintrust.dev/docs/start/frameworks", "title": "Frameworks - Docs - Start - Braintrust", "text": "Trace with existing frameworks\nTrace your apps using existing frameworks to quickly add observability. This guide walks you through the supported frameworks and how to configure them for maximum observability and insight.\nTrace your apps using existing frameworks to quickly add observability. This guide walks you through the supported frameworks and how to configure them for maximum observability and insight.", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/opentelemetry", "anchor": "OpenTelemetry (OTel)"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/vercel-ai-sdk", "anchor": "Vercel AI SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/openai-agents-sdk", "anchor": "OpenAI Agents SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/instructor", "anchor": "Instructor"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/langchain", "anchor": "LangChain"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/langgraph", "anchor": "LangGraph"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/google", "anchor": "Google ADK"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Trace with existing frameworks"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/opentelemetry", "anchor": "OpenTelemetry"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/vercel-ai-sdk", "anchor": "Vercel AI SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/openai-agents-sdk", "anchor": "Agents SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/instructor", "anchor": "Instructor"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/langchain", "anchor": "LangChain"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/langgraph", "anchor": "LangGraph"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/google", "anchor": "Google ADK"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}], "depth": 2}, "https://www.braintrust.dev/docs/providers": {"url": "https://www.braintrust.dev/docs/providers", "title": "Model providers - Docs - Braintrust", "text": "Model providers\nConfigure model and cloud providers to run AI models in playgrounds, experiments, and online scores.\nBraintrust supports a wide range of model providers out of the box via the Braintrust API Proxy. This allows you to add custom providers to work with any AI service. Braintrust provides the logging, evaluation, and observability tools you need regardless of which models you choose. Learn more about custom providers.\nProxy model providers\nBraintrust comes with several pre-configured providers that you can use to interact with different language models.\nProxy cloud providers\nBraintrust also supports several cloud providers by default.", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/providers/openai", "anchor": "OpenAI"}, {"href": "https://www.braintrust.dev/docs/providers/anthropic", "anchor": "Anthropic"}, {"href": "https://www.braintrust.dev/docs/providers/gemini", "anchor": "Gemini"}, {"href": "https://www.braintrust.dev/docs/providers/mistral", "anchor": "Mistral"}, {"href": "https://www.braintrust.dev/docs/providers/together", "anchor": "Together"}, {"href": "https://www.braintrust.dev/docs/providers/fireworks", "anchor": "Fireworks"}, {"href": "https://www.braintrust.dev/docs/providers/perplexity", "anchor": "Perplexity"}, {"href": "https://www.braintrust.dev/docs/providers/xai", "anchor": "xAI"}, {"href": "https://www.braintrust.dev/docs/providers/groq", "anchor": "Groq"}, {"href": "https://www.braintrust.dev/docs/providers/lepton", "anchor": "Lepton"}, {"href": "https://www.braintrust.dev/docs/providers/cerebras", "anchor": "Cerebras"}, {"href": "https://www.braintrust.dev/docs/providers/replicate", "anchor": "Replicate"}, {"href": "https://www.braintrust.dev/docs/providers/baseten", "anchor": "Baseten"}, {"href": "https://www.braintrust.dev/docs/providers/custom", "anchor": "Custom providers"}, {"href": "https://www.braintrust.dev/docs/providers/google", "anchor": "Vertex AI"}, {"href": "https://www.braintrust.dev/docs/providers/azure", "anchor": "Azure OpenAI"}, {"href": "https://www.braintrust.dev/docs/providers/databricks", "anchor": "Databricks"}, {"href": "https://www.braintrust.dev/docs/providers/bedrock", "anchor": "AWS Bedrock"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "Braintrust API Proxy"}, {"href": "https://www.braintrust.dev/docs/providers/custom", "anchor": "custom providers"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Proxy model providers"}, {"href": "https://www.braintrust.dev/docs/providers/openai", "anchor": "OpenAI"}, {"href": "https://www.braintrust.dev/docs/providers/anthropic", "anchor": "Anthropic"}, {"href": "https://www.braintrust.dev/docs/providers/gemini", "anchor": "Gemini"}, {"href": "https://www.braintrust.dev/docs/providers/mistral", "anchor": "Mistral"}, {"href": "https://www.braintrust.dev/docs/providers/together", "anchor": "Together"}, {"href": "https://www.braintrust.dev/docs/providers/fireworks", "anchor": "Fireworks"}, {"href": "https://www.braintrust.dev/docs/providers/perplexity", "anchor": "Perplexity"}, {"href": "https://www.braintrust.dev/docs/providers/xai", "anchor": "xAI"}, {"href": "https://www.braintrust.dev/docs/providers/groq", "anchor": "Groq"}, {"href": "https://www.braintrust.dev/docs/providers/lepton", "anchor": "Lepton"}, {"href": "https://www.braintrust.dev/docs/providers/cerebras", "anchor": "Cerebras"}, {"href": "https://www.braintrust.dev/docs/providers/replicate", "anchor": "Replicate"}, {"href": "https://www.braintrust.dev/docs/providers/baseten", "anchor": "Baseten"}, {"href": "https://www.braintrust.dev/docs/providers/custom", "anchor": "Custom"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Proxy cloud providers"}, {"href": "https://www.braintrust.dev/docs/providers/bedrock", "anchor": "AWS Bedrock"}, {"href": "https://www.braintrust.dev/docs/providers/google", "anchor": "Vertex AI"}, {"href": "https://www.braintrust.dev/docs/providers/azure", "anchor": "Azure"}, {"href": "https://www.braintrust.dev/docs/providers/databricks", "anchor": "Databricks"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}], "depth": 2}, "https://www.braintrust.dev/docs/guides/experiments": {"url": "https://www.braintrust.dev/docs/guides/experiments", "title": "Experiments - Docs - Guides - Experiments - Braintrust", "text": "Experiments\nExperiments let you snapshot the performance of your AI application so you can improve it over time. In traditional software, performance usually refers to speed, like for example, how many milliseconds it takes to complete a request. In AI, it often refers to other measurements in addition to speed, including accuracy or quality. These types of metrics are harder to define and measure, especially at scale. Assessing the performance of an LLM application is known as evaluation.\nBraintrust supports two types of evaluations:\n- Offline evals are structured experiments used to compare and improve your app systematically.\n- Online evals run scorers on live requests to monitor performance in real time.\nBoth types of evals are important for building quality AI applications.\nWhy are evals important?\nIn AI development, it's hard for teams to understand how an update will impact performance. This breaks the typical software development loop, making iteration feel like guesswork instead of engineering.\nEvaluations solve this, helping you distill the non-deterministic outputs of AI applications into an effective feedback loop that enables you to ship more reliable, higher quality products.\nSpecifically, great evals help you:\n- Understand whether an update is an improvement or a regression\n- Quickly drill down into good / bad examples\n- Diff specific examples vs. prior runs\n- Avoid playing whack-a-mole\nBreaking down evals\nEvals consist of 3 parts:\n- Data: a set of examples to test your application on\n- Task: the AI function you want to test (any function that takes in an\ninput\nand returns anoutput\n) - Scores: a set of scoring functions that take an\ninput\n,output\n, and optionalexpected\nvalue and compute a score\nYou can establish an Eval()\nfunction with these 3 pieces:\nFor more details, try the full tutorial.\nViewing experiments\nRunning your Eval\nfunction will automatically create an experiment in Braintrust,\ndisplay a summary in your Terminal, and populate the UI:\nThis gives you great visibility into how your AI application performed. You can:\n- Preview each test case and score in a table\n- Filter by high/low scores\n- Click into any individual example and see detailed tracing\n- See high level scores\n- Sort by improvements or regressions\nWhere to go from here\nNow that you understand the basics of evals and experiments, you can dive deeper into the following topics:", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "Write evals"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/run", "anchor": "Run evals"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/interpret", "anchor": "Interpret evals"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Why are evals important?"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Breaking down evals"}, {"href": "https://www.braintrust.dev/docs/welcome/start", "anchor": "full tutorial"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Viewing experiments"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Where to go from here"}, {"href": "https://www.braintrust.dev/docs/welcome/start", "anchor": "Run your first eval with a full tutorial"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "Writing evals"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/run", "anchor": "Running evals locally, in CI, or in production"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/interpret", "anchor": "Interpreting eval results"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}], "depth": 2}, "https://www.braintrust.dev/docs/guides/logs": {"url": "https://www.braintrust.dev/docs/guides/logs", "title": "Logs - Docs - Guides - Braintrust", "text": "Logs\nLogs are the recorded data and metadata from an AI routine. We record the inputs and outputs of your LLM calls to help you evaluate model performance on set of predefined tasks, identify patterns, and diagnose issues.\nIn Braintrust, logs consist of traces, which roughly correspond to a single request or interaction in your application. Traces consist of one or more spans, each of which corresponds to a unit of work in your application, like an LLM call, for example. You typically collect logs while running your application, both in staging (internal) and production (external) environments, using them to debug issues, monitor user behavior, and gather data for building datasets.\nWhy log in Braintrust?\nBy logging in Braintrust, you can create a feedback loop between real-world observations (logs) and offline evaluations (experiments). This feedback loop is crucial for refining your model's performance and building high-quality AI applications.\nBy design, logs are exactly the same data structure as experiments. This leads to a number of useful properties:\n- If you instrument your code to run evals, you can reuse this instrumentation to generate logs\n- Your logged traces capture exactly the same data as your evals\n- You can reuse automated and human review scores across both experiments and logs\nWhere to go from here\nNow that you know the basics of logging in Braintrust, dig into some more complex capabilities:", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/logs/write", "anchor": "Write logs"}, {"href": "https://www.braintrust.dev/docs/guides/logs/score", "anchor": "Score logs"}, {"href": "https://www.braintrust.dev/docs/guides/logs/view", "anchor": "View logs"}, {"href": "https://www.braintrust.dev/docs/guides/logs/advanced", "anchor": "Advanced"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "datasets"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Why log in Braintrust?"}, {"href": "https://www.braintrust.dev/docs/guides/evals", "anchor": "experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Where to go from here"}, {"href": "https://www.braintrust.dev/docs/guides/logs/write", "anchor": "Logging user feedback"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "Online evaluation"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Logging multimodal content"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Customizing your traces"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}], "depth": 2}, "https://www.braintrust.dev/docs/guides/traces": {"url": "https://www.braintrust.dev/docs/guides/traces", "title": "Tracing - Docs - Guides - Braintrust", "text": "Tracing\nTracing is an invaluable tool for exploring the sub-components of your program which produce each top-level input and output. We currently support tracing in logging and evaluations.\nAnatomy of a trace\nA trace represents a single independent request, and is made up of several spans.\nA span represents a unit of work, with a start and end time, and optional fields like input, output, metadata, scores, and metrics (the same fields you can log in an experiment). Each span contains one or more children that are usually run within their parent span, like for example, a nested function call. Common examples of spans include LLM calls, vector searches, the steps of an agent chain, and model evaluations.\nEach trace can be expanded to view all of the spans inside. Well-designed traces make it easy to understand the flow of your application, and to debug issues when they arise. The tracing API works the same way whether you are logging online (production logging) or offline (evaluations).\nWhere to go from here\nLearn more about tracing in Braintrust:", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Customize traces"}, {"href": "https://www.braintrust.dev/docs/guides/traces/view", "anchor": "View traces"}, {"href": "https://www.braintrust.dev/docs/guides/traces/extend", "anchor": "Extend traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Tracing"}, {"href": "https://www.braintrust.dev/docs/guides/logging", "anchor": "logging"}, {"href": "https://www.braintrust.dev/docs/guides/evals", "anchor": "evaluations"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Anatomy of a trace"}, {"href": "https://www.braintrust.dev/docs/guides/evals", "anchor": "experiment"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Where to go from here"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Wrapping LLM clients (OpenAI and others)"}, {"href": "https://www.braintrust.dev/docs/guides/traces/integrations", "anchor": "OpenTelemetry and other popular library integrations"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Troubleshooting"}, {"href": "https://www.braintrust.dev/docs/guides/traces/view", "anchor": "Viewing traces"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}], "depth": 2}, "https://www.braintrust.dev/docs/guides/playground": {"url": "https://www.braintrust.dev/docs/guides/playground", "title": "Playgrounds - Docs - Guides - Braintrust", "text": "Eval playgrounds\nPlaygrounds are a powerful workspace for rapidly iterating on AI engineering primitives. Tune prompts, models, scorers and datasets in an editor-like interface, and run full evaluations in real-time, side by side.\nUse playgrounds to build and test hypotheses and evaluation configurations in a flexible environment. Playgrounds leverage the same underlying Eval\nstructure as experiments, with support for running thousands of dataset rows directly in the browser. Collaborating with teammates is also simple with a shared URL.\nPlaygrounds are designed for quick prototyping of ideas. When a playground is run, its previous generations are overwritten. You can create experiments from playgrounds when you need to capture an immutable snapshot of your evaluations for long-term reference or point-in-time comparison.\nYou can try the playground without signing up. Any work you do in a demo playground will be saved if you make an account.\nCreating a playground\nA playground includes one or more evaluation tasks, one or more scorers, and optionally, a dataset.\nYou can create a playground by navigating to Evaluations > Playgrounds, or by selecting Create playground with prompt at the bottom of a prompt dialog.\nTasks\nTasks define LLM instructions. There are four types of tasks:\n-\nPrompts: AI model, prompt messages, parameters, and tools.\n-\nAgents: A chain of prompts.\n-\nRemote evals: Prompts and scorers from external sources.\n-\nScorers: Prompts or heuristics used to evaluate the output of LLMs. Running scorers as tasks is useful to validate and iterate on them.\nNote the difference between scorers-as-tasks and scorers used to evaluate tasks. You can even score your scorers-as-tasks in the playground.\nAn empty playground will prompt you to create a base task, and optional comparison tests. The base task is used as the source when diffing output traces.\nWhen you select Run (or the keyboard shortcut Cmd/Ctrl+Enter), each task runs in parallel and the results stream into the grid below. You can also choose to view in list or summary layout.\nAI providers must be configured before playgrounds can be run.\nFor multimodal workflows, supported attachments will have a preview shown in the inline embedded view.\nScorers\nScorers quantify the quality of evaluation outputs using an LLM judge or code. You can use built-in autoevals for common evaluation scenarios to help you get started quickly, or write custom scorers tailored to your use case.\nTo add a scorer, select + Scorer and choose from the list or create a custom scorer.\nDatasets\nDatasets provide structured inputs, expected values, and metadata for evaluations.\nA playground can be run without a dataset to view a single set of task outputs, or with a dataset to view a matrix of outputs for many inputs.\nDatasets can be linked to a playground by selecting existing library datasets, or creating/importing a new one.\nOnce you link a dataset, you will see a new row in the grid for each record in the dataset. You can reference the\ndata from each record in your prompt using the input\n, expected\n, and metadata\nvariables. The playground uses\nmustache syntax for templating:\nEach value can be arbitrarily complex JSON, for example, {{input.formula}}\n. If you want to preserve double curly brackets {{\nand }}\nas plain text in your prompts, you can change the delimiter tags to any custom\nstring of your choosing. For example, if you want to change the tags to <%\nand %>\n, insert {{=<% %>=}}\ninto the message,\nand all strings below in the message block will respect these delimiters:\nDataset edits in playgrounds edit the original dataset.\nFor scorers-as-task\nWhen evaluating scorers in the playground, ensure that your dataset input schema adheres to scorer convention. Like when a scorer is used on a prompt or agent, the input to the scorer should have the shape { input, expected, metadata, output }\n.\nUnlike other task types, those reserved dataset keywords are hoisted into the global scope, meaning you can use your saved scorers in the playground and reference variables without any changes.\nFor example, to tune a scorer with the prompt:\nThen, your dataset rows should look something like:\nRunning a playground\nTo run a playground, select the Run button at the top of the playground to run all tasks and all dataset rows. You can also run a single task individually, or run a single dataset row.\nViewing traces\nSelect a row in the results table to compare evaluation traces side-by-side. This allows you to identify differences in outputs, scores, metrics, and input data.\nFrom this view, you can also run a single row by selecting Run row.\nDiffing\nDiffing allows you to visually compare variations across models, prompts, or agents to quickly understand differences in outputs.\nTo turn on diff mode, select the diff toggle.\nCreating experiment snapshots\nExperiments formalize evaluation results for comparison and historical reference. While playgrounds are better for fast, iterative exploration, experiments are immutable, point-in-time evaluation snapshots ideal for detailed analysis and reporting.\nTo create an experiment from a playground, select + Experiment. Each playground task will map to its own experiment.\nAdvanced options\nAppended dataset messages\nYou may sometimes have additional messages in a dataset that you want to append to a prompt. This option lets you specify a path to a messages array in the dataset. For example, if input\nis specified as the appended messages path and a dataset row has the following input, all prompts in the playground will run with additional messages.\nTo append messages from a dataset to your prompts, open the advanced settings menu next to your dataset selection and enter the path to the messages you want to append.\nMax concurrency\nThe maximum number of tasks/scorers that will be run concurrently in the playground. This is useful for avoiding rate limits (429 - Too many requests) from AI providers.\nStrict variables\nWhen this option is enabled, evaluations will fail if the dataset row does not include all of the variables referenced in prompts.\nCollaboration\nPlaygrounds are designed for collaboration and automatically synchronize in real-time.\nTo share a playground, copy the URL and send it to your collaborators. Your collaborators must be members of your organization to view the playground. You can invite users from the settings page.\nReasoning\nIf you are on a hybrid deployment, reasoning support is available starting with v0.0.74\n.\nReasoning models like OpenAI\u2019s o4, Anthropic\u2019s Claude 3.5 Sonnet, and Google\u2019s Gemini 2.5 Flash generate intermediate reasoning steps before producing a final response. Braintrust provides unified support for these models, so you can work with reasoning outputs no matter which provider you choose.\nWhen you enable reasoning, models generate \"thinking tokens\" that show their step-by-step reasoning process. This is useful for complex tasks like math problems, logical reasoning, coding, and multi-step analysis.\nIn playgrounds, you can configure reasoning parameters directly in the model settings.\nTo enable reasoning in a playground:\n- Select a reasoning-capable model (like\nclaude-3-7-sonnet-latest\n,o4-mini\n, orpublishers/google/models/gemini-2.5-flash-preview-04-17\n(Gemini provided by Vertex AI)) - In the model parameters section, configure your reasoning settings:\n- Set\nreasoning_effort\nto low, medium, or high - Or enable\nreasoning_enabled\nand specify areasoning_budget\n- Set\n- Run your prompt to see reasoning in action", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Eval playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "experiments"}, {"href": "https://www.braintrust.dev/playground", "anchor": "try the playground"}, {"href": "https://www.braintrust.dev/signup", "anchor": "make an account"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Creating a playground"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Tasks"}, {"href": "https://www.braintrust.dev/docs/guides/functions/prompts", "anchor": "Prompts"}, {"href": "https://www.braintrust.dev/docs/guides/functions/agents", "anchor": "Agents"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/functions/scorers", "anchor": "Scorers"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "scorers"}, {"href": "https://www.braintrust.dev/docs/reference/organizations", "anchor": "AI providers"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "attachments"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Scorers"}, {"href": "https://www.braintrust.dev/docs/reference/autoevals", "anchor": "autoevals"}, {"href": "https://www.braintrust.dev/docs/guides/functions/scorers", "anchor": "custom scorers"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "For scorers-as-task"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Running a playground"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Viewing traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Diffing"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Creating experiment snapshots"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Advanced options"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Appended dataset messages"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Max concurrency"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Strict variables"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Collaboration"}, {"href": "https://www.braintrust.dev/app/settings?subroute=team", "anchor": "settings"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Reasoning"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Eval playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Creating a playground"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Tasks"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Scorers"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "For scorers-as-task"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Running a playground"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Viewing traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Diffing"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Creating experiment snapshots"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Advanced options"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Appended dataset messages"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Max concurrency"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Strict variables"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Collaboration"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Reasoning"}], "depth": 2}, "https://www.braintrust.dev/docs/guides/loop": {"url": "https://www.braintrust.dev/docs/guides/loop", "title": "Loop - Docs - Guides - Braintrust", "text": "Loop\nLoop is an AI assistant in Braintrust playgrounds, experiments, datasets, and logs.\nIn playgrounds, it helps you optimize and generate prompts, datasets and evals. On the experiments page, it helps you read and interpret the experiments in a project. In datasets, you can generate and edit datapoint rows at scale. In logs, it helps you find analytical insights about your project.\nLoop is in public beta and is off by default. To turn it on, flip the feature flag in your settings. If you are on a hybrid deployment, Loop is available starting with v0.0.74\n.\nSelecting a model\nLoop uses the AI models available in your Braintrust account via the Braintrust API Proxy. We currently support the following models:\n- claude-4-sonnet\n- claude-4.1-opus\n- gpt-5\n- gpt-4.1\n- o3\n- o4-mini\n- claude-3-5-sonnet\nTo choose a model, navigate to the gear icon in the Loop chat window and select from the list of available models.\nAvailable tools\nLoop currently has the following tools. Tool availability changes based on the page you are viewing:\n- Get summarized results: fetch summarized data of current page contents\n- Get detailed results: retrieve detailed data of current page contents (evaluation results, dataset rows, ...etc)\n- Edit prompt: generate and modify prompts in the playground\n- Run eval: Execute evaluations in the playground\n- Edit data: Generate and modify datasets\n- Get scorers: Get all available scorers in the project\n- Edit scorers: Edit scorer selection in the playground\n- Create code scorer: Create or edit code-based scorer\n- Create LLM judge scorer: Create or edit LLM judge scorer\n- BTQL query: Generate and run btql query against project logs\n- Infer schema: Inspect project logs and create an understanding of the shape of the data\n- Continue execution: Resume tasks after Loop has run out of iteration\nYou can remove any of these tools from your Loop workflow by selecting the gear icon and deselecting a tool from the available list.\nGenerating and optimizing prompts\nLoop can help you generate a prompt from scratch. To do so, make sure you have an empty task open, then use Loop to generate a prompt.\nIf you have existing prompts, you can optimize them using Loop.\nTo optimize a prompt, ask Loop in the chat window, or select the Loop icon in the top bar of any existing task. From there, you can add the prompt to your chat, or quick optimize.\nAfter Loop provides a suggested optimization, you can review and accept the suggestion or keep iterating.\nGenerating and optimizing datasets\nIf no dataset exists, Loop can create one automatically. You must have a task in order for Loop to generate a tailored dataset for the evaluation task.\nYou can review the dataset and further refine it as needed.\nAfter you run your playground, you can also ask Loop to optimize your dataset. The agent will provide various areas for optimizations based on an analysis of your current dataset.\nAnalyze project logs\nLoop can understand the shape of your project's logs data and make arbitrary queries to answer questions about your logs data. This ability can be used to find analytical insights or used in conjunction with Loop's other abilities.\nFor analytical insights, you can ask things like \"what are the most common errors\", \"what are the most common inputs from users\", and \"what user retention trends do you see?\" and Loop will gather the necessary data from your logs to answer your question.\nFor using this in conjunction with Loop's other abilities, you might navigate to the dataset page and ask Loop, \"Can you find the most common errors users face and generate dataset rows based on the findings? Follow the formatting of existing rows you see in this dataset\", and Loop will gather the context necessary from logs and generate your dataset based on the findings.\nGenerating and editing scorers\nIf no scorers exist, Loop can create one for you. You must have a dataset and a task in order for Loop to generate a scorer that is specific to your use case. The agent will begin by checking what data you have, what existing scorers are available, and fetching some sample results to understand the data structure.\nIf you select Accept, the new scorer will be added to the playground.\nLoop can also help you improve and edit existing scorers.\nTune scorers based on target classification\nLoop can take manually labelled target classification from evaluations in the playground and adjust scorer classification behavior.\nSelect the rows that the scorers did not perform expectedly on, then select Tune scorer.\nSelect the desired classification, provide optional additional instruction and submit to Loop to tune the scorer. Loop will adjust the scorer based on the provided context.\nRun and assess evals\nAfter your tasks, dataset, and scorers are set up, Loop can run an evaluation for you, analyze it, and suggest further improvements.\nAnalyze and interpret your experiments\nLoop can read the results of your experiment(s), summarize the results, and help discover new insights.\nMode\nBy default, Loop will ask you for confirmation before executing certain tool calls, like running an evaluation. If you'd like Loop to run evaluations without confirmation, you can turn off this setting in the agent mode menu.\nContinuous agent\nIn continuous agent mode, Loop will execute tools and make edit suggestions one after the other.", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Selecting a model"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "Braintrust API Proxy"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Available tools"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Generating and optimizing prompts"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Generating and optimizing datasets"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Analyze project logs"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Generating and editing scorers"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Tune scorers based on target classification"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Run and assess evals"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Analyze and interpret your experiments"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Mode"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Continuous agent"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Selecting a model"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Available tools"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Generating and optimizing prompts"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Generating and optimizing datasets"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Analyze project logs"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Generating and editing scorers"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Tune scorers based on target classification"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Run and assess evals"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Analyze and interpret your experiments"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Mode"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Continuous agent"}], "depth": 2}, "https://www.braintrust.dev/docs/guides/datasets": {"url": "https://www.braintrust.dev/docs/guides/datasets", "title": "Datasets - Docs - Guides - Datasets - Braintrust", "text": "Datasets\nDatasets allow you to collect data from production, staging, evaluations, and even manually, and then use that data to run evaluations and track improvements over time.\nFor example, you can use Datasets to:\n- Store evaluation test cases for your eval script instead of managing large JSONL or CSV files\n- Log all production generations to assess quality manually or using model graded evals\n- Store user reviewed (, ) generations to find new test cases\nIn Braintrust, datasets have a few key properties:\n- Integrated. Datasets are integrated with the rest of the Braintrust platform, so you can use them in evaluations, explore them in the playground, and log to them from your staging/production environments.\n- Versioned. Every insert, update, and delete is versioned, so you can pin evaluations to a specific version of the dataset via the SDK.\n- Scalable. Datasets are stored in a modern cloud data warehouse, so you can collect as much data as you want without worrying about storage or performance limits.\n- Secure. If you run Braintrust in your cloud environment, datasets are stored in your warehouse and never touch our infrastructure.\nManaging datasets with the SDK\nCreating a dataset\nRecords in a dataset are stored as JSON objects, and each record has three top-level fields:\ninput\nis a set of inputs that you could use to recreate the example in your application. For example, if you're logging examples from a question answering model, the input might be the question.expected\n(optional) is the output of your model. For example, if you're logging examples from a question answering model, this might be the answer. You can accessexpected\nwhen running evaluations as theexpected\nfield; however,expected\ndoes not need to be the ground truth.metadata\n(optional) is a set of key-value pairs that you can use to filter and group your data. For example, if you're logging examples from a question answering model, the metadata might include the knowledge source that the question came from.\nDatasets are created automatically when you initialize them in the SDK.\nReading a dataset\nTo read a dataset, use the same method as above for creating a dataset, but pass the name of the dataset you want to retrieve.\nInserting records\nYou can use the SDK to insert into a dataset:\nUpdating records\nIn the above example, each insert()\nstatement returns an id\n. You can use this id\nto update the record using update()\n:\nThe update()\nmethod applies a merge strategy: only the fields you provide will be updated, and all other existing fields in the record will remain unchanged.\nDeleting records\nYou can delete records via code by id\n:\nTo delete an entire dataset, use the API command.\nFlushing\nIn both TypeScript and Python, the Braintrust SDK flushes records as fast as possible and installs an exit handler that tries\nto flush records, but these hooks are not always respected (e.g. by certain runtimes, or if you exit\na process yourself). If\nyou need to ensure that records are flushed, you can call flush()\non the dataset.\nMultimodal datasets\nYou may want to store or process images in your datasets. There are currently three ways to use images in Braintrust:\n- Image URLs (most performant)\n- Base64 (least performant)\n- Attachments (easiest to manage, stored in Braintrust)\n- External attachments (access files in your own object stores)\nIf you're building a dataset of large images in Braintrust, we recommend using image URLs. This keeps your dataset lightweight and allows you to preview or process them without storing heavy binary data directly.\nIf you prefer to keep all data within Braintrust, create a dataset of attachments instead. In addition to images, you can create datasets of attachments that have any arbitrary data type, including audio and PDFs. You can then use these datasets in evaluations.\nTo invoke this script, run this in your terminal:\nAttachments are not yet supported in the playground. To explore images in the playground, we recommend using image URLs.\nManaging datasets in the UI\nIn addition to managing datasets through the API, you can also manage them in the Braintrust UI.\nViewing a dataset\nYou can view a dataset in the Braintrust UI by navigating to the project and then clicking on the dataset.\nFrom the UI, you can filter records, create new ones, edit values, and delete records. You can also copy records between datasets and from experiments into datasets. This feature is commonly used to collect interesting or anomalous examples into a golden dataset.\nCreate custom columns\nWhen viewing a dataset, create custom columns to extract values from the root span.\nCreating a dataset\nThe easiest way to create a dataset is to upload a CSV file.\nUpdating records\nOnce you've uploaded a dataset, you can update records or add new ones directly in the UI.\nLabeling records\nIn addition to updating datasets through the API, you can edit and label them in the UI. Like experiments and logs, you can configure categorical fields to allow human reviewers to rapidly label records.\nThis requires you to first configure human review in the Configuration tab of your project.\nDeleting records\nTo delete a record, navigate to Library \u2192 Datasets and select the dataset. Select the check box next to the individual record you'd like to delete, and then select the Trash icon.\nYou can follow the same steps to delete an entire dataset from the Library > Datasets page.\nUsing a dataset in an evaluation\nYou can use a dataset in an evaluation by passing it directly to the Eval()\nfunction.\nYou can also manually iterate through a dataset's records and run your tasks,\nthen log the results to an experiment. Log the id\ns to link each dataset record\nto the corresponding result.\nYou can also use the results of an experiment as baseline data for future experiments by calling the asDataset()\n/as_dataset()\nfunction, which converts the experiment into dataset format (input\n, expected\n, and metadata\n).\nFor a more advanced overview of how to use an experiment as a baseline for other experiments, see hill climbing.\nLogging from your application\nTo log to a dataset from your application, you can use the SDK and call insert()\n. Braintrust logs\nare queued and sent asynchronously, so you don't need to worry about critical path performance.\nSince the SDK uses API keys, it's recommended that you log from a privileged environment (e.g. backend server), instead of client applications directly.\nThis example walks through how to track / from feedback:", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "in your cloud environment"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Managing datasets with the SDK"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Creating a dataset"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Reading a dataset"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Inserting records"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Updating records"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Deleting records"}, {"href": "https://www.braintrust.dev/docs/reference/api/Datasets", "anchor": "API command"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Flushing"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Multimodal datasets"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "use these datasets in evaluations"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Managing datasets in the UI"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Viewing a dataset"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Create custom columns"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/interpret", "anchor": "custom columns"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Creating a dataset"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Updating records"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Labeling records"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "categorical fields"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "configure human review"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Deleting records"}, {"href": "https://www.braintrust.dev/docs/guides/datasets/delete-dataset-record.mp4", "anchor": "Video"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Using a dataset in an evaluation"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "hill climbing"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Logging from your application"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Managing datasets with the SDK"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Creating a dataset"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Reading a dataset"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Inserting records"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Updating records"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Deleting records"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Flushing"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Multimodal datasets"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Managing datasets in the UI"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Viewing a dataset"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Create custom columns"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Creating a dataset"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Updating records"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Labeling records"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Deleting records"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Using a dataset in an evaluation"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Logging from your application"}], "depth": 2}, "https://www.braintrust.dev/docs/guides/views": {"url": "https://www.braintrust.dev/docs/guides/views", "title": "Views - Docs - Guides - Braintrust", "text": "Views\nYou'll often want to create a view that shows data organized and visualized a certain way on the same underlying data. Views are saved table configurations that preserve filters, sorts, column order and column visibility. All table-based layouts, including logs, experiments, datasets and projects support configured views.\nDefault locked views\nSome table layouts include default views for convenience. These views are locked and cannot be modified or deleted.\n- All rows corresponds to all of the records in a given table. This is the default, unfiltered view.\nOn experiment and logs pages:\n- Non-errors corresponds to all of the records in a given table that do not contain errors.\n- Errors corresponds to all of the records in a given table that contain errors.\nOn experiment pages:\n- Unreviewed hides items that have already been human-reviewed.\nCreating and managing custom views\nIn the UI\nTo create a custom view, start by applying the filters, sorts, and columns that you would like to have visible in your view. Then, navigate to the Views dropdown and select Create view.\nAfter entering a view, select Save view in the view dropdown menu to save any changes you make to the filters, sorts, and columns.\nTo rename, duplicate, delete, or set as default, use the Manage view submenu in the view dropdown.\nIn code\nViews can also be created and managed programmatically via the API.\nAccess\nViews are accessible and configurable by any member of the organization.\nBest practices\nUse views when:\n- You frequently reapply the same filters.\n- You want to standardize what your team sees.\n- You want to review only a subset of records.\nMake sure to use clear, descriptive names so your team can quickly understand the purpose of each view. Some example views might be:\n- \"Logs with Factuality < 50%\"\n- \"Unreviewed high-priority traces\"\n- \"Failing test cases\"\n- \"Tagged with 'Customer Support'\"\n- \"Lisa's test cases\"\nUsing views with custom columns\nIf you regularly filter by complex or nested JSON queries or metadata, consider creating custom columns. Custom columns let you surface frequently-used or computed values directly as columns, simplifying repetitive filtering tasks. Custom columns are also rendered in trace spans, with their own span field view type (for example, JSON, Markdown, or HTML).\nFor example, you can analyze data across multiple models within a single experiment view:\n- First, define a custom column extracting the model name from your metadata.\n- Then, apply the custom column, sort, and any additional standard filters, then save this configuration as a view.\n- Lastly, use the filter dropdown to quickly toggle between models.", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Default locked views"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Creating and managing custom views"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "In the UI"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "In code"}, {"href": "https://www.braintrust.dev/docs/reference/api/Views", "anchor": "via the API"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Access"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Best practices"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Using views with custom columns"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/interpret", "anchor": "custom columns"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Default locked views"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Creating and managing custom views"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "In the UI"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "In code"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Access"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Best practices"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Using views with custom columns"}], "depth": 2}, "https://www.braintrust.dev/docs/guides/functions": {"url": "https://www.braintrust.dev/docs/guides/functions", "title": "Functions - Docs - Guides - Functions - Braintrust", "text": "Functions\nBraintrust functions are atomic, reusable building blocks for executing AI-related logic. Functions are hosted and remotely executed in a performant serverless environment and are fully intended to be used in production. Functions can be invoked through the API, SDK, or the UI, and have built-in support for streaming and structured outputs.\nThere are currently three types of functions in Braintrust:\n- Prompts\nTemplated messages to send to an LLM. - Tools\nGeneral purpose code that can be invoked by LLMs. - Scorers\nFunctions for scoring the quality of LLM outputs (a number from 0 to 1).\nComposability\nFunctions can be composed together to produce sophisticated applications that would otherwise require brittle orchestration logic.\nIn this diagram, a prompt is being invoked with an input and is calling two different tools and scorers to ultimately produce a streaming output. Out of the box, you also get automatic tracing, including the tool calls and scores.\nAny function can be used as a tool, which can be called, and its output added to the chat history. For example, a RAG agent can be defined as just two components:\n- A vector search tool,\ntoolRAG\n, implemented in TypeScript or Python, which embeds a query, searches for relevant documents, and returns them\n- A system prompt containing instructions for how to retrieve content and synthesize answers using the tool\nTo dig more into this example, check out the cookbook for Using functions to build a RAG agent.\nSyncing functions via the SDK\nYou can sync functions between the Braintrust UI and your local codebase using the Braintrust SDK. Currently, this works for any prompts and tools written in TypeScript.\nYou can push tools and prompts written in Python to Braintrust using braintrust push\n, but pulling from Braintrust is not yet available.\nTo push a change from your codebase to the UI, run npx braintrust push <filename>\nfrom the command line. You can push one or more files or directories to Braintrust. If you specify a directory, all .ts\nfiles under that directory are pushed.\nTo pull a change from the UI to your codebase, run npx braintrust pull\n. For example, you can use the pull\ncommand to:\n- Download functions to public projects so others can use them\n- Pin your production environment to a specific prompt version without running them through Braintrust on the request path\n- Review changes to functions in pull requests\nCode bundling\nBraintrust bundles your code together with any libraries and dependencies for serverless execution.\nBraintrust uses esbuild\nto bundle your code. Bundling works by creating a single JavaScript file that contains all the necessary code, reducing the risk of version mismatches and dependency errors when deploying functions.\nSince esbuild\nstatically analyzes your code, it cannot handle dynamic imports or runtime code modifications.\nOnce code is bundled and uploaded to the Braintrust UI, you cannot edit it directly in the UI. Any changes must be made locally in your codebase and pushed via the SDK.\nRuntimes\nThere are three runtimes available for functions:\n- TypeScript (Node.js v18, v20, v22)\n- Python (Python 3.11, 3.12, 3.13)\n- Calling model providers with a prompt via the AI proxy\nDefault Python packages\nWe include a set of Python packages available in the Braintrust code editor by default:\nbraintrust\n(latest)autoevals\n(latest)requests\n2.32.2openai\n1.40.8\nUploading code to create a Python function will attempt to use the versions of the above packages (as well as pydantic\n) in your local environment.", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/functions/prompts", "anchor": "Prompts"}, {"href": "https://www.braintrust.dev/docs/guides/functions/tools", "anchor": "Tools"}, {"href": "https://www.braintrust.dev/docs/guides/functions/scorers", "anchor": "Scorers"}, {"href": "https://www.braintrust.dev/docs/guides/functions/agents", "anchor": "Agents"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/reference/api/Functions", "anchor": "API"}, {"href": "https://www.braintrust.dev/docs/guides/functions/prompts", "anchor": "Prompts"}, {"href": "https://www.braintrust.dev/docs/guides/functions/tools", "anchor": "Tools"}, {"href": "https://www.braintrust.dev/docs/guides/functions/scorers", "anchor": "Scorers"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Composability"}, {"href": "https://www.braintrust.dev/docs/cookbook/recipes/ToolRAG", "anchor": "Using functions to build a RAG agent"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Syncing functions via the SDK"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Code bundling"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Runtimes"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Default Python packages"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}], "depth": 2}, "https://www.braintrust.dev/docs/guides/attachments": {"url": "https://www.braintrust.dev/docs/guides/attachments", "title": "Attachments - Docs - Guides - Braintrust", "text": "Attachments\nYou can log arbitrary binary data, like images, audio, video, and PDFs, as attachments. Attachments are useful for building multimodal evaluations, and can enable advanced scenarios like summarizing visual content or analyzing document metadata.\nUploading attachments\nYou can upload attachments from either your code or the UI. Your files are securely stored in an object store and associated with the uploading user\u2019s organization. Only you can access your attachments.\nVia code\nTo upload an attachment, create a new Attachment\nobject to represent the file path or in-memory buffer that you want to upload:\nYou can place the Attachment\nanywhere in a log, dataset, or feedback log.\nBehind the scenes, the Braintrust SDK automatically detects and uploads attachments in the background, in parallel to the original logs. This ensures that the latency of your logs isn\u2019t affected by any additional processing.\nUsing external files as attachments\nBraintrust also supports references to files in external object stores with the ExternalAttachment\nobject. You can use this anywhere you would use an Attachment\n. Currently S3 is the only supported option for external files.\nJust like attachments uploaded to Braintrust, external attachments can be previewed and downloaded for local viewing.\nIn the UI\nYou can upload attachments directly through the UI for any editable span field. This includes:\n- Any dataset fields, including datasets in playgrounds\n- Log span fields\n- Experiment span fields\nYou can also include attachments in prompt messages when using models that support multimodal inputs.\nInline attachments\nSometimes your attachments are pre-hosted files which you do not want to upload explicitly, but would like\nto display as if they were attachments. Inline attachments allow you to do this, by specifying the URL and content\ntype of the file. Create a JSON object anywhere in the log data with type: \"inline_attachment\"\nand src\nand\ncontent_type\nfields. The filename\nfield is optional.\nViewing attachments\nYou can preview most images, audio files, videos, or PDFs in the Braintrust UI. You can also download any file to view it locally. We provide built-in support to preview attachments directly in playground input cells and traces.\nIn the playground, you can preview attachments in an inline embedded view for easy visual verification during experimentation:\nIn the trace pane, attachments appear as an additional list under the data viewer:", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Uploading attachments"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Via code"}, {"href": "https://www.braintrust.dev/docs/guides/tracing", "anchor": "upload an attachment"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/classes/Attachment", "anchor": "Braintrust SDK"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Using external files as attachments"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "In the UI"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Inline attachments"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Viewing attachments"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Uploading attachments"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Via code"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Using external files as attachments"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "In the UI"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Inline attachments"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Viewing attachments"}], "depth": 2}, "https://www.braintrust.dev/docs/guides/projects": {"url": "https://www.braintrust.dev/docs/guides/projects", "title": "Projects - Docs - Guides - Braintrust", "text": "Projects\nA project is analogous to an AI feature in your application. Some customers create separate projects for development and production to help track workflows. Projects contain all experiments, logs, datasets and playgrounds for the feature.\nFor example, a project might contain:\n- An experiment that tests the performance of a new version of a chatbot\n- A dataset of customer support conversations\n- A prompt that guides the chatbot's responses\n- A tool that helps the chatbot answer customer questions\n- A scorer that evaluates the chatbot's responses\n- Logs that capture the chatbot's interactions with customers\nProject configuration\nProjects can also house configuration settings that are shared across the project.\nTags\nBraintrust supports tags that you can use throughout your project to curate logs, datasets, and even experiments. You can filter based on tags in the UI to track various kinds of data across your application, and how they change over time. Tags can be created in the Configuration tab by selecting Add tag and entering a tag name, selecting a color, and adding an optional description.\nFor more information about using tags to curate logs, see the logging guide.\nHuman review\nYou can define scores and labels for manual human review, either as feedback from your users (through the API) or directly through the UI. Scores you define on the Configuration page will be available in every experiment and log in your project.\nTo create a new score, select Add human review score and enter a name and score type. You can add multiple options and decide if you want to allow writing to the expected field instead of the score, or multiple choice.\nTo learn more about human review, check out the full guide.\nAggregate scores\nAggregate scores are formulas that combine multiple scores into a single value. This can be useful for creating a single score that represents the overall experiment.\nTo create an aggregate score, select Add aggregate score and enter a name, formula, and description. Braintrust currently supports three types of aggregate scores:\nBraintrust currently supports three types of aggregate scores:\n- Weighted average - A weighted average of selected scores.\n- Minimum - The minimum value among the selected scores.\n- Maximum - The maximum value among the selected scores.\nTo learn more about aggregate scores, check out the experiments guide.\nOnline scoring\nBraintrust supports server-side online evaluations that are automatically run asynchronously as you upload logs. To create an online evaluation, select Add rule and input the rule name, description, and which scorers and sampling rate you'd like to use. You can choose from custom scorers available in this project and others in your organization, or built-in scorers. Decide if you'd like to apply the rule to the root span or any other spans in your traces.\nFor more information about online evaluations, check out the logging guide.\nSpan iframes\nYou can configure span iframes from your project settings. For more information, check out the extend traces guide.\nComparison key\nWhen comparing multiple experiments, you can customize the expression you're using to evaluate test cases by changing the comparison key. It defaults to \"input,\" but you can change it in your project's Configuration tab.\nFor more information about the comparison key, check out the evaluation guide.\nRename project\nYou can rename your project at any time in the Configuration tab.", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/evals", "anchor": "experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logging", "anchor": "logs"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "datasets"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Project configuration"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Tags"}, {"href": "https://www.braintrust.dev/docs/guides/logging", "anchor": "logging guide"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "full guide"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Aggregate scores"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/interpret", "anchor": "experiments guide"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Online scoring"}, {"href": "https://www.braintrust.dev/docs/guides/logging", "anchor": "logging guide"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Span iframes"}, {"href": "https://www.braintrust.dev/docs/guides/traces/extend", "anchor": "extend traces"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Comparison key"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/interpret", "anchor": "evaluation guide"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Rename project"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Project configuration"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Tags"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Aggregate scores"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Online scoring"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Span iframes"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Comparison key"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Rename project"}], "depth": 2}, "https://www.braintrust.dev/docs/guides/environments": {"url": "https://www.braintrust.dev/docs/guides/environments", "title": "Environments - Docs - Guides - Braintrust", "text": "Environments\nEnvironments in Braintrust allow you to manage different versions of prompts across your development lifecycle. You can pin specific versions of prompts to environments like development, staging, and production, enabling controlled deployment and testing workflows.\nOverview\nAn environment is a named collection that associates specific versions of prompts with a deployment context. This enables you to:\n- Maintain version control: Pin stable prompt versions to production while testing new versions in development\n- Enable staged deployments: Promote prompt versions through dev/staging/production pipelines\n- Support A/B testing: Compare different prompt versions across environments\n- Isolate changes: Test prompt modifications without affecting production systems\nCurrently, environments work with prompts only.\nCreate environments\nEnvironments are configured through the Braintrust UI in your organization settings. Each environment has:\n- Name: A human-readable name (e.g., \"Development\", \"Production\")\n- Slug: A unique identifier used in API calls (e.g., \"dev\", \"prod\")\n- Description: Optional details about the environment's purpose\nTo create an environment:\n- Navigate to your organization settings\n- Go to the Environments section\n- Click Add Environment\n- Enter the name, slug, and optional description\n- Save the environment\nAssociate prompts with environments\nOnce you have environments set up, you can associate specific versions of prompts with them. This creates a mapping that tells Braintrust which version of a prompt to return when queried with an environment parameter.\nYou can make the association using the activity tab on the prompt view.\nLoad prompts with environments\nUsing the SDK\nThe Braintrust SDK supports loading prompts with environment parameters:\nUsing the REST API\nYou can load prompts with environment parameters directly via HTTP:", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Overview"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Create environments"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Associate prompts with environments"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Load prompts with environments"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Using the SDK"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Using the REST API"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Overview"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Create environments"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Associate prompts with environments"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Load prompts with environments"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Using the SDK"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Using the REST API"}], "depth": 2}, "https://www.braintrust.dev/docs/guides/monitor": {"url": "https://www.braintrust.dev/docs/guides/monitor", "title": "Monitor logs and experiments - Docs - Guides - Braintrust", "text": "Monitor page\nThe Monitor page shows aggregate metrics data for both the logs and experiments in a given project. The included preset charts show values related to the selected time period for request count, latency, token count, time to first token, cost, scores, and tools. Custom charts can also be created.\nFilter and group data\nSelect filter and group by options on the top of the page to apply to all charts.\nCreate custom charts\nOpen the chart editor by clicking '+ Chart' button on top right, or by the pencil icon on any chart header. Measures and filters correspond to the btql options of the same name. Group by option is a btql dimension.\nSelect a timeframe\nSelect a timeframe from the given options to see the data associated with that time period. Or click and drag on a chart to select a fixed timeframe to zoom in on. Double click a chart to zoom out.\nView management\nState is stored in the corresponding view selected in the upper left. Views are specific to the project and project type you have selected.\nClick to view traces\nTo see specific traces, click on a data point on any chart. It will redirect you to the logs or experiments page filtered to the corresponding time bucket and series.", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor page"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Filter and group data"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Create custom charts"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Select a timeframe"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "View management"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Click to view traces"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor page"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Filter and group data"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Create custom charts"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Select a timeframe"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "View management"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Click to view traces"}], "depth": 2}, "https://www.braintrust.dev/docs/guides/assignment": {"url": "https://www.braintrust.dev/docs/guides/assignment", "title": "Assignment and mentions - Docs - Guides - Braintrust", "text": "Assignment and mentions\nAssignment in Braintrust allows you to assign rows to team members for review, analysis, or follow-up action. You can assign specific logs, experiment rows, or dataset entries to individuals and track progress by quickly filtering for annotation workflows.\nAssigning rows\nYou can assign any row in Braintrust to a team member across logs, experiments, or datasets. To assign a row, select the assignment column or use the row actions menu, and select the team member.\nFiltering by assignment\nUse assignment filters to focus on relevant work. Filter by Assigned to me to see your current assignments, or select specific team members to view their workload. You can also filter by assignment status: assigned, unassigned, or completed rows. When entering human review mode, only the rows in the specified view will be shown.\nMentions in comments\nMention team members in comments by typing @\nfollowed by their name and selecting from the autocomplete dropdown. Mentioned users receive email notifications with direct links to the specific row and comment.\nEmail notifications\nBraintrust automatically sends email notifications when rows are assigned to you or when someone mentions you in comments.\nAssignment workflows\nAssignment works particularly well with human review workflows, where you can assign specific rows that need human evaluation and distribute review work across multiple team members. You can also use assignment for quality assurance processes by assigning low-scoring results for investigation and tracking resolution of identified issues.\nFor collaborative evaluation, assignment enables coordination between engineering and product teams by assigning rows to subject matter experts for specialized review and ensuring comprehensive coverage of evaluation datasets.", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assigning rows"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Filtering by assignment"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Mentions in comments"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Email notifications"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment workflows"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "human review"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assigning rows"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Filtering by assignment"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Mentions in comments"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Email notifications"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment workflows"}], "depth": 2}, "https://www.braintrust.dev/docs/guides/human-review": {"url": "https://www.braintrust.dev/docs/guides/human-review", "title": "Human review - Docs - Guides - Braintrust", "text": "Human review\nAlthough Braintrust helps you automatically evaluate AI software, human review is a critical part of the process. Braintrust seamlessly integrates human feedback from end users, subject matter experts, and product teams in one place. You can use human review to evaluate/compare experiments, assess the efficacy of your automated scoring methods, and curate log events to use in your evals. As you add human review scores, your logs will update in real time.\nConfiguring human review\nTo set up human review, define the scores you want to collect in your project's Configuration tab.\nSelect Add human review score to configure a new score. A score can be one of\n- Continuous number value between\n0%\nand100%\n, with a slider input control. - Categorical value where you can define the possible options and their scores. Categorical value options\nare also assigned a unique percentage value between\n0%\nand100%\n(stored as 0 to 1). - Free-form text where you can write a string value to the\nmetadata\nfield at a specified path.\nCreated human review scores will appear in the Human review section in every experiment and log trace in the project. Categorical scores configured to \"write to expected\" and free-form scores will also appear on dataset rows.\nWriting to expected fields\nYou may choose to write categorical scores to the expected\nfield of a span instead of a score.\nTo enable this, check the Write to expected field instead of score option. There is also\nan option to Allow multiple choice when writing to the expected field.\nA numeric score will not be assigned to the categorical options when writing to the expected field. If there is an existing object in the expected field, the categorical value will be appended to the object.\nIn addition to categorical scores, you can always directly edit the structured output for the expected\nfield of any span through the UI.\nReviewing logs and experiments\nTo manually review results from your logs or experiment, select a row to open trace view. There, you can edit the human review scores you previously configured.\nAs you set scores, they will be automatically saved and reflected in the summary metrics. The process is the same whether you're reviewing logs or experiments.\nLeaving comments\nIn addition to setting scores, you can also add comments to spans and update their expected\nvalues. These updates\nare tracked alongside score updates to form an audit trail of edits to a span.\nIf you leave a comment that you want to share with a teammate, you can copy a link that will deeplink to the comment.\nFocused review mode\nIf you or a subject matter expert is reviewing a large number of logs or experiments, you can use Review mode to enter a UI that's optimized specifically for review. To enter review mode, hit the \"r\" key or the expand () icon next to the Human review header in a span.\nIn review mode, you can set scores, leave comments, and edit expected values. Review mode is optimized for keyboard navigation, so you can quickly move between scores and rows with keyboard shortcuts. You can also share a link to the review mode view with other team members, and they'll drop directly into review mode.\nReviewing data that matches a specific criteria\nTo easily review a subset of your logs or experiments that match a given criteria, you can filter using English or BTQL, then enter review mode.\nIn addition to filters, you can use tags to mark items for Triage\n, and then review them all at once.\nYou can also save any filters, sorts, or column configurations as views. Views give you a standardized place to see any current or future logs that match a given criteria, for example, logs with a Factuality score less than 50%. Once you create your view, you can enter review mode right from there.\nReviewing is a common task, and therefore you can enter review mode from any experiment or log view. You can also re-enter review mode from any view to audit past reviews or update scores.\nBenefits over an annotation queue\n-\nDesigned for optimal productivity: The combination of views and human review mode simplifies the review process with intuitive filters, reusable configurations, and keyboard navigation, enabling faster, more efficient log evaluation and feedback.\n-\nDynamic and flexible views: Views dynamically update with new logs matching saved criteria, eliminating the need to set up and maintain complex automation rules.\n-\nEasy collaboration: Sharing review mode links allows for team collaboration without requiring intricate permissions or setup overhead.\nFiltering using feedback\nIn the UI, you can filter on log events with specific scores by adding a filter using the filter button, like \"Preference is greater than 75%\", and then add the matching rows to a dataset for further investigation.\nYou can also programmatically filter log events using the API using a query and the project ID:\nThis is a powerful way to utilize human feedback to improve your evals.\nCapturing end-user feedback\nThe same set of updates \u2014 scores, comments, and expected values \u2014 can be captured from end-users as well. See the Logging guide for more details.", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Configuring human review"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Writing to expected fields"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Reviewing logs and experiments"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Leaving comments"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Focused review mode"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Reviewing data that matches a specific criteria"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "BTQL"}, {"href": "https://www.braintrust.dev/docs/guides/logging", "anchor": "tags"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Benefits over an annotation queue"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Filtering using feedback"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Capturing end-user feedback"}, {"href": "https://www.braintrust.dev/docs/guides/logs/write", "anchor": "Logging guide"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Configuring human review"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Writing to expected fields"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Reviewing logs and experiments"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Leaving comments"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Focused review mode"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Reviewing data that matches a specific criteria"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Benefits over an annotation queue"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Filtering using feedback"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Capturing end-user feedback"}], "depth": 2}, "https://www.braintrust.dev/docs/guides/remote-evals": {"url": "https://www.braintrust.dev/docs/guides/remote-evals", "title": "Remote evals - Docs - Guides - Braintrust", "text": "Remote evals\nIf you have existing infrastructure for running evaluations that isn't easily adaptable to the Braintrust Playground, you can use remote evals to expose a remote endpoint. This lets you run evaluations directly in the playground, iterate quickly across datasets, run scorers, and compare results with other tasks. You can also run multiple instances of your remote eval side-by-side with different parameters and compare results. Parameters defined in the remote eval will be exposed in the playground UI.\nExpose remote Eval\nTo expose an Eval\nrunning at a remote URL or your local machine, pass in the --dev\nflag.\nRun npx braintrust eval parameters.eval.ts --dev\nto start the dev server and expose http://localhost:8300\n.\nThe dev host and port can also be configured:\n--dev-host DEV_HOST\n: The host to bind the dev server to. Defaults to localhost. Set to 0.0.0.0 to bind to all interfaces.--dev-port DEV_PORT\n: The port to bind the dev server to. Defaults to 8300.\nRunning a remote eval from a playground\nTo run a remote eval from a playground, select + Remote from the Task pane and choose from the evals exposed in localhost or remote sources.\nConfigure remote eval sources\nTo configure remote eval source URLs for a project, navigate to Configuration > Remote evals. Then, select + Remote eval source to configure a new remote eval source for your project.\nLanguage considerations\nWhen implementing remote evals, be aware of these language-specific patterns:\nLimitations\n- The dataset defined in your remote eval will be ignored. Scorers defined in remote evals will be concatenated with playground scorers.\n- Remote evals are supported in both TypeScript and Python.", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Expose remote Eval"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Running a remote eval from a playground"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Configure remote eval sources"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Language considerations"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Limitations"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Expose remote Eval"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Running a remote eval from a playground"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Configure remote eval sources"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Language considerations"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Limitations"}], "depth": 2}, "https://www.braintrust.dev/docs/guides/automations": {"url": "https://www.braintrust.dev/docs/guides/automations", "title": "Automations - Docs - Guides - Braintrust", "text": "Automations\nAutomations let you trigger actions based on specific events in Braintrust. This makes it easier for you to execute common actions and integrate Braintrust with your existing tools and workflows.\nAutomation types\nBraintrust currently supports the following types of automations:\n- Alerts: Send a JSON payload to a specified webhook URL when a condition on your logs is met\n- S3 export: Export data to an AWS S3 bucket in\nJSONL\nor Parquet format - Data retention: Define time-based retention policies on logs in your project", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/automations/alerts", "anchor": "Alerts"}, {"href": "https://www.braintrust.dev/docs/guides/automations/data-management", "anchor": "Data management"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automation types"}, {"href": "https://www.braintrust.dev/docs/guides/automations/alerts", "anchor": "Alerts"}, {"href": "https://www.braintrust.dev/docs/guides/automations/data-management", "anchor": "S3 export"}, {"href": "https://www.braintrust.dev/docs/guides/automations/data-management", "anchor": "Data retention"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}], "depth": 2}, "https://www.braintrust.dev/docs/guides/access-control": {"url": "https://www.braintrust.dev/docs/guides/access-control", "title": "Access control - Docs - Guides - Braintrust", "text": "Access control\nBraintrust has a robust and flexible access control system. You can grant permissions to users or service accounts either at the organization level or on specific Braintrust objects (projects, experiments, logs, datasets, prompts, and playgrounds).\nPermission groups\nThe core concept of Braintrust's access control system is the permission group. Permission groups are collections of users that can be granted specific permissions. Braintrust has three pre-configured Permission Groups that are scoped to the organization.\n- Owners - Unrestricted access to the organization, its data, and its settings. Can add, modify, and delete projects and all other resources. Can invite and remove members and can manage group membership.\n- Engineers - Can access, create, update, and delete projects and all resources within projects. Cannot invite or remove members or manage access to resources.\n- Viewers - Can access projects and all resources within projects. Cannot create, update, or delete any resources. Cannot invite or remove members or manage access to resources.\nIf your access control needs are simple and you do not need to restrict access to individual projects, these ready-made permission groups may be all that you need.\nA new user can be added to one of these three groups when you invite them to your organization.\nCreating custom permission groups\nIn addition to the built-in permission groups, it's possible to create your own groups as well. To do so, go to the 'Permission groups' page of Settings and click on the 'Create permission group' button. Give your group a name and a description and then click 'Create'.\nTo set organization-level permissions for your new group, find the group in the groups list and click on the Permissions button.\nThe 'Manage Access' permission should be granted judiciously as it is a super-user permission.\nIt gives the user the ability to add and remove permissions, thus any user with 'Manage Access' gains the ability to grant all other permissions to themselves.\nThe 'Manage Settings' permission grants users the ability to change organization-level settings like the API URL.\nTo set group-level permissions for your new group, i.e. who can read, delete, and add members to this group, find the group in the groups list and click on the Group access button.\nProject scoped permissions\nTo limit access to a specific project, create a new permission group from the Settings page.\nNavigate to the Configuration page of that project, and click on the Permissions link in the context menu.\nSearch for your group by typing in the text input at the top of the page, and then click the pencil icon next to the group to set permissions.\nSet the project-level permissions for your group and click Save.\nObject scoped permissions\nTo limit access to a particular object (experiment, dataset, or playground) within a project, first create a permission group for those users on the 'Permission groups' section of Settings.\nNext, navigate to the Configuration page of the project that holds that object and grant the group 'Read' permission at the project level. This will allow users in that group to navigate to the project in the Braintrust UI.\nFinally, navigate to your object and select Permissions from the context menu in the top-right of that object's page.\nFind the permission group via the search input, and click the pencil icon to set permissions for the group.\nSet the desired permissions for the group scoped to this specific object.\nService accounts and service tokens\nService accounts are designed for system integrations and automation. Unlike regular user accounts, service accounts are not tied to individual people and can be assigned granular permissions for specific use cases. Service accounts can inherit permissions from groups or be granted permissions like users.\nService tokens are the authentication mechanism for service accounts. They use the bt-st-\nprefix to distinguish them from regular API keys (sk-\nprefix).\nService tokens can be used anywhere API keys can be used in the SDK, AI proxy, and API requests.\nYou must be in the Owner group of your organization to manage service accounts and service tokens.\nFor hybrid deployments you must configure a service token for the data plane to enable features like data retention. See the data plane manager docs for more details.\nAPI support\nTo automate the creation of permission groups and their access control rules, you can use the Braintrust API. For more information on using the API to manage permission groups, check out the API reference for groups and for permissions.", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Permission groups"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Creating custom permission groups"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Project scoped permissions"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Object scoped permissions"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Service accounts and service tokens"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting/advanced", "anchor": "data plane manager docs"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "API support"}, {"href": "https://www.braintrust.dev/docs/reference/api/Groups", "anchor": "API reference for groups"}, {"href": "https://www.braintrust.dev/docs/reference/api", "anchor": "permissions"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Permission groups"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Creating custom permission groups"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Project scoped permissions"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Object scoped permissions"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Service accounts and service tokens"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "API support"}], "depth": 2}, "https://www.braintrust.dev/docs/guides/proxy": {"url": "https://www.braintrust.dev/docs/guides/proxy", "title": "AI proxy - Docs - Guides - Braintrust", "text": "AI proxy\nThe Braintrust AI Proxy is a powerful tool that enables you to access models from OpenAI, Anthropic, Google, AWS, Mistral, and third-party inference providers like Together which offer open source models like LLaMa 3 \u2014 all through a single, unified API.\nWith the AI proxy, you can:\n- Simplify your code by accessing many AI providers through a single API.\n- Reduce your costs by automatically caching results when possible.\n- Increase observability by optionally logging your requests to Braintrust.\nBest of all, the AI proxy is free to use, even if you don't have a Braintrust account.\nTo read more about why we launched the AI proxy, check out our blog post announcing the feature.\nThe AI proxy is free for all users. You can access it without a Braintrust account by using your API key from any of the supported providers. With a Braintrust account, you can use a single Braintrust API key to access all AI providers.\nQuickstart\nThe Braintrust Proxy is fully compatible with applications written using the\nOpenAI SDK. You can get started without making any code changes. Just set the\nAPI URL to https://api.braintrust.dev/v1/proxy\n.\nTry running the following script in your favorite language, twice:\nAnthropic users can pass their Anthropic API key with a model such as\nclaude-3-5-sonnet-20240620\n.\nThe second run will be significantly faster because the proxy served your request from its cache, rather than rerunning the AI provider's model. Under the hood, your request is served from a Cloudflare Worker that caches your request with end-to-end encryption.\nKey features\nThe proxy is a drop-in replacement for the OpenAI API, with a few killer features:\n- Automatic caching of results, with configurable semantics\n- Interopability with other providers, including a wide range of open source models\n- Run reasoning models across providers with a single call\n- API key management\nThe proxy also supports the Anthropic and Gemini APIs for making requests to Anthropic and Gemini models.\nCaching\nThe proxy automatically caches results, and reuses them when possible. Because the proxy runs on the edge, you can expect cached requests to be returned in under 100ms. This is especially useful when you're developing and frequently re-running or evaluating the same prompts many times.\nCache modes\nThere are three caching modes: auto\n(default), always\n, never\n:\n- In\nauto\nmode, requests are cached if they havetemperature=0\nor theseed\nparameter set and they are one of the supported paths. - In\nalways\nmode, requests are cached as long as they are one of the supported paths. - In\nnever\nmode, the cache is never read or written to.\nThe supported paths are:\n/auto\n/embeddings\n/chat/completions\n/completions\n/moderations\nYou can set the cache mode by passing the x-bt-use-cache\nheader to your request.\nCache TTL\nBy default, cached results expire after 1 week. The TTL for individual requests can be set by passing the x-bt-cache-ttl\nheader to your request. The TTL is specified in seconds and must be between 1 and 604800 (7 days).\nCache control\nThe proxy supports a limited set of Cache-Control directives:\n- To bypass the cache, set the\nCache-Control\nheader tono-cache, no-store\n. Note that this is semantically equivalent to setting thex-bt-use-cache\nheader tonever\n. - To force a fresh request, set the\nCache-Control\nheader tono-cache\n. Note that without theno-store\ndirective the response will be cached for subsequent requests. - To request a cached response with a maximum age, set the\nCache-Control\nheader tomax-age=<seconds>\n. If the cached data is older than the specified age that the cache will be bypassed and a new response will be generated. Combine this withno-store\nto bypass the cache for a request without overwriting the currently cached response.\nWhen cache control directives conflict with the x-bt-use-cache\nheader, the cache control directives take precedence.\nThe proxy will return the x-bt-cached\nheader in the response with HIT\nor MISS\nto indicate whether the response was served from the cache, the Age\nheader to indicate the age of the cached response, and the Cache-Control\nheader with the max-age\ndirective to return the TTL/max age of the cached response.\nFor example, to set the cache mode to always\nwith a TTL of 2 days,\nEncryption\nWe use AES-GCM to encrypt the cache, using a key derived from your API key. Results are cached for 1 week unless otherwise specified in request headers.\nThis design ensures that the cache is only accessible to you, and that we cannot see your data. We also do not store or log API keys.\nBecause the cache's encryption key is your API key, cached results are scoped to an individual user. However, Braintrust customers can opt-into sharing cached results across users within their organization.\nTracing\nTo log requests that you make through the proxy, you can specify an x-bt-parent\nheader with the project or\nexperiment you'd like to log to. While tracing, you must also use a BRAINTRUST_API_KEY\nrather than a provider's\nkey. Behind the scenes, the proxy will derive your provider's key and facilitate tracing using the BRAINTRUST_API_KEY\n.\nFor example,\nThe x-bt-parent\nheader sets the trace's parent project or experiment. You can use\na prefix like project_id:\n, project_name:\n, or experiment_id:\nhere, or pass in\na span slug\n(span.export()\n) to nest the trace under a span within the parent object.\nTo find your project ID, navigate to your project's configuration page and find the Copy Project ID button at the bottom of the page.\nSupported models\nThe proxy supports over 100 models, including popular models like o4-mini, Claude 4 Sonnet, Llama 2, and Gemini Pro 2.5. It also supports third-party inference providers, including the Azure OpenAI Service, Amazon Bedrock, and Together AI. See the full list of models and providers at the bottom of this page.\nWe are constantly adding new models. If you have a model you'd like to see supported, please let us know!\nSupported protocols\nHTTP-based models\nOn the /auto\n, and /chat/completions\nendpoints,\nthe proxy receives HTTP requests in the OpenAI API schema and automatically\ntranslates OpenAI requests into various providers' APIs. That means you can\ninteract with other providers like Anthropic by using OpenAI client libraries\nand API calls.\nFor example,\nThe proxy can also receive requests in the Anthropic and Gemini API schemas for making requests to those respective models.\nFor example, you can make an Anthropic request with the following curl command:\nNote that the anthropic-version\nand x-api-key\nheaders do not need to be set.\nSimilarly, you can make a Gemini request with the following curl command:\nReasoning models\nIf you are on a hybrid deployment, reasoning support is available starting with v0.0.74\n.\nThe Braintrust proxy lets you write one chat completion call that works across multiple providers by standardizing support for reasoning-specific features.\n- Supported providers: We support reasoning models from OpenAI, Anthropic, and Google.\n- Unified parameters: We use a consistent set of parameters related to reasoning:\nreasoning_effort\n: Compatible with OpenAI'sreasoning_effort\n, this parameter allows you to specify the desired level of reasoning complexity.reasoning_enabled\n: An explicit flag to enable or disable reasoning output. Note: has no effect when using an OpenAI model.reasoning_budget\n: Allows you to specify a budget for the reasoning process. Note: you must provide eitherreasoning_effort\norreasoning_enabled\n(for models that support it).\n- Structured reasoning output: Responses from models that support reasoning will include a list of\nreasoning\nobjects as part of the assistant's message. Each object contains thecontent\nof the reasoning step and a uniqueid\n. You can include thesereasoning\nobjects from previous turns in subsequent requests to maintain context in multi-turn conversations. - Streaming support: For streaming responses, a new\nreasoning_delta\nis available, allowing you to process reasoning output as it is generated by the model. - Type safety: To provide a better developer experience when using SDKs like OpenAI's, we offer type augmentations. For JavaScript/TypeScript, use the\n@braintrust/proxy/types\nmodule to extend OpenAI's types. For Python, thebraintrust-proxy\npackage provides casting utilities for input parameters and output objects, helping avoid type errors in your IDEs.\nNon-streaming request with reasoning parameters\nHere's a non-streaming chat completion request using a Google model, explicitly enabling reasoning with reasoning_enabled\nand setting a reasoning_budget\n:\nStreaming request with reasoning delta\nThis example shows how to handle the reasoning_delta\nwhen streaming chat completion responses:\nWebSocket-based models\nThe proxy supports the OpenAI Realtime API at the\n/realtime\nendpoint. To use the proxy with the OpenAI Reference\nClient, set the url\nto\nhttps://braintrustproxy.com/v1/realtime\nwhen constructing the\nRealtimeClient\nor RealtimeAPI\nclasses:\nFor developers trying out the OpenAI Realtime Console sample app, we maintain a fork that demonstrates how to modify the sample code to use the proxy.\nYou can continue to use your OpenAI API key as usual if you are creating the\nRealtimeClient\nin your backend. If you would like to run the RealtimeClient\nin your frontend or in a mobile app, we recommend passing temporary\ncredentials to your frontend to\navoid exposing your API key.\nAuthorization\nThe proxy allows you to use either a provider's API key or your Braintrust API key.\nIf you use a provider's API key, you can use the proxy without a Braintrust account to take advantage of low-latency edge caching (scoped to your API key).\nUsing Braintrust API keys\nIf you use a Braintrust API key, you can access multiple model providers through the proxy and manage all your API keys in one place. To do so, sign up for an account and add each provider's API key on the AI providers page in your settings. Then create an API key to use in your requests.\nThe proxy response will return the x-bt-used-endpoint\nheader, which specifies\nwhich of your configured providers was used to complete the request.\nCustom models\nIf you have custom models as part of your OpenAI or other accounts, you can use\nthem with the proxy by adding a custom provider. For example, if you have a\ncustom model called gpt-3.5-acme\n, you can add it to your\norganization settings by navigating to\nSettings > Organization > AI providers:\nAny headers you add to the configuration will be passed through in the request to the custom endpoint.\nThe values of the headers can also be templated using Mustache syntax.\nCurrently, the supported template variables are {{email}}\nand {{model}}\n.\nwhich will be replaced with the email of the user whom the Braintrust API key belongs to and the model name, respectively.\nIf the endpoint is non-streaming, set the Endpoint supports streaming\nflag to false. The proxy will\nconvert the response to streaming format, allowing the models to work in the playground.\nEach custom model must have a flavor (chat\nor completion\n) and format (openai\n, anthropic\n, google\n, window\nor js\n). Additionally, they can\noptionally have a boolean flag if the model is multimodal and an input cost and output cost, which will only be used to calculate and display estimated\nprices for experiment runs.\nSpecifying an org\nIf you are part of multiple organizations, you can specify which organization to use by passing the x-bt-org-name\nheader in the SDK:\nTemporary credentials for end user access\nA temporary credential converts your Braintrust API key (or model provider API key) to a time-limited credential that can be safely shared with end users.\n- Temporary credentials can also carry additional information to limit access to a particular model and/or enable logging to Braintrust.\n- They can be used in the\nAuthorization\nheader anywhere you'd use a Braintrust API key or a model provider API key.\nUse temporary credentials if you'd like your frontend or mobile app to send AI requests to the proxy directly, minimizing latency without exposing your API keys to end users.\nIssue temporary credential in code\nYou can call the /credentials\nendpoint from a privileged\nlocation, such as your app's backend, to issue temporary credentials. The\ntemporary credential will be allowed to make requests on behalf of the\nBraintrust API key (or model provider API key) provided in the Authorization\nheader.\nThe body should specify the restrictions to be applied to the temporary\ncredentials as a JSON object. Additionally, if the logging\nkey is present, the\nproxy will log to Braintrust any requests made with this temporary credential.\nSee the /credentials\nAPI spec for details.\nThe following example grants access to gpt-4o-realtime-preview-2024-10-01\non\nbehalf of the key stored in the BRAINTRUST_API_KEY\nenvironment variable for 10\nminutes, logging the requests to the project named \"My project.\"\nIssue temporary credential in browser\nYou can also generate a temporary credential using the form below:\nimport { OpenAI } from \"openai\";\nconst client = new OpenAI({\nbaseURL: \"https://api.braintrust.dev/v1/proxy\",\napiKey: \"YOUR_TEMPORARY_CREDENTIAL\",\n// It is safe to store temporary credentials in the browser because they have\n// limited lifetime and access.\ndangerouslyAllowBrowser: true,\n});\nasync function main() {\nconst response = await client.chat.completions.create({\nmodel: \"gpt-4o-mini\",\nmessages: [{ role: \"user\", content: \"What is a proxy?\" }],\n});\nconsole.log(response.choices[0].message.content);\n}\nmain();\nInspect temporary credential grants\nThe temporary credential is formatted as a JSON Web Token (JWT).\nYou can inspect the JWT's payload using a library such as\njsonwebtoken\nor a web-based tool like JWT.io to\ndetermine the expiration time and granted models.\nDo not modify the JWT payload. This will invalidate the signature. Instead,\nissue a new temporary credential using the /credentials\nendpoint.\nLoad balancing\nIf you have multiple API keys for a given model type, e.g. OpenAI and Azure for gpt-4o\n, the proxy will\nautomatically load balance across them. This is a useful way to work around per-account rate limits and provide\nresiliency in case one provider is down.\nYou can setup endpoints directly on the secrets page in your Braintrust account by adding endpoints:\nPDF input\nThe proxy extends the OpenAI API to support PDF input.\nTo use it, pass the PDF's URL or base64-encoded PDF data with MIME type application/pdf\nin the request body.\nFor example,\nor\nAdvanced configuration\nThe following headers allow you to configure the proxy's behavior:\nx-bt-use-cache\n:auto | always | never\n. See Cachingx-bt-use-creds-cache\n:auto | always | never\n. Similar tox-bt-use-cache\n, but controls whether to cache the credentials used to access the provider's API. This is useful if you are rapidly tweaking credentials and don't want to wait ~60 seconds for the credentials cache to expire.x-bt-org-name\n: Specify if you are part of multiple organizations and want to use API keys/log to a specific org.x-bt-endpoint-name\n: Specify to use a particular endpoint (by its name).\nIntegration with Braintrust platform\nSeveral features in Braintrust are powered by the proxy. For example, when you create a playground, the proxy handles running the LLM calls. Similarly, if you create a prompt, when you preview the prompt's results, the proxy is used to run the LLM. However, the proxy is not required when you:\n- Run evals in your code\n- Load prompts to run in your code\n- Log traces to Braintrust\nIf you'd like to use it in your code to help with caching, secrets management, and other features, follow the instructions above to set it as the base URL in your OpenAI client.\nSelf-hosting\nIf you're self-hosting Braintrust, your API service (serverless functions or containers) contain a built-in proxy that runs within your own environment. See the self-hosting docs for more information on how to set up self-hosting.\nOpen source\nThe AI proxy is open source. You can find the code on GitHub.\nAppendix\nList of supported models and providers\n- gpt-5 (openai, azure)\n- gpt-5-2025-08-07 (openai, azure)\n- gpt-5-mini (openai, azure)\n- gpt-5-mini-2025-08-07 (openai, azure)\n- gpt-5-nano (openai, azure)\n- gpt-5-nano-2025-08-07 (openai, azure)\n- gpt-5-chat-latest (openai, azure)\n- gpt-4o (openai, azure)\n- gpt-4o-2024-11-20 (openai, azure)\n- gpt-4o-2024-08-06 (openai, azure)\n- gpt-4o-2024-05-13 (openai, azure)\n- gpt-4.1 (openai, azure)\n- gpt-4.1-2025-04-14 (openai, azure)\n- gpt-4o-mini (openai, azure)\n- gpt-4o-mini-2024-07-18 (openai, azure)\n- gpt-4.1-mini (openai, azure)\n- gpt-4.1-mini-2025-04-14 (openai, azure)\n- gpt-4.1-nano (openai, azure)\n- gpt-4.1-nano-2025-04-14 (openai, azure)\n- o4-mini (openai, azure)\n- o4-mini-2025-04-16 (openai, azure)\n- o3-mini (openai, azure)\n- o3-mini-2025-01-31 (openai, azure)\n- o3-pro (openai, azure)\n- o3-pro-2025-06-10 (openai, azure)\n- o3 (openai, azure)\n- o3-2025-04-16 (openai, azure)\n- o1 (openai, azure)\n- o1-2024-12-17 (openai, azure)\n- o1-mini (openai, azure)\n- o1-mini-2024-09-12 (openai, azure)\n- o1-pro (openai, azure)\n- o1-pro-2025-03-19 (openai, azure)\n- chatgpt-4o-latest (openai, azure)\n- gpt-4-turbo (openai, azure)\n- gpt-4-turbo-2024-04-09 (openai, azure)\n- gpt-4-turbo-preview (openai, azure)\n- gpt-4 (openai, azure)\n- gpt-4-0125-preview (openai, azure)\n- gpt-4-1106-preview (openai, azure)\n- gpt-4-0613 (openai, azure)\n- gpt-4-0314 (openai, azure)\n- gpt-4.5-preview (openai, azure)\n- gpt-4.5-preview-2025-02-27 (openai, azure)\n- o1-preview (openai, azure)\n- o1-preview-2024-09-12 (openai, azure)\n- gpt-4o-search-preview (openai, azure)\n- gpt-4o-search-preview-2025-03-11 (openai, azure)\n- gpt-4o-mini-search-preview (openai, azure)\n- gpt-4o-mini-search-preview-2025-03-11 (openai, azure)\n- gpt-3.5-turbo-0125 (openai, azure)\n- gpt-3.5-turbo (openai, azure)\n- gpt-35-turbo (azure)\n- gpt-3.5-turbo-1106 (openai, azure)\n- gpt-3.5-turbo-instruct (openai, azure)\n- gpt-3.5-turbo-instruct-0914 (openai, azure)\n- gpt-4-32k (openai, azure)\n- gpt-4-32k-0613 (openai, azure)\n- gpt-4-32k-0314 (openai, azure)\n- gpt-4-vision-preview (openai, azure)\n- gpt-4-1106-vision-preview (openai, azure)\n- gpt-3.5-turbo-16k (openai, azure)\n- gpt-35-turbo-16k (azure)\n- gpt-3.5-turbo-16k-0613 (openai, azure)\n- gpt-3.5-turbo-0613 (openai, azure)\n- gpt-3.5-turbo-0301 (openai, azure)\n- text-davinci-003 (openai, azure)\n- claude-sonnet-4-20250514 (anthropic)\n- claude-4-sonnet-20250514 (anthropic)\n- claude-3-7-sonnet-latest (anthropic)\n- claude-3-7-sonnet-20250219 (anthropic)\n- claude-3-5-haiku-latest (anthropic)\n- claude-3-5-haiku-20241022 (anthropic)\n- claude-3-5-sonnet-latest (anthropic)\n- claude-3-5-sonnet-20241022 (anthropic)\n- claude-3-5-sonnet-20240620 (anthropic)\n- claude-opus-4-1-20250805 (anthropic)\n- claude-opus-4-20250514 (anthropic)\n- claude-4-opus-20250514 (anthropic)\n- claude-3-opus-latest (anthropic)\n- claude-3-opus-20240229 (anthropic)\n- claude-3-sonnet-20240229 (anthropic)\n- claude-3-haiku-20240307 (anthropic)\n- claude-instant-1.2 (anthropic)\n- claude-instant-1 (anthropic)\n- claude-2.1 (anthropic)\n- claude-2.0 (anthropic)\n- claude-2 (anthropic)\n- openai/gpt-oss-120b (together, groq, baseten)\n- openai/gpt-oss-20b (groq)\n- accounts/fireworks/models/gpt-oss-120b (fireworks)\n- accounts/fireworks/models/gpt-oss-20b (fireworks)\n- gpt-oss-120b (cerebras)\n- meta/llama-2-70b-chat (replicate)\n- mistral (ollama)\n- phi (ollama)\n- sonar (perplexity)\n- sonar-pro (perplexity)\n- sonar-reasoning (perplexity)\n- sonar-reasoning-pro (perplexity)\n- r1-1776 (perplexity)\n- meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8 (together)\n- meta-llama/Llama-4-Scout-17B-16E-Instruct (together)\n- meta-llama/Llama-3.3-70B-Instruct-Turbo (together)\n- meta-llama/Llama-3.3-70B-Instruct-Turbo-Free (together)\n- meta-llama/Llama-3.2-90B-Vision-Instruct-Turbo (together)\n- meta-llama/Llama-3.2-11B-Vision-Instruct-Turbo (together)\n- meta-llama/Llama-Vision-Free (together)\n- meta-llama/Llama-3.2-3B-Instruct-Turbo (together)\n- meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo (together)\n- meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo (together)\n- meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo (together)\n- meta-llama/Llama-3-70b-chat-hf (together)\n- meta-llama/Meta-Llama-3-70B-Instruct-Turbo (together)\n- meta-llama/Meta-Llama-3-70B-Instruct-Lite (together)\n- meta-llama/Llama-3-8b-chat-hf (together)\n- meta-llama/Meta-Llama-3-8B-Instruct-Turbo (together)\n- meta-llama/Meta-Llama-3-8B-Instruct-Lite (together)\n- google/gemma-2-27b-it (together)\n- google/gemma-2-9b-it (together)\n- google/gemma-2b-it (together)\n- mistralai/Mistral-Small-24B-Instruct-2501 (together)\n- mistralai/Mistral-7B-Instruct-v0.3 (together)\n- mistralai/Mistral-7B-Instruct-v0.2 (together)\n- mistralai/Mistral-7B-Instruct-v0.1 (together)\n- mistralai/Mixtral-8x22B-Instruct-v0.1 (together)\n- mistralai/Mixtral-8x7B-Instruct-v0.1 (together)\n- deepseek-ai/DeepSeek-V3 (together)\n- deepseek-ai/DeepSeek-R1 (together)\n- deepseek-ai/DeepSeek-R1-Distill-Llama-70B (together)\n- deepseek-ai/DeepSeek-R1-Distill-Llama-70B-Free (together)\n- deepseek-ai/DeepSeek-R1-Distill-Qwen-14B (together)\n- deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B (together)\n- deepseek-ai/deepseek-llm-67b-chat (together)\n- Qwen/Qwen2.5-72B-Instruct-Turbo (together)\n- Qwen/Qwen2.5-7B-Instruct-Turbo (together)\n- Qwen/Qwen2.5-Coder-32B-Instruct (together)\n- Qwen/QwQ-32B (together)\n- Qwen/Qwen2-VL-72B-Instruct (together)\n- Qwen/Qwen2-72B-Instruct (together)\n- nvidia/Llama-3.1-Nemotron-70B-Instruct-HF (together)\n- microsoft/WizardLM-2-8x22B (together)\n- databricks/dbrx-instruct (together)\n- NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO (together)\n- Gryphe/MythoMax-L2-13b (together)\n- Gryphe/MythoMax-L2-13b-Lite (together)\n- meta-llama/Meta-Llama-3-70B (together)\n- meta-llama/Llama-3-8b-hf (together)\n- meta-llama/Llama-2-70b-chat-hf (together)\n- deepseek-ai/deepseek-coder-33b-instruct (together)\n- Qwen/QwQ-32B-Preview (together)\n- NousResearch/Nous-Hermes-2-Yi-34B (together)\n- magistral-medium-latest (mistral)\n- magistral-medium-2506 (mistral)\n- magistral-small-latest (mistral)\n- magistral-small-2506 (mistral)\n- mistralai/mixtral-8x7b-32kseqlen (together)\n- mistralai/Mixtral-8x7B-Instruct-v0.1-json (together)\n- mistralai/Mixtral-8x22B (together)\n- devstral-small-latest (mistral)\n- devstral-small-2507 (mistral)\n- mistral-large-latest (mistral)\n- mistral-large-2411 (mistral)\n- pixtral-large-latest (mistral)\n- pixtral-large-2411 (mistral, vertex)\n- mistral-medium-latest (mistral)\n- mistral-medium-2505 (mistral)\n- mistral-small-latest (mistral)\n- mistral-small-2501 (mistral)\n- codestral-latest (mistral)\n- codestral-2501 (mistral)\n- ministral-8b-latest (mistral)\n- ministral-8b-2410 (mistral)\n- ministral-3b-latest (mistral)\n- ministral-3b-2410 (mistral)\n- mistral-saba-latest (mistral)\n- mistral-saba-2502 (mistral)\n- pixtral-12b-2409 (mistral)\n- open-mistral-nemo (mistral)\n- open-mistral-nemo-2407 (mistral)\n- open-codestral-mamba (mistral)\n- open-mixtral-8x22b (mistral)\n- mistral-tiny (mistral)\n- mistral-small (mistral)\n- mistral-medium (mistral)\n- llama-3.3-70b-versatile (groq)\n- llama-3.1-8b-instant (groq)\n- llama3-70b-8192 (groq)\n- llama3-8b-8192 (groq)\n- llama-guard-3-8b (groq)\n- gemma2-9b-it (groq)\n- meta-llama/llama-4-maverick-17b-128e-instruct (groq)\n- meta-llama/llama-4-scout-17b-16e-instruct (groq)\n- llama-3.3-70b-specdec (groq)\n- llama-3.2-90b-vision-preview (groq)\n- llama-3.2-11b-vision-preview (groq)\n- llama-3.2-3b-preview (groq)\n- llama-3.2-1b-preview (groq)\n- mistral-saba-24b (groq)\n- deepseek-r1-distill-llama-70b (groq, cerebras)\n- deepseek-r1-distill-llama-70b-specdec (groq)\n- deepseek-r1-distill-qwen-32b (groq)\n- qwen-2.5-32b (groq)\n- qwen-2.5-coder-32b (groq)\n- qwen-qwq-32b (groq)\n- gemma-7b-it (groq)\n- llama-3.1-70b-versatile (groq)\n- llama-3.1-405b-reasoning (groq)\n- llama2-70b-4096 (groq)\n- mixtral-8x7b-32768 (groq)\n- llama-4-scout-17b-16e-instruct (cerebras)\n- llama3-3-70b (lepton)\n- llama3-2-3b (lepton)\n- llama3-2-1b (lepton)\n- llama3-1-70b (lepton)\n- llama3-1-8b (lepton)\n- llama3-70b (lepton)\n- llama3-8b (lepton)\n- mistral-7b (lepton)\n- mixtral-8x7b (lepton)\n- wizardlm-2-7b (lepton)\n- wizardlm-2-8x22b (lepton)\n- nous-hermes-llama2-13b (lepton)\n- dolphin-mixtral-8x7b (lepton)\n- accounts/fireworks/models/llama4-maverick-instruct-basic (fireworks)\n- accounts/fireworks/models/llama4-scout-instruct-basic (fireworks)\n- accounts/fireworks/models/llama-v3p3-70b-instruct (fireworks)\n- accounts/fireworks/models/llama-v3p2-90b-vision-instruct (fireworks)\n- accounts/fireworks/models/llama-v3p2-11b-vision-instruct (fireworks)\n- accounts/fireworks/models/llama-v3p2-3b-instruct (fireworks)\n- accounts/fireworks/models/llama-v3p1-405b-instruct (fireworks)\n- accounts/fireworks/models/llama-v3p1-405b-instruct-long (fireworks)\n- accounts/fireworks/models/llama-v3p1-70b-instruct (fireworks)\n- accounts/fireworks/models/llama-v3p1-8b-instruct (fireworks)\n- accounts/fireworks/models/mistral-small-24b-instruct-2501 (fireworks)\n- accounts/fireworks/models/mixtral-8x22b-instruct (fireworks)\n- accounts/fireworks/models/mixtral-8x7b-instruct (fireworks)\n- accounts/fireworks/models/phi-3-vision-128k-instruct (fireworks)\n- accounts/fireworks/models/deepseek-v3 (fireworks)\n- accounts/fireworks/models/deepseek-v3-0324 (fireworks)\n- accounts/fireworks/models/deepseek-r1 (fireworks)\n- accounts/fireworks/models/deepseek-r1-basic (fireworks)\n- accounts/fireworks/models/qwen2p5-72b-instruct (fireworks)\n- accounts/fireworks/models/qwen2p5-coder-32b-instruct (fireworks)\n- accounts/fireworks/models/qwq-32b (fireworks)\n- accounts/fireworks/models/qwen2-vl-72b-instruct (fireworks)\n- accounts/fireworks/models/qwen-qwq-32b-preview (fireworks)\n- llama3.3-70b (cerebras)\n- llama3.1-8b (cerebras)\n- gemini-2.5-flash (google)\n- gemini-2.5-pro (google)\n- gemini-2.5-flash-preview-05-20 (google)\n- gemini-2.5-flash-preview-04-17 (google)\n- gemini-2.5-pro-preview-06-05 (google)\n- gemini-2.5-pro-preview-05-06 (google)\n- gemini-2.5-pro-preview-03-25 (google)\n- gemini-2.5-pro-exp-03-25 (google)\n- gemini-2.5-flash-lite-preview-06-17 (google)\n- gemini-2.5-flash-lite (google)\n- gemini-2.0-flash (google)\n- gemini-2.0-flash-001 (google)\n- gemini-2.0-flash-lite (google)\n- gemini-2.0-flash-lite-001 (google)\n- gemini-1.5-flash (google)\n- gemini-1.5-flash-latest (google)\n- gemini-1.5-flash-001 (google)\n- gemini-1.5-flash-002 (google)\n- gemini-1.5-flash-8b (google)\n- gemini-1.5-flash-8b-latest (google)\n- gemini-1.5-flash-8b-001 (google)\n- gemini-1.5-pro (google)\n- gemini-1.5-pro-latest (google)\n- gemini-1.5-pro-001 (google)\n- gemini-1.5-pro-002 (google)\n- gemini-2.0-pro-exp-02-05 (google)\n- gemini-2.0-flash-exp (google)\n- gemini-2.0-flash-thinking-exp-01-21 (google)\n- learnlm-1.5-pro-experimental (google)\n- gemini-exp-1206 (google)\n- gemini-1.0-pro (google)\n- gemini-pro (google)\n- grok-4 (xAI)\n- grok-4-latest (xAI)\n- grok-4-0709 (xAI)\n- grok-3 (xAI)\n- grok-3-latest (xAI)\n- grok-3-beta (xAI)\n- grok-3-mini (xAI)\n- grok-3-mini-latest (xAI)\n- grok-3-mini-beta (xAI)\n- grok-3-mini-fast (xAI)\n- grok-3-mini-fast-latest (xAI)\n- grok-3-mini-fast-beta (xAI)\n- grok-3-fast-beta (xAI)\n- grok-3-fast-latest (xAI)\n- grok-2-vision (xAI)\n- grok-2-vision-latest (xAI)\n- grok-2-vision-1212 (xAI)\n- grok-2 (xAI)\n- grok-2-latest (xAI)\n- grok-2-1212 (xAI)\n- grok-vision-beta (xAI)\n- grok-beta (xAI)\n- amazon.nova-pro-v1:0 (bedrock)\n- us.amazon.nova-pro-v1:0 (bedrock)\n- amazon.nova-micro-v1:0 (bedrock)\n- us.amazon.nova-micro-v1:0 (bedrock)\n- amazon.nova-lite-v1:0 (bedrock)\n- us.amazon.nova-lite-v1:0 (bedrock)\n- amazon.titan-text-premier-v1:0 (bedrock)\n- amazon.titan-text-express-v1 (bedrock)\n- amazon.titan-text-lite-v1 (bedrock)\n- anthropic.claude-sonnet-4-20250514-v1:0 (bedrock)\n- us.anthropic.claude-sonnet-4-20250514-v1:0 (bedrock)\n- anthropic.claude-3-7-sonnet-20250219-v1:0 (bedrock)\n- us.anthropic.claude-3-7-sonnet-20250219-v1:0 (bedrock)\n- anthropic.claude-3-5-haiku-20241022-v1:0 (bedrock)\n- us.anthropic.claude-3-5-haiku-20241022-v1:0 (bedrock)\n- anthropic.claude-3-5-sonnet-20241022-v2:0 (bedrock)\n- us.anthropic.claude-3-5-sonnet-20241022-v2:0 (bedrock)\n- apac.anthropic.claude-3-5-sonnet-20241022-v2:0 (bedrock)\n- anthropic.claude-3-5-sonnet-20240620-v1:0 (bedrock)\n- us.anthropic.claude-3-5-sonnet-20240620-v1:0 (bedrock)\n- apac.anthropic.claude-3-5-sonnet-20240620-v1:0 (bedrock)\n- eu.anthropic.claude-3-5-sonnet-20240620-v1:0 (bedrock)\n- anthropic.claude-opus-4-1-20250805-v1:0 (anthropic)\n- us.anthropic.claude-opus-4-1-20250805-v1:0 (anthropic)\n- anthropic.claude-opus-4-20250514-v1:0 (bedrock)\n- us.anthropic.claude-opus-4-20250514-v1:0 (bedrock)\n- anthropic.claude-3-opus-20240229-v1:0 (bedrock)\n- us.anthropic.claude-3-opus-20240229-v1:0 (bedrock)\n- anthropic.claude-3-sonnet-20240229-v1:0 (bedrock)\n- us.anthropic.claude-3-sonnet-20240229-v1:0 (bedrock)\n- apac.anthropic.claude-3-sonnet-20240229-v1:0 (bedrock)\n- eu.anthropic.claude-3-sonnet-20240229-v1:0 (bedrock)\n- anthropic.claude-3-haiku-20240307-v1:0 (bedrock)\n- us.anthropic.claude-3-haiku-20240307-v1:0 (bedrock)\n- apac.anthropic.claude-3-haiku-20240307-v1:0 (bedrock)\n- eu.anthropic.claude-3-haiku-20240307-v1:0 (bedrock)\n- meta.llama3-3-70b-instruct-v1:0 (bedrock)\n- us.meta.llama3-3-70b-instruct-v1:0 (bedrock)\n- meta.llama3-2-90b-instruct-v1:0 (bedrock)\n- us.meta.llama3-2-90b-instruct-v1:0 (bedrock)\n- meta.llama3-2-11b-instruct-v1:0 (bedrock)\n- us.meta.llama3-2-11b-instruct-v1:0 (bedrock)\n- meta.llama3-2-3b-instruct-v1:0 (bedrock)\n- us.meta.llama3-2-3b-instruct-v1:0 (bedrock)\n- eu.meta.llama3-2-3b-instruct-v1:0 (bedrock)\n- meta.llama3-2-1b-instruct-v1:0 (bedrock)\n- us.meta.llama3-2-1b-instruct-v1:0 (bedrock)\n- eu.meta.llama3-2-1b-instruct-v1:0 (bedrock)\n- meta.llama3-1-405b-instruct-v1:0 (bedrock)\n- us.meta.llama3-1-405b-instruct-v1:0 (bedrock)\n- meta.llama3-1-70b-instruct-v1:0 (bedrock)\n- us.meta.llama3-1-70b-instruct-v1:0 (bedrock)\n- meta.llama3-1-8b-instruct-v1:0 (bedrock)\n- us.meta.llama3-1-8b-instruct-v1:0 (bedrock)\n- meta.llama3-70b-instruct-v1:0 (bedrock)\n- meta.llama3-8b-instruct-v1:0 (bedrock)\n- mistral.mistral-large-2402-v1:0 (bedrock)\n- mistral.mistral-small-2402-v1:0 (bedrock)\n- mistral.mixtral-8x7b-instruct-v0:1 (bedrock)\n- mistral.mistral-7b-instruct-v0:2 (bedrock)\n- cohere.command-r-plus-v1:0 (bedrock)\n- cohere.command-r-v1:0 (bedrock)\n- cohere.command-text-v14 (bedrock)\n- cohere.command-light-text-v14 (bedrock)\n- publishers/google/models/gemini-2.0-flash (vertex)\n- publishers/google/models/gemini-2.0-flash-001 (vertex)\n- publishers/google/models/gemini-2.0-flash-lite (vertex)\n- publishers/google/models/gemini-2.0-flash-lite-001 (vertex)\n- publishers/google/models/gemini-1.5-pro (vertex)\n- publishers/google/models/gemini-1.5-pro-002 (vertex)\n- publishers/google/models/gemini-1.5-pro-001 (vertex)\n- publishers/google/models/gemini-1.5-flash (vertex)\n- publishers/google/models/gemini-1.5-flash-002 (vertex)\n- publishers/google/models/gemini-1.5-flash-001 (vertex)\n- publishers/google/models/gemini-1.0-pro-vision (vertex)\n- publishers/google/models/gemini-1.0-pro-vision-001 (vertex)\n- publishers/google/models/gemini-1.0-pro (vertex)\n- publishers/google/models/gemini-1.0-pro-002 (vertex)\n- publishers/google/models/gemini-1.0-pro-001 (vertex)\n- publishers/anthropic/models/claude-sonnet-4 (vertex)\n- publishers/anthropic/models/claude-sonnet-4@20250514 (vertex)\n- publishers/anthropic/models/claude-3-7-sonnet (vertex)\n- publishers/anthropic/models/claude-3-7-sonnet@20250219 (vertex)\n- publishers/anthropic/models/claude-3-5-haiku (vertex)\n- publishers/anthropic/models/claude-3-5-haiku@20241022 (vertex)\n- publishers/anthropic/models/claude-3-5-sonnet-v2 (vertex)\n- publishers/anthropic/models/claude-3-5-sonnet-v2@20241022 (vertex)\n- publishers/anthropic/models/claude-3-5-sonnet (vertex)\n- publishers/anthropic/models/claude-3-5-sonnet@20240620 (vertex)\n- publishers/anthropic/models/claude-opus-4 (vertex)\n- publishers/anthropic/models/claude-opus-4@20250514 (vertex)\n- publishers/anthropic/models/claude-3-opus (vertex)\n- publishers/anthropic/models/claude-3-opus@20240229 (vertex)\n- publishers/anthropic/models/claude-3-haiku (vertex)\n- publishers/anthropic/models/claude-3-haiku@20240307 (vertex)\n- publishers/meta/models/llama-3.1-401b-instruct-maas (vertex)\n- publishers/mistralai/models/mistral-large-2411 (vertex)\n- publishers/mistralai/models/mistral-nemo (vertex)\n- publishers/mistralai/models/codestral-2501 (vertex)\n- publishers/google/models/gemini-2.5-pro (vertex)\n- publishers/google/models/gemini-2.5-pro-preview-05-06 (vertex)\n- publishers/google/models/gemini-2.5-pro-preview-03-25 (vertex)\n- publishers/google/models/gemini-2.5-pro-exp-03-25 (vertex)\n- publishers/google/models/gemini-2.5-flash (vertex)\n- publishers/google/models/gemini-2.5-flash-preview-05-20 (vertex)\n- publishers/google/models/gemini-2.5-flash-preview-04-17 (vertex)\n- publishers/google/models/gemini-2.5-flash-lite-preview-06-17 (vertex)\n- publishers/google/models/gemini-2.5-flash-lite (vertex)\n- publishers/google/models/gemini-2.0-flash-thinking-exp-01-21 (vertex)\n- publishers/meta/models/llama-3.3-70b-instruct-maas (vertex)\n- publishers/meta/models/llama-3.2-90b-vision-instruct-maas (vertex)\n- publishers/meta/models/llama-3.1-70b-instruct-maas (vertex)\n- publishers/meta/models/llama-3.1-8b-instruct-maas (vertex)\n- publishers/google/models/gemini-2.0-flash-lite-preview-02-05 (vertex)\n- databricks-claude-3-7-sonnet (databricks)\n- databricks-meta-llama-3-3-70b-instruct (databricks)\n- databricks-meta-llama-3-1-405b-instruct (databricks)\n- databricks-meta-llama-3-1-8b-instruct (databricks)\n- Qwen3-Coder-480B-A35B-Instruct (baseten)\n- moonshotai/Kimi-K2-Instruct (baseten)\n- deepseek-ai/DeepSeek-V3-0324 (baseten)\n- grok-code-fast (xAI)\n- grok-code-fast-1 (xAI)\n- grok-code-fast-1-0825 (xAI)\nWe are constantly adding new models. If you have a model you'd like to see supported, please let us know!", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/blog/ai-proxy", "anchor": "blog post"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "Quickstart"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "Key features"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "Caching"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "Cache modes"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "Cache TTL"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "Cache control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "Encryption"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "Tracing"}, {"href": "https://www.braintrust.dev/docs/guides/tracing", "anchor": "span slug"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "Supported models"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "full list of models and providers"}, {"href": "mailto:support@braintrust.dev", "anchor": "let us know"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "Supported protocols"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "HTTP-based models"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "Reasoning models"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "Non-streaming request with reasoning parameters"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "Streaming request with reasoning delta"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "WebSocket-based models"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "temporary credentials"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "Using Braintrust API keys"}, {"href": "https://www.braintrust.dev/signup", "anchor": "sign up for an account"}, {"href": "https://www.braintrust.dev/app/settings?subroute=secrets", "anchor": "AI providers"}, {"href": "https://www.braintrust.dev/app/settings?subroute=api-keys", "anchor": "API key"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "Custom models"}, {"href": "https://www.braintrust.dev/docs/reference/organizations", "anchor": "organization settings"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "Specifying an org"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "Temporary credentials for end user access"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "Issue temporary credential in code"}, {"href": "https://www.braintrust.dev/docs/reference/api/Proxy", "anchor": "/credentials endpoint"}, {"href": "https://www.braintrust.dev/docs/reference/api/Proxy", "anchor": "/credentials API spec"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "Issue temporary credential in browser"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "Inspect temporary credential grants"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "Load balancing"}, {"href": "https://www.braintrust.dev/app/settings?subroute=secrets", "anchor": "secrets page"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "PDF input"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "Advanced configuration"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "Caching"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "Integration with Braintrust platform"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "playground"}, {"href": "https://www.braintrust.dev/docs/guides/prompts", "anchor": "create a prompt"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "instructions above"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "Open source"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "Appendix"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "List of supported models and providers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "let us know"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "Quickstart"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "Key features"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "Caching"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "Cache modes"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "Cache TTL"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "Cache control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "Encryption"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "Tracing"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "Supported models"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "Supported protocols"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "HTTP-based models"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "Reasoning models"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "Non-streaming request with reasoning parameters"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "Streaming request with reasoning delta"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "WebSocket-based models"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "Using Braintrust API keys"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "Custom models"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "Specifying an org"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "Temporary credentials for end user access"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "Issue temporary credential in code"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "Issue temporary credential in browser"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "Inspect temporary credential grants"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "Load balancing"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "PDF input"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "Advanced configuration"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "Integration with Braintrust platform"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "Open source"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "Appendix"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "List of supported models and providers"}], "depth": 2}, "https://www.braintrust.dev/docs/guides/self-hosting": {"url": "https://www.braintrust.dev/docs/guides/self-hosting", "title": "Self-hosting - Docs - Guides - Braintrust", "text": "Self-hosting Braintrust\nBraintrust supports self-hosting through its unique hybrid architecture. In this guide, we outline important considerations and trade-offs for the various self-hosting options, along with how we can most effectively support you.\nOverview\nIn general, there are two ways to self-host Braintrust:\n- Using our official infrastructure packages (Terraform, CloudFormation1)\n- Using our docker containers\nWe strongly recommend using our Terraform module because it's kept up-to-date with best practices, mirrors our fully hosted offering (proven at scale), and minimizes configuration issues. This is important because when troubleshooting performance or operational issues, we first need to understand what unique characteristics of your deployment might be contributing factors. If you use our Terraform modules, there are entire classes of issues that simply cannot exist. For example, if an API instance is using too many resources, this will not cause downstream issues, because API requests run via AWS lambda and are isolated.\nA common piece of feedback we hear is that the Terraform module uses practices that differ from your specific infrastructure. While that may be true, we\u2019d ask that you consider two points before rejecting it:\n- A common practice is to run Braintrust inside a separate AWS account, so that it does not clutter or otherwise affect your main account. In many cases, this is enough to alleviate concerns about its design.\n- If there are small tweaks that would make it work well in your infrastructure, it is significantly easier to fork (and even upstream) changes to the Terraform module than it is to reinvent the infra yourself from scratch.\nIf you choose not to use our Terraform module, we will still support you, but you are now equally responsible for the uptime, security, and performance of your deployment as we are. Expect to put in significantly more work to keep your system running well.\nAzure, Google Cloud Platform (GCP)\nWe now have a terraform template available for Azure and plan to build one for GCP. If this is of interest to you, please reach out.\nKubernetes (k8s)\nOur AWS and Azure terraform submodules can be used in combination with existing k8s clusters as an option for hosting compute services. We provide a helm chart for deploying our core services in k8s and submodules for deploying data stores such as PostgreSQL and Redis. We plan to support k8s for the long term. In AWS, we still do not recommend using k8s because you must handle auto scaling yourself, rather than using the built-in scaling from lambda functions.\nRoles and responsibilities\nWhen you self-host, uptime becomes a shared responsibility between your team and ours. It is our responsibility to respond quickly when you have issues, collaboratively resolve them with you, and fix bugs/improve quality so that you encounter fewer issues in the future. It is your responsibility to follow our documentation, assign infrastructure resources on your team, and make sure that in the event of an incident, you have staff who are familiar with Braintrust and can work with our team to share context and resolve issues.\nIf you use our Terraform, we can help you resolve issues much more efficiently, since we can rule out infrastructure configuration as a root cause and assume you have deployed with best practices. Our Terraform template also has built-in support for enabling temporary, secure remote access for our staff.\nIf you use our Docker containers, you should be prepared to:\n- Collect logs and make them available to us upon request\n- Make sure one or more people on your team can access the docker containers, database (via\npsql\n), and storage buckets to perform ad-hoc checks and updates. - Ensure your on-call resources are familiar with Braintrust.\n- If possible, give us temporary remote access when required to resolve complex issues. We understand this is not always possible, but if not, it\u2019s important to make sure that someone on your team can access the system.\nOur product and the AI space as a whole is evolving quickly and therefore not perfect \u2014 issues will happen. Therefore, it\u2019s important that as a shared team, we make sure that you and your users are set up for success. Review this section and make sure that you can commit to what is required for the deployment option you pick. If you do not feel like you have the resources or bandwidth to commit to what\u2019s required, then pick an easier option (e.g. Terraform rather than Docker, or fully hosted rather than Terraform).\nHere\u2019s a quick summary of expectations with each deployment option. There are more details in the sections that follow.\nMonitoring\nWe have learned some important lessons from customers at varying scale:\n- There are a handful of Braintrust-specific metrics (like Brainstore indexing lag) that tell us whether the system is struggling.\n- Generally, when customers find issues, they immediately notify us and we work together on gathering context from their logs. The time between identifying an issue and getting us the relevant bits of information is usually a major contributor to downtime.\n- Many issues, if identified proactively or quickly, can be resolved with little or no downtime. Ideally we can issue alerts as soon as we detect them.\nTo address these in the most efficient way possible, self-hosted Braintrust (Terraform and Docker) now automatically sends a subset of telemetry back to our control plane. By default, we send metrics and status information but not logs or traces. Each service supports the following:\n- Sending metrics, logs, and traces to Braintrust's control plane. These requests are automatically authorized using your license key and therefore tied to your account. We will use this information to monitor the health of your deployment for you.\n- While we are careful not to include any PII in logs or traces, we understand that you may not feel comfortable sending them to us. Each type of telemetry (metrics, logs, and traces) can be individually enabled or disabled.\n- Metrics and traces can be sent to an OpenTelemetry destination of your choice (via HTTP). You can collect logs directly from the Lambda functions and Docker containers. We can provide high level guidance on what to look for, but we're not planning to build integrations into specific observability tools for you to monitor the system.\n- There are a few endpoints that Braintrust's engineering team can access to debug issues and monitor system health. Specifically, the\n/brainstore/backfill/*\nendpoints which report system metrics about the backfill and compaction status of Brainstore segments. Note that these endpoints do not access or expose any data, just metadata from Brainstore. You can disable these endpoints by setting theDISABLE_SYSADMIN_TELEMETRY\nenvironment variable totrue\n. - There is an optional,\nTELEMETRY_ENABLED\nflag which sends billing and usage data to Braintrust. This is disabled by default, but it may be required depending on your contract with Braintrust. It may default to enabled in the future.\nIn general, our approach will be to send ourselves enough information to (a) proactively notify ourselves and you when something is going wrong and (b) have the information required to diagnose issues without asking you to dig it up for us. We also plan to explore visualizing system health in an admin dashboard directly in our UI.\nUpgrades\nWe release new versions of the data plane around once per week, often with incremental changes that improve the performance of Brainstore, add support for new UI features, and improve logging. You do not need to update this often, but here is a framework for how often you should update:\n- Generally speaking, customers update about once per month\n- You must update at least once per quarter\n- If you are collaborating closely with us, e.g. on improving the performance of a query, you may need to update more often (each time we release a new version). If so, we\u2019ll be in close contact with you about updating.\nWhile upgrading, if you use our built-in Terraform modules, run terraform apply\n. This will make any relevant infra changes, as well as update the versions of Braintrust\u2019s code. If you are deploying via Docker, then you should:\n- Make sure to update both the API and Brainstore services to the same version. Running mismatched versions can result in downtime.\n- Periodically review the Terraform template and docs to make sure you are following best practices and have all of the necessary infrastructure components in place.\nRemote access\nThere are occasionally issues that will require ad-hoc debugging or running manual commands against the container, Postgres database, or storage buckets to repair the state of the system. Customers who give us remote access (as needed) have experienced much faster resolutions when such issues occur, because our team can connect directly and resolve things. We understand that this is not always possible, but if not, we kindly ask you to factor this into your uptime calculations. Said another way, if uptime of Braintrust is a key metric for you, then you should strongly consider making remote access, as needed, available to our team.\nIf you cannot set up remote access, then make sure that you can swiftly spin up a terminal that can perform the following:\n- Access the containers directly (\ndocker exec\n, update them, view logs, restart them, view host metrics like CPU, network, memory, and disk utilization) - Run SQL queries against Postgres\n- Connect to Redis\n- Run read, write, and list commands against your storage buckets\nIt\u2019s important that your on-call staff have basic familiarity with Braintrust and the ability to perform all of these operations.\n1 CloudFormation was our original infrastructure option and is not recommended for new deployments, but will continue to be supported for existing users. All of the trade-offs and considerations that apply to Terraform apply to CloudFormation as well.", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting/aws", "anchor": "Self-host on AWS"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting/docker", "anchor": "Deploy with Docker"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting/advanced", "anchor": "Advanced self-hosting topics"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting Braintrust"}, {"href": "https://www.braintrust.dev/docs/reference/platform/architecture", "anchor": "architecture"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Overview"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "1"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting/docker", "anchor": "docker containers"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Azure, Google Cloud Platform (GCP)"}, {"href": "mailto:support@braintrust.dev", "anchor": "reach out"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Kubernetes (k8s)"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Roles and responsibilities"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Monitoring"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Upgrades"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Remote access"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}], "depth": 2}, "https://www.braintrust.dev/docs/guides/api": {"url": "https://www.braintrust.dev/docs/guides/api", "title": "API walkthrough - Docs - Guides - Braintrust", "text": "API walkthrough\nThe Braintrust REST API is available via an OpenAPI spec published at https://github.com/braintrustdata/braintrust-openapi. This guide walks through a few common use cases, and should help you get started with using the API. Each example is implemented in a particular language, for legibility, but the API itself is language-agnostic.\nTo learn more about the API, see the full API spec. If you are looking for a language-specific wrapper over the bare REST API, we support several different languages.\nRunning an experiment\nFetching experiment results\nLet's say you have a human review workflow and you want to determine if an experiment has been fully reviewed. You can do this by running a Braintrust query language (BTQL) query:\nTo do this in Python, you can use the btql\nendpoint:\nFetch specific child spans based on trace-level metadata\nLet's say you want to retrieve child spans based on trace-level metadata. You can do this via the API by first filtering the spans you want to look\nat by whatever metadata\nyou are interested in, and then iterating through those spans to find the one(s) you are interested in.\nThe example below shows how you can calculate how long a given child span took to run that are in traces having a metadata key of \"orgName\" set to \"qawolf\".\nPaginating a large dataset\nIf you're using the Python or TypeScript SDK, pagination is handled automatically. Only use this code if you're developing with other tools.\nDeleting logs\nTo delete logs, you have to issue log requests with the _object_delete\nflag set to true\n.\nFor example, to find all logs matching a specific criteria, and then delete them, you can\nrun a script like the following:\nImpersonating a user for a request\nUser impersonation allows a privileged user to perform an operation on behalf of another user, using the impersonated user's identity and permissions. For example, a proxy service may wish to forward requests coming in from individual users to Braintrust without requiring each user to directly specify Braintrust credentials. The privileged service can initiate the request with its own credentials and impersonate the user so that Braintrust runs the operation with the user's permissions.\nTo this end, all API requests accept a header x-bt-impersonate-user\n, which you\ncan set to the ID or email of the user to impersonate. Currently impersonating\nanother user requires that the requesting user has specifically been granted the\nOwner\nrole over all organizations that the impersonated user belongs to. This\ncheck guarantees the requesting user has at least the set of permissions that\nthe impersonated user has.\nConsider the following code example for configuring ACLs and running a request with user impersonation.\nPostman\nPostman is a popular tool for interacting with HTTP APIs. You can load Braintrust's API spec into Postman by importing the OpenAPI spec's URL.\nTracing with the REST API SDKs\nIn this section, we demonstrate the basics of logging with tracing using the\nlanguage-specific REST API SDKs. The end result of running each example should\nbe a single log entry in a project called tracing_test\n, which looks like the\nfollowing:", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/api/spec", "anchor": "API spec"}, {"href": "https://www.braintrust.dev/docs/reference/api", "anchor": "languages"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "Running an experiment"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "Fetching experiment results"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "human review"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "Braintrust query language (BTQL)"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "Fetch specific child spans based on trace-level metadata"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "Paginating a large dataset"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "Deleting logs"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "Impersonating a user for a request"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "Postman"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "Tracing with the REST API SDKs"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "Running an experiment"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "Fetching experiment results"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "Fetch specific child spans based on trace-level metadata"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "Paginating a large dataset"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "Deleting logs"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "Impersonating a user for a request"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "Postman"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "Tracing with the REST API SDKs"}], "depth": 2}, "https://www.braintrust.dev/docs/best-practices/scorers": {"url": "https://www.braintrust.dev/docs/best-practices/scorers", "title": "Writing scorers - Docs - Braintrust", "text": "Writing scorers\nTo accurately evaluate the quality of your AI systems, you need to write good scorers. Scorers allow you to evaluate the output of LLMs based on a set of criteria. These can include both heuristics (expressed as code) or prompts (expressed as LLM-as-a-judge). Scorers help you assign a performance score between 0 and 100% to assess how well the AI outputs match expected results. While many scorers are available out of the box through the open-source autoevals library, most create their own custom scorers based on their specific use case.\nThis guide outlines a structured approach and best practices, based on insights from Loom's implementation, to help you build reliable scorers tailored to your AI features.\nMental models\nScorers are a crucial element of both offline and online evaluations:\n- Offline evaluations are used to proactively identify and resolve issues before deployment.\n- Online evaluation involves running scorers on live requests to diagnose problems, monitor performance, and capture user feedback in real-time.\nWriting good scorers is important for both parts of the LLM software development lifecycle. Any scorer you create for offline evaluation can also be run on a live request.\nDefine clear criteria\nBefore beginning to write scorers, clearly identify the criteria users will use to evaluate the generated output. You can start by defining:\n- Input: The data or prompt given to the model.\n- Output: The expected result from the model.\nThen, specify traits that users would expect and value in the output. Think of this like the product requirements you put together when developing a new feature in your product. Common traits might include:\n- Accuracy of information\n- Conciseness\n- Clarity and readability\n- Appropriate tone\n- Correct grammar and spelling\n- Bias and safety\n- Adherence to specific formatting\nIn more complex, agentic workflows, it's possible that each step will have its own inputs and outputs. This just means that you might have different criteria, and therefore different scorers, for each step. Braintrust will automatically aggregate scores across spans for each trace.\nApply common quality checks\nYou will certainly have success criteria that are unique to your product and use case, but many evaluation scenarios also benefit from common quality checks. Check out this list of common checks, and verify if they apply to your use case:\n- Relevance: Does the output reflect the source input accurately?\n- Readability: Is the language clear and easy to understand?\n- Structure and formatting: Does the output follow required formats, such as structured lists or JSON schemas?\n- Factuality: Is the provided information correct and verifiable?\n- Safety: Is the content free from biased or offensive language?\n- Language accuracy: Does the output match the requested language?\nThen, consider if you'd need to tailor any of these checks specifically for your application. For example, you might want a specific structure or formatting, or be pulling information from an external resource. Getting as specific as possible in determining what you're looking for improves the reliability of your application.\nAutomate with code-based checks\nWhere possible, implement deterministic quality checks through code-based scoring functions. Code-based scorers are reliable and consistent, execute quickly and efficiently, and reduce variability from human or model judgments. Code-based scorers in Braintrust can be written in either TypeScript or Python, via either the UI or SDK. They return a score between 0\nand 1\n.\nSome examples of code-based checks include:\n- Verifying valid JSON structure\n- Checking text length constraints (for example, less than 100 characters)\n- Ensuring outputs match predefined patterns (for example, a bullet-point list of exactly three items)\nSchema validation libraries like pydantic\nor jsonschema\nare useful for formatting requirements.\nDevelop and align LLM-based scorers\nFor more subjective and nuanced criteria that code can not capture, like tone appropriateness or creativity, you can use LLM-based scorers.\nWhen building these, it's important to:\n- Design judge prompts with explicit instructions, examples of good vs. bad outputs, and a clear scoring rubric\n- Use chain of thought to understand why the model is assigning a specific score\n- Use more granular scoring when necessary\n- Choose the model that is best suited for the evaluation, which may be different from the model used in the task\nFor example:\nWhen you create your LLM-based scorer, you will assign each choice in the rubric to a specific score between 0 and 1. Binary scoring is often recommended as it's easier to define and creates less confusion among human reviewers during alignment. However, when you need more nuanced evaluation, be sure to clearly explain what each choice score corresponds to like in the example above.\nLLMs can also help you generate good scorer prompts.\nTo calibrate your LLM-based scorer, test it on a small but representative dataset that covers edge cases, different user personas, and a good variety of inputs. Compare the results with human spot checks to make sure they are aligned.\nIn Braintrust, you can enable chain of thought (CoT) with a toggle or flag from the UI or SDK, respectively.\nIterate on your initial set of criteria\nScorer development is an ongoing process. After assessing your initial scorers, you should review low-score outputs to identify missing criteria or edge-case behaviors. Based on what you find, you can refine your definitions and add new scorers for uncovered aspects. You can also rerun the calibration step on an expanded example set, and adjust prompts, model providers, or code as needed.\nBy tightly coupling development, evaluation, and refinement, you can make sure that your scorers stay aligned with evolving product needs and user inputs.\nBest practices for scorer design\n- Provide clear rationale: When using language-model-based scorers, enable detailed rationale explanations to understand scoring decisions and refine scorer behavior.\n- Single-aspect scorers: Create separate scorers for each distinct evaluation aspect, such as accuracy versus style.\n- Weighted scoring: Use weighted averages when combining scores, prioritizing critical criteria over less important ones.\n- Appropriate scoring scales: Match the scoring scale to evaluation complexity. Use binary scoring (yes/no) for simple checks and multi-point scales for nuanced assessments.\nEvaluating agents\nWhen evaluating agents, scorers should assess not only individual responses but also overall agent behavior and performance:\n- Goal completion: Did the agent accomplish the assigned task?\n- Efficiency: Did the agent complete the task within acceptable resource or time constraints?\n- Interaction quality: Was the interaction coherent, helpful, and aligned with user expectations?\n- Error handling: Did the agent handle unexpected situations gracefully and recover effectively?\nConsider using simulations or controlled scenarios to thoroughly evaluate agent performance across these dimensions.\nFor more information on evaluating agents, check out the full guide.\nBenefits of effective scorers\nBy following a structured evaluation cycle (define, implement, evaluate, refine), you can:\n- Get closer to deterministic model behavior\n- Quickly iterate and improve AI features\n- Scale evaluations without manual overhead\nReliable scorers are the backbone of high\u2011quality, user\u2011aligned AI products.", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/blog/loom", "anchor": "insights from Loom's implementation"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Mental models"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "be run on a live request"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Define clear criteria"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Apply common quality checks"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Automate with code-based checks"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Develop and align LLM-based scorers"}, {"href": "https://www.braintrust.dev/docs/guides/functions/scorers", "anchor": "chain of thought"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Iterate on your initial set of criteria"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Best practices for scorer design"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/blog/evaluating-agents", "anchor": "full guide"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Benefits of effective scorers"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Mental models"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Define clear criteria"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Apply common quality checks"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Automate with code-based checks"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Develop and align LLM-based scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Iterate on your initial set of criteria"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Best practices for scorer design"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Benefits of effective scorers"}], "depth": 2}, "https://www.braintrust.dev/docs/best-practices/agents": {"url": "https://www.braintrust.dev/docs/best-practices/agents", "title": "Evaluating agents - Docs - Braintrust", "text": "Evaluating agents\nAgent-based systems are inherently complex because they often break down tasks into multiple steps to reach a final result. Some agents operate almost entirely autonomously, repeatedly leveraging available tools to find a satisfactory answer, while others follow more predefined, static workflows. Regardless of the approach, it\u2019s important to evaluate these systems both as a whole (for example, did the agent\u2019s plan make sense, and was the final answer correct?) and at each individual step (for example, did the agent choose the right tool, did the retrieval component provide relevant information, and in multi-agent setups, did it direct the request to the correct model or sub-agent?).\nEvaluating agents can range from targeted unit-like tests to comprehensive end-to-end scenarios. Here\u2019s how to structure those evaluations specifically tailored to agent-based AI systems, ranging from simple to complex.\nKey questions for evaluating agents\nWhen evaluating sophisticated agent behaviors, ask questions like:\n- If the agent starts by providing a plan of actions to take in answering the user's query, does that plan make sense given the user's objective?\n- If the agent provides reasoning steps, are those intermediate thoughts expected?\n- Did the agent choose the correct next step or defer to a human as expected?\n- Did the agent invoke the correct tools?\n- When invoking a tool, did the agent properly build up the arguments to invoke it?\n- When examining a tool's output, did the agent properly utilize it to provide an answer or move to the next expected step?\nErrors can surface at any point in an agentic system. To debug and understand these errors it's important to capture the inputs at each step as well as the outputs.\nTypes of evaluations\nOffline evaluations\nOffline evaluations proactively identify issues in agent behavior before deployment. These function similarly to unit tests or integration tests, emphasizing reproducibility and stability. You can use datasets to test both the end-to-end performance of your agent and its intermediate steps. For instance, you might create a specific dataset to test a retrieval step in a RAG pipeline, or one that checks whether generated SQL adheres to security constraints. Once you\u2019ve created a \u201cgolden dataset\u201d with ground truth examples, you can apply either code-based scorers or LLM-as-a-judge scorers to evaluate outputs systematically.\nRecommended approach:\n- Stub external dependencies: Snapshot sufficient state from production or staging environments to simulate databases, APIs, and infrastructure.\n- Isolate specific agent actions: Create deterministic scenarios to evaluate critical behaviors reliably.\n- Assess incremental behavior: Evaluate individual agent steps, including tool calls, parameter accuracy, and responses.\nOnline evaluations\nOnline evaluations continuously monitor real-time performance, capturing live user interactions, and diagnosing issues as they arise. Here, there is no ground truth to evaluate the overall performance of the agent or any of its steps, so in general, we rely on LLM-as-a-judge scorers for evaluation.\nRecommended approach:\n-\nReal environment usage: Always evaluate in your actual production environment for accurate user experience insights.\n-\nIncorporate user feedback: Allow users to like or dislike agent responses and provide comments. This can be invaluable for error analysis and informed sampling traces for evaluation. Refer to the user feedback docs for implementation details.\n-\nReal-time scoring: Implement continuous monitoring for key behaviors like hallucinations, tool accuracy, and goal completion. More information is available in the online scoring documentation.\n-\nAdaptive sampling: Start by scoring all requests, then adjust sample rates based on agent stability and usage volume. For details on how to control sampling from your logs, check out the online scoring docs.\n-\nFeedback integration: Use both low-scoring and anomalously high-scoring examples to feed new test scenarios into offline evaluations.\nStructuring agent evaluations\nEnd-to-end:\n- Use real or simulated environments to evaluate complete task flows.\n- Focus on goal success, coherence, and robustness.\nIf you need to incorporate intermediate results in your agents to evaluate the final result, you can use the hooks\nargument in your eval's task function to add the results to your\ntrace's metadata, which can then be used in any of your eval's scorers to evaluate the final output, like this:\nSingle-step:\n- Use snapshotted scenarios with stubbed infrastructure to test specific decisions in isolation.\n- Make sure you include the inputs from the preceding step as sometimes a \"step failure\" may really be due to a problem with the previous step's output.\n- Target precise behaviors, ensuring reproducibility and reliability.\nYou can accomplish this by including \"inline scorers\" into your code. For example, you can run an inline scorer only if the agent\nchooses to initiate a tool call\n:\nTo see the full example, check out the API Agent cookbook.\nFor more complex and interrelated tool calling scenarios, this idea can be extended. For example, imagine one tool first generates SQL, a second tool executes that SQL, and a third tool translates the results into plain language. By attaching a separate inline scorer to each stage, you'll have the granular feedback needed to pinpoint and analyze errors in every part of your agent pipeline.\nAdditional resources\n- An agent that runs OpenAPI commands\n- Using functions to build a RAG agent\n- A field guide to rapidly improving AI products\nDesigning comprehensive agent evaluations\nFor agents managing complex, multi-step interactions, make sure evaluations account for variability and context-dependence:\n- Snapshotting state: Capture tool calls and responses from live environments for accurate offline evaluation scenarios.\n- Incremental assessment: Evaluate each step individually to manage non-deterministic agent interactions effectively.\n- Goal-oriented evaluation: For complex sequences, prioritize evaluations based on the agent's ultimate success or failure in achieving its intended outcome.\nEvolving your evaluation suite\nEvaluations should evolve alongside your agent\u2019s behavior and product goals.\nStart with simple scenarios, using stubbed environments to isolate key decisions.\nAdd complex flows using simulated or real data to test agents under realistic conditions.\nFor data-intensive agents (for example, manipulating and loading data into databases), define custom success criteria, like:\n- Schema compliance\n- Data transformation correctness\n- Deterministic output formats\nUse continuous feedback loops to:\n- Iterate on scorers\n- Expand your dataset coverage\n- Adapt to new agent workflows\nBy combining offline and online evaluations, and balancing end-to-end testing with single-step checks, you\u2019ll build a solid evaluation architecture. You'll be able to catch issues early, debug faster, and continuously improve your agent based on real-world user expectations.", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Key questions for evaluating agents"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Types of evaluations"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Offline evaluations"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Online evaluations"}, {"href": "https://www.braintrust.dev/docs/guides/logs/write", "anchor": "user feedback"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "online scoring"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "online scoring"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Structuring agent evaluations"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "hooks argument"}, {"href": "https://www.braintrust.dev/docs/cookbook/recipes/APIAgent-Py", "anchor": "API Agent cookbook"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Additional resources"}, {"href": "https://www.braintrust.dev/docs/cookbook/recipes/APIAgent-Py", "anchor": "An agent that runs OpenAPI commands"}, {"href": "https://www.braintrust.dev/docs/cookbook/recipes/ToolRAG", "anchor": "Using functions to build a RAG agent"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Designing comprehensive agent evaluations"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evolving your evaluation suite"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Key questions for evaluating agents"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Types of evaluations"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Offline evaluations"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Online evaluations"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Structuring agent evaluations"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Additional resources"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Designing comprehensive agent evaluations"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evolving your evaluation suite"}], "depth": 2}, "https://www.braintrust.dev/docs/guides/evals": {"url": "https://www.braintrust.dev/docs/guides/evals", "title": "Experiments - Docs - Guides - Experiments - Braintrust", "text": "Experiments\nExperiments let you snapshot the performance of your AI application so you can improve it over time. In traditional software, performance usually refers to speed, like for example, how many milliseconds it takes to complete a request. In AI, it often refers to other measurements in addition to speed, including accuracy or quality. These types of metrics are harder to define and measure, especially at scale. Assessing the performance of an LLM application is known as evaluation.\nBraintrust supports two types of evaluations:\n- Offline evals are structured experiments used to compare and improve your app systematically.\n- Online evals run scorers on live requests to monitor performance in real time.\nBoth types of evals are important for building quality AI applications.\nWhy are evals important?\nIn AI development, it's hard for teams to understand how an update will impact performance. This breaks the typical software development loop, making iteration feel like guesswork instead of engineering.\nEvaluations solve this, helping you distill the non-deterministic outputs of AI applications into an effective feedback loop that enables you to ship more reliable, higher quality products.\nSpecifically, great evals help you:\n- Understand whether an update is an improvement or a regression\n- Quickly drill down into good / bad examples\n- Diff specific examples vs. prior runs\n- Avoid playing whack-a-mole\nBreaking down evals\nEvals consist of 3 parts:\n- Data: a set of examples to test your application on\n- Task: the AI function you want to test (any function that takes in an\ninput\nand returns anoutput\n) - Scores: a set of scoring functions that take an\ninput\n,output\n, and optionalexpected\nvalue and compute a score\nYou can establish an Eval()\nfunction with these 3 pieces:\nFor more details, try the full tutorial.\nViewing experiments\nRunning your Eval\nfunction will automatically create an experiment in Braintrust,\ndisplay a summary in your Terminal, and populate the UI:\nThis gives you great visibility into how your AI application performed. You can:\n- Preview each test case and score in a table\n- Filter by high/low scores\n- Click into any individual example and see detailed tracing\n- See high level scores\n- Sort by improvements or regressions\nWhere to go from here\nNow that you understand the basics of evals and experiments, you can dive deeper into the following topics:", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "Write evals"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/run", "anchor": "Run evals"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/interpret", "anchor": "Interpret evals"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/evals", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/evals", "anchor": "Why are evals important?"}, {"href": "https://www.braintrust.dev/docs/guides/evals", "anchor": "Breaking down evals"}, {"href": "https://www.braintrust.dev/docs/welcome/start", "anchor": "full tutorial"}, {"href": "https://www.braintrust.dev/docs/guides/evals", "anchor": "Viewing experiments"}, {"href": "https://www.braintrust.dev/docs/guides/evals", "anchor": "Where to go from here"}, {"href": "https://www.braintrust.dev/docs/welcome/start", "anchor": "Run your first eval with a full tutorial"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "Writing evals"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/run", "anchor": "Running evals locally, in CI, or in production"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/interpret", "anchor": "Interpreting eval results"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}], "depth": 2}, "https://www.braintrust.dev/docs/guides/logging": {"url": "https://www.braintrust.dev/docs/guides/logging", "title": "Logs - Docs - Guides - Braintrust", "text": "Logs\nLogs are the recorded data and metadata from an AI routine. We record the inputs and outputs of your LLM calls to help you evaluate model performance on set of predefined tasks, identify patterns, and diagnose issues.\nIn Braintrust, logs consist of traces, which roughly correspond to a single request or interaction in your application. Traces consist of one or more spans, each of which corresponds to a unit of work in your application, like an LLM call, for example. You typically collect logs while running your application, both in staging (internal) and production (external) environments, using them to debug issues, monitor user behavior, and gather data for building datasets.\nWhy log in Braintrust?\nBy logging in Braintrust, you can create a feedback loop between real-world observations (logs) and offline evaluations (experiments). This feedback loop is crucial for refining your model's performance and building high-quality AI applications.\nBy design, logs are exactly the same data structure as experiments. This leads to a number of useful properties:\n- If you instrument your code to run evals, you can reuse this instrumentation to generate logs\n- Your logged traces capture exactly the same data as your evals\n- You can reuse automated and human review scores across both experiments and logs\nWhere to go from here\nNow that you know the basics of logging in Braintrust, dig into some more complex capabilities:", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/logs/write", "anchor": "Write logs"}, {"href": "https://www.braintrust.dev/docs/guides/logs/score", "anchor": "Score logs"}, {"href": "https://www.braintrust.dev/docs/guides/logs/view", "anchor": "View logs"}, {"href": "https://www.braintrust.dev/docs/guides/logs/advanced", "anchor": "Advanced"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/logging", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "datasets"}, {"href": "https://www.braintrust.dev/docs/guides/logging", "anchor": "Why log in Braintrust?"}, {"href": "https://www.braintrust.dev/docs/guides/evals", "anchor": "experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logging", "anchor": "Where to go from here"}, {"href": "https://www.braintrust.dev/docs/guides/logs/write", "anchor": "Logging user feedback"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "Online evaluation"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Logging multimodal content"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Customizing your traces"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}], "depth": 2}, "https://www.braintrust.dev/home": {"url": "https://www.braintrust.dev/home", "title": "Braintrust - The evals and observability platform for building reliable AI agents", "text": "Agents fail in unpredictable ways\nHow do you know your AI feature works?\nEvals test your AI with real data and score the results. You can determine whether changes improve or hurt performance.\nAre bad responses reaching users?\nProduction monitoring tracks live model responses and alerts you when quality drops or incorrect outputs increase.\nCan your team improve quality without guesswork?\nSide-by-side diffs allow you to compare the scores of different prompts and models, and see exactly why one version performs better than another.\nIntuitive mental model\nAll evals are composed of a dataset, task, and scorers. This framework gives teams a shared understanding for testing and improving AI applications systematically.\nCross-functional collaboration\nEngineers write code-based tests. Product managers prototype in the UI. Everyone can review results and debug issues together in real time.\nBuilt for scale\nReliable, fast infrastructure handles high-volume production traffic and complex testing workflows.\n\u201cI've never seen a workflow transformation like the one that incorporates evals into \u2018mainstream engineering\u2019 processes before. It's astonishing.\u201d\nFast prompt engineering\nTune prompts, swap models, edit scorers, and run evaluations directly in the browser. Compare traces side-by-side to see exactly what changed.\nBatch testing\nRun your prompts against hundreds or thousands of real or synthetic examples to understand performance across scenarios.\nAI-assisted workflows\nAutomate writing and optimizing prompts, scorers, and datasets with Loop, our built-in agent.\nQuantifiable progress\nMeasure changes against your own benchmarks to make data-driven decisions.\nQuality and safety gates\nPrevent quality regressions and unsafe outputs from reaching users.\nAutomated and human scoring\nRun automated tests on every change, then layer human feedback to capture the nuance machines miss.\nLive performance monitoring\nTrack latency, cost, and custom quality metrics as real traffic flows through your application.\nAutomations and alerts\nConfigure alerts that trigger when quality thresholds are crossed or safety rails trip.\nScalable log ingestion\nIngest and store all application logs with Brainstore, purpose-built for searching and analyzing AI interactions at enterprise scale.\nLoop\nPrompt optimization\nLoop analyzes your prompts and generates better-performing versions so you can hit your quality targets faster.\nSynthetic data generation\nLoop creates evaluation datasets tailored to your use case with the volume and variety needed for thorough testing.\nScorer building\nLoop builds and refines scorers to measure the specific quality metrics that matter for your application.\nBrainstore is built for AI data\nSecurity and compliance at scale\nGranular permissions\nRole-based access control with org-level permissions and project isolation to meet your security and compliance requirements.\nHybrid deployment\nSelf-hosting options to maintain full control over your AI data and meet strict compliance requirements.\nOutsized impact for the biggest brands in AI\n\u201cEvery new AI project starts with evals in Braintrust\u2014it's a game changer.\u201d", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Start for free"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Get started with evals"}, {"href": "https://www.braintrust.dev/playground", "anchor": "Try playground"}, {"href": "https://www.braintrust.dev/blog/brainstore", "anchor": "View benchmarks"}, {"href": "https://trust.braintrust.dev", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/blog/coursera", "anchor": "How Coursera builds next-generation learning tools"}, {"href": "https://www.braintrust.dev/blog/best-practices", "anchor": "Webinar: Eval best practices"}, {"href": "https://www.braintrust.dev/blog/loom", "anchor": "How Loom auto-generates video titles"}, {"href": "https://www.braintrust.dev/blog/brainstore", "anchor": "Brainstore: the database designed for AI engineering"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 2}, "https://www.braintrust.dev/docs/reference/changelog": {"url": "https://www.braintrust.dev/docs/reference/changelog", "title": "Changelog - Docs - Braintrust", "text": "Changelog\nWeek of 2025-09-08\n- Trace tree is now visible in human review mode.\n- BTQL sandbox improvements\n- Loop is now on the page and can write queries, debug errors and answer syntax questions\n- Tabs\n- Simple charts\n- Improved auto-complete\n- Updated UI color palette\n- Custom charts added to the monitor page (requires data plane 1.1.22)\n- View state changes for non-saved views\n- Before: We would attempt to restore any previous edited view state to the URL\n- After: With a few exceptions, edited view state for non-saved views is only represented in the URL\nSDK Integrations: LangChain (JS) (version 0.0.7)\n- Fixed dependency issue that prevented the integration from using the latest braintrust SDK.\nPython SDK version 0.2.7 (upcoming)\n- Fixed an OpenAI Agents concurrency bug that incorrectly handled root propagation of input/output.\n- Fixed parent span precedence issues for better trace hierarchy\n- Support locking down remote evals via\n--dev-org-name\nto only accept users from your org. - Added\nupdate-stack-url\nCLI option to explicitly change the URL of the data plane.\nTypeScript SDK version 0.3.8 (upcoming)\n- Prevents logging Braintrust API keys when logging Span objects.\n- Improved error messages when we fail to find evaluators or code definitions.\nData plane (1.1.22)\n- Added ability to create and edit custom charts in the monitor dashboard\n- Added support for more Grok models and improved model refresh handling in\n/invoke\nendpoint - Added support for\nIN\nclause in BTQL queries - Improved processing of pydantic-ai OpenTelemetry spans with tool names in span names and proper input/output field mapping\n- Added OpenAI Agents logs formatter for better span rendering in the UI\n- Added retention support for Postgres WAL and object WAL (write-ahead logs)\n- Add S3 lifecycle policies to reclaim additional space from bucket\n- Added authentication support for remote evaluation endpoints\n- Improved ability to fetch all datasets efficiently\n- New\nMAX_LIMIT_FOR_QUERIES\nparameter to set the maximum allowable limit for BTQL queries. Larger result sets can still be queried through pagination\nAutoevals PY (version 0.0.130)\n- Fold the\nbraintrust_core\nexternal package into theautoevals\npackage, since it is the only user ofbraintrust_core\n. Future braintrust packages will not depend on thebraintrust_core\npy package.\nWeek of 2025-09-01\n- Loop can search through Braintrust's docs and blog posts to help you answer questions about how to use Braintrust, including generating sample code.\nWeek of 2025-08-25\n- Traces in the trace viewer on the logs page can now show all associated traces based on a metadata field or tag.\nTypeScript SDK version 0.3.7\n- Support locking down remote evals via\n--dev-org-name\nto only accept users from your org. - Fixed parent span precedence issues for better trace hierarchy\n- Improved propagation of parentSpanId into parentSpanContext for OpenTelemetry JS v2 compatibility\n- Fold the\n@braintrust/core\npackage intobraintrust\n. This package consists of a small set of utility functions that is more easily-managed as part of the mainbraintrust\npackage. After version0.3.7\n, you should no longer need a dependency on@braintrust/core\n.\nPython SDK version 0.2.6\n- Python SDK now correctly nests spans logged from inside tool calls in OpenAI Agents\nTypeScript SDK version 0.3.6\n- OpenAI responses wrapper no longer filters out span data fields when logging\n- Fixed\nwithResponse\nandwrapOpenAI\ninteraction to not hide response data\nData plane (1.1.21)\n- Process pydantic-ai OTel spans\n- AI proxy now supports temperature > 1 for models which allow it\n- Preview of data retention on logs, datasets, and experiments\nWeek of 2025-08-18\n- Monitor page layout changed to be more responsive to screen size\n- Various UX improvements to prompt dialog\n- Improved onboarding experience\n- Trace timeline layout improvements\nData plane (1.1.20)\n- Brainstore vacuum is enabled by default. This will reclaim space from object storage. As a bonus, vacuum also cleans up more data (segment-level write-ahead logs)\n- AI proxy now dynamically fetches updates to the model registry\n- Performance improvements to summary,\nIS NOT NULL\n, and!= NULL\nqueries - Handle cancelled BTQL queries earlier and optimize schema inference queries\n- Added a REST API for managing service tokens. See docs\n- Support custom columns on the experiments page\n- Aggregate custom metrics and include more built-in agent metrics in experiments and logs\n- Preview of data retention on logs. You can define a per-project policy on logs which will be deleted on a schedule and no longer available in the UI and API\nWeek of 2025-08-18\nPython SDK version 0.2.5\n- Support data masking (see docs)\n- Remote evals in Python SDK\n- Support tags in Eval hooks\n- Validate attachment file readability at creation time\nTypeScript SDK version 0.2.5\n- Support data masking (see docs)\n- Support tags in Eval hooks\n- Validate attachment file readability at creation time\nSDK Integrations: Google ADK (Python) (version 0.1.1)\n- Added integration with Google Agent Development Kit (ADK).\nPython SDK version 0.2.4\n- Allow non-batch span processors in\nBraintrustSpanProcessor\n.\nWeek of 2025-08-11\n- Pro plan organizations can now downgrade to the Free plan via the settings page without contacting support\n- Prevent read-only users from downloading data from the UI\nPython SDK version 0.2.3\n- Fix openai-agents to inherit the right tracing context\nTypeScript SDK version 0.2.4\n- Support OpenAI Agents SDK\nSDK Integrations: OpenAI Agents (TS) (version 0.0.2)\n- Fix openai-agents to inherit the right tracing context\nData plane (1.1.19)\n- Add support for GPT-5 models\n- OTel tracing support for Google Agent Development Kit\n- OTel support for deleting fields\n- Fix binder error handling for malformed BTQL queries\n- Enable environment tags on prompt versions\nWeek of 2025-08-04\n- @mention team members in comments to notify them via email. To mention someone, type \"@\" and a team member's name or email in any comment input.\n- You can now assign users to rows in experiments, logs, and datasets. Once assigned, you can filter rows by a specific user or a group of users.\n- View configuration has been changed to no longer auto-save changes. It now shows a dirty state and you have the option of saving or resetting those changes back to the base view.\nPython SDK version 0.2.2\n- Added\nenvironment\nparameter toload_prompt\n- The Otel SpanProcessor now keeps\ntraceloop.*\nspans by default. - Experiments can now be run without sending results to the server.\n- Span creation is significantly faster in Python.\nTypeScript SDK version 0.2.3\n- Added\nenvironment\nparameter toload_prompt\n- The Otel SpanProcessor now keeps\ntraceloop.*\nspans by default. - Experiments can now be run without sending results to the server.\n- Fix\nnpx braintrust pull\nfor large prompts\nTypeScript SDK version 0.2.2\n- Fix ai-sdk tool call formatting in output\n- Log OpenAI Agents input and output to root span\n- Wrap OpenAI responses.parse\n- Add wrapTraced support for generator functions\nPython SDK version 0.2.1\n- Fix langchain-py integration tracing when users use a @traced method\n- Wrap OpenAI responses.parse\n- Add @traced support for generator functions\nWeek of 2025-07-28\n- New improved UI for trace tree.\n- Token and cost metrics are computed per sub-tree in the trace viewer.\n- Download BTQL sandbox results as JSON or CSV\nData plane (1.1.18)\nThis is our largest data plane release in a while, and it includes several significant performance improvements, bug fixes, and new features:\n- Improve performance for non-selective searches. Eg make\nfoo != 'bar'\nfaster. - Improve performance for score filters. Eg make\nscores.correctness = 0\nfaster. - Improve group by performance. This should make the monitor page and project summary page significantly faster.\n- Add syntax for explicit casting. You can now use explicit casting functions to cast data to any datatype. e.g.\nto_number(input.foo)\n,to_datetime(input.foo)\n, etc. - Fix ILIKE queries on nested json: ILIKE queries previously returned incorrect results on nested json objects. ILIKE now works as expected for all json objects.\n- Improve backfill performance. New objects should get picked up faster.\n- Improve compaction latency. Indexing should kick in much faster, and in particular, this means data gets indexed a lot faster.\n- Improved support for OTel mappings, including the new GenAI Agent conventions and strands framework.\n- Add Gemini 2.5 Flash-Lite GA, GPT-OSS models on several providers, and Claude Opus 4.1.\nWeek of 2025-07-21\n- Moved monitor chart legends to the bottom and increased chart heights.\n- Fixed a monitor chart issue where the series toggle selector would filter the incorrect series.\n- Improved monitor fullscreen experience: charts now open faster and retain their series filter state.\n- Loop is now available in the experiments page and has a new ability to render interactive components inside the chat that will help you find the UI element that Loop is referencing.\n- You can now use remote evals with the \"+Experiment\" button to create a new experiment. Previously, they were only available in the playground.\nTypeScript SDK version 0.2.1\n- Fix support for the\nopenai.chat.completions.parse\nmethod when used withwrapOpenAI\n. - Added support for ai-sdk@beta with new\nBraintrustMiddleware\n- Support running remote evals as full experiments.\nTypeScript SDK version 0.2.0\n- When running multiple trials per input (\ntrial_count > 1\n), you can now access the current trial index (0-based) viahooks.trialIndex\nin your task function. - Added\nBraintrustExporter\nin addition toBraintrustSpanProcessor\n. - Bound max ancestors in git to 1,000.\nPython SDK version 0.2.0\n- When running multiple trials per input (\ntrial_count > 1\n), you can now access the current trial index (0-based) viahooks.trial_index\nin your task function. - New LiteLLM\nwrap_litellm\nwrapper. - Increase max ancestors in git to 1,000.\nData plane (1.1.15)\n- Add ability to run scorers as tasks in the playground\n- You can now use object storage, instead of Redis, as a locks manager.\n- Support async python in inline code functions\n- Don't re-trigger online scoring on existing traces if only metadata fields like\ntags\nchange.\nWeek of 2025-07-14\n- Add monitor page UTC timezone toggle\n- Improved trace view loading performance for large traces.\nPython SDK version 0.1.8\n- Added\nBraintrustSpanProcessor\nto simplify Braintrust's integration with OpenTelemetry.\nTypeScript SDK version 0.1.1\n- Added\nBraintrustSpanProcessor\nto simplify integration with OpenTelemetry.\nData plane (1.1.14)\n- Switch the default query shape from\ntraces\ntospans\nin the API. This means that btql queries will now return 1 row per span, rather than per trace. This change also applies to the REST API. - Service tokens with scoped, user-independent credentials for system integrations.\n- Fix a bug where very large experiments (run through the API) would drop spans if they could not flush data fast enough.\n- Support built-in OTel metrics (contact your account team for more details)\n- New parallel backfiller improves performance of loading data into Brainstore across many projects.\nPython SDK version 0.1.7\n- Added support for loading prompts by ID via the\nload_prompt\nfunction. You can now load prompts directly by their unique identifier:\nTypeScript SDK version 0.1.0\n- Fix a bug where large experiments would drop spans if they could not flush data fast enough.\n- Fix bug in attachment uploading in evals executed with\nnpx braintrust eval\n. - Upgrading zod dependency from\n^3.22.4\nto^3.25.3\n- Added support for loading prompts by ID via the\nloadPrompt\nfunction. You can now load prompts directly by their unique identifier:\nWeek of 2025-07-07\n- Loop can now create custom code scorers in playgrounds\n- Schema builder UI for structured outputs\n- Sort datasets when the\nFaster tables\nfeature flag is enabled - Change LLM duration to be the sum, not average, of LLM duration across spans\n- Add support for Grok 4 and Mistral's Devstral Small Latest\nData plane (1.1.13)\n- Fix support for\nCOALESCE\nwith variadic arguments - Add option to select logs for online scoring with a BTQL filter\n- Add ability to test online scoring configuration on existing logs\n- Mmap based indexing optimization enabled by default for Brainstore\nData plane (1.1.12) [skipped]\nWeek of 2025-06-30\n- Time range filters on the logs page\nData plane (1.1.11)\n- Add support for LLaMa 4 Scout for Cerebras\n- Turn on index validation (which enables self-healing failed compactions) in the Cloudformation by default.\nWeek of 2025-06-23\n- Add support for multi-factor authentication\n- Fix a bug with Vertex AI calls when the request includes the anthropic-beta header\n- Add Zapier integration to trigger Zaps when there's a new automation event or a new project.\nData plane (1.1.7)\n- Improve performance of error count queries in Brainstore\n- Automatically heal segments that fail to compact\n- Add support for new models including o3 pro\n- Improve error messages for LLM-originated errors in the proxy\nAutoevals.js v0.0.130\n- Remove dependency on\n@braintrust/core\nTypeScript SDK version 0.0.209\n- Ensure SpanComponentsV3 encoding works in the browser.\nTypeScript SDK version 0.0.208\n- Ensure running remote evals (i.e.\nrunDevServer\n) works without the CLI wrapper. - Add span + parent ids to\nStartSpanArgs\nWeek of 2025-06-16\n- Add OpenAI's o3-pro model to the playground and AI proxy.\n- View parameters are now present in the url when viewing a default view\n- Experiments charting controls have been added into views\n- Experiment objects now support tags through the API and on the experiments view\n- Add support for Gemini 2.5 Pro, Gemini 2.5 Flash, and Gemini 2.5 Flash Lite\nPython SDK version 0.1.5\n- The SDK's under-the-hood log queue will not block when full\nand has a default size of 25000 logs. You can configure the max size by setting\nBRAINTRUST_LOG_QUEUE_MAX_SIZE\nin your environment. The environment variableBRAINTRUST_QUEUE_DROP_WHEN_FULL\nis no longer used. - Improvements to the logging of parallel tool calls.\n- Attachments are now converted to base64 data URLs, making it easier to work with image attachments in prompts.\nTypeScript SDK version 0.0.207\n- The SDK's under-the-hood queue for sending logs now has a default size of 5000 logs.\nYou can configure the max size by setting\nBRAINTRUST_LOG_QUEUE_MAX_SIZE\nin your environment. - Improvements to the logging of parallel tool calls.\n- Attachments are now converted to base64 data URLs, making it easier to work with image attachments in prompts.\nData plane (1.1.6)\n- Patch a bug in 1.1.5 related to the\nrealtime_state\nfield in the API response.\nData plane (1.1.5)\n- Default query timeout in Brainstore is now 32 seconds.\n- Auto-recompact segments which have been rendered unusable due to an S3-related issue.\n- Gemini 2.5 models\nData plane (1.1.4)\n- Optimize \"Activity\" (audit log) queries, which reduces the query workload on Postgres for large traces (even if you are using Brainstore).\n- Automatically convert base64 payloads to attachments in the data plane. This\nreduces the amount of data that needs to be stored in the data plane and\nimproves page load times. You can disable this by setting\nDISABLE_ATTACHMENT_OPTIMIZATION=true\norDisableAttachmentOptimization=true\nin your stack. - Improve AI proxy errors for status codes 401->409\n- Increase real-time query memory limit to 10GB in Brainstore\nWeek of 2025-06-09\n- Correctly propagate\nexpected\nandmetadata\nvalues to function calls when runninginvoke\n. This means that if you provideexpected\normetadata\n,input\nrefers to the top-level input argument. If you are passing in a value like{input: \"a\"}\n, then you must now use{{input.input}}\nto refer to the string \"a\", if you pass inexpected\normetadata\n. This should have no effect on the playground or scorers. - Chat-like thread layout that simplifies thread display to LLM and score data\n- Enable all agent nodes to access dataset variables with the mustache variable\n{{dataset}}\n. For example, to accessmetadata.foo\nin the third prompt in an agent, you can use{{dataset.metadata.foo}}\n. - Improve reliability of online scoring when logging high volumes of data to a project.\n- Tags can now be sorted in the project configuration page which will change their display order in other parts of the UI.\n- System-only messages are now supported in Anthropic and Bedrock models.\n- Logs page UI can now filter nested data fields in\nmetadata\n,input\n,output\n, andexpected\n.\nPython SDK version 0.1.4\n- Add\nproject.publish()\nto directlypush\nprompts to Braintrust (without runningbraintrust push\n). @traced\nnow works correctly with async generator functions.- The OpenAI and Anthropic wrappers set\nprovider\nmetadata.\nTypeScript SDK version 0.0.206\n- Add support for\nproject.publish()\nto directlypush\nprompts to Braintrust (without runningbraintrust push\n). - The OpenAI and Anthropic wrappers set\nprovider\nmetadata.\nWeek of 2025-06-02\n- Support reasoning params and reasoning tokens in streaming and non-streaming responses in the AI proxy and across the product (requires a stack update to 0.0.74).\n- New braintrust-proxy Python library to help developers integrate with their IDEs to support new reasoning input and output types.\n- New\n@braintrust/proxy/types\nmodule to augment OpenAI libraries with reasoning input and output types. - New streaming protocol between Brainstore and the API server speeds up queries.\n- Time brushing interaction enabled on Monitor page charts.\n- Can create user-defined views in the monitoring page.\n- Live updating time mode added to the monitoring page.\n- The\nanthropic\npackage is now included by default in Python functions. - Audit log queries must now specify an\nid\nfilter for the set of rows to fetch. These queries will only return the audit log for the specified rows, rather than the whole trace. - (Beta) continuously export logs, experiments, and datasets to S3.\n- Enable passing\nmetadata\nandexpected\nas arguments to the first agent prompt node.\nPython SDK version 0.1.3\n- Improve retry logic in the control plane connection (used to create new experiments and datasets).\nWeek of 2025-05-26\n- The \"Faster tables\" flag is now the default (you may need to update your data plane if you are self-hosted). You should notice experiments, datasets, and the logs page load much faster.\n- Add Claude 4 models in Bedrock and Vertex to the AI proxy and playground.\n- Braintrust now incorporates cached tokens into the cost calculations for experiments and logs. The monitor page also now includes separate lines so you can track costs and counts for uncached, cached, and cache creation tokens.\n- Native support for thinking parameters in the playground.\nPython SDK version 0.1.2\n- Added support for\nmetadata\nandtags\narguments toinvoke\n. - The SDK now gracefully handles OpenAI's\nNotGiven\nparameter. - Added\nspan.link()\nto synchronously generate permalinks.\nTypeScript SDK version 0.0.206 [upcoming]\n- Add support for\nmetadata\nandtags\narguments toinvoke\n.\nWeek of 2025-05-19\n- Improved playground prompt editor stability and performance.\n- Capture cached tokens from OpenAI and Anthropic models in a unified format and surface them in the UI.\n- Create experiments from the experiments list page using saved prompts/agents.\n- New BTQL sandbox page and editor with autocomplete\n- Fullscreen-able monitor charts\n- Added a 'Copy page' button to the top of every docs page.\n- Brainstore now supports vacuuming data from object storage to reclaim space. If you are self-hosted, please reach out to Braintrust support to learn more about the feature.\n- Organization owners can manage API keys for all users in their organization in the UI.\n- Add endpoint for admins to list all ACLs within an org.\nTypeScript SDK version 0.0.205\n- Make the\n_xact_id\nfield inorigin\noptional. - Added\nspan.link()\nas a synchronous means of generating permalinks.\nPython SDK version 0.1.1\n- Update cached token accounting in\nwrap_anthropic\nto correctly capture cached tokens. - Pull additional metadata in\nbraintrust pull\nfor prompts and functions to improve tracing.\nTypeScript SDK version 0.0.204\n- Update cached token accounting in\nwrapAnthropic\nto correctly capture cached tokens.\nWeek of 2025-05-12\n- Collapsible sidebar navigation\n- Command bar (CMD/CTRL+K) to quickly navigate and between pages and projects\n- View monitor page logs across all projects in an organization\nSDK (version 0.1.0)\n- Allow custom model descriptions in Braintrust.\n- Improve support for PDF attachments to multimodal OpenAI models.\nThe Python library no longer has a dependency on braintrust_core\n.\nbraintrust_core\nwill be deprecated in the near future, but the package will\nremain on PyPI. If you wrote code that directly imports from braintrust_core\n, you can\neither:\n- Change your imports to\nfrom braintrust.score import Score, Scorer\n(preferred) - or, add\nbraintrust_core\nto your project's dependencies.\nThe TypeScript SDK also includes a small packaging bugfix.\nSDK (version 0.0.203)\n- Add new reasoning to OpenAI messages\nWeek of 2025-05-05\n- Added Mistral Medium 3 and Gemini 2.5 Pro Preview to the AI proxy and playground.\n- Self-hosted builds now log in a structured JSON format that is easier to parse.\nSDK (version 0.0.202)\n- Gracefully handle experiment summarization failures in Eval()\n- Fix a bug where\nwrap_openai\nwas breakingpydantic_ai run_stream\nfunc. - Add tracing to the\nclient.beta.messages\ncalls in the TypeScript Anthropic library. - Fix some deprecation warnings in the Python SDK.\nWeek of 2025-04-28\n- Permission groups settings page now allows admins to set group-level permissions (i.e. which users can read, delete, and add/remove members from a particular group)\n- Automations alpha: trigger webhooks based on log events\nWeek of 2025-04-21\n- Preview attachments in playground input cells.\n- Playground now support list mode which includes score and metric summaries.\n- Handle structured outputs from OpenAI's responses API in the \"Try prompt\" experience.\nSDK (version 0.0.201)\n- Support OpenAI\nclient.beta.chat.completions.parse\nin the Python wrapper.\nSDK (version 0.0.200)\n- Ensure the prompt cache properly handles any manner of prompt names.\n- Ensure the output of\nanthropic.messages.create\nis properly traced when called withstream=True\nin an async program.\nWeek of 2025-04-14\n- Allow users to remove themselves from any organization they are part of using\nthe\n/v1/organization/members\nREST endpoint. - Group monitor page charts by metadata path.\n- Download playground contents as CSV.\n- Add pending and streaming state indicators to playground cells.\n- Distinguish per-row and global playground progress.\n- Added GPT-4.1, o4-mini and o3 to the AI proxy and playground.\n- On the monitor page, add aggregate values to chart legends.\n- Add Gemini 2.5 Flash Preview model to the AI proxy and playground.\n- Add support for audio and video inputs for Gemini models in the AI proxy and playground.\n- Add support for PDF files for OpenAI models.\n- Native tracing support in the proxy has finally arrived! Read more in the docs\n- Upload attachments directly in the UI in datasets, playgrounds, and prompts (requires a stack update to 0.0.67).\nSDK (version 0.0.199)\n- Fix a bug that broke async calls to the Python version of\nanthropic.messages.create\n. - Store detailed metrics from OpenAI's\nchat.completion\nTypeScript API.\nSDK (version 0.0.198)\n- Trace the\nopenai.responses\nendpoint in the Typescript SDK. - Store the\ntoken_details\nmetrics return by theopenai/responses\nAPI.\nWeek of 2025-04-07\n- Playground option to append messages from a dataset to the end of a prompt\n- A new toggle that lets you skip tracing scoring info for online scoring. This is useful when you are scoring old logs and don't want to hurt search performance as a result.\n- GIF and image support in comments\n- Add embedded view and download action for inline attachments of supported file types\nAPI (version 0.0.65)\n- Improve error messages when trying to insert invalid unicode\nSDK (version 0.0.197)\n- Fix a bug in\ninit_function\nin the Python SDK which prevented theinput\nargument from being passed to the function correctly when it was used as a scorer. - Support setting\ndescription\nandsummarizeScores\n/summarize_scores\ninEval(...)\n.\nAPI (version 0.0.65)\n- Backend support for appending messages.\nWeek of 2025-03-31\n- Many improvements to the playground experience:\n- Fixed many crashes and infinite loading spinner states\n- Improved performance across large datasets\n- Better support for running single rows for the first time\n- Fixed re-ordering prompts\n- Fixed adding and removing dataset rows\n- You can now re-run specific prompts for individual cells and columns\n- You can now do \"does not contain\" filters for tags in experiments and datasets. Coming soon to logs!\n- When you\ninvoke()\na function, inline base64 payloads will be automatically logged as attachments. - Add a strict mode to evals and functions which allows you to fail test cases when a variable is not present in a prompt. Without strict mode, prompts will always render (and sometimes miss variables). With strict mode on, these variables show clearly as errors in the playground and experiments.\n- Add Fireworks' DeepSeek V3 03-24 and DeepSeek R1 (Basic), along with Qwen QwQ 32B in Fireworks and Together.ai, to the playground and AI proxy.\n- Fix bug that prevented Databricks custom provider form from being submitted without toggling authentication types.\n- Unify Vertex AI, Azure, and Databricks custom provider authentication inputs.\n- Add Llama 4 Maverick and Llama 4 Scout models to Together.ai, Fireworks, and Groq providers in the playground and AI proxy.\n- Add Mistral Saba and Qwen QwQ 32B models to the Groq provider in the playground and AI proxy.\n- Add Gemini 2.5 Pro Experimental and Gemini 2.0 Flash Thinking Mode models to the Vertex provider in the playground and AI proxy.\nAPI (version 0.0.64)\n- Brainstore is now set as the default storage option\n- Improved backfilling performance and overall database load\n- Enabled relaxed search mode for ClickHouse to improve query flexibility\n- Added strict mode option to prompts that fails when required template arguments are missing\n- Enhanced error reporting for missing functions and eval failures\n- Fixed streaming errors that previously resulted in missing cells instead of visible error states\n- Abort evaluations on server when stopped from playground\n- Added support for external bucket attachments\n- Improved handling of large base64 images by converting them to attachments\n- Fixed proper handling of UTF-8 characters in attachment filenames\n- Added the ability to set telemetry URL through admin settings\nSDK (version 0.0.196)\n- Adding Anthropic tracing for our TypeScript SDK. See\nbraintrust.wrapAnthropic\n. - The SDK now paginates datasets and experiments, which should improve performance for large datasets and experiments.\n- Add\nstrict\nflag toinvoke\nwhich implements the strict mode described above. - Raise if a Python tool is pushed without without defined parameters, instead of silently not showing the tool in the UI.\n- Fix Python OpenAI wrapper to work for older versions of the OpenAI library without\nresponses\n. - Set time_to_first_token correctly from AI SDK wrapper\nWeek of 2025-03-24\n- Add OpenAI's o1-pro model to the playground and AI proxy.\n- Support OpenAI Responses API in the AI proxy.\n- Add support for the Gemini 2.5 Pro Experimental model in the playground and AI proxy.\n- Option to disable the experiment comparison auto-select behavior\n- Add support for Databricks custom provider as a default cloud provider in the playground and AI proxy.\n- Allow supplying a base API URL for Mistral custom providers in the playground and AI proxy.\n- Support pushed code bundles larger than 50MB.\nSDK (version 0.0.195)\n- Improve the metadata collected by the Anthropic client.\n- Anthropic client can now be run with\nbraintrust.wrap_anthropic\n- Fix a bug when\nmessages.create\nwas called withstream=True\nSDK (version 0.0.194)\n- Add Anthropic tracing to the Python SDK with\nwrap_anthropic_client\n- Fix a bug calling\nbraintrust.permalink\nwithNoopSpan\nSDK (version 0.0.193)\n- Fix retry bug when downloading large datasets/experiments from the SDK\n- Background logger will load environment variables upon first use rather than when module is imported.\nWeek of 2025-03-17\n- The OTEL endpoint now understands structured output calls from the Vercel AI\nSDK. Logging via\ngenerateObject\nandstreamObject\nwill populate the schema in Braintrust, allowing the full prompt to be run. - Added support for\nconcat\n,lower\n, andupper\nstring functions in BTQL. - Correctly propagate Bedrock streaming errors through the AI proxy and playground.\n- Online scoring supports sampling rates with decimal precision.\nSDK (version 0.0.192)\n- Improve default retry handler in the python SDK to cover more network-related exceptions.\nAutoevals (version 0.0.124)\n- Added\ninit\nto set a global default client for all evaluators (Python and Node.js). - Added\nclient\nargument to all evaluators to specify the client to use. - Improved the Autoevals docs with more examples, and Python reference docs now include moderation, ragas, and other evaluators that were missing from the initial release.\nWeek of 2025-03-10\n- Added support for OpenAI GPT-4o Search Preview and GPT-4o mini Search Preview in the playground and AI proxy.\n- Add support for making Anthropic and Google-format requests to corresponding models in the AI proxy.\n- Fix bug in model provider key modal that prevents submitting a Vertex provider with an empty base URL.\n- Add column menu in grid layout with sort and visibility options.\n- Enable logging the\norigin\nfield through the REST API\nAutoevals (version 0.0.123)\n- Swapped\npolyleven\nforlevenshtein\nfor faster string matching.\nSDK Integrations: LangChain (Python) (version 0.0.2)\n- Add a new\nbraintrust-langchain\nintegration with an improvedBraintrustCallbackHandler\nandset_global_handler\nto set the handler globally for all LangChain components.\nSDK Integrations: LangChain.js (version 0.0.6)\n- Small improvement to avoid logging unhelpful LangGraph spans.\n- Updated peer dependencies with LangChain core that fixes the global handler for LangGraph runs.\nSDK Integrations: Val Town\n- New\nval.town\nintegration with example vals to quickly get started with Braintrust.\nSDK (version 0.0.190)\n- Fix\nprompt pull\nfor long prompts. - Fix a bug in the Python SDK which would not retry requests that were severed after a connection timeout.\nSDK (version 0.0.189)\n- Added integration with OpenAI Agents SDK.\nSDK (version 0.0.188)\n- Deprecated\nbraintrust.wrapper.langchain\nin favor of the newbraintrust-langchain\npackage.\nWeek of 2025-03-03\n- Add support for \"image\" pdfs in the AI proxy. See the proxy docs for more details.\n- Fix issue in which code function executions could hang indefinitely.\n- Add support for custom base URLs for Vertex AI providers.\n- Add dataset column to experiments table.\n- Add python3.13 support to user-defined functions.\n- Fix bug that prevented calling Python functions from the new unified playground.\nSDK (version 0.0.187)\n- Always bundle default python packages when pushing code with\nbraintrust push\n. - Fix bug in the TypeScript SDK where\nasyncFlush\nwas not correctly defaulted to false. - Fix a bug where\nspan_attributes\nfailed to propagate to child spans through propagated events.\nWeek of 2025-02-24\n- Add support for removing all permissions for a group/user on an object with a single click.\n- Add support for Claude 3.7 Sonnet model.\n- Add llms.txt for docs content.\n- Enable spellcheck for prompt message editors.\n- Add support for Anthropic Claude models in Vertex AI.\n- Add support for Claude 3.7 Sonnet in Bedrock and Vertex AI.\n- Add support for Perplexity R1 1776, Mistral Saba, Gemini LearnLM, and more Groq models.\n- Support system instructions in Gemini models.\n- Add support for Gemini 2.0 Flash-Lite, and remove preview model, which no longer serves requests.\n- Add support for default Bedrock cross-region inference profiles in the playground and AI proxy.\n- Move score distribution charts to the experiment sidebar.\n- Add support for OpenAI GPT-4.5 model in the playground and AI proxy.\n- Add deprecation warning for\n_parent_id\nfield in the REST API (docs). This field will be removed in a future release.\nAPI (version 0.0.63)\n- Support for Claude 3.7 Sonnet, Gemini 2.0 Flash-Lite, and several other models in the proxy.\n- Stability and performance improvements for ETL processes.\n- A new\n/status\nendpoint to check the health of Braintrust services.\nSDK (version 0.0.187)\n- Added support for handling score values when an Eval has errored.\nWeek of 2025-02-17\n- Add support for stop sequences in Anthropic, Bedrock, and Google models.\n- Resolve JSON Schema references when translating structured outputs to Gemini format.\n- Add button to copy table cell contents to clipboard.\n- Add support for basic Cache-Control headers in the AI proxy.\n- Add support for selecting all or none in the categories of permission dialogs.\n- Respect Bedrock providers not supporting streaming in the AI proxy.\nSDK (version 0.0.187)\n- Improve support for binary packages in\nnpx braintrust eval\n. - Support templated structured outputs.\n- Fix dataset summary types in Typescript.\nWeek of 2025-02-10\n- Store table grouping, row height, and layout options in the view configuration.\n- Add the ability to set a default table view.\n- Add support for Google Cloud Vertex AI in the playground and proxy. Google Cloud auth is supported for principals and service accounts via either OAuth 2.0 token or service account key.\n- Add default cloud providers section to the organization AI providers page.\n- Support streaming responses from OpenAI o1 models in the playground and AI proxy.\nWeek of 2025-02-03\n- Add complete support for Bedrock models in the playground and AI proxy; this includes support for system prompts, tool calls, and multimodal inputs.\n- Fix model provider configuration issues in which custom models could clobber default models, and different providers of the same type could clobber each other.\n- Fix bug in streaming JSON responses from non-OpenAI providers.\n- Supported templated structured outputs in experiments run from the playground.\n- Support structured outputs in the playground and AI proxy for Anthropic models, Bedrock models, and any OpenAI-flavored models that support tool calls, e.g. LLaMa on Together.ai.\n- Support templated custom headers for custom AI providers. See the proxy docs for more details.\n- Added and updated models across all providers in the playground and AI proxy.\n- Support tool usage and structured outputs for Gemini models in the playground and AI proxy.\n- Simplify playground model dropdown by showing model variations in a nested dropdown.\nWeek of 2025-01-27\n- Add support for duplicating prompts, scorers, and tools.\n- Fix pagination for the\n/v1/prompt\nREST API endpoint. - \"Unreviewed\" default view on experiment and logs tables to filter out rows that have been human reviewed.\n- Add o3-mini to the AI proxy and playground.\n- Scorer dropdown now supports using custom scoring functions across projects.\nSDK Integrations: LangChain.js (version 0.0.5)\n- Less noisy logging from the LangChain.js integration.\n- You can now pass a\nNOOP_SPAN\nto theBraintrustCallbackHandler\nto disable logging. - Fixes a bug where the LangChain.js integration could not handle null/undefined values in chain inputs/outputs.\nSDK (version 0.0.184)\nspan.export()\nwill no longer throw if braintrust is down- Improvement to the Python prompt rendering to correctly render formatted messages, LLM tool calls, and other structured outputs.\nWeek of 2025-01-20\n- Drag and drop to reorder span fields in experiment/log traces and dataset rows. On wider screens, fields can also be arranged side-by-side.\n- Small convenience improvement to the BTQL Sandbox to avoid having to add include\nfilter:\nto an advanced filter clause. - Add an attachments browser to view all attachments for a span in a sidebar. To open the attachments browser, expand the trace and click the arrow icon in the attachments section. It will only be visible when the trace panel is wide enough.\nSDK (version 0.0.183)\n- Fix a bug related to\ninitDataset()\nin the Typescript SDK creating links inEval()\ncalls. - Fix a few type checking issues in the Python SDK.\nWeek of 2025-01-13\n- Add support for setting a baseline experiment for experiment comparisons. If a baseline experiment is set, it will be chosen by default as the comparison when clicking on an experiment.\n- UI updates to experiment and log tables.\n- Trace audit log now displays granular changes to span data.\n- Start/end columns shown as dates/times.\n- Non-existent trace records display an error message instead of loading indefinitely.\nSDK Integrations: LangChain.js (version 0.0.4)\n- Support logging spans from inside evals in the LangChain.js integration.\nSDK (version 0.0.182)\n- Improved logging for moderation models from the SDK wrappers.\nWeek of 2025-01-06\n- Creating an experiment from a playground now correctly renders prompts with\ninput\n,metadata\n,expected\n, andoutput\nmapped fields. - Fixes small bug where\ninput.output\ndata could pollute the dataset'soutput\nwhen rendering the prompts. - The AI proxy now includes\nx-bt-used-endpoint\nas a response header. It specifies which of your configured AI providers was used to complete the request. - Add support for deeplinking to comments within spans, allowing users to easily copy and share links to comments.\n- In Human Review mode, display all scores in a form.\n- Experiment table rows can now be sorted based on score changes and regressions for each group, relative to a selected comparison experiment.\n- The OTEL endpoint now converts attributes under the\nbraintrust\nnamespace directly to the corresponding Braintrust fields. For example,braintrust.input\nwill appear asinput\nin Braintrust. See the tracing guide for more details. - New OTEL attributes that accept JSON-serialized values have been added for convenience:\ngen_ai.prompt_json\ngen_ai.completion_json\nbraintrust.input_json\nbraintrust.output_json\nFor more details, see the tracing guide.\n- Experiment tables and individual traces now support comparing trial data between experiments.\nSDK (version 0.0.181)\n- Add\nReadonlyAttachment.metadata\nhelper method to fetch a signed URL for downloading the attachment metadata.\nSDK (version 0.0.179)\n- New\nhook.expected\nfor reading and updating expected values in the Eval framework. - Small type improvements for\nhook\nobjects. - Fixed a bug to enable support for\ninit_function\nwith LLM scorers in Python. - Support nested attachments in Python.\nWeek of 2024-12-30\n- Add support for free-form human review scores (written to the\nmetadata\nfield).\nSDK (version 0.0.179) (unreleased)\n- Add support for imports in Python functions pushed to Braintrust via\nbraintrust push\n.\nSDK (version 0.0.178)\n- Cache prompts locally in a two-layered memory/disk cache, and attempt to use this cache if the prompt cannot be fetched from the Braintrust server.\n- Support for using custom functions that are stored in Braintrust in evals. See the docs for more details.\n- Add support for running traced functions in a\nThreadPoolExecutor\nin the Python SDK. See the customize traces guide for more information. - Improved formatting of spans logged from the Vercel AI SDK's\ngenerateObject\nmethod. The logged output now matches the format of OpenAI's structured outputs. - Default to\nasyncFlush: true\nin the TypeScript SDK. This is usually safe since Vercel and Cloudflare both havewaitUntil\n, and async flushes mean that clients will not be blocked if Braintrust is down.\nSDK integrations: LangChain.js (version 0.0.2)\n- Add support for initializing global LangChain callback handler to avoid manually passing the handler to each LangChain object.\nWeek of 2024-12-16\nAPI (version 0.0.61)\n- Upgraded to Node.js 22 in Docker containers.\nSDK (version 0.0.177)\n- Support for creating and pushing custom scorers from your codebase\nwith\nbraintrust push\n. Read the guides to scorers for more information.\nWeek of 2024-12-09\n- Add support for structured outputs in the playground.\n- Sparkline charts added to the project home page.\n- Better handling of missing data points in monitor charts.\n- Clicking on monitor charts now opens a link to traces filtered to the selected time range.\n- Add\nEndpoint supports streaming\nflag to custom provider configuration. The AI proxy will convert non-streaming endpoints to streaming format, allowing the provider's models to be used in the playground. - Experiments chart can be resized vertically by dragging the bottom of the chart.\n- BTQL sandbox to explore project data using Braintrust Query Language.\n- Add support for updating span data from custom span iframes.\nAutoevals (version 0.0.110)\n- Python Autoevals now support custom clients when calling evaluators. See docs for more details.\nSDK (version 0.0.176)\n- New\nhook.metadata\nfor reading and updating Eval metadata when using theEval\nframework. Previoushook.meta\nis now deprecated.\nSDK integrations: LangChain.js (version 0.0.1)\n- New LangChain.js integration to export traces from\nlangchainjs\nruns.\nSDK integrations: LangChain.js (version 0.0.1)\n- New LangChain.js integration to export traces from\nlangchainjs\nruns.\nWeek of 2024-12-02\n- Significantly speed up loading performance for experiments and logs, especially with lots of spans.\nThis speed up comes with a few changes in behavior:\n- Searches inside experiments will only work over content in the tabular view, rather than over the full trace.\n- While searching on the logs page, realtime updates are disabled.\n- Starring rows in experiment and dataset tables now supported.\n- \"Order by regression\" option in experiment column menu can now be toggled on and off without losing previous order.\n- Add expanded timeline view for traces.\n- Added a 'Request count' chart to the monitor page.\n- Add headers to custom provider configuration which the AI proxy will include in the request to the custom endpoint.\n- The logs viewer now supports exporting the currently loaded rows as a CSV or JSON file.\nAPI (version 0.0.60)\n- Make PG_URL configuration more uniform between nodeJS and python clients.\nSDK (version 0.0.175)\n- Fix bug with serializing ReadonlyAttachment in logs\nWeek of 2024-11-25\n- Experiment columns can now be reordered from the column menu.\n- You can now customize legends in monitor charts. Select a legend item to highlight its data, Shift (\u21e7) + Click to select multiple items, or Command (\u2318) / Ctrl (\u2303) + Click to deselect.\nSDK (version 0.0.174)\n- AI SDK fixes: support for image URLs and properly formatted tool calls so \"Try prompt\" works in the UI.\nSDK (version 0.0.173)\n- Attachments can now be loaded when iterating an experiment or dataset.\nSDK (version 0.0.172)\n- Fix a bug where\nbraintrust eval\ndid not respect certain configuration options, likebase_experiment_id\n. - Fix a bug where\ninvoke\nin the Python SDK did not properly stream responses.\nWeek of 2024-11-18\n- The Traceloop OTEL integration now uses the input and output attributes to populate the corresponding fields in Braintrust.\n- The monitor page now supports querying experiment metrics.\n- Removed the\nfilters\nparam from the REST API fetch endpoint. For complex queries, we recommend using the/btql\nendpoint (docs). - New experiment summary layout option, a url-friendly view for experiment summaries that respects all filters.\n- Add a default limit of 10 to all fetch and\n/btql\nrequests for project_logs. - You can now export your prompts from the playground as code snippets and run them through the AI proxy.\n- Add a fallback for the \"add prompt\" dropdown button in the playground, which will search for prompts within the current project if the cross-org prompts query fails.\nSDK (version 0.0.171)\n- Add a\n.data\nmethod to theAttachment\nclass, which lets you inspect the loaded attachment data.\nWeek of 2024-11-12\n- Support for creating and pushing custom Python tools and prompts from your codebase with\nbraintrust push\n. Read the guides to tools and prompts for more information. - You can now view grouped summary data for all experiments by selecting Include comparisons in group from the Group by dropdown inside an experiment.\n- The experiments page now supports downloading as CSV/JSON.\n- Downloading or duplicating a dataset in the UI now properly copies all dataset rows.\n- You can now view a score data as a bar chart for your experiments data by selecting Score comparison from the X axis selector.\n- Trials information is now shown as a separate column in diff mode in the experiment table.\n- Cmd/Ctrl + S hotkey to save from prompts in the playground and function dialogs.\nSDK (version 0.0.170)\n- Support uploading file attachments in the Python SDK.\n- Log, feedback, and dataset inputs to the Python SDK are now synchronously deep-copied for more consistent logging.\nSDK (version 0.0.169)\n- The Python SDK\nEval()\nfunction has been split intoEval()\nandEvalAsync()\nto make it clear which one should be called in an asynchronous context. The behavior ofEval()\nremains unchanged. However,Eval()\ncallers running in an asynchronous context are strongly recommended to switch toEvalAsync()\nto improve type safety. - Improved type annotations in the Python SDK.\nSDK (version 0.0.168)\n- A new\nSpan.permalink()\nmethod allows you to format a permalink for the current span. See TypeScript docs or Python docs for details. braintrust push\nsupport for Python tools and prompts.\nWeek of 2024-11-04\n- The Braintrust AI Proxy now supports the OpenAI Realtime API, providing observability for voice-to-voice model sessions and simplifying backend infrastructure.\n- Add \"Group by\" functionality to the monitor page.\n- The experiment table can now be visualized in a grid layout, where each column represents an experiment to compare long-form outputs side-by-side.\n- 'Select all' button in permission dialogs\n- Create custom columns on dataset, experiment and logs tables from\nJSON\nvalues ininput\n,output\n,expected\n, ormetadata\nfields.\nAPI (version 0.0.59)\n- Fix permissions bug with updating org-scoped env vars\nWeek of 2024-10-28\n- The Braintrust AI Proxy can now issue temporary credentials to access the proxy for a limited time. This can be used to make AI requests directly from frontends and mobile apps, minimizing latency without exposing your API keys.\n- Move experiment score summaries to the table column headers. To view improvements and regressions per metadata or input group, first group the table by the relevant field. Sooo much room for [table] activities!\n- You now receive a clear error message if you run out of free tier capacity while running an experiment from the playground.\n- Filters on JSON fields now support array indexing, e.g.\nmetadata.foo[0] = 'bar'\n. See docs.\nSDK (version 0.0.168)\ninitDataset()\n/init_dataset()\nused inEval()\nnow tracks the dataset ID and links to each row in the dataset properly.\nWeek of 2024-10-21\n- Preview file attachments in the trace view.\n- View and filter by comments in the experiment table.\n- Add table row numbers to experiments, logs, and datasets.\nSDK (version 0.0.167)\n- Support uploading file attachments in the TypeScript SDK.\n- Log, feedback, and dataset inputs to the TypeScript SDK are now synchronously deep-copied for more consistent logging.\n- Address an issue where the TypeScript SDK could not make connections when running in a Cloudflare Worker.\nAPI (version 0.0.59)\n- Support uploading file attachments.\n- You can now export OpenTelemetry (OTel) traces to Braintrust. See the tracing guide for more details.\nWeek of 2024-10-14\n- The Monitor page now shows an aggregate view of log scores over time.\n- Improvement/Regression filters between experiments are now saved to the URL.\n- Add\nmax_concurrency\nandtrial_count\nto the playground when kicking off evals.max_concurrency\nis useful to avoid hitting LLM rate limits, andtrial_count\nis useful for evaluating applications that have non-deterministic behavior. - Show a button to scroll to a single search result in a span field when using trace search.\n- Indicate spans with errors in the trace span list.\nSDK (version 0.0.166)\n- Allow explicitly specifying git metadata info in the Eval framework.\nSDK (version 0.0.165)\n- Support specifying dataset-level metadata in\ninitDataset/init_dataset\n.\nSDK (version 0.0.164)\n- Add\nbraintrust.permalink\nfunction to create deep links pointing to particular spans in the Braintrust UI.\nWeek of 2024-10-07\n- After using \"Copy to Dataset\" to create a new dataset row, the audit log of the new row now links back to the original experiment, log, or other dataset.\n- Tools now stream their\nstdout\nandstderr\nto the UI. This is helpful for debugging. - Fix prompt, scorer, and tool dropdowns to only show the correct function types.\nSDK (version 0.0.163)\n- Fix Python SDK compatibility with Python 3.8.\nSDK (version 0.0.162)\n- Fix Python SDK compatibility with Python 3.9 and older.\nSDK (version 0.0.161)\n- Add utility function\nspanComponentsToObjectId\nfor resolving the object ID from an exported span slug.\nWeek of 2024-09-30\n- The Github action now supports Python runtimes.\n- Add support for Cerebras models in the proxy, playground, and saved prompts.\n- You can now create span iframe viewers to visualize span data in a custom iframe. In this example, the \"Table\" section is a custom span iframe.\nNOT LIKE\n,NOT ILIKE\n,NOT INCLUDES\n, andNOT CONTAINS\nsupported in BTQL.- Add \"Upload Rows\" button to insert rows into an existing dataset from CSV or JSON.\n- Add \"Maximum\" aggregate score type.\n- The experiment table now supports grouping by input (for trials) or by a metadata field.\n- The Name and Input columns are now pinned\n- Gemini models now support multimodal inputs.\nWeek of 2024-09-23\n- Basic monitor page that shows aggregate values for latency, token count, time to first token, and cost for logs.\n- Create custom tools to use in your prompts and in the playground. See the docs for more details.\n- Set org-wide environment variables to use in these tools\n- Pull your prompts to your codebase using the\nbraintrust pull\ncommand. - Select and compare multiple experiments in the experiment view using the\ncompared with\ndropdown. - The playground now displays aggregate scores (avg/max/min) for each prompt and supports sorting rows by a score.\n- Compare span field values side-by-side in the trace viewer when fullscreen and diff mode is enabled.\nSDK (version 0.0.160)\n- Fix a bug with\nsetFetch()\nin the TypeScript SDK.\nSDK (version 0.0.159)\n- In Python, running the CLI with\n--verbose\nnow uses theINFO\nlog level, while still printing full stack traces. Pass the flag twice (-vv\n) to use theDEBUG\nlog level. - Create and push custom tools from your codebase with\nbraintrust push\n. See docs for more details. TypeScript only for now. - A long awaited feature: you can now pull prompts to your codebase using the\nbraintrust pull\ncommand. TypeScript only for now.\nAPI (version 0.0.56)\n- Hosted tools are now available in the API.\n- Environment variables are now supported in the API (not yet in the standard REST API). See the docker compose file for information on how to configure the secret used to encrypt them if you are using Docker.\n- Automatically backfill\nfunction_data\nfor prompts created via the API.\nWeek of 2024-09-16\n- The tag picker now includes tags that were added dynamically via API, in addition to the tags configured for your project.\n- Added a REST API for managing AI secrets. See docs.\nSDK (version 0.0.158)\n- A dedicated\nupdate\nmethod is now available for datasets. - Fixed a Python-specific error causing experiments to fail initializing when git diff --cached encounters invalid or inaccessible Git repositories.\n- Token counts have the correct units when printing\nExperimentSummary\nobjects. - In Python,\nMetricSummary.metric\ncould have anint\nvalue. The type annotation has been updated.\nWeek of 2024-09-09\n- You can now create server-side online evaluations for your logs. Online evals support both autoevals and custom scorers you define as LLM-as-a-judge, TypeScript, or Python functions. See docs for more details.\n- New member invitations now support being added to multiple permission groups.\n- Move datasets and prompts to a new Library navigation tab, and include a list of custom scorers.\n- Clean up tree view by truncating the root preview and showing a preview of a node only if collapsed.\n- Automatically save changes to table views.\nWeek of 2024-09-02\n- You can now upload typescript evals from the command line as functions, and then use them in the playground.\n- Click a span field line to highlight it and pin it to the URL.\n- Copilot tab autocomplete for prompts and data in the Braintrust UI.\nAPI (version 0.0.54)\n- Support for bundled eval uploads.\n- The\nPATCH\nendpoint for prompts now supports updating theslug\nfield.\nSDK (version 0.0.157)\n- Enable the\n--bundle\nflag forbraintrust eval\nin the TypeScript SDK.\nWeek of 2024-08-26\n- Basic filter UI (no BTQL necessary)\n- Add to dataset dropdown now supports adding to datasets across projects.\n- Add REST endpoint for batch-updating ACLs:\n/v1/acl/batch_update\n. - Cmd/Ctrl click on a table row to open it in a new tab.\n- Show the last 5 basic filters in the filter editor.\n- You can now explicitly set and edit prompt slugs.\nAutoevals (version 0.0.86)\n- Add support for Azure OpenAI in node.\nSDK (version 0.0.155)\n- The client wrappers\nwrapOpenAI()\n/wrap_openai()\nnow support Structured Outputs.\nAPI (version 0.0.54)\n- Don't fail insertion requests if realtime broadcast fails\nWeek of 2024-08-19\n- Fixed comment deletion.\n- You can now use\n%\nin BTQL queries to represent percent values. E.g.50%\nwill be interpreted as0.5\n.\nAPI (version 0.0.54)\n- Performance optimizations to filters on\nscores\n,metrics\n, andcreated\nfields. - Performance optimizations to filter subfields of\nmetadata\nandspan_attributes\n.\nWeek of 2024-08-12\n- You can now create custom LLM and code (TypeScript and Python) evaluators in the playground.\n- Fullscreen trace toggle\n- Datasets now accept JSON file uploads\n- When uploading a CSV/JSON file to a dataset, columns/fields named\ninput\n,expected\n, andmetadata\nare now auto-assigned to the corresponding dataset fields - Fix bug in logs/dataset viewer when changing the search params.\nAPI (version 0.0.53)\n- The API now supports running custom LLM and code (TypeScript and Python) functions. To enable this in the:\n- AWS Cloudformation stack: turn on the\nEnableQuarantine\nparameter - Docker deployment: set the\nALLOW_CODE_FUNCTION_EXECUTION\nenvironment variable totrue\n- AWS Cloudformation stack: turn on the\nWeek of 2024-08-05\n- Full text search UI for all span contents in a trace\n- New metrics in the UI and summary API: prompt tokens, completion tokens, total tokens, and LLM duration\n- These metrics, along with cost, now exclude LLM calls used in autoevals (as of 0.0.85)\n- Switching organizations via the header navigates to the same-named project in the selected organization\n- Added\nMarkAsyncWrapper\nto the Python SDK to allow explicitly marking functions which return awaitable objects as async\nAutoevals (version 0.0.85)\n- LLM calls used in autoevals are now marked with\nspan_attributes.purpose = \"scorer\"\nso they can be excluded from metric and cost calculations.\nAutoevals (version 0.0.84)\n- Fix a bug where\nrationale\nwas incorrectly formatted in Python. - Update the\nfull\ndocker deployment configuration to bundle the metadata DB (supabase) inside the main docker compose file. Thus no separate supabase cluster is required. See docs for details. If you are upgrading an existing full deployment, you will likely want to mark the supabase db volumesexternal\nto continue using your existing data (see comments in thedocker-compose.full.yml\nfile for more details).\nSDK (version 0.0.151)\nEval()\ncan now take a base experiment. Provide eitherbaseExperimentName\n/base_experiment_name\norbaseExperimentId\n/base_experiment_id\n.\nWeek of 2024-07-29\n- Errors now show up in the trace viewer.\n- New cookbook recipe on benchmarking LLM providers.\n- Viewer mode selections will no longer automatically switch to a non-editable view if the field is editable and persist across trace/span changes.\n- Show\n%\nin diffs instead ofpp\n. - Add rename, delete and copy current project id actions to the project dropdown.\n- Playgrounds can now be shared publicly.\n- Duration now reflects the \"task\" duration not the overall test case duration (which also includes scores).\n- Duration is now also displayed in the experiment overview table.\n- Add support for Fireworks and Lepton inference providers.\n- \"Jump to\" menu to quickly navigate between span sections.\n- Speed up queries involving metadata fields, e.g.\nmetadata.foo ILIKE '%bar%'\n, using the columnstore backend if it is available. - Added\nproject_id\nquery param to REST API queries which already acceptproject_name\n. E.g. GET experiments. - Update to include the latest Mistral models in the proxy/playground.\nSDK (version 0.0.148)\n- While tracing, if your code errors, the error will be logged to the span. You can also manually log the\nerror\nfield through the API or the logging SDK.\nSDK (version 0.0.147)\nproject_name\nis nowprojectName\n, etc. in theinvoke(...)\nfunction in TypeScriptEval()\nreturn values are printed in a nicer format (e.g. in Notebooks)updateSpan()\n/update_span()\nallows you to update a span's fields after it has been created.\nWeek of 2024-07-22\n- Categorical human review scores can now be re-ordered via Drag-n-Drop.\n- Human review row selection is now a free text field, enabling a quick jump to a specific row.\n- Added REST endpoint for managing org membership. See docs.\nAPI (version 0.0.51)\n- The proxy is now a first-class citizen in the API service, which simplifies deployment and sets the groundwork for some\nexciting new features. Here is what you need to know:\n- The updates are available as of API version 0.0.51.\n- The proxy is now accessible at\nhttps://api.braintrust.dev/v1/proxy\n. You can use this as a base URL in your OpenAI client, instead ofhttps://braintrustproxy.com/v1\n. [NOTE: The latter is still supported, but will be deprecated in the future.] - If you are self-hosting, the proxy is now bundled into the API service. That means you no longer need to deploy the proxy as a separate service.\n- If you have deployed through AWS, after updating the Cloudformation, you'll need to grab the \"Universal API URL\" from the \"Outputs\" tab.\n- Then, replace that in your settings page settings page\n- If you have a Docker-based deployment, you can just update your containers.\n- Once you see the \"Universal API\" indicator, you can remove the proxy URL from your settings page, if you have it set.\nSDK (version 0.0.146)\n- Add support for\nmax_concurrency\nin the Python SDK - Hill climbing evals that use a\nBaseExperiment\nas data will use that as the default base experiment.\nWeek of 2024-07-15\n- In preparation for auth changes, we are making a series of updates that may affect self-deployed instances:\n- Preview URLs will now be subdomains of\n*.preview.braintrust.dev\ninstead ofvercel.app\n. Please add this domain to your allow list. - To continue viewing preview URLs, you will need to update your stack (to update the allow list to include the new domain pattern).\n- The data plane may make requests back to\n*.preview.braintrust.dev\nURLs. This allows you to test previews that include control plane changes. You may need to whitelist traffic from the data plane to*.preview.braintrust.dev\ndomains. - Requests will optionally send an additional\nx-bt-auth-token\nheader. You may need to whitelist this header. - User impersonation through the\nx-bt-impersonate-user\nheader now accepts either the user's id or email. Previously only user id was accepted.\n- Preview URLs will now be subdomains of\nAutoevals (version 0.0.80)\n- New\nExactMatch\nscorer for comparing two values for exact equality.\nAutoevals (version 0.0.77)\n- Officially switch the default model to be\ngpt-4o\n. Our testing showed that it performed on average 10% more accurately thangpt-3.5-turbo\n! - Support claude models (e.g. claude-3-5-sonnet-20240620). You can use them by simply specifying the\nmodel\nparam in any LLM based evaluator.- Under the hood, this will use the proxy, so make sure to configure your Anthropic API keys in your settings.\nWeek of 2024-07-08\n- Human review scores are now sortable from the project configuration page.\n- Streaming support for tool calls in Anthropic models through the proxy and playground.\n- The playground now supports different \"parsing\" modes:\nauto\n: (same as before) the completion text and the first tool call arguments, if anyparallel\n: the completion text and a list of all tool callsraw\n: the completion in the OpenAI non-streaming formatraw_stream\n: the completion in the OpenAI streaming format\n- Cleaned up environment variables in the public docker deployment. Functionally, nothing has changed.\nAutoevals (version 0.0.76)\n- New\n.partial(...)\nsyntax to initialize a scorer with partial arguments likecriteria\ninClosedQA\n. - Allow messages to be inserted in the middle of a prompt.\nWeek of 2024-07-01\n- Table views can now be saved, persisting the BTQL filters, sorts, and column state.\n- Add support for the new\nwindow.ai\nmodel into the playground. - Use push history when navigating table rows to allow for back button navigation.\n- In the experiments list, grouping by a metadata field will group rows in the table as well.\n- Allow the trace tree panel to be resized.\n- Port the log summary query to BTQL. This should speed up the query, especially if you have clickhouse configured in your cloud environment. This functionality requires upgrading your data backend to version 0.0.50.\nSDK (version 0.0.140)\n- New\nwrapTraced\nfunction allows you to trace javascript functions in a more ergonomic way.\nSDK (version 0.0.138)\n- The TypeScript SDK's\nEval()\nfunction now takes amaxConcurrency\nparameter, which bounds the number of concurrent tasks that run. braintrust install api\nnow sets up your API and Proxy URL in your environment.- You can now specify a custom\nfetch\nimplementation in the TypeScript SDK.\nWeek of 2024-06-24\n- Update the experiment progress and experiment score distribution chart layouts\n- Format table column headers with icons\n- Move active filters to the table toolbar\n- Enable RBAC for all users. When inviting a new member, prompt to add that member to an RBAC Permission group.\n- Use btql to power the datasets list, making it significantly faster if you have multiple large datasets.\n- Experiments list chart supports click interactions. Left click to select an experiment, right click to add an annotation.\n- Jump into comparison view between 2 experiments by selecting them in the table an clicking \"Compare\"\nDeployment\n- The proxy service now supports more advanced functionality which requires setting the\nPG_URL\nandREDIS_URL\nparameters. If you do not set them, the proxy will still run without caching credentials or requests.\nWeek of 2024-06-17\n- Add support for labeling expected fields using human review.\n- Create and edit descriptions for datasets.\n- Create and edit metadata for prompts.\n- Click scores and attributes (tree view only) in the trace view to filter by them.\n- Highlight the experiments graph to filter down the set of experiments.\n- Add support for new models including Claude 3.5 Sonnet.\nWeek of 2024-06-10\n- Improved empty state and instructions for custom evaluators in the playground.\n- Show query examples when filtering/sorting.\n- Custom comparison keys for experiments.\n- New model dropdown in the playground/prompt editor that is organized by provider and model type.\nWeek of 2024-06-03\n- You can now collapse the trace tree. It's auto collapsed if you have a single span.\n- Improvements to the experiment chart including greyed out lines for inactive scores and improved legend.\n- Show diffs when you save a new prompt version.\nWeek of 2024-05-27\n- You can now see which users are viewing the same traces as you are in real-time.\n- Improve whitespace and presentation of diffs in the trace view.\n- Show markdown previews in score editor.\n- Show cost in spans and display the average cost on experiment summaries and diff views.\n- Published a new Text2SQL eval recipe\n- Add groups view for RBAC.\nWeek of 2024-05-20\n- Deprecate the legacy dataset format (\noutput\nin place ofexpected\n) in a new version of the SDK (0.0.130). For now, data can still be fetched in the legacy format by setting theuseOutput\n/use_output\nflag to false when usinginitDataset()\n/init_dataset()\n. We recommend updating your code to use datasets withexpected\ninstead ofoutput\nas soon as possible. - Improve the UX for saving and updating prompts from the playground.\n- New hide/show column controls on all tables.\n- New model comparison cookbook recipe.\n- Add support for model / metadata comparison on the experiments view.\n- New experiment picker dropdown.\n- Markdown support in the LLM message viewer.\nWeek of 2024-05-13\n- Support copying to clipboard from\ninput\n,output\n, etc. views - Improve the empty-state experience for datasets.\n- New multi-dimensional charts on the experiment page for comparing models and model parameters.\n- Support\nHTTPS_PROXY\n,HTTP_PROXY\n, andNO_PROXY\nenvironment variables in the API containers. - Support infinite scroll in the logs viewer and remove dataset size limitations.\nWeek of 2024-05-06\n- Denser trace view with span durations built in.\n- Rework pagination and fix scrolling across multiple pages in the logs viewer.\n- Make BTQL the default search method.\n- Add support for Bedrock models in the playground and the proxy.\n- Add \"copy code\" buttons throughout the docs.\n- Automatically overflow large objects (e.g. experiments) to S3 for faster loading and better performance.\nWeek of 2024-04-29\n- Show images in LLM view, adding the ability to display images in the LLM view in the trace viewer.\n- Send an invite email when you invite a new user to your organization.\n- Support selecting/deselecting scores in the experiment view.\n- Roll out Braintrust Query Language (BTQL) for querying logs and traces.\nWeek of 2024-04-22\n- Smart relative time labels for dates (\n1h ago\n,3d ago\n, etc.) - Added double quoted string literals support, e.g.,\ntags contains \"foo\"\n. - Jump to top button in trace details for easier navigation.\n- Fix a race condition in distributed tracing, in which subspans could hit the backend before their parent span, resulting in an inaccurate trace structure.\nAs part of this change, we removed the parent_id\nargument from the latest SDK,\nwhich was previously deprecated in favor of parent\n. parent_id\nis only able\nto use the race-condition-prone form of distributed tracing, so we felt it would\nbe best for folks to upgrade any of their usages from parent_id\nto parent\n.\nBefore upgrading your SDK, if you are currently using parent_id\n, you can port\nover to using parent\nby changing any exported IDs from span.id\nto\nspan.export()\nand then changing any instances of parent_id=[span_id]\nto\nparent=[exported_span]\n.\nFor example, if you had distributed tracing code like the following:\nIt would now look like this:\nWeek of 2024-04-15\n- Incremental support for roles-based access control (RBAC) logic within the API server backend.\nAs part of this change, we removed certain API endpoints which are no longer in\nuse. In particular, the /crud/{object_type}\nendpoint. For the handful of\nusages of these endpoints in old versions of the SDK libraries, we added\nbackwards-compatibility routes, but it is possible we may have missed a few.\nPlease let us know if your code is trying to use an endpoint that no longer\nexists and we can remediate.\n- Changed the semantics of experiment initialization with\nupdate=True\n. Previously, we would require the experiment to already exist, now we will create the experiment if it doesn't already exist otherwise return the existing one.\nThis change affects the semantics of the PUT /v1/experiment\noperation, so that\nit will not replace the contents of an existing experiment with a new one, but\ninstead just return the existing one, meaning it behaves the same as POST /v1/experiment\n. Eventually we plan to revise the update semantics for other\nobject types as well. Therefore, we have deprecated the PUT\nendpoint across\nthe board and plan to remove it in a future revision of the API.\nWeek of 2024-04-08\n- Added support for new multimodal models (\ngpt-4-turbo\n,gpt-4-vision-preview\n,gpt-4-1106-vision-preview\n,gpt-4-turbo-2024-04-09\n,claude-3-opus-20240229\n,claude-3-sonnet-20240229\n,claude-3-haiku-20240307\n). - Introduced REST API for RBAC (Role-Based Access Control) objects including CRUD operations on roles, groups, and permissions, and added a read-only API for users.\n- Improved AI search and added positive/negative tag filtering in AI search. To positively filter, prefix the tag with\n+\n, and to negatively filter, prefix the tag with-\n.\nWe are making some systematic changes to the search experience, and the search syntax is subject to change.\nWeek of 2024-04-01\n- Added functionality for distributed tracing. See the docs for more details.\nAs part of this change, we had to rework the core logging implementation in the\nSDKs to rely on some newer backend API features. Therefore, if you are hosting\nBraintrust on-prem, before upgrading your SDK to any version >= 0.0.115\n, make\nsure your API version is >= 0.0.35\n. You can query the version of the on-prem\nserver with curl [api-url]/version\n, where the API URL can be found on the settings page.\nWeek of 2024-03-25\n- Introduce multimodal support for OpenAI and Anthropic models in the prompt playground and proxy. You can now pass image URLs, base64-encoded image strings, or mustache template variables to models that support multimodal inputs.\n- The REST API now gzips responses.\n- You can now return dynamic arrays of scores in\nEval()\nfunctions (docs). - Launched Reporters, a way to summarize and report eval results in a custom format.\n- New coat of paint in the trace view.\n- Added support for Clickhouse as an additional storage backend, offering a more scalable solution for handling large datasets and performance improvements for certain query types. You can enable it by\nsetting the\nUseManagedClickhouse\nparameter totrue\nin the CloudFormation template or installing the docker container. - Implemented realtime checks using a WebSocket connection and updated proxy configurations to include CORS support.\n- Introduced an API version checker tool so you know when your API version is outdated.\nWeek of 2024-03-18\n- Add new database parameters for external databases in the CloudFormation template.\n- Faster optimistic updates for large writes in the UI.\n- \"Open in playground\" now opens a lighter weight modal instead of the full playground.\n- Can create a new prompt playground from the prompt viewer.\nWeek of 2024-03-11\n- Shipped support for prompt management.\n- Moved playground sessions to be within projects. All existing sessions are now in the \"Playground Sessions\" project.\n- Allowed customizing proxy and real-time URLs through the web application, adding flexibility for different deployment scenarios.\n- Improved documentation for Docker deployments.\n- Improved folding behavior in data editors.\nWeek of 2024-03-04\n- Support custom models and endpoint configuration for all providers.\n- New add team modal with support for multiple users.\n- New information architecture to enable faster project navigation.\n- Experiment metadata now visible in the experiments table.\n- Improve UI write performance with batching.\n- Log filters now apply to any span.\n- Share button for traces\n- Images now supported in the tree view (see tracing docs for more).\nWeek of 2024-02-26\n- Show auto scores before manual scores (matching trace) in the table\n- New logo is live!\n- Any span can now submit scores, which automatically average in the trace. This makes it easier to label scores in the spans where they originate.\n- Improve sidebar scrolling behavior.\n- Add AI search for datasets and logs.\n- Add tags to the SDK.\n- Support viewing and updating metadata on the experiment page.\nWeek of 2024-02-19\nWe rolled out a breaking change to the REST API that renames the\noutput\nfield to expected\non dataset records. This change brings\nthe API in line with last week's update to\nthe Braintrust SDK. For more information, refer to the REST API docs\nfor dataset records (insert\nand fetch).\n- Add support for tags.\n- Score fields are now sorted alphabetically.\n- Add support for Groq ModuleResolutionKind.\n- Improve tree viewer and XML parser.\n- New experiment page redesign\nWeek of 2024-02-12\nWe are rolling out a change to dataset records that renames the output\nfield to expected\n. If you are using the SDK, datasets will still fetch\nrecords using the old format for now, but we recommend future-proofing\nyour code by setting the useOutput\n/ use_output\nflag to false when\ncalling initDataset()\n/ init_dataset()\n, which will become the default\nin a future version of Braintrust.\nWhen you set useOutput\nto false, your dataset records will contain\nexpected\ninstead of output\n. This makes it easy to use them with\nEval(...)\nto provide expected outputs for scoring, since you'll\nno longer have to manually rename output\nto expected\nwhen passing\ndata to the evaluator:\nHere's an example of how to insert and fetch dataset records using the new format:\n- Support duplicate\nEval\nnames. - Fallback to\nBRAINTRUST_API_KEY\nifOPENAI_API_KEY\nis not set. - Throw an error if you use\nexperiment.log\nandexperiment.start_span\ntogether. - Add keyboard shortcuts (j/k/p/n) for navigation.\n- Increased tooltip size and delay for better usability.\n- Support more viewing modes: HTML, Markdown, and Text.\nWeek of 2024-02-05\n- Tons of improvements to the prompt playground:\n- A new \"compact\" view, that shows just one line per row, so you can quickly scan across rows. You can toggle between the two modes.\n- Loading indicators per cell\n- The run button transforms into a \"Stop\" button while you are streaming data\n- Prompt variables are now syntax highlighted in purple and use a monospace font\n- Tab now autocompletes\n- We no longer auto-create variables as you're typing (was causing more trouble than helping)\n- Slider params like\nmax_tokens\nare now optional\n- Cloudformation now supports more granular RDS configuration (instance type, storage, etc)\n- Support optional slider params\n- Made certain parameters like\nmax_tokens\noptional. - Accompanies pull request https://github.com/braintrustdata/braintrust-proxy/pull/23.\n- Made certain parameters like\n- Lots of style improvements for tables.\n- Fixed filter bar styles.\n- Rendered JSON cell values using monospace type.\n- Adjusted margins for horizontally scrollable tables.\n- Implemented a smaller size for avatars in tables.\n- Deleting a prompt takes you back to the prompts tab\nWeek of 2024-01-29\n- New REST API.\n- Cookbook of common use cases and examples.\n- Support for custom models in the playground.\n- Search now works across spans, not just top-level traces.\n- Show creator avatars in the prompt playground\n- Improved UI breadcrumbs and sticky table headers\nWeek of 2024-01-22\n- UI improvements to the playground.\n- Added an example of closed QA / extra fields.\n- New YAML parser and new syntax highlighting colors for data editor.\n- Added support for enabling/disabling certain git fields from collection (in org settings and the SDK).\n- Added new GPT-3.5 and 4 models to the playground.\n- Fixed scrolling jitter issue in the playground.\n- Made table fields in the prompt playground sticky.\nWeek of 2024-01-15\n- Added ability to download dataset as CSV\n- Added YAML support for logging and visualizing traces\n- Added JSON mode in the playground\n- Added span icons and improved readability\n- Enabled shift modifier for selecting multiple rows in Tables\n- Improved tables to allow editing expected fields and moved datasets to trace view\nWeek of 2024-01-08\n- Released new Docker deployment method for self hosting\n- Added ability to manually score results in the experiment UI\n- Added comments and audit log in the experiment UI\nWeek of 2024-01-01\n- Added ability to upload dataset CSV files in prompt playgrounds\n- Published new guide for tracing and logging your code\n- Added support to download experiment results as CSVs\nWeek of 2023-12-25\n- API keys are now scoped to organizations, so if you are part of multiple orgs, new API keys will only permit access to the org they belong to.\n- You can now search for experiments by any metadata, including their name, author, or even git metadata.\n- Filters are now saved in URL state so you can share a link to a filtered view of your experiments or logs.\n- Improve performance of project page by optimizing API calls.\nWe made several cleanups and improvements to the low-level typescript and python SDKs (0.0.86). If you use the Eval framework, nothing should change for you, but keep in mind the following differences if you use the manual logging functionality:\n- Simplified the low-level tracing API (updated docs coming soon!)\n- The current experiment and current logger are now maintained globally\nrather than as async-task-local variables. This makes it much simpler to\nstart tracing with minimal code modification. Note that creating\nexperiments/loggers with\nwithExperiment\n/withLogger\nwill now set the current experiment globally (visible across all async tasks) rather than local to a specific task. You may passsetCurrent: false/set_current=False\nto avoid setting the global current experiment/logger. - In python, the\n@traced\ndecorator now logs the function input/output by default. This might interfere with code that already logs input/output inside thetraced\nfunction. You may passnotrace_io=True\nas an argument to@traced\nto turn this logging off. - In typescript, the\ntraced\nmethod can start spans under the global logger, and is thus async by default. You may passasyncFlush: true\nto these functions to make the traced function synchronous. Note that if the function tries to trace under the global logger, it must also haveasyncFlush: true\n. - Removed the\nwithCurrent\n/with_current\nfunctions - In typescript, the\nSpan.traced\nmethod now acceptsname\nas an optional argument instead of a required positional param. This matches the behavior of all other instances oftraced\n.name\nis also now optional in python, but this doesn't change the function signature.\n- The current experiment and current logger are now maintained globally\nrather than as async-task-local variables. This makes it much simpler to\nstart tracing with minimal code modification. Note that creating\nexperiments/loggers with\nExperiments\nandDatasets\nare now lazily-initialized, similar toLoggers\n. This means all write operations are immediate and synchronous. But any metadata accessor methods ([Experiment|Logger].[id|name|project]\n) are now async.- Undo auto-inference of\nforce_login\niflogin\nis invoked with different params than last time. Nowlogin\nwill only re-login ifforceLogin: true/force_login=True\nis provided.\nWeek of 2023-12-18\n- Dropped the official 2023 Year-in-Review dashboard. Check out yours here!\n- Improved ergonomics for the Python SDK:\n- The\n@traced\ndecorator will automatically log inputs/outputs - You no longer need to use context managers to scope experiments or loggers.\n- The\n- Enable skew protection in frontend deploys, so hopefully no more hard refreshes.\n- Added syntax highlighting in the sidepanel to improve readability.\n- Add\njsonl\nmode to the eval CLI to log experiment summaries in an easy-to-parse format.\nWeek of 2023-12-11\n- Released new trials feature to rerun each input multiple times and collect aggregate results for a more robust score.\n- Added ability to run evals in the prompt playground. Use your existing dataset and the autoevals functions to score playground outputs.\n- Released new version of SDK (0.0.81) including a small breaking change. When setting the experiment name in the\nEval\nfunction, theexprimentName\nkey pair should be moved to a top level argument. before:\nafter:\n- Added support for Gemini and Mistral Platform in AI proxy and playground\nWeek of 2023-12-4\n- Enabled the prompt playground and datasets for free users\n- Added Together.ai models including Mixtral to AI Proxy\n- Turned prompts tab on organization view into a list\n- Removed data row limit for the prompt playground\n- Enabled configuration for dark mode and light mode in settings\n- Added automatic logging of a diff if an experiment is run on a repo with uncommitted changes\nWeek of 2023-11-27\n- Added experiment search on project view to filter by experiment name\n- Upgraded AI Proxy to support tracking Prometheus metrics\n- Modified Autoevals library to use the AI proxy\n- Upgraded Python braintrust library to parallelize evals\n- Optimized experiment diff view for performance improvements\nWeek of 2023-11-20\n- Added support for new Perplexity models (ex: pplx-7b-online) to playground\n- Released AI proxy: access many LLMs using one API w/ caching\n- Added load balancing endpoints to AI proxy\n- Updated org-level view to show projects and prompt playground sessions\n- Added ability to batch delete experiments\n- Added support for Claude 2.1 in playground\nWeek of 2023-11-13\n- Made experiment column resized widths persistent\n- Fixed our libraries including Autoevals to work with OpenAI\u2019s new libraries\n- Added support for function calling and tools in our prompt playground\n- Added tabs on a project page for datasets, experiments, etc.\nWeek of 2023-11-06\n- Improved selectors for diffing and comparison modes on experiment view\n- Added support for new OpenAI models (GPT4 preview, 3.5turbo-1106) in playground\n- Added support for OS models (Mistral, Codellama, Llama2, etc.) in playground using Perplexity's APIs\nWeek of 2023-10-30\n- Improved experiment sidebar to be fully responsive and resizable\n- Improved tooltips within the web UI\n- Multiple performance optimizations and bug fixes\nWeek of 2023-10-23\n-\nImproved prompt playground variable handling and visualization\n-\nAdded time duration statistics per row to experiment summaries\n- Multiple performance optimizations and bug fixes\nWeek of 2023-10-16\n- Launched new tracing feature: log and visualize complex LLM chains and executions.\n- Added a new \u201ctext-block\u201d prompt type in the playground that just returns a string or variable back without a LLM call (useful for chaining prompts and debugging)\n- Increased default # of rows per page from 10 to 100 for experiments\n- UI fixes and improvements for the side panel and tooltips\n- The experiment dashboard can be customized to show the most relevant charts\nWeek of 2023-10-09\n- Performance improvements related to user sessions\nWeek of 2023-10-02\n- All experiment loading HTTP requests are 100-200ms faster\n- The prompt playground now supports autocomplete\n- Dataset versions are now displayed on the datasets page\n- Projects in the summary page are now sorted alphabetically\n- Long text fields in logged data can be expanded into scrollable blocks\n- We evaluated the Alpaca evals leaderboard in Braintrust\n- New tutorial for finetuning GPT3.5 and evaluating with Braintrust\nWeek of 2023-09-18\n- The Eval framework is now supported in Python! See the updated evals guide for more information:\n- Onboarding and signup flow for new users\n- Switch product font to Inter\nWeek of 2023-09-11\n-\nBig performance improvements for registering experiments (down from ~5s to <1s). Update the SDK to take advantage of these improvements.\n-\nNew graph shows aggregate accuracy between experiments for each score.\n-\nThrow errors in the prompt playground if you reference an invalid variable.\n-\nA significant backend database change which significantly improves performance while reducing costs. Please contact us if you have not already heard from us about upgrading your deployment.\n-\nNo more record size constraints (previously, strings could be at most 64kb long).\n-\nNew autoevals for numeric diff and JSON diff\nWeek of 2023-09-05\n- You can duplicate prompt sessions, prompts, and dataset rows in the prompt playground.\n- You can download prompt sessions as JSON files (including the prompt templates, prompts, and completions).\n- You can adjust model parameters (e.g. temperature) in the prompt playground.\n- You can publicly share experiments (e.g. Alpaca Evals).\n- Datasets now support editing, deleting, adding, and copying rows in the UI.\n- There is no longer a 64KB limit on strings.\nWeek of 2023-08-28\n- The prompt playground is now live! We're excited to get your feedback as we continue to build this feature out. See the docs for more information.\nWeek of 2023-08-21\n- A new chart shows experiment progress per score over time.\n- The eval CLI now supports\n--watch\n, which will automatically re-run your evaluation when you make changes to your code. - You can now edit datasets in the UI.\nWeek of 2023-08-14\n- Introducing datasets! You can now upload datasets to Braintrust and use them in your experiments. Datasets are versioned, and you can use them in multiple experiments. You can also use datasets to compare your model's performance against a baseline. Learn more about how to create and use datasets in the docs.\n- Fix several performance issues in the SDK and UI.\nWeek of 2023-08-07\n- Complex data is now substantially more performant in the UI. Prior to this change, we ran schema\ninference over the entire\ninput\n,output\n,expected\n, andmetadata\nfields, which could result in complex structures that were slow and difficult to work with. Now, we simply treat these fields asJSON\ntypes. - The UI updates in real-time as new records are logged to experiments.\n- Ergonomic improvements to the SDK and CLI:\n- The JS library is now Isomorphic and supports both Node.js and the browser.\n- The Evals CLI warns you when no files match the\n.eval.[ts|js]\npattern.\nWeek of 2023-07-31\n- You can now break down scores by metadata fields:\n-\nImprove performance for experiment loading (especially complex experiments). Prior to this change, you may have seen experiments take 30s+ occasionally or even fail. To enable this, you'll need to update your CloudFormation.\n-\nSupport for renaming and deleting experiments:\n- When you expand a cell in detail view, the row is now highlighted:\nWeek of 2023-07-24\n- A new framework for expressing evaluations in a much simpler way:\nBesides being much easier than the logging SDK, this framework sets the foundation for evaluations that can be run automatically as your code changes, built and run in the cloud, and more. We are very excited about the use cases it will open up!\ninputs\nis nowinput\nin the SDK (>= 0.0.23) and UI. You do not need to make any code changes, although you should gradually start using theinput\nfield instead ofinputs\nin your SDK calls, asinputs\nis now deprecated and will eventually be removed.- Improved diffing behavior for nested arrays.\nWeek of 2023-07-17\n- A couple of SDK updates (>= v0.0.21) that allow you to update an existing experiment\ninit(..., update=True)\nand specify an id inlog(..., id='my-custom-id')\n. These tools are useful for running an experiment across multiple processes, tasks, or machines, and idempotently logging the same record (identified by itsid\n).- Note: If you have Braintrust installed in your own cloud environment, make sure to update the CloudFormation (available at https://braintrust-cf.s3.amazonaws.com/braintrust-latest.yaml).\n- Tables with lots and lots of columns are now visually more compact in the UI:\nBefore:\nAfter:\nWeek of 2023-07-10\n- A new Node.js SDK (npm) which mirrors the Python SDK. As this SDK is new, please let us know if you run into any issues or have any feedback.\nIf you have Braintrust installed in your own cloud environment, make sure to update the CloudFormation (available at https://braintrust-cf.s3.amazonaws.com/braintrust-latest.yaml) to include some functionality the Node.js SDK relies on.\nYou can do this in the AWS console, or by running the following command (with the braintrust\ncommand included in the Python SDK).\n- You can now swap the primary and comparison experiment with a single click.\n- You can now compare\noutput\nvs.expected\nwithin an experiment.\n- Version 0.0.19 is out for the SDK. It is an important update that throws an error if your payload is larger than 64KB in size.\nWeek of 2023-07-03\n-\nSupport for real-time updates, using Redis. Prior to this, Braintrust would wait for your data warehouse to sync up with Kafka before you could view an experiment, often leading to a minute or two of time before a page loads. Now, we cache experiment records as your experiment is running, making experiments load instantly. To enable this, you'll need to update your CloudFormation.\n-\nNew settings page that consolidates team, installation, and API key settings. You can now invite team members to your Braintrust account from the \"Team\" page.\n-\nThe experiment page now shows commit information for experiments run inside of a git repository.\nWeek of 2023-06-26\n- Experiments track their git metadata and automatically find a \"base\" experiment to compare against, using your repository's base branch.\n- The Python SDK's\nsummarize()\nmethod now returns anExperimentSummary\nobject with score differences against the base experiment (v0.0.10). - Organizations can now be \"multi-tenant\", i.e. you do not need to install in your cloud account. If you start with a multi-tenant account to try out Braintrust, and decide to move it into your own account, Braintrust can migrate it for you.\nWeek of 2023-06-19\n-\nNew scatter plot and histogram insights to quickly analyze scores and filter down examples.\n-\nAPI keys that can be set in the SDK (explicitly or through an environment variable) and do not require user login. Visit the settings page to create an API key.\n- Update the braintrust Python SDK to version 0.0.6 and the CloudFormation template (https://braintrust-cf.s3.amazonaws.com/braintrust-latest.yaml) to use the new API key feature.\nWeek of 2023-06-12\n- New\nbraintrust install\nCLI for installing the CloudFormation - Improved performance for event logging in the SDK\n- Auto-merge experiment fields with different types (e.g.\nnumber\nandstring\n)\nWeek of 2023-06-05\n-\nAutomatically refresh cognito tokens in the Python client\n-\nNew filter and sort operators on the experiments table:\n- Filter experiments by changes to scores (e.g. only examples with a lower score than another experiment)\n- Custom SQL filters\n- Filter and sort bubbles to visualize/clear current operations\n-\n[Alpha] SQL query explorer to run arbitrary queries against one or more experiments", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2025-09-08"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK Integrations: LangChain (JS) (version 0.0.7)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Python SDK version 0.2.7 (upcoming)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "TypeScript SDK version 0.3.8 (upcoming)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Data plane (1.1.22)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Autoevals PY (version 0.0.130)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2025-09-01"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2025-08-25"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "TypeScript SDK version 0.3.7"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Python SDK version 0.2.6"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "TypeScript SDK version 0.3.6"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Data plane (1.1.21)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2025-08-18"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Data plane (1.1.20)"}, {"href": "https://www.braintrust.dev/docs/reference/api/ServiceTokens", "anchor": "docs"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2025-08-18"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Python SDK version 0.2.5"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "docs"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "TypeScript SDK version 0.2.5"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "docs"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK Integrations: Google ADK (Python) (version 0.1.1)"}, {"href": "https://www.braintrust.dev/docs/guides/integrations", "anchor": "Google Agent Development Kit (ADK)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Python SDK version 0.2.4"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2025-08-11"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Python SDK version 0.2.3"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "TypeScript SDK version 0.2.4"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK Integrations: OpenAI Agents (TS) (version 0.0.2)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Data plane (1.1.19)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2025-08-04"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Python SDK version 0.2.2"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "TypeScript SDK version 0.2.3"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "TypeScript SDK version 0.2.2"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Python SDK version 0.2.1"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2025-07-28"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Data plane (1.1.18)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2025-07-21"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "TypeScript SDK version 0.2.1"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "TypeScript SDK version 0.2.0"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Python SDK version 0.2.0"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Data plane (1.1.15)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2025-07-14"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Python SDK version 0.1.8"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "TypeScript SDK version 0.1.1"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Data plane (1.1.14)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Python SDK version 0.1.7"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "TypeScript SDK version 0.1.0"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2025-07-07"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Data plane (1.1.13)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Data plane (1.1.12) [skipped]"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2025-06-30"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Data plane (1.1.11)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2025-06-23"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Data plane (1.1.7)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Autoevals.js v0.0.130"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "TypeScript SDK version 0.0.209"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "TypeScript SDK version 0.0.208"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2025-06-16"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Python SDK version 0.1.5"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "TypeScript SDK version 0.0.207"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Data plane (1.1.6)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Data plane (1.1.5)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Data plane (1.1.4)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2025-06-09"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Python SDK version 0.1.4"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "TypeScript SDK version 0.0.206"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2025-06-02"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Python SDK version 0.1.3"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2025-05-26"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Python SDK version 0.1.2"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "TypeScript SDK version 0.0.206 [upcoming]"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2025-05-19"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "TypeScript SDK version 0.0.205"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Python SDK version 0.1.1"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "TypeScript SDK version 0.0.204"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2025-05-12"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.1.0)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.203)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2025-05-05"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.202)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2025-04-28"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2025-04-21"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.201)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.200)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2025-04-14"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "the docs"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.199)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.198)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2025-04-07"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "API (version 0.0.65)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.197)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "API (version 0.0.65)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2025-03-31"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "API (version 0.0.64)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.196)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2025-03-24"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.195)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.194)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.193)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2025-03-17"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.192)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Autoevals (version 0.0.124)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2025-03-10"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Autoevals (version 0.0.123)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK Integrations: LangChain (Python) (version 0.0.2)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK Integrations: LangChain.js (version 0.0.6)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK Integrations: Val Town"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.190)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.189)"}, {"href": "https://www.braintrust.dev/docs/guides/traces/integrations", "anchor": "OpenAI Agents SDK"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.188)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2025-03-03"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "proxy docs"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.187)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2025-02-24"}, {"href": "https://www.braintrust.dev/docs/llms.txt", "anchor": "llms.txt"}, {"href": "https://www.braintrust.dev/docs/reference/api/Logs", "anchor": "docs"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "API (version 0.0.63)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.187)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2025-02-17"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.187)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2025-02-10"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2025-02-03"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "proxy docs"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2025-01-27"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK Integrations: LangChain.js (version 0.0.5)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.184)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2025-01-20"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.183)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2025-01-13"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK Integrations: LangChain.js (version 0.0.4)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.182)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2025-01-06"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/tracing/integrations", "anchor": "tracing guide"}, {"href": "https://www.braintrust.dev/docs/guides/tracing/integrations", "anchor": "tracing guide"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.181)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.179)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-12-30"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.179) (unreleased)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.178)"}, {"href": "https://www.braintrust.dev/docs/guides/evals/write", "anchor": "docs"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "customize traces guide"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK integrations: LangChain.js (version 0.0.2)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-12-16"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "API (version 0.0.61)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.177)"}, {"href": "https://www.braintrust.dev/docs/guides/functions/scorers", "anchor": "scorers"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-12-09"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "Braintrust Query Language"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Autoevals (version 0.0.110)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.176)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK integrations: LangChain.js (version 0.0.1)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK integrations: LangChain.js (version 0.0.1)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-12-02"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "API (version 0.0.60)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.175)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-11-25"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.174)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.173)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.172)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-11-18"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "docs"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.171)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-11-12"}, {"href": "https://www.braintrust.dev/docs/guides/functions/tools", "anchor": "tools"}, {"href": "https://www.braintrust.dev/docs/guides/functions/prompts", "anchor": "prompts"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.170)"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "file attachments in the Python SDK"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.169)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.168)"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/Span", "anchor": "TypeScript docs"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "Python docs"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-11-04"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI Proxy"}, {"href": "https://www.braintrust.dev/docs/guides/evals/interpret", "anchor": "grid layout"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "API (version 0.0.59)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-10-28"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI Proxy"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "issue temporary credentials"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "docs"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.168)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-10-21"}, {"href": "https://www.braintrust.dev/docs/guides/tracing", "anchor": "file attachments"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.167)"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/classes/Attachment", "anchor": "file attachments in the TypeScript SDK"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "API (version 0.0.59)"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/classes/Attachment", "anchor": "file attachments"}, {"href": "https://www.braintrust.dev/docs/guides/tracing/integrations", "anchor": "tracing guide"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-10-14"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.166)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.165)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.164)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-10-07"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.163)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.162)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.161)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-09-30"}, {"href": "https://www.braintrust.dev/docs/guides/evals/run", "anchor": "Github action"}, {"href": "https://www.braintrust.dev/docs/guides/tracing", "anchor": "span iframe viewers"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-09-23"}, {"href": "https://www.braintrust.dev/docs/guides/prompts", "anchor": "docs"}, {"href": "https://www.braintrust.dev/app/settings?subroute=env-vars", "anchor": "Set org-wide environment variables"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.160)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.159)"}, {"href": "https://www.braintrust.dev/docs/guides/prompts", "anchor": "docs"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "API (version 0.0.56)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-09-16"}, {"href": "https://www.braintrust.dev/docs/reference/api/AiSecrets", "anchor": "docs"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.158)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-09-09"}, {"href": "https://www.braintrust.dev/docs/reference/autoevals", "anchor": "autoevals"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "custom scorers"}, {"href": "https://www.braintrust.dev/docs/guides/evals/write", "anchor": "docs"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-09-02"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "API (version 0.0.54)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.157)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-08-26"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Autoevals (version 0.0.86)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.155)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "API (version 0.0.54)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-08-19"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "API (version 0.0.54)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-08-12"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "API (version 0.0.53)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-08-05"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Autoevals (version 0.0.85)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Autoevals (version 0.0.84)"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting/docker", "anchor": "docs"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.151)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-07-29"}, {"href": "https://www.braintrust.dev/docs/cookbook/recipes/ProviderBenchmark", "anchor": "benchmarking LLM providers"}, {"href": "https://www.braintrust.dev/docs/reference/api/Experiments", "anchor": "GET experiments"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.148)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.147)"}, {"href": "https://www.braintrust.dev/docs/guides/tracing", "anchor": "updateSpan() / update_span()"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-07-22"}, {"href": "https://www.braintrust.dev/docs/reference/api/Organizations", "anchor": "docs"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "API (version 0.0.51)"}, {"href": "https://www.braintrust.dev/app/settings?subroute=api-url", "anchor": "settings page"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.146)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-07-15"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Autoevals (version 0.0.80)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Autoevals (version 0.0.77)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-07-08"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Autoevals (version 0.0.76)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-07-01"}, {"href": "https://www.braintrust.dev/docs/reference/views", "anchor": "can now be saved"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.140)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.138)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-06-24"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Deployment"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-06-17"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "expected fields using human review"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-06-10"}, {"href": "https://www.braintrust.dev/docs/guides/evals/interpret", "anchor": "Custom comparison keys"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-06-03"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-05-27"}, {"href": "https://www.braintrust.dev/docs/cookbook/recipes/Text2SQL-Data", "anchor": "Text2SQL eval recipe"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-05-20"}, {"href": "https://www.braintrust.dev/docs/cookbook/recipes/ModelComparison", "anchor": "model comparison"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-05-13"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-05-06"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-04-29"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "Braintrust Query Language"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-04-22"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-04-15"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-04-08"}, {"href": "https://www.braintrust.dev/docs/api/spec", "anchor": "REST API for RBAC"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-04-01"}, {"href": "https://www.braintrust.dev/docs/guides/tracing", "anchor": "docs"}, {"href": "https://www.braintrust.dev/app/settings?subroute=api-url", "anchor": "settings page"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-03-25"}, {"href": "https://www.braintrust.dev/docs/guides/evals", "anchor": "docs"}, {"href": "https://www.braintrust.dev/docs/guides/evals", "anchor": "Reporters"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-03-18"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-03-11"}, {"href": "https://www.braintrust.dev/docs/guides/prompts", "anchor": "prompt management"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-03-04"}, {"href": "https://www.braintrust.dev/docs/guides/tracing", "anchor": "tracing docs"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-02-26"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-02-19"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "last week's update"}, {"href": "https://www.braintrust.dev/docs/api/spec", "anchor": "insert"}, {"href": "https://www.braintrust.dev/docs/api/spec", "anchor": "fetch"}, {"href": "https://www.braintrust.dev/docs/guides/logging", "anchor": "tags"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-02-12"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-02-05"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-01-29"}, {"href": "https://www.braintrust.dev/docs/api/spec", "anchor": "REST API"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "custom models"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-01-22"}, {"href": "https://www.braintrust.dev/docs/guides/evals", "anchor": "closed QA / extra fields"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-01-15"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-01-08"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-01-01"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2023-12-25"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2023-12-18"}, {"href": "https://www.braintrust.dev/app/year-in-review", "anchor": "here"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2023-12-11"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2023-12-4"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2023-11-27"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2023-11-20"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "load balancing endpoints"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2023-11-13"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2023-11-06"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2023-10-30"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2023-10-23"}, {"href": "https://www.braintrust.dev/docs/release-notes/ReleaseNotes-2023-10-PromptPlaygroundVar.mp4", "anchor": "Improved prompt playground variable handling and visualization"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2023-10-16"}, {"href": "https://www.braintrust.dev/docs/guides/evals", "anchor": "Launched new tracing feature: log and visualize complex LLM chains and executions."}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2023-10-09"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2023-10-02"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2023-09-18"}, {"href": "https://www.braintrust.dev/docs/guides/evals", "anchor": "evals guide"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2023-09-11"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2023-09-05"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2023-08-28"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "the docs"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2023-08-21"}, {"href": "https://www.braintrust.dev/docs/guides/evals", "anchor": "eval CLI"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2023-08-14"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "how to create and use datasets in the docs"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2023-08-07"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2023-07-31"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2023-07-24"}, {"href": "https://www.braintrust.dev/docs/guides/evals", "anchor": "framework"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2023-07-17"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2023-07-10"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Node.js SDK"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "Python SDK"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2023-07-03"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2023-06-26"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "summarize()"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "ExperimentSummary"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2023-06-19"}, {"href": "https://www.braintrust.dev/app/settings?subroute=api-keys", "anchor": "settings page"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2023-06-12"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2023-06-05"}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Tutorial guide + notebook"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2025-09-08"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK Integrations: LangChain (JS) (version 0.0.7)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Python SDK version 0.2.7 (upcoming)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "TypeScript SDK version 0.3.8 (upcoming)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Data plane (1.1.22)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Autoevals PY (version 0.0.130)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2025-09-01"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2025-08-25"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "TypeScript SDK version 0.3.7"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Python SDK version 0.2.6"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "TypeScript SDK version 0.3.6"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Data plane (1.1.21)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2025-08-18"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Data plane (1.1.20)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2025-08-18"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Python SDK version 0.2.5"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "TypeScript SDK version 0.2.5"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK Integrations: Google ADK (Python) (version 0.1.1)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Python SDK version 0.2.4"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2025-08-11"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Python SDK version 0.2.3"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "TypeScript SDK version 0.2.4"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK Integrations: OpenAI Agents (TS) (version 0.0.2)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Data plane (1.1.19)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2025-08-04"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Python SDK version 0.2.2"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "TypeScript SDK version 0.2.3"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "TypeScript SDK version 0.2.2"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Python SDK version 0.2.1"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2025-07-28"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Data plane (1.1.18)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2025-07-21"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "TypeScript SDK version 0.2.1"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "TypeScript SDK version 0.2.0"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Python SDK version 0.2.0"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Data plane (1.1.15)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2025-07-14"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Python SDK version 0.1.8"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "TypeScript SDK version 0.1.1"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Data plane (1.1.14)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Python SDK version 0.1.7"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "TypeScript SDK version 0.1.0"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2025-07-07"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Data plane (1.1.13)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Data plane (1.1.12) [skipped]"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2025-06-30"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Data plane (1.1.11)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2025-06-23"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Data plane (1.1.7)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Autoevals.js v0.0.130"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "TypeScript SDK version 0.0.209"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "TypeScript SDK version 0.0.208"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2025-06-16"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Python SDK version 0.1.5"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "TypeScript SDK version 0.0.207"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Data plane (1.1.6)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Data plane (1.1.5)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Data plane (1.1.4)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2025-06-09"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Python SDK version 0.1.4"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "TypeScript SDK version 0.0.206"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2025-06-02"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Python SDK version 0.1.3"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2025-05-26"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Python SDK version 0.1.2"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "TypeScript SDK version 0.0.206 [upcoming]"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2025-05-19"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "TypeScript SDK version 0.0.205"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Python SDK version 0.1.1"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "TypeScript SDK version 0.0.204"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2025-05-12"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.1.0)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.203)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2025-05-05"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.202)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2025-04-28"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2025-04-21"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.201)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.200)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2025-04-14"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.199)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.198)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2025-04-07"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "API (version 0.0.65)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.197)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "API (version 0.0.65)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2025-03-31"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "API (version 0.0.64)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.196)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2025-03-24"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.195)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.194)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.193)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2025-03-17"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.192)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Autoevals (version 0.0.124)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2025-03-10"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Autoevals (version 0.0.123)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK Integrations: LangChain (Python) (version 0.0.2)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK Integrations: LangChain.js (version 0.0.6)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK Integrations: Val Town"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.190)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.189)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.188)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2025-03-03"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.187)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2025-02-24"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "API (version 0.0.63)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.187)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2025-02-17"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.187)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2025-02-10"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2025-02-03"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2025-01-27"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK Integrations: LangChain.js (version 0.0.5)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.184)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2025-01-20"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.183)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2025-01-13"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK Integrations: LangChain.js (version 0.0.4)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.182)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2025-01-06"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.181)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.179)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-12-30"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.179) (unreleased)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.178)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK integrations: LangChain.js (version 0.0.2)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-12-16"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "API (version 0.0.61)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.177)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-12-09"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Autoevals (version 0.0.110)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.176)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK integrations: LangChain.js (version 0.0.1)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK integrations: LangChain.js (version 0.0.1)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-12-02"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "API (version 0.0.60)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.175)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-11-25"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.174)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.173)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.172)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-11-18"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.171)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-11-12"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.170)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.169)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.168)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-11-04"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "API (version 0.0.59)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-10-28"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.168)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-10-21"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.167)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "API (version 0.0.59)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-10-14"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.166)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.165)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.164)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-10-07"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.163)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.162)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.161)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-09-30"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-09-23"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.160)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.159)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "API (version 0.0.56)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-09-16"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.158)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-09-09"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-09-02"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "API (version 0.0.54)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.157)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-08-26"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Autoevals (version 0.0.86)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.155)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "API (version 0.0.54)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-08-19"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "API (version 0.0.54)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-08-12"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "API (version 0.0.53)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-08-05"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Autoevals (version 0.0.85)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Autoevals (version 0.0.84)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.151)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-07-29"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.148)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.147)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-07-22"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "API (version 0.0.51)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.146)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-07-15"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Autoevals (version 0.0.80)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Autoevals (version 0.0.77)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-07-08"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Autoevals (version 0.0.76)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-07-01"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.140)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "SDK (version 0.0.138)"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-06-24"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Deployment"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-06-17"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-06-10"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-06-03"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-05-27"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-05-20"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-05-13"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-05-06"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-04-29"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-04-22"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-04-15"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-04-08"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-04-01"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-03-25"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-03-18"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-03-11"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-03-04"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-02-26"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-02-19"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-02-12"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-02-05"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-01-29"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-01-22"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-01-15"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-01-08"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2024-01-01"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2023-12-25"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2023-12-18"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2023-12-11"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2023-12-4"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2023-11-27"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2023-11-20"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2023-11-13"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2023-11-06"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2023-10-30"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2023-10-23"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2023-10-16"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2023-10-09"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2023-10-02"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2023-09-18"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2023-09-11"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2023-09-05"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2023-08-28"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2023-08-21"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2023-08-14"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2023-08-07"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2023-07-31"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2023-07-24"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2023-07-17"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2023-07-10"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2023-07-03"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2023-06-26"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2023-06-19"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2023-06-12"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Week of 2023-06-05"}], "depth": 2}, "https://status.braintrust.dev/": {"url": "https://status.braintrust.dev/", "title": "Braintrust Status", "text": "Report a problem\nSubscribe to updates\nReport a problem\nSubscribe to updates\nWe\u2019re fully operational\nWe\u2019re not aware of any issues affecting our systems.\nSystem status\nJun 2025\n-\nSep 2025\nWeb UI / Control Plane\n99.94\n% uptime\nWeb UI / Control Plane\nCentrally-Hosted Data Plane\n99.95\n% uptime\nCentrally-Hosted Data Plane\nCalendar\nSep 2025\nLoading...\nM\nT\nW\nT\nF\nS\nS\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\nPowered by\nPrivacy policy\n\u00b7\nTerms of service", "links": [{"href": "https://status.braintrust.dev/", "anchor": ""}, {"href": "mailto: info@braintrust.dev", "anchor": "Report a problem"}, {"href": "mailto: info@braintrust.dev", "anchor": "Report a problem"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy policy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of service"}], "depth": 2}, "https://trust.braintrust.dev/": {"url": "https://trust.braintrust.dev/", "title": "Trust Center", "text": "", "links": [], "depth": 2}, "https://www.braintrust.dev/careers?ashby_jid=bade7f86-8304-4515-bb39-5671bed35010": {"url": "https://www.braintrust.dev/careers?ashby_jid=bade7f86-8304-4515-bb39-5671bed35010", "title": "Cloud Infrastructure Engineer - Careers - Braintrust", "text": "Cloud Infrastructure Engineer\nAbout the company\nBraintrust is building the modern platform for evaluating and deploying AI systems. Our mission is to help enterprises build trust in their AI by making it easy to test, monitor, and improve models using real-world evaluation frameworks. We work with cutting-edge customers in finance, healthcare, and tech who are building production-grade AI systems.\nAbout the role\nWe\u2019re looking for a Cloud Infrastructure Engineer to help us build reliable, scalable infrastructure and give developers a world-class platform to ship code with speed and confidence. You\u2019ll lead efforts across Terraform, Kubernetes, CI/CD, observability, and support, and play a key role in how we scale Braintrust both internally and for customers self-hosting our platform.\nThis is a high-impact role where you\u2019ll contribute across our internal AWS environment and help customers deploy our stack in AWS, Azure, and GCP.\nWhat you\u2019ll do\nBuild and maintain Terraform modules for both internal infrastructure and customer deployments\nWork directly with customers in Slack to support self-hosting and troubleshoot infrastructure issues. Build tools to make it easier for them to support themselves.\nOwn and improve our CI/CD pipeline: reduce build times, improve failure visibility, and enable safer, faster releases\nCentralize and scale observability - including logs, metrics, dashboards, and alerts\nPartner with engineering teams to build and evolve a secure, developer-friendly infrastructure platform\nSupport multi-cloud deployment patterns (AWS primarily, with Azure and GCP support for enterprise customers)\nImplement tools and automation to improve deployment, rollback, and infrastructure reliability\nIdeal candidate credentials\n5+ years of experience in DevOps, SRE, or Infrastructure Engineering roles\nDeep experience with Terraform and at least one major cloud provider (AWS strongly preferred)\nStrong Kubernetes skills: deploying, debugging, and scaling real workloads\nProficient in scripting or programming (Python, Typescript, or Go)\nExperience supporting production systems and responding to incidents\nComfortable working directly with customers in a support or deployment context\nBonus: experience with multi-cloud environments or self-hosted enterprise software\nBenefits include\nMedical, dental, and vision insurance\n401k plan\nDaily lunch, snacks, and beverages\nFlexible time off\nCompetitive salary and equity\nAI Stipend\nEqual opportunity\nBraintrust is an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 2}, "https://www.braintrust.dev/careers?ashby_jid=d0ba915c-85c4-4ac5-bedf-7dd7459ec14d": {"url": "https://www.braintrust.dev/careers?ashby_jid=d0ba915c-85c4-4ac5-bedf-7dd7459ec14d", "title": "Commercial Account Executive - Careers - Braintrust", "text": "Commercial Account Executive\nAbout the company\nBraintrust is building the modern platform for evaluating and deploying AI systems. Our mission is to help enterprises build trust in their AI by making it easy to test, monitor, and improve models using real-world evaluation frameworks. We work with cutting-edge customers in finance, healthcare, and tech who are building production-grade AI systems.\nAbout the role\nWe're looking for a Commercial Account Executive who will be responsible for prospecting and closing new business. You will identify, nurture, and close opportunities with both new and existing customers.\nWhat you\u2019ll do\nAs part of the foundational sales team, you'll help develop our GTM strategy and close key customer accounts.\nMeet and exceed individual quarterly and annual sales goals\nManage all aspects of the sales process - prospecting, sales meetings, and negotiations)\nBuild and own the entire sales process including engaging with potential customers, negotiating and closing contracts, customer retention, and renewals.\nNavigate complex sales and win trust through consultative sales conversations with technical teams.\nIdeal candidate credentials\nYou have 3+ years of full cycle sales experience, selling to technical audiences.\nExcellent communication skills through all channels (in person, online, in writing) and able to form strong working relationships both in person and virtually.\nYou have the drive and willingness to go deep on our product to carry a technical conversation with AI teams.\nYou are ready to roll up your sleeves and drive impact to exceed revenue targets.\nYou have a strong worth ethic, competitive drive, and you naturally inspire and motivate those around you.\nYou have thrived in environments where things are sometimes loosely defined and may have competing priorities or deadlines.\nBenefits include\nMedical, dental, and vision insurance\n401k plan\nDaily lunch, snacks, and beverages\nFlexible time off\nCompetitive salary and equity\nAI Stipend\nEqual opportunity\nBraintrust is an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 2}, "https://www.braintrust.dev/careers?ashby_jid=b2131234-080c-4c5b-85e1-56e3edafa4e3": {"url": "https://www.braintrust.dev/careers?ashby_jid=b2131234-080c-4c5b-85e1-56e3edafa4e3", "title": "Customer Solutions Architect - Careers - Braintrust", "text": "Customer Solutions Architect\nAbout the company\nBraintrust is building the modern platform for evaluating and deploying AI systems. Our mission is to help enterprises build trust in their AI by making it easy to test, monitor, and improve models using real-world evaluation frameworks. We work with cutting-edge customers in finance, healthcare, and tech who are building production-grade AI systems.\nAbout the Role\nWe're looking for a Customer Solutions Architect to join our Field Engineering team. This is a deeply technical, customer-facing role focused on helping teams deploy and operate Braintrust in production environments.\nYou\u2019ll work with platform, DevOps, and ML infrastructure teams at leading AI-forward organizations to ensure Braintrust is running reliably \u2014 whether in our managed cloud or in their own environments. Braintrust is used to evaluate and monitor LLM-powered applications, and you\u2019ll play a critical role in enabling those evaluation workflows by ensuring the underlying infrastructure is sound. You\u2019ll support deployments, troubleshoot issues across layers, and guide best practices for scale, security, and maintainability.\nThis role blends solution design, proactive support, and infrastructure guidance \u2014 ideal for someone who enjoys solving real-world problems with code, systems knowledge, and deep customer empathy.\nWhat You\u2019ll Do\nCustomer Enablement & Support\nAct as a trusted advisor to customers deploying and scaling Braintrust in production environments.\nLead onboarding and implementation engagements, including assisting with setup of customer-hosted infrastructure, configuration of observability pipelines, evaluation workflows, and integrations with third-party tools.\nHelp debug customer issues across layers (e.g., SDKs, cloud infrastructure, model outputs), coordinating with product and engineering when needed.\nOwn proactive health checks, upgrade support, and reliability best practices for customers running self-hosted deployments.\nSolution Design & Implementation\nDesign and document end-to-end customer evaluation pipelines using Braintrust SDKs and APIs.\nBuild technical artifacts (scripts, templates, notebooks) that accelerate onboarding and our ability to support customers, or demonstrate platform capabilities.\nConsult on architectural decisions, including LLM evaluation, prompt versioning, or how to structure evals for reliability and reproducibility.\nInternal & Cross-functional Collaboration\nTranslate real-world usage and customer friction into actionable feedback for our product and engineering teams.\nPartner with sales, support, and engineering to ensure seamless handoffs from pre-sales to post-sales and back.\nContribute to internal runbooks, implementation guides, and technical documentation.\nWhat We\u2019re Looking For\nStrong interest in AI technology\nExperience in customer-facing technical roles (e.g., Solutions Architect, Forward Deployed Engineer, Escalation Engineer, Technical Account Manager).\nFluency in Python or TypeScript and comfort debugging code, APIs, and infrastructure.\nStrong grasp of DevOps infrastructure and cloud architecture, including technologies like Docker, Terraform, Kubernetes, and major clouds (AWS, GCP, Azure)\nAbility to explain technical concepts clearly to both engineers and non-technical stakeholders.\nComfortable working autonomously in a fast-paced, startup environment.\nNice to Have\nExperience with self-hosted or hybrid SaaS deployments.\nFamiliarity with LLM/AI development workflows and common tools in the ecosystem\nFamiliarity with Terraform, CI/CD pipelines, and networking/security best practices.\nWhy Join Braintrust\nYou\u2019ll be joining a small, senior team solving one of the most important problems in modern AI development\u2014ensuring reliability. We work with cutting-edge AI teams and move fast to deliver value. You\u2019ll have broad ownership, a strong voice in the product, and the opportunity to help define how the world measures AI quality.\nBenefits include\nMedical, dental, and vision insurance\n401k plan\nDaily lunch, snacks, and beverages\nFlexible time off\nCompetitive salary and equity\nAI Stipend\nEqual opportunity\nBraintrust is an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 2}, "https://www.braintrust.dev/careers?ashby_jid=57487086-68ae-4c81-8c62-c6157c20e4ac": {"url": "https://www.braintrust.dev/careers?ashby_jid=57487086-68ae-4c81-8c62-c6157c20e4ac", "title": "Design Engineer - Careers - Braintrust", "text": "Design Engineer\nAbout the company\nBraintrust is building the modern platform for evaluating and deploying AI systems. Our mission is to help enterprises build trust in their AI by making it easy to test, monitor, and improve models using real-world evaluation frameworks. We work with cutting-edge customers in finance, healthcare, and tech who are building production-grade AI systems.\nAbout the role\nWe're looking for a design engineer who loves working on complex problems and designing products that power-users love to use on a daily basis. Our primary users are engineers and PMs at innovative tech companies who spend multiple hours a day using our product to run experiments, analyze user behavior, and try out new ideas, seamlessly shifting between their code and our UI.\nIn this role, you will:\nPlay a significant role in developing the core user experience of our product. You will work across UI, API, and SDK, ensuring that Braintrust is a seamless tool in every AI engineer\u2019s work day.\nWork closely with our users to deeply understand their needs and contribute to the product roadmap. You are comfortable owning the entire product lifecycle from gathering requirements to ideating, designing, and implementing.\nPartner closely with our product and engineering teams. You\u2019ll work to establish and implement a design system that enables rapid product iteration.\nIdeal candidate credentials\nYou have experience designing zero to one B2B products with complex requirements.\nYou enjoy building things and are familiar with modern devtools (send us a link to your Github!).\nYou have owned problems end-to-end, and are willing to pick up whatever knowledge you\u2019re missing to get the job done.\nYou have thrived in environments where things are sometimes loosely defined and may have competing priorities or deadlines.\nBenefits include\nMedical, dental, and vision insurance\n401k plan\nDaily lunch, snacks, and beverages\nFlexible time off\nCompetitive salary and equity\nAI Stipend\nEqual opportunity\nBraintrust is an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 2}, "https://www.braintrust.dev/careers?ashby_jid=3ae60654-eeaa-4827-8724-8ff060e2e2e4": {"url": "https://www.braintrust.dev/careers?ashby_jid=3ae60654-eeaa-4827-8724-8ff060e2e2e4", "title": "Enterprise Account Executive  - Careers - Braintrust", "text": "Enterprise Account Executive\nAbout the company\nBraintrust is building the modern platform for evaluating and deploying AI systems. Our mission is to help enterprises build trust in their AI by making it easy to test, monitor, and improve models using real-world evaluation frameworks. We work with cutting-edge customers in finance, healthcare, and tech who are building production-grade AI systems.\nAbout the role\nBraintrust is looking for an Enterprise Account Executive who will be responsible for prospecting and closing new business across the West Region of the US.\nBraintrust already has a strong base of early customers including Stripe, Notion, Instacart, and Airtable, which you'll be able to build on to help us expand. Our primary users are software engineers, so you'll need to be comfortable working with a technical audience.\nWhat you\u2019ll do\nAs part of the foundational sales team, you'll help develop our GTM strategy and close key customer accounts.\nBuild and own the entire sales process including engaging with potential customers, negotiating and closing contracts, customer retention, and renewals.\nNavigate complex sales and win trust through consultative sales conversations with technical teams.\nIdeal candidate credentials\nYou have 5+ years of full cycle sales experience, selling to technical audiences.\nYou have the drive and willingness to go deep on our product to carry a technical conversation with AI teams.\nYou are ready to roll up your sleeves and drive impact to exceed revenue targets.\nYou have a strong worth ethic, competitive drive, and you naturally inspire and motivate those around you.\nYou have thrived in environments where things are sometimes loosely defined and may have competing priorities or deadlines.\nBenefits include\nMedical, dental, and vision insurance\n401k plan\nDaily lunch, snacks, and beverages\nFlexible time off\nCompetitive salary and equity\nAI Stipend\nEqual opportunity\nBraintrust is an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 2}, "https://www.braintrust.dev/careers?ashby_jid=002956ad-7692-4159-a54d-a211c972524f": {"url": "https://www.braintrust.dev/careers?ashby_jid=002956ad-7692-4159-a54d-a211c972524f", "title": "Head of Revenue Operations  - Careers - Braintrust", "text": "Head of Revenue Operations\nAbout the company\nBraintrust is building the modern platform for evaluating and deploying AI systems. Our mission is to help enterprises build trust in their AI by making it easy to test, monitor, and improve models using real-world evaluation frameworks. We work with cutting-edge customers in finance, healthcare, and tech who are building production-grade AI systems.\nAbout the role\nAt Braintrust, we believe revenue operations is about creating the clarity and alignment that enables our go-to-market (GTM) teams to thrive.\nThe Head of Revenue Operations is the strategic architect of Braintrust\u2019s growth engine, serving as the critical connector between strategy and execution. In this role, you will build the systems, processes, and frameworks that bring consistency and scalability to how our teams operate. By aligning sales, marketing, and field engineering, you\u2019ll ensure we have a unified foundation for growth \u2014 improving efficiency, driving execution, and enabling sustainable results. This role requires someone who can think strategically about the long-term direction of our revenue engine, while also rolling up their sleeves to execute and deliver impact day-to-day.\nWe\u2019re looking for a Head of Revenue Operations to bring structure, insights, and operational excellence to our growing organization.\nWhat you'll do:\nCommercial Operations and insights\nBuild and maintain dashboards that provide real-time visibility into pipeline health, conversion rates, sales velocity, and productivity.\nConduct deep-dive analyses to answer critical business questions (e.g., \u201cWhy did conversion drop in this segment?\u201d).\nSupport deal desk and pricing strategy\nCreate stakeholder-specific dashboards and materials for sales leadership, customer success, and executives.\nDevelop early warning systems with leading and lagging indicators of revenue health.\nStrategic alignment\nOwn and optimize Salesforce and the GTM tech stack, ensuring clean data and scalable workflows.\nPartner with Marketing to align funnel metrics and attribution.\nCollaborate with Finance on forecasting, revenue modeling, and compensation tracking.\nLead planning processes around quotas, territories, and resource allocation.\nYou might be a fit if:\nComfortable balancing strategic framing with hands-on execution.\nThrive in startup environments and enjoy bringing structure to ambiguity.\nDeep experience in revenue operations, sales operations, or business operations, ideally in dev tools, B2B SaaS, and/or AI.\nStrong command of Salesforce and GTM reporting tools\nHighly analytical with the ability to turn data into actionable insights.\nStrong communicator who can tailor messaging for reps, managers, and executives alike.\nBonus points for:\nExperience in devtools, AI infrastructure, or early-stage GTM environments.\nA track record of connecting Marketing + Sales funnels with clear attribution.\nPrior work designing comp plans or coverage models that improved performance.\nFamiliarity with using AI/automation to streamline reporting and analytics.\nBenefits include\nMedical, dental, and vision insurance\n401k plan\nDaily lunch, snacks, and beverages\nFlexible time off\nCompetitive salary and equity\nAI Stipend\nEqual opportunity\nBraintrust is an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 2}, "https://www.braintrust.dev/careers?ashby_jid=2a6c4ee9-063f-45d4-83ba-faf64b1f1a60": {"url": "https://www.braintrust.dev/careers?ashby_jid=2a6c4ee9-063f-45d4-83ba-faf64b1f1a60", "title": "Open Source Engineer - Go - Careers - Braintrust", "text": "Open Source Engineer - Go\nAbout the company\nBraintrust is building the modern platform for evaluating and deploying AI systems. Our mission is to help enterprises build trust in their AI by making it easy to test, monitor, and improve models using real-world evaluation frameworks. We work with cutting-edge customers in finance, healthcare, and tech who are building production-grade AI systems.\nAbout the Role\nWe\u2019re hiring engineers to build the open-source libraries that customers use to observe, understand, and optimize how AI operates inside their production applications. These SDKs are a critical part of Braintrust\u2019s platform. They power evaluation and observability pipelines across a wide range of environments, frameworks, and AI providers.\nThis role is ideal for engineers who are passionate about developer experience, API craftsmanship, and building fast, reliable, and idiomatic libraries. You\u2019ll own the full SDK lifecycle: design, implementation, release, documentation, and community support. Your work will be embedded in thousands of applications and help define how AI is adopted and operationalized across industries.\nWhat You\u2019ll Do\nBuild elegant, idiomatic, and resilient SDKs that power Braintrust\u2019s LLM evaluation and AI observability platform.\nEnsure our libraries are easy to use, efficient to run, and a delight to work with, prioritizing developer experience and performance.\nIntegrate with major AI providers, frameworks, and platforms our customers rely on, such as OpenAI, Anthropic, and Gemini.\nBuild tools and automation to improve testing, profiling and simplify release workflows.\nCollaborate closely with backend, platform, and product teams to ensure a cohesive and polished developer experience.\nBe a great community ambassador: talk with our users, understand their issues, help them get their fixes merged and have deep empathy for our users.\nYou Might Be a Fit If You:\nAre customer-obsessed and passionate about solving real-world problems for developers.\nHave deep expertise in Go and understand what it takes to build fast, idiomatic, and reliable libraries in that ecosystem.\nAre proficient with the tooling needed to build robust SDKs, such as testing frameworks, profilers, CI/CD pipelines, and packaging systems.\nCare deeply about developer experience, from clean APIs and clear docs to thoughtful error handling and backward compatibility.\nKnow how to wrap, patch, or extend existing frameworks and SDKs to inject observability and control without breaking developer workflows.\nHave experience maintaining or contributing to production-grade open-source libraries, ideally used across multiple environments.\nBonus: You\u2019ve contributed to popular open source projects, led SDK adoption at your organization, are familiar with tracing and observability tools like OpenTelemetry, or worked on tools in the LLM/AI ecosystem.\nBenefits include\nMedical, dental, and vision insurance\n401k plan\nDaily lunch, snacks, and beverages\nFlexible time off\nCompetitive salary and equity\nAI Stipend\nEqual opportunity\nBraintrust is an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 2}, "https://www.braintrust.dev/careers?ashby_jid=5d761ea6-bb57-4ae6-ad7d-de39edb0aed6": {"url": "https://www.braintrust.dev/careers?ashby_jid=5d761ea6-bb57-4ae6-ad7d-de39edb0aed6", "title": "Open Source Engineer - Java  - Careers - Braintrust", "text": "Open Source Engineer - Java\nAbout the company\nBraintrust is building the modern platform for evaluating and deploying AI systems. Our mission is to help enterprises build trust in their AI by making it easy to test, monitor, and improve models using real-world evaluation frameworks. We work with cutting-edge customers in finance, healthcare, and tech who are building production-grade AI systems.\nAbout the Role\nWe\u2019re hiring engineers to build the open-source libraries that customers use to observe, understand, and optimize how AI operates inside their production applications. These SDKs are a critical part of Braintrust\u2019s platform. They power evaluation and observability pipelines across a wide range of environments, frameworks, and AI providers.\nThis role is ideal for engineers who are passionate about developer experience, API craftsmanship, and building fast, reliable, and idiomatic libraries. You\u2019ll own the full SDK lifecycle: design, implementation, release, documentation, and community support. Your work will be embedded in thousands of applications and help define how AI is adopted and operationalized across industries.\nWhat You\u2019ll Do\nBuild elegant, idiomatic, and resilient SDKs that power Braintrust\u2019s LLM evaluation and AI observability platform.\nEnsure our libraries are easy to use, efficient to run, and a delight to work with, prioritizing developer experience and performance.\nIntegrate with major AI providers, frameworks, and platforms our customers rely on, such as OpenAI, Anthropic, and Gemini.\nBuild tools and automation to improve testing, profiling and simplify release workflows.\nCollaborate closely with backend, platform, and product teams to ensure a cohesive and polished developer experience.\nBe a great community ambassador: talk with our users, understand their issues, help them get their fixes merged and have deep empathy for our users.\nYou Might Be a Fit If You:\nAre customer-obsessed and passionate about solving real-world problems for developers.\nHave deep expertise in Java and understand what it takes to build fast, idiomatic, and reliable libraries in that ecosystem.\nAre proficient with the tooling needed to build robust SDKs, such as testing frameworks, profilers, CI/CD pipelines, and packaging systems.\nCare deeply about developer experience, from clean APIs and clear docs to thoughtful error handling and backward compatibility.\nKnow how to wrap, patch, or extend existing frameworks and SDKs to inject observability and control without breaking developer workflows.\nHave experience maintaining or contributing to production-grade open-source libraries, ideally used across multiple environments.\nBonus: You\u2019ve contributed to popular open source projects, led SDK adoption at your organization, are familiar with tracing and observability tools like OpenTelemetry, or worked on tools in the LLM/AI ecosystem.\nBenefits include\nMedical, dental, and vision insurance\n401k plan\nDaily lunch, snacks, and beverages\nFlexible time off\nCompetitive salary and equity\nAI Stipend\nEqual opportunity\nBraintrust is an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 2}, "https://www.braintrust.dev/careers?ashby_jid=ba4d6676-f65c-42c9-84ee-285f2da15e0d": {"url": "https://www.braintrust.dev/careers?ashby_jid=ba4d6676-f65c-42c9-84ee-285f2da15e0d", "title": "Open Source Engineer - Python - Careers - Braintrust", "text": "Open Source Engineer - Python\nAbout the company\nBraintrust is building the modern platform for evaluating and deploying AI systems. Our mission is to help enterprises build trust in their AI by making it easy to test, monitor, and improve models using real-world evaluation frameworks. We work with cutting-edge customers in finance, healthcare, and tech who are building production-grade AI systems.\nAbout the Role\nWe\u2019re hiring engineers to build the open-source libraries that customers use to observe, understand, and optimize how AI operates inside their production applications. These SDKs are a critical part of Braintrust\u2019s platform. They power evaluation and observability pipelines across a wide range of environments, frameworks, and AI providers.\nThis role is ideal for engineers who are passionate about developer experience, API craftsmanship, and building fast, reliable, and idiomatic libraries. You\u2019ll own the full SDK lifecycle: design, implementation, release, documentation, and community support. Your work will be embedded in thousands of applications and help define how AI is adopted and operationalized across industries.\nWhat You\u2019ll Do\nBuild elegant, idiomatic, and resilient SDKs that power Braintrust\u2019s LLM evaluation and AI observability platform.\nEnsure our libraries are easy to use, efficient to run, and a delight to work with, prioritizing developer experience and performance.\nIntegrate with major AI providers, frameworks, and platforms our customers rely on, such as OpenAI, Anthropic, and Gemini.\nBuild tools and automation to improve testing, profiling and simplify release workflows.\nCollaborate closely with backend, platform, and product teams to ensure a cohesive and polished developer experience.\nBe a great community ambassador: talk with our users, understand their issues, help them get their fixes merged and have deep empathy for our users.\nYou Might Be a Fit If You:\nAre customer-obsessed and passionate about solving real-world problems for developers.\nHave deep expertise in Python and understand what it takes to build fast, idiomatic, and reliable libraries in that ecosystem.\nAre proficient with the tooling needed to build robust SDKs, such as testing frameworks, profilers, CI/CD pipelines, and packaging systems.\nCare deeply about developer experience, from clean APIs and clear docs to thoughtful error handling and backward compatibility.\nKnow how to wrap, patch, or extend existing frameworks and SDKs to inject observability and control without breaking developer workflows.\nHave experience maintaining or contributing to production-grade open-source libraries, ideally used across multiple environments.\nBonus: You\u2019ve contributed to popular open source projects, led SDK adoption at your organization, are familiar with tracing and observability tools like OpenTelemetry, or worked on tools in the LLM/AI ecosystem.\nBenefits include\nMedical, dental, and vision insurance\n401k plan\nDaily lunch, snacks, and beverages\nFlexible time off\nCompetitive salary and equity\nAI Stipend\nEqual opportunity\nBraintrust is an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 2}, "https://www.braintrust.dev/careers?ashby_jid=48cf6a86-8b76-45ea-8fdc-033e49204bef": {"url": "https://www.braintrust.dev/careers?ashby_jid=48cf6a86-8b76-45ea-8fdc-033e49204bef", "title": "Open Source Engineer - Ruby - Careers - Braintrust", "text": "Open Source Engineer - Ruby\nAbout the company\nBraintrust is building the modern platform for evaluating and deploying AI systems. Our mission is to help enterprises build trust in their AI by making it easy to test, monitor, and improve models using real-world evaluation frameworks. We work with cutting-edge customers in finance, healthcare, and tech who are building production-grade AI systems.\nAbout the Role\nWe\u2019re hiring engineers to build the open-source libraries that customers use to observe, understand, and optimize how AI operates inside their production applications. These SDKs are a critical part of Braintrust\u2019s platform. They power evaluation and observability pipelines across a wide range of environments, frameworks, and AI providers.\nThis role is ideal for engineers who are passionate about developer experience, API craftsmanship, and building fast, reliable, and idiomatic libraries. You\u2019ll own the full SDK lifecycle: design, implementation, release, documentation, and community support. Your work will be embedded in thousands of applications and help define how AI is adopted and operationalized across industries.\nWhat You\u2019ll Do\nBuild elegant, idiomatic, and resilient SDKs that power Braintrust\u2019s LLM evaluation and AI observability platform.\nEnsure our libraries are easy to use, efficient to run, and a delight to work with, prioritizing developer experience and performance.\nIntegrate with major AI providers, frameworks, and platforms our customers rely on, such as OpenAI, Anthropic, and Gemini.\nBuild tools and automation to improve testing, profiling and simplify release workflows.\nCollaborate closely with backend, platform, and product teams to ensure a cohesive and polished developer experience.\nBe a great community ambassador: talk with our users, understand their issues, help them get their fixes merged and have deep empathy for our users.\nYou Might Be a Fit If You:\nAre customer-obsessed and passionate about solving real-world problems for developers.\nHave deep expertise in Ruby and understand what it takes to build fast, idiomatic, and reliable libraries in that ecosystem.\nAre proficient with the tooling needed to build robust SDKs, such as testing frameworks, profilers, CI/CD pipelines, and packaging systems.\nCare deeply about developer experience, from clean APIs and clear docs to thoughtful error handling and backward compatibility.\nKnow how to wrap, patch, or extend existing frameworks and SDKs to inject observability and control without breaking developer workflows.\nHave experience maintaining or contributing to production-grade open-source libraries, ideally used across multiple environments.\nBonus: You\u2019ve contributed to popular open source projects, led SDK adoption at your organization, are familiar with tracing and observability tools like OpenTelemetry, or worked on tools in the LLM/AI ecosystem.\nBenefits include\nMedical, dental, and vision insurance\n401k plan\nDaily lunch, snacks, and beverages\nFlexible time off\nCompetitive salary and equity\nAI Stipend\nEqual opportunity\nBraintrust is an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 2}, "https://www.braintrust.dev/careers?ashby_jid=f93a8de6-f9c4-47dc-a8a9-df8f8dec7223": {"url": "https://www.braintrust.dev/careers?ashby_jid=f93a8de6-f9c4-47dc-a8a9-df8f8dec7223", "title": "Open Source Engineer - Typescript/Javascript - Careers - Braintrust", "text": "Open Source Engineer - Typescript/Javascript\nAbout the company\nBraintrust is building the modern platform for evaluating and deploying AI systems. Our mission is to help enterprises build trust in their AI by making it easy to test, monitor, and improve models using real-world evaluation frameworks. We work with cutting-edge customers in finance, healthcare, and tech who are building production-grade AI systems.\nAbout the Role\nWe\u2019re hiring engineers to build the open-source libraries that customers use to observe, understand, and optimize how AI operates inside their production applications. These SDKs are a critical part of Braintrust\u2019s platform. They power evaluation and observability pipelines across a wide range of environments, frameworks, and AI providers.\nThis role is ideal for engineers who are passionate about developer experience, API craftsmanship, and building fast, reliable, and idiomatic libraries. You\u2019ll own the full SDK lifecycle: design, implementation, release, documentation, and community support. Your work will be embedded in thousands of applications and help define how AI is adopted and operationalized across industries.\nWhat You\u2019ll Do\nBuild elegant, idiomatic, and resilient SDKs that power Braintrust\u2019s LLM evaluation and AI observability platform.\nEnsure our libraries are easy to use, efficient to run, and a delight to work with, prioritizing developer experience and performance.\nIntegrate with major AI providers, frameworks, and platforms our customers rely on, such as OpenAI, Anthropic, and Gemini.\nBuild tools and automation to improve testing, profiling and simplify release workflows.\nCollaborate closely with backend, platform, and product teams to ensure a cohesive and polished developer experience.\nBe a great community ambassador: talk with our users, understand their issues, help them get their fixes merged and have deep empathy for our users.\nYou Might Be a Fit If You:\nAre customer-obsessed and passionate about solving real-world problems for developers.\nHave deep expertise in Typescript/Javascript and understand what it takes to build fast, idiomatic, and reliable libraries in that ecosystem.\nAre proficient with the tooling needed to build robust SDKs, such as testing frameworks, profilers, CI/CD pipelines, and packaging systems.\nCare deeply about developer experience, from clean APIs and clear docs to thoughtful error handling and backward compatibility.\nKnow how to wrap, patch, or extend existing frameworks and SDKs to inject observability and control without breaking developer workflows.\nHave experience maintaining or contributing to production-grade open-source libraries, ideally used across multiple environments.\nBonus: You\u2019ve contributed to popular open source projects, led SDK adoption at your organization, are familiar with tracing and observability tools like OpenTelemetry, or worked on tools in the LLM/AI ecosystem.\nBenefits include\nMedical, dental, and vision insurance\n401k plan\nDaily lunch, snacks, and beverages\nFlexible time off\nCompetitive salary and equity\nAI Stipend\nEqual opportunity\nBraintrust is an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 2}, "https://www.braintrust.dev/careers?ashby_jid=2dc80993-3029-49bd-b95c-c8f832002619": {"url": "https://www.braintrust.dev/careers?ashby_jid=2dc80993-3029-49bd-b95c-c8f832002619", "title": "Recruiting Coordinator - Careers - Braintrust", "text": "Recruiting Coordinator\nAbout the company\nBraintrust is building the modern platform for evaluating and deploying AI systems. Our mission is to help enterprises build trust in their AI by making it easy to test, monitor, and improve models using real-world evaluation frameworks. We work with cutting-edge customers in finance, healthcare, and tech who are building production-grade AI systems.\nAbout the Role\nWe\u2019re looking for our first recruiting coordinator to own and lay the foundation for the candidate experience across all teams at Braintrust. We're looking for someone who\u2019s organized, proactive, and excited to build recruiting systems that scale. If you\u2019re a natural with people, thrive in fast-paced environments, and get energy from making things more efficient - this could be a great role for you.\nThis is an in-person role based in San Francisco.\nWhat You\u2019ll Do\nOwn scheduling and logistics for all interviews - across teams, time zones, and formats (virtual, onsite, hybrid)\nGuide candidates through the interview process, keeping communication timely, warm, and informative\nManage the behind-the-scenes coordination from first touch to offer, ensuring a smooth and polished experience\nBuild and maintain the systems, templates, and workflows that power recruiting operations\nPartner with the team to keep job descriptions, interview loops, and process documentation current and clear\nSupport onboarding to ensure new hires have a smooth transition from candidate to team member\nSpot inefficiencies in our hiring process and take the lead in fixing them\nIdeal Credentials\n1+ years of experience in recruiting coordination, people operations, or a high-touch customer-facing role\nExcellent verbal and written communication skills\nStrong attention to detail, with a love for streamlining and automating where you can\nFast learner with the ability to pick up new tools and systems quickly\nGreat at juggling multiple priorities - you ask for help or communicate early when something is slipping\nEnergized by working with people and making processes run smoothly\nBenefits include\nMedical, dental, and vision insurance\n401k plan\nDaily lunch, snacks, and beverages\nFlexible time off\nCompetitive salary and equity\nAI Stipend\nEqual opportunity\nBraintrust is an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 2}, "https://www.braintrust.dev/careers?ashby_jid=319c6491-53c1-46cf-868d-f3b4acba650c": {"url": "https://www.braintrust.dev/careers?ashby_jid=319c6491-53c1-46cf-868d-f3b4acba650c", "title": "Sales Development Representative - Careers - Braintrust", "text": "Sales Development Representative\nAbout the company\nBraintrust is building the modern platform for evaluating and deploying AI systems. Our mission is to help enterprises build trust in their AI by making it easy to test, monitor, and improve models using real-world evaluation frameworks. We work with cutting-edge customers in finance, healthcare, and tech who are building production-grade AI systems.\nAbout the role\nWe're looking for sales development representatives to find and engage new prospects for Braintrust. The ideal candidate will be driven, curious, and eager to dive deep into the AI tech landscape, and capable of engaging in meaningful, technical conversations with prospective customers.\nWhat you\u2019ll do\nResearch, identify, and reach out to prospects within targeted accounts through multi-channel outreach (calls, email, LinkedIn).\nConsistently deliver on established monthly meeting targets that will contribute to revenue growth.\nIdentify key decision makers and champions within new large enterprise and strategic accounts.\nStay up-to-date with industry trends and AI developments to better understand and engage with prospects.\nIdeal candidate credentials\nBachelor's degree or equivalent practical experience in a relevant field.\n1+ years in a sales or relevant role with a track record of success and high performance.\nAbility to work independently in a fast-paced environment, managing multiple priorities with a results-driven approach.\nStrong interpersonal and communication skills, capable of effectively engaging and building relationships with a technical audience.\nA love for making an impact and working with a team to hit key goals and metrics.\nBenefits include\nMedical, dental, and vision insurance\n401k plan\nDaily lunch, snacks, and beverages\nFlexible time off\nCompetitive salary and equity\nAI Stipend\nEqual opportunity\nBraintrust is an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 2}, "https://www.braintrust.dev/careers?ashby_jid=9728653e-49b9-4f5c-b6cd-7dbc6a6d5fcc": {"url": "https://www.braintrust.dev/careers?ashby_jid=9728653e-49b9-4f5c-b6cd-7dbc6a6d5fcc", "title": "Software Engineer, Backend - Careers - Braintrust", "text": "Software Engineer, Backend\nAbout the company\nBraintrust is building the modern platform for evaluating and deploying AI systems. Our mission is to help enterprises build trust in their AI by making it easy to test, monitor, and improve models using real-world evaluation frameworks. We work with cutting-edge customers in finance, healthcare, and tech who are building production-grade AI systems.\nAbout the role\nWe\u2019re looking for a backend engineer who\u2019s excited to build the infrastructure behind cutting-edge AI development tools.\nUnder the hood, Braintrust is a real-time, high-availability data platform that operates in both SaaS and self-hosted environments. You\u2019ll help scale our ingestion pipelines, evolve our open source libraries, and ensure everything runs reliably, observably, and fast. If you thrive in a fast-paced environment and enjoy solving hard systems problems with a product mindset, we\u2019d love to work with you.\nWhat you'll do\nAs a backend engineer at Braintrust, you\u2019ll help build the core platform powering LLM-native development:\nDesign and ship features that give users deep insight into their LLM usage and performance.\nIntegrate, proxy, cache, and collect data from OpenAI, Anthropic, Gemini, and other major model providers that our customers rely on.\nDevelop and evolve robust, efficient open source libraries for tracing and evaluating LLM calls inside customer applications. Check out our SDKs\nBuild highly available real-time data pipelines to ingest, store, and query large volumes of structured and semi-structured AI usage data.\nYou will also collaborate closely with our product engineers to ship polished, end-to-end features that solve real problems for our customers, such as:\nBundling and uploading javascript and python code snippets to be used as online, on-demand function calls for LLM workflows.\nSupport for parsing and uploading attachments from LLM multimodal outputs into long term storage.\nRole-based access controls and data retention policies for enterprise use cases.\nIdeal candidate credentials\n5+ years of backend engineering experience across multiple parts of the stack\nStrong systems thinking: you\u2019ve owned or contributed to infrastructure projects and know how to design for scale, uptime, and data integrity\nProficient in modern backend technologies (e.g., Node.js, Go, Python, Rust, Postgres, Redis, Terraform, Docker, AWS)\nExperience with observability tooling (e.g., Grafana, Prometheus, Datadog, or similar) and a passion for making systems operable, available and rock solid.\nComfortable with ambiguous problems and excited to partner across the company to ship impactful work\nClear communicator who documents decisions, shares context, and helps lift the team\nBenefits include\nMedical, dental, and vision insurance\n401k plan\nDaily lunch, snacks, and beverages\nFlexible time off\nCompetitive salary and equity\nAI Stipend\nEqual opportunity\nBraintrust is an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 2}, "https://www.braintrust.dev/careers?ashby_jid=7d0a5b62-a554-48f0-b2f7-8cbac0e98ae6": {"url": "https://www.braintrust.dev/careers?ashby_jid=7d0a5b62-a554-48f0-b2f7-8cbac0e98ae6", "title": "Software Engineer, Product - Careers - Braintrust", "text": "Software Engineer, Product\nAbout the company\nBraintrust is building the modern platform for evaluating and deploying AI systems. Our mission is to help enterprises build trust in their AI by making it easy to test, monitor, and improve models using real-world evaluation frameworks. We work with cutting-edge customers in finance, healthcare, and tech who are building production-grade AI systems.\nAbout the role\nWe're looking for a product engineer who is passionate about building tools that users love and use every day. You'll work on the core product, hang out with users (developers, product managers, and designers working in AI), and play a key role in shaping the roadmap.\nUnder the hood, Braintrust is a high performance, local-first, visualization-heavy UI built with modern Typescript and React. Our users work at some of the world's best technology companies and expect our product to be lightning fast, reliable, and intuitive.\nWhat you'll do\nCore responsibilities:\nCreate a dialog with end-users to directly solve customer problems and gather feedback about the product.\nPlay a significant role in the foundational UI architecture, technology choices, and implementation.\nBuild the user-facing pieces of Braintrust, such as:\nCharting and data visualization for our logging and monitoring page.\nA first-class prompt playground experience supporting multiple models across hundreds of thousands of inputs.\nLLM output analysis and comparison across arbitrarily large datasets.\nIdeal candidate credentials\nExpert-level understanding of Typescript, React, HTML, CSS, SQL, NextJS\nPrevious experience founding or working at startups is a plus\nHas written prompts / tinkered with GPT models and apps\nBenefits include\nMedical, dental, and vision insurance\n401k plan\nDaily lunch, snacks, and beverages\nFlexible time off\nCompetitive salary and equity\nAI Stipend\nEqual opportunity\nBraintrust is an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 2}, "https://www.braintrust.dev/careers?ashby_jid=8b9cfa26-627f-442c-a358-783b0e4ef930": {"url": "https://www.braintrust.dev/careers?ashby_jid=8b9cfa26-627f-442c-a358-783b0e4ef930", "title": "Software Engineer, Systems - Careers - Braintrust", "text": "Software Engineer, Systems\nAbout the company\nBraintrust is building the modern platform for evaluating and deploying AI systems. Our mission is to help enterprises build trust in their AI by making it easy to test, monitor, and improve models using real-world evaluation frameworks. We work with cutting-edge customers in finance, healthcare, and tech who are building production-grade AI systems.\nAbout the role\nWe're looking for a software engineer who loves to build high performance data processing systems. Our customers are scaled up companies who send complex, semi-structured data which they need to process and analyze in real-time. Our unique architecture allows them to store this data on-prem, while creating complex visualizations that load instantly. Check out our Brainstore blog post\nIf you've worked on database systems, compilers, networks, or storage systems, and want to translate this knowledge to the AI field, this could be the role for you. You will play a significant role in the foundational system architecture, technology choices, and implementation. Our founding team has deep expertise in database and ML systems, and you'll be empowered to both collaborate closely with them and explore your own ideas.\nWhat you'll do\nAs a systems engineer at Braintrust, you\u2019ll help build the core systems powering Braintrust's ability to process and query massive volumes of unstructured data at enterprise scale. Areas of ownership include:\nContribute to the storage, indexing, and query execution performance of Brainstore.\nExpanding Braintrust's btql query language.\nOptimizing query patterns to improve performance across our platform.\nIdeal candidate credentials\nExpert-level understanding of systems programming (C++ or Rust, concurrency, databases, operating systems)\nPrevious experience founding or working at startups is a plus\nHas written prompts / tinkered with GPT models and apps\nBenefits include\nMedical, dental, and vision insurance\n401k plan\nDaily lunch, snacks, and beverages\nFlexible time off\nCompetitive salary and equity\nAI Stipend\nEqual opportunity\nBraintrust is an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/blog/brainstore", "anchor": "Check out our Brainstore blog post"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "btql query language"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 2}, "https://www.braintrust.dev/careers?ashby_jid=f877a92d-e7ab-4664-abf5-c60fafa789ef": {"url": "https://www.braintrust.dev/careers?ashby_jid=f877a92d-e7ab-4664-abf5-c60fafa789ef", "title": "Solutions Engineer - Careers - Braintrust", "text": "Solutions Engineer\nAbout the company\nBraintrust is building the modern platform for evaluating and deploying AI systems. Our mission is to help enterprises build trust in their AI by making it easy to test, monitor, and improve models using real-world evaluation frameworks. We work with cutting-edge customers in finance, healthcare, and tech who are building production-grade AI systems.\nAbout the role\nWe are looking to hire a solutions engineer to partner with our sales team to navigate every part of the sales process. In this role, you'll working closely with technical stakeholders at companies ranging from small startups to massive enterprises. Our primary users are software engineers and PMs at world-class tech companies who are working to leverage GenAI.\nWhat you'll do\nPartner with our sales team to articulate the overall Braintrust value proposition, vision, and strategy to customers\nOwn the technical engagement with customers and help close complex opportunities through advanced competitive knowledge, technical skill and credibility\nDeliver product and technical presentations to potential and existing customers\nEffectively communicate with customers and internal teams to provide feedback on our products and the competitive landscape\nWork with the team to enhance documentation, write blog posts, record videos, contribute knowledge base articles and create other public or private enablement material\nIdeal candidate credentials\n5+ years of experience as a sales engineer or in solutions consulting\nAbility to connect a customer's specific problems to Braintrust's solutions\nProficiency in Typescript and Python\nHas written prompts / tinkered with GPT models and apps\nBenefits include\nMedical, dental, and vision insurance\n401k plan\nDaily lunch, snacks, and beverages\nFlexible time off\nCompetitive salary and equity\nAI Stipend\nEqual opportunity\nBraintrust is an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 2}, "https://www.braintrust.dev/careers?ashby_jid=6a9b8666-77f2-4405-8ef6-09488074caf4": {"url": "https://www.braintrust.dev/careers?ashby_jid=6a9b8666-77f2-4405-8ef6-09488074caf4", "title": "Talent Sourcer - Careers - Braintrust", "text": "Talent Sourcer\nAbout the company\nBraintrust is building the modern platform for evaluating and deploying AI systems. Our mission is to help enterprises build trust in their AI by making it easy to test, monitor, and improve models using real-world evaluation frameworks. We work with cutting-edge customers in finance, healthcare, and tech who are building production-grade AI systems.\nAbout the role:\nBraintrust is looking for a motivated and experienced Talent Sourcer to play a critical role in scaling both our Engineering and Go-to-Market (GTM) teams.\nAs a Talent Sourcer, you'll partner closely with recruiters and hiring managers to identify and engage top-tier talent across a variety of roles - from software engineers and design engineers to enterprise sellers and professional services. You\u2019ll bring a consultative mindset and a sharp eye for high-signal profiles, helping us build world-class teams across both technical and commercial functions.\nWhat you'll do:\nPartner with recruiters and hiring managers across Engineering, Sales, Marketing, and Field Engineering to understand role requirements, team dynamics, and hiring priorities\nDevelop and execute strategic sourcing plans to build strong candidate pipelines for engineering and GTM functions\nIdentify and engage passive candidates through channels like LinkedIn, GitHub, job boards, referrals, and online communities\nCommunicate compelling narratives that reflect Braintrust\u2019s mission, product vision, and career opportunities\nStay up to date on market trends, talent movements, and competitive intelligence across both technical and commercial talent markets\nMaintain and organize talent pools for high-priority and evergreen roles, ensuring a strong pipeline when hiring needs arise\nProvide weekly updates on sourcing progress, market insights, and candidate funnel health to recruiters and hiring teams\nIdeal candidate credentials\n2-5 years of sourcing or full-cycle recruiting experience, with exposure to both technical and GTM roles\nFamiliarity with sourcing tools and techniques, including Boolean search, LinkedIn Recruiter, Ashby, and up-and-coming AI tools\nExceptional communication skills - you can write compelling outreach and engage candidates across disciplines\nHighly organized and adaptable - you can context-switch between technical and non-technical roles without missing a beat\nProactive, self-driven, and excited to work in a fast-paced, high-impact startup environment\nA passion for building high-performing teams and contributing to a talent-first culture\nBenefits include\nMedical, dental, and vision insurance\n401k plan\nDaily lunch, snacks, and beverages\nFlexible time off\nCompetitive salary and equity\nAI Stipend\nEqual opportunity\nBraintrust is an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 2}, "https://www.braintrust.dev/careers?ashby_jid=5b64bc4a-7005-4134-9718-8b9772782194": {"url": "https://www.braintrust.dev/careers?ashby_jid=5b64bc4a-7005-4134-9718-8b9772782194", "title": "Technical Support Engineer - Careers - Braintrust", "text": "Technical Support Engineer\nAbout the company\nBraintrust is building the modern platform for evaluating and deploying AI systems. Our mission is to help enterprises build trust in their AI by making it easy to test, monitor, and improve models using real-world evaluation frameworks. We work with cutting-edge customers in finance, healthcare, and tech who are building production-grade AI systems.\nAbout the role\nAt Braintrust, we believe world-class customer support is not just a function \u2014 it\u2019s a strategic advantage. Our customers are developers building LLM-powered applications, and they move fast. We win by helping them move faster.\nWe're looking for a Technical Support Engineer who\u2019s obsessed with making customers successful. Someone who will dig in, debug, escalate, build a workaround, and write a doc \u2014 all in the name of helping a customer ship. You\u2019ll be the first line of support for developers using Braintrust, and a crucial feedback loop between our users and the rest of the company.\nThis role blends technical problem solving, async communication, and tight collaboration with Engineering, Solutions, and Product. If you love making tools work, explaining complex things clearly, and running through walls to unblock users \u2014 we want to talk to you.\nWhat you'll do\nOwn and respond to technical support issues from developers using Braintrust \u2014 primarily via Slack and email\nDebug API and SDK issues across Python and TypeScript codebases\nReproduce bugs and work closely with Engineering to prioritize and resolve them\nWrite clear, friendly explanations and surface improvements to docs, guides, and error messages\nHelp users design successful evaluation workflows, even when they\u2019re unsure what\u2019s going wrong\nOccasionally jump in to write a script, fix a bug, or contribute to internal tools when needed\nCapture and relay customer feedback to Product and Engineering to shape roadmap and priorities\nYou might be a fit if:\nYou\u2019ve supported technical users before (ideally developers) and know what good async support looks like\nYou can write, debug, and reason through Python (bonus: TypeScript too)\nYou\u2019re comfortable learning and reasoning about APIs, SDKs, and tools quickly\nYou take ownership of problems \u2014 even when they cross team lines \u2014 and follow through until users are unblocked\nYou write clearly, kindly, and effectively \u2014 especially when things are confusing\nYou\u2019re curious about LLMs and excited to support tools that make AI more measurable, reliable, and trustworthy\nBonus points for:\nExperience supporting developer tools, ML infrastructure, or observability products\nFamiliarity with OpenAI, Anthropic, or similar LLM APIs\nComfort working with datasets, evaluation metrics, or prompt engineering\nContributions to support tooling, docs, or product-led growth initiatives\nBenefits include\nMedical, dental, and vision insurance\n401k plan\nDaily lunch, snacks, and beverages\nFlexible time off\nCompetitive salary and equity\nAI Stipend\nEqual opportunity\nBraintrust is an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up for free"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/articles", "anchor": "Articles"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Contact us"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}], "depth": 2}, "https://www.braintrust.dev/docs/guides/prompts": {"url": "https://www.braintrust.dev/docs/guides/prompts", "title": "Prompts - Docs - Guides - Functions - Braintrust", "text": "Prompts\nPrompt engineering is a core activity in AI engineering. Braintrust allows you to create prompts, test them out in the playground, use them in your code, update them, and track their performance over time. Our goal is to provide a world-class authoring experience in Braintrust, seamlessly, securely, and reliably integrate them into your code, and debug issues as they arise.\nCreating a prompt\nTo create a prompt, navigate to your Library in the top menu bar and select Prompts, then Create prompt. Pick a name and unique slug for your prompt. The slug is an identifier that you can use to reference it in your code. As you change the prompt's name, description, or contents, its slug stays constant.\nPrompts can use mustache templating syntax to refer to variables. These variables are substituted\nautomatically in the API, playground, and using the .build()\nfunction in your code. More on that below.\nIn code\nTo create a prompt in code, you can write a script and push\nit to Braintrust:\nEach prompt change is versioned, e.g. 5878bd218351fb8e\n. You can use this identifier to pin a specific\nversion of the prompt in your code.\nYou can use this identifier to refer to a specific version of the prompt in your code.\nAdding few-shot examples to a prompt\nYou can also use mustache syntax to add few-shot examples to your prompt. For example:\nTesting in the playground\nWhile developing a prompt, it can be useful to test it out on real-world data in the Playground. You can open a prompt in the playground, tweak it, and save a new version once you're ready.\nStructured outputs\nWhen using prompts in the playground, you can also define the JSON schema for the structured output of the prompt. Like tool calls, the returned value is parsed as a JSON object automatically.\nFor example:\nThis JSON object corresponds to the response_format.json_schema\nargument in the OpenAI API, so this feature currently only works for OpenAI models.\nStructured outputs can also be defined using the visual schema builder UI.\nUsing tools\nYou can use any custom tools you've created during prompt execution. To reference a tool when creating a prompt via the SDK, add the names of the tools you want to use to the tools\nparameter:\nIn Python, the prompt and the tool need to be defined in the same file and pushed to Braintrust together. In TypeScript, they can be defined and pushed separately.\nTo add a tool to a prompt via the UI, select the Tools dropdown in the prompt creation window and select a tool from your library, then save the prompt.\nFor more information about creating and using tools, check out the Tools guide.\nUsing prompts in your code\nExecuting directly\nIn Braintrust, a prompt is a simple function that can be invoked directly through the SDK and REST API. When invoked, prompt functions leverage the proxy to access a wide range of providers and models with managed secrets, and are automatically traced and logged to your Braintrust project. All functions are fully managed and versioned via the UI and API.\nFunctions are a broad concept that encompass prompts, code snippets, HTTP endpoints, and more. When using the functions API, you can use a prompt's slug or ID as the function's slug or ID, respectively. To learn more about functions, see the functions reference.\nThe return value, result\n, is a string unless you have tool calls, in which case it returns the arguments\nof the first tool call. In TypeScript, you can assert this by using the schema\nargument, which ensures your\ncode matches a particular zod schema:\nAdding extra messages\nIf you're building a chat app, it's often useful to send back additional messages of context as you gather them. You can provide\nOpenAI-style messages to the invoke\nfunction by adding messages\n, which are appended to the end of the built-in messages:\nStreaming\nYou can also stream results in an easy-to-parse format.\nVercel AI SDK\nIf you're using Next.js and the Vercel AI SDK, you can use the Braintrust\nadapter by installing the @braintrust/vercel-ai-sdk\npackage and converting the stream to Vercel's format:\nYou can also use streamText\nto leverage the Vercel AI SDK directly. Be sure to have the OpenTelemetry environment variables configured to log these requests to Braintrust.\nLogging\ninvoke\nuses the active logging state of your application, just like any function decorated with @traced\nor wrapTraced\n.\nThis means that if you initialize a logger while calling invoke\n, it will automatically log spans to Braintrust. By default, invoke\nrequests will log to a root span, but you can customize the name of a span using the name\nargument. For example:\nwill generate a log like this:\nYou can also pass in the parent\nargument, which is a string that you can\nderive from span.export()\nwhile doing distributed tracing.\nFetching in code\nIf you'd like to run prompts directly, you can fetch them using the Braintrust SDK. The loadPrompt()\n/load_prompt()\nfunction loads a prompt into a simple format that you can pass along to the OpenAI client.\nloadPrompt\nalso caches prompts with a two-layered cache,\nand attempts to use this cache if the prompt cannot be fetched from the Braintrust server:\n- A memory cache, which stores up to\nBRAINTRUST_PROMPT_CACHE_MEMORY_MAX\nprompts in memory. This defaults to 1024. - A disk cache, which stores up to\nBRAINTRUST_PROMPT_CACHE_DISK_MAX\nprompts on disk. This defaults to 1048576.\nYou can also configure the directory used by disk cache\nby setting the BRAINTRUST_PROMPT_CACHE_DIR\nenvironment variable.\nIf you need to use another model provider, then you can use the Braintrust\nproxy to access a wide range of models using the OpenAI\nformat. You can also grab the messages\nand other parameters directly from\nthe returned object to use a model library of your choice.\nPinning a specific version\nTo pin a specific version of a prompt, use the loadPrompt()\n/load_prompt()\nfunction with the version identifier.\nPulling prompts locally\nYou can also download prompts to your local filesystem and ensure a specific version is used via version control. You should\nuse the pull\ncommand to:\n- Download prompts to public projects so others can use them\n- Pin your production environment to a specific version without running them through Braintrust on the request path\n- Review changes to prompts in pull requests\nCurrently, braintrust pull\nonly supports TypeScript.\nWhen you run braintrust pull\n, you can specify a project name, prompt slug, or version to pull. If you don't specify\nany of these, all prompts across projects will be pulled into a separate file per project. For example, if you have a\nproject named Summary\nwill generate the following file:\nTo pin your production environment to a specific version, you can run braintrust pull\nwith the --version\nflag.\nUsing a pulled prompt\nThe prompts.create\nfunction generates the same Prompt\nobject as the loadPrompt\nfunction.\nThis means you can use a pulled prompt in the same way you would use a normal prompt, e.g. by\nrunning prompt.build()\nand passing the result to client.chat.completions.create()\ncall.\nPushing prompts\nJust like with tools, you can push prompts to Braintrust using the push\ncommand by changing\nthe prompt definition and running braintrust push\nfrom the command line. Braintrust automatically generates\na new version for each pushed prompt.\nWhen you run npx braintrust push\n, you can specify one or more files or directories to push. If you specify a directory, all .ts\nfiles under that directory are pushed.\nSee the example in the guide to tools for more details.\nPushing from code\nSometimes it's more convenient to write a script that pushes prompts, rather than using the command line. You can do this by\nrunning project.publish()\ndirectly from your code.\nDeployment strategies\nIt is often useful to use different versions of a prompt in different environments. For example, you might want to use the latest\nversion locally and in staging, but pin a specific version in production. This is simple to setup by conditionally passing a version\nto loadPrompt()\n/load_prompt()\nbased on the environment.\nGet prompt versions\nUse the getPromptVersions()\nfunction to get a list of available versions for a specific prompt. This function returns transaction IDs that can be used with the version\nparameter when loading prompts.\nChat vs. completion format\nIn Python, prompt.build()\nreturns a dictionary with chat or completion parameters, depending on the prompt type. In TypeScript, however,\nprompt.build()\naccepts an additional parameter (flavor\n) to specify the format. This allows prompt.build\nto be used in a more type-safe\nmanner. When you specify a flavor, the SDK also validates that the parameters are correct for that format.\nOpening from traces\nWhen you use a prompt in your code, Braintrust automatically links spans to the prompt used to generate them. This allows you to click to open a span in the playground, and see the prompt that generated it alongside the input variables. You can even test and save a new version of the prompt directly from the playground.\nThis workflow is very powerful. It effectively allows you to debug, iterate, and publish changes to your prompts directly within Braintrust. And because Braintrust flexibly allows you to load the latest prompt, a specific version, or even a version controlled artifact, you have a lot of control over how these updates propagate into your production systems.\nUsing the API\nThe full lifecycle of prompts - creating, retrieving, modifying, etc. - can be managed through the REST API. See the API docs for more details.", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/functions/prompts", "anchor": "Prompts"}, {"href": "https://www.braintrust.dev/docs/guides/functions/tools", "anchor": "Tools"}, {"href": "https://www.braintrust.dev/docs/guides/functions/scorers", "anchor": "Scorers"}, {"href": "https://www.braintrust.dev/docs/guides/functions/agents", "anchor": "Agents"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/prompts", "anchor": "Prompts"}, {"href": "https://www.braintrust.dev/docs/guides/prompts", "anchor": "Creating a prompt"}, {"href": "https://www.braintrust.dev/docs/guides/prompts", "anchor": "In code"}, {"href": "https://www.braintrust.dev/docs/guides/prompts", "anchor": "Adding few-shot examples to a prompt"}, {"href": "https://www.braintrust.dev/docs/guides/prompts", "anchor": "Testing in the playground"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playground"}, {"href": "https://www.braintrust.dev/docs/guides/prompts", "anchor": "Structured outputs"}, {"href": "https://www.braintrust.dev/docs/guides/prompts", "anchor": "Using tools"}, {"href": "https://www.braintrust.dev/docs/guides/functions/tools", "anchor": "Tools guide"}, {"href": "https://www.braintrust.dev/docs/guides/prompts", "anchor": "Using prompts in your code"}, {"href": "https://www.braintrust.dev/docs/guides/prompts", "anchor": "Executing directly"}, {"href": "https://www.braintrust.dev/docs/reference/api/Functions", "anchor": "REST API"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "proxy"}, {"href": "https://www.braintrust.dev/docs/reference/functions", "anchor": "functions reference"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/InvokeFunctionArgs", "anchor": "schema"}, {"href": "https://www.braintrust.dev/docs/guides/prompts", "anchor": "Adding extra messages"}, {"href": "https://www.braintrust.dev/docs/guides/prompts", "anchor": "Streaming"}, {"href": "https://www.braintrust.dev/docs/guides/prompts", "anchor": "Vercel AI SDK"}, {"href": "https://www.braintrust.dev/docs/guides/traces/integrations", "anchor": "OpenTelemetry environment variables"}, {"href": "https://www.braintrust.dev/docs/guides/prompts", "anchor": "Logging"}, {"href": "https://www.braintrust.dev/docs/guides/functions/prompts", "anchor": "initialize a logger"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "distributed tracing"}, {"href": "https://www.braintrust.dev/docs/guides/prompts", "anchor": "Fetching in code"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "loadPrompt()"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "load_prompt()"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "Braintrust proxy"}, {"href": "https://www.braintrust.dev/docs/guides/prompts", "anchor": "Pinning a specific version"}, {"href": "https://www.braintrust.dev/docs/guides/prompts", "anchor": "Pulling prompts locally"}, {"href": "https://www.braintrust.dev/docs/guides/prompts", "anchor": "Using a pulled prompt"}, {"href": "https://www.braintrust.dev/docs/guides/prompts", "anchor": "Pushing prompts"}, {"href": "https://www.braintrust.dev/docs/guides/functions/tools", "anchor": "guide to tools"}, {"href": "https://www.braintrust.dev/docs/guides/prompts", "anchor": "Pushing from code"}, {"href": "https://www.braintrust.dev/docs/guides/prompts", "anchor": "Deployment strategies"}, {"href": "https://www.braintrust.dev/docs/guides/prompts", "anchor": "Get prompt versions"}, {"href": "https://www.braintrust.dev/docs/guides/prompts", "anchor": "Chat vs. completion format"}, {"href": "https://www.braintrust.dev/docs/guides/prompts", "anchor": "Opening from traces"}, {"href": "https://www.braintrust.dev/docs/guides/prompts", "anchor": "Using the API"}, {"href": "https://www.braintrust.dev/docs/api/spec", "anchor": "API docs"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}, {"href": "https://www.braintrust.dev/docs/guides/prompts", "anchor": "Prompts"}, {"href": "https://www.braintrust.dev/docs/guides/prompts", "anchor": "Creating a prompt"}, {"href": "https://www.braintrust.dev/docs/guides/prompts", "anchor": "In code"}, {"href": "https://www.braintrust.dev/docs/guides/prompts", "anchor": "Adding few-shot examples to a prompt"}, {"href": "https://www.braintrust.dev/docs/guides/prompts", "anchor": "Testing in the playground"}, {"href": "https://www.braintrust.dev/docs/guides/prompts", "anchor": "Structured outputs"}, {"href": "https://www.braintrust.dev/docs/guides/prompts", "anchor": "Using tools"}, {"href": "https://www.braintrust.dev/docs/guides/prompts", "anchor": "Using prompts in your code"}, {"href": "https://www.braintrust.dev/docs/guides/prompts", "anchor": "Executing directly"}, {"href": "https://www.braintrust.dev/docs/guides/prompts", "anchor": "Adding extra messages"}, {"href": "https://www.braintrust.dev/docs/guides/prompts", "anchor": "Streaming"}, {"href": "https://www.braintrust.dev/docs/guides/prompts", "anchor": "Vercel AI SDK"}, {"href": "https://www.braintrust.dev/docs/guides/prompts", "anchor": "Logging"}, {"href": "https://www.braintrust.dev/docs/guides/prompts", "anchor": "Fetching in code"}, {"href": "https://www.braintrust.dev/docs/guides/prompts", "anchor": "Pinning a specific version"}, {"href": "https://www.braintrust.dev/docs/guides/prompts", "anchor": "Pulling prompts locally"}, {"href": "https://www.braintrust.dev/docs/guides/prompts", "anchor": "Using a pulled prompt"}, {"href": "https://www.braintrust.dev/docs/guides/prompts", "anchor": "Pushing prompts"}, {"href": "https://www.braintrust.dev/docs/guides/prompts", "anchor": "Pushing from code"}, {"href": "https://www.braintrust.dev/docs/guides/prompts", "anchor": "Deployment strategies"}, {"href": "https://www.braintrust.dev/docs/guides/prompts", "anchor": "Get prompt versions"}, {"href": "https://www.braintrust.dev/docs/guides/prompts", "anchor": "Chat vs. completion format"}, {"href": "https://www.braintrust.dev/docs/guides/prompts", "anchor": "Opening from traces"}, {"href": "https://www.braintrust.dev/docs/guides/prompts", "anchor": "Using the API"}], "depth": 2}, "https://www.braintrust.dev/signin?redirect_url=%2Fapp%2Fcreate-playground": {"url": "https://www.braintrust.dev/signin?redirect_url=%2Fapp%2Fcreate-playground", "title": "Braintrust - The evals and observability platform for building reliable AI agents", "text": "Sign in to Braintrust\nEmail address\nContinue\nOr\nContinue with Google\nDon't have an account?\nSign up\nBraintrust - The evals and observability platform for building reliable AI agents", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/signup", "anchor": "Sign up"}], "depth": 3}, "https://www.braintrust.dev/signup?redirect_url=%2Fapp%2Fsetup%3Freferrer%3Dplayground": {"url": "https://www.braintrust.dev/signup?redirect_url=%2Fapp%2Fsetup%3Freferrer%3Dplayground", "title": "Braintrust - The evals and observability platform for building reliable AI agents", "text": "Create your Braintrust account\nEmail address\nPassword\nContinue\nOr\nContinue with Google\nAlready have a Braintrust account?\nSign in\nBy signing up, you agree to our\nPrivacy Policy\nand\nTerms of Service\nTrusted by leading AI teams\nBraintrust - The evals and observability platform for building reliable AI agents", "links": [{"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/signin", "anchor": "Sign in"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy Policy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms of Service"}], "depth": 3}, "https://www.braintrust.dev/docs/start/frameworks/opentelemetry": {"url": "https://www.braintrust.dev/docs/start/frameworks/opentelemetry", "title": "OpenTelemetry (OTel) - Docs - Start - Braintrust", "text": "OpenTelemetry (OTel)\nTo set up Braintrust as an OpenTelemetry backend, you'll need to route the traces to Braintrust's OpenTelemetry endpoint, set your API key, and specify a parent project or experiment.\nBraintrust supports configuring OTel with our SDK, as well as libraries like OpenLLMetry and the Vercel AI SDK. You can also use OTel's built-in exporters to send traces to Braintrust if you don't want to install additional libraries or write code. OpenLLMetry supports a range of languages including Python, TypeScript, Java, and Go, so you can start logging to Braintrust from many different environments.\nPython SDK configuration\nInstall the Braintrust Python SDK with OpenTelemetry support:\nuv add braintrust[otel]\nFor Python applications, use the BraintrustSpanProcessor\nfor simplified configuration:\nFor more advanced configuration, you can pass in the following arguments to BraintrustSpanProcessor\n:\napi_key\n: The API key to use for Braintrust. Defaults to theBRAINTRUST_API_KEY\nenvironment variable.api_url\n: The URL of the Braintrust API. Defaults to theBRAINTRUST_API_URL\nenvironment variable orhttps://api.braintrust.dev\nif not set.parent\n: The parent project or experiment to use for Braintrust. Defaults to theBRAINTRUST_PARENT\nenvironment variable.filter_ai_spans\n: Defaults toFalse\n. IfTrue\n, only AI-related spans will be sent to Braintrust.custom_filter\n: A function that gives you fine-grained control over which spans are sent to Braintrust. It takes a span and returns a boolean. IfTrue\n, the span will be sent to Braintrust. IfFalse\n, the span will be dropped. IfNone\n, don't influence the sampling decision.\nTypeScript SDK configuration\nInstall the Braintrust TypeScript SDK with the following OpenTelemetry dependencies:\npnpm add braintrust @opentelemetry/api @opentelemetry/sdk-node @opentelemetry/sdk-trace-base\nFor TypeScript/JavaScript applications, use the BraintrustSpanProcessor\nwith NodeSDK:\nOr configure it manually with a custom tracer provider:\nFor more advanced configuration, you can pass in the following arguments to BraintrustSpanProcessor\n:\napiKey\n: The API key to use for Braintrust. Defaults to theBRAINTRUST_API_KEY\nenvironment variable.apiUrl\n: The URL of the Braintrust API. Defaults to theBRAINTRUST_API_URL\nenvironment variable orhttps://api.braintrust.dev\nif not set.parent\n: The parent project or experiment to use for Braintrust. Defaults to theBRAINTRUST_PARENT\nenvironment variable.filterAISpans\n: Defaults tofalse\n. Iftrue\n, only AI-related spans will be sent to Braintrust.customFilter\n: A function that gives you fine-grained control over which spans are sent to Braintrust. It takes a span and returns a boolean. Iftrue\n, the span will be sent to Braintrust. Iffalse\n, the span will be dropped. Ifnull\n, don't influence the sampling decision.\nOTLP configuration\nIf you are using a different language or want to use pure OTel code, you can set up the OpenTelemetry Protocol Exporter (OTLP) to send traces to Braintrust.\nOnce you set up an OTLP exporter to send traces to Braintrust, we automatically\nconvert LLM calls into Braintrust LLM\nspans, which\ncan be saved as prompts\nand evaluated in the playground.\nFor JavaScript/TypeScript applications, you can use the BraintrustExporter\ndirectly:\nFor collectors that use the OpenTelemetry SDK to export traces, set the following environment variables:\nThe trace endpoint URL is https://api.braintrust.dev/otel/v1/traces\n. If your exporter\nuses signal-specific environment variables, you'll need to set the full path:\nOTEL_EXPORTER_OTLP_TRACES_ENDPOINT=https://api.braintrust.dev/otel/v1/traces\nIf you're self-hosting Braintrust, substitute your stack's Universal API URL. For example:\nOTEL_EXPORTER_OTLP_ENDPOINT=https://dfwhllz61x709.cloudfront.net/otel\nThe x-bt-parent\nheader sets the trace's parent project or experiment. You can use\na prefix like project_id:\n, project_name:\n, or experiment_id:\nhere, or pass in\na span slug\n(span.export()\n) to nest the trace under a span within the parent object.\nTo find your project ID, navigate to your project's configuration page and find the Copy Project ID button at the bottom of the page.\nVercel AI SDK\nThe Vercel AI SDK natively supports OpenTelemetry and works out of the box with Braintrust, either via Next.js or Node.js.\nNext.js\nIf you are using Next.js, use the Braintrust exporter with @vercel/otel\n:\nTraced LLM calls will appear under the Braintrust project or experiment provided in the parent\nfield.\nWhen you call the AI SDK, make sure to set experimental_telemetry\n:\nThe integration supports streaming functions like streamText\n. Each streamed call will produce ai.streamText\nspans in Braintrust.\nNode.js\nIf you are using Node.js without a framework, you must configure the NodeSDK\ndirectly. Here, it's more straightforward\nto use the BraintrustSpanProcessor\n.\nFirst, install the necessary dependencies:\npnpm add ai @ai-sdk/openai braintrust @opentelemetry/sdk-node @opentelemetry/sdk-trace-base zod\nThen, set up the OpenTelemetry SDK:\nTraceloop\nTo export OTel traces from Traceloop OpenLLMetry to Braintrust, set the following environment variables:\nWhen setting the bearer token, be sure to encode the space between \"Bearer\" and your API key using %20\n.\nTraces will then appear under the Braintrust project or experiment provided in\nthe x-bt-parent\nheader.\nLlamaIndex\nTo trace LLM calls with LlamaIndex, you can use the OpenInference LlamaIndexInstrumentor\nto send OTel traces directly to Braintrust. Configure your environment and set the OTel endpoint:\nNow traced LLM calls will appear under the provided Braintrust project or experiment.\nMastra\nTo use Braintrust with Mastra, configure these environment variables:\nWhen you create your agent, enable telemetry and export the data using OpenTelemetry:\nThis will automatically send interactions, tool calls, and performance metrics to Braintrust for monitoring and evaluation.\nManual tracing\nIf you want to log LLM calls directly to the OTel endpoint, you can set up a custom OpenTelemetry tracer and add the appropriate attributes to your spans. This gives you fine-grained control over what data gets logged.\nBraintrust implements the OpenTelemetry GenAI semantic conventions. When you send traces with these attributes, they are automatically mapped to Braintrust fields.\nYou can also use the braintrust\nnamespace to set fields in Braintrust directly:\nFields mapped from braintrust.*\nattributes are deleted and translated into Braintrust's native format.\nHere's an example of how to set up manual tracing:\nTroubleshooting\nWhy are my traces not showing up?\nThere are a few common reasons why your traces may not show up in Braintrust:\n- Braintrust's logs table only shows traces that have a root span (i.e.\nspan_parents\nis empty). If you only send children spans, they will not appear in the logs table. A common reason for this is only sending spans to Braintrust which have atraceparent\nheader. To fix this, make sure to send a root span for every trace you want to appear in the UI. - If you are self-hosting Braintrust, make sure you do not use\nhttps://api.braintrust.dev\nand instead use your custom API URL as theOTLP_ENDPOINT\n, for examplehttps://dfwhllz61x709.cloudfront.net/otel\n. - You must explicitly set up OpenTelemetry in your application. If you're using Next.js, then follow the Next.js OpenTelemetry guide. If you are using Node.js without a framework, then follow this example to set up a basic exporter.", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/opentelemetry", "anchor": "OpenTelemetry (OTel)"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/vercel-ai-sdk", "anchor": "Vercel AI SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/openai-agents-sdk", "anchor": "OpenAI Agents SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/instructor", "anchor": "Instructor"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/langchain", "anchor": "LangChain"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/langgraph", "anchor": "LangGraph"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/google", "anchor": "Google ADK"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/opentelemetry", "anchor": "OpenTelemetry (OTel)"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/opentelemetry", "anchor": "Python SDK configuration"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/opentelemetry", "anchor": "TypeScript SDK configuration"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Braintrust TypeScript SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/opentelemetry", "anchor": "OTLP configuration"}, {"href": "https://www.braintrust.dev/docs/guides/functions/prompts", "anchor": "prompts"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "playground"}, {"href": "https://www.braintrust.dev/docs/guides/tracing", "anchor": "span slug"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/opentelemetry", "anchor": "Vercel AI SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/opentelemetry", "anchor": "Next.js"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/opentelemetry", "anchor": "Node.js"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/opentelemetry", "anchor": "Traceloop"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/opentelemetry", "anchor": "LlamaIndex"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/opentelemetry", "anchor": "Mastra"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/opentelemetry", "anchor": "Manual tracing"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/opentelemetry", "anchor": "Troubleshooting"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/opentelemetry", "anchor": "Why are my traces not showing up?"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/opentelemetry", "anchor": "OpenTelemetry (OTel)"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/opentelemetry", "anchor": "Python SDK configuration"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/opentelemetry", "anchor": "TypeScript SDK configuration"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/opentelemetry", "anchor": "OTLP configuration"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/opentelemetry", "anchor": "Vercel AI SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/opentelemetry", "anchor": "Next.js"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/opentelemetry", "anchor": "Node.js"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/opentelemetry", "anchor": "Traceloop"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/opentelemetry", "anchor": "LlamaIndex"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/opentelemetry", "anchor": "Mastra"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/opentelemetry", "anchor": "Manual tracing"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/opentelemetry", "anchor": "Troubleshooting"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/opentelemetry", "anchor": "Why are my traces not showing up?"}], "depth": 3}, "https://www.braintrust.dev/docs/start/frameworks/vercel-ai-sdk": {"url": "https://www.braintrust.dev/docs/start/frameworks/vercel-ai-sdk", "title": "Vercel AI SDK - Docs - Start - Braintrust", "text": "Vercel AI SDK\nThe Vercel AI SDK is an elegant tool for building AI-powered applications. Braintrust natively supports tracing requests made with the Vercel AI SDK.\nVercel AI SDK v5 (wrapAISDK)\nwrapAISDK\nwraps the top-level AI SDK functions (generateText\n, streamText\n, generateObject\n, streamObject\n) and automatically creates spans with full input/output logging, metrics, and tool call tracing.\nTool calls with wrapAISDK\nwrapAISDK\nautomatically traces both the LLM's tool call suggestions and the actual tool executions. It supports both the array-based and object-based tools\nformats from the AI SDK:\nVercel AI SDK v4 (model-level wrapper)\nTo wrap individual models, you can use wrapAISDKModel\nwith specific model instances:\nWrapping tools\nWrap tool implementations with wrapTraced\n. Here is a full example, modified from the Node.js Quickstart.\nWhen you run this code, you'll see traces like this in the Braintrust UI:", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/opentelemetry", "anchor": "OpenTelemetry (OTel)"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/vercel-ai-sdk", "anchor": "Vercel AI SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/openai-agents-sdk", "anchor": "OpenAI Agents SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/instructor", "anchor": "Instructor"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/langchain", "anchor": "LangChain"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/langgraph", "anchor": "LangGraph"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/google", "anchor": "Google ADK"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/vercel-ai-sdk", "anchor": "Vercel AI SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/vercel-ai-sdk", "anchor": "Vercel AI SDK v5 (wrapAISDK)"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/vercel-ai-sdk", "anchor": "Tool calls with wrapAISDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/vercel-ai-sdk", "anchor": "Vercel AI SDK v4 (model-level wrapper)"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/vercel-ai-sdk", "anchor": "Wrapping tools"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/vercel-ai-sdk", "anchor": "Vercel AI SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/vercel-ai-sdk", "anchor": "Vercel AI SDK v5 (wrapAISDK)"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/vercel-ai-sdk", "anchor": "Tool calls with wrapAISDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/vercel-ai-sdk", "anchor": "Vercel AI SDK v4 (model-level wrapper)"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/vercel-ai-sdk", "anchor": "Wrapping tools"}], "depth": 3}, "https://www.braintrust.dev/docs/guides/experiments/write": {"url": "https://www.braintrust.dev/docs/guides/experiments/write", "title": "Write evaluations - Docs - Guides - Experiments - Braintrust", "text": "Write evals\nRunning Eval()\ncreates a new experiment in your Braintrust project. There can be multiple eval statements in a single file.\nThe first argument is the name of the project, and the second argument is an object with the following properties:\ndata\n, a function that returns an evaluation dataset: a list of inputs, expected outputs (optional), and metadatatask\n, a function that takes a single input and returns an output (usually an LLM completion)scores\n, a set of scoring functions that take an input, output, and expected output (optional) and return a scoremetadata\nabout the experiment, like the model you're using or configuration valuesexperimentName\na name to use for the experiment. Braintrust will automatically add a unique suffix if this name already exists.\nThe return value of Eval()\nincludes the full results of the eval as well as a summary that you can use to\nsee the average scores, duration, improvements, regressions, and other metrics.\nFor a full list of parameters, see the SDK docs.\nData\nAn evaluation dataset is a list of test cases. Each has an input and optional expected output, metadata, and tags. The key fields in a data record are:\n- Input: The arguments that uniquely define a test case (an arbitrary, JSON serializable object). Braintrust uses the\ninput\nto know whether two test cases are the same between evaluation runs, so the cases should not contain run-specific state. A simple rule of thumb is that if you run the same eval twice, theinput\nshould be identical. - Expected. (Optional) the ground truth value (an arbitrary, JSON serializable object) that you'd compare to\noutput\nto determine if youroutput\nvalue is correct or not. Braintrust currently does not compareoutput\ntoexpected\nfor you, since there are many different ways to do that correctly. For example, you may use a subfield inexpected\nto compare to a subfield inoutput\nfor a certain scoring function. Instead, these values are just used to help you navigate your evals while debugging and comparing results. - Metadata. (Optional) a dictionary with additional data about the test example, model outputs, or just about anything else that's relevant, that you can use\nto help find and analyze examples later. For example, you could log the\nprompt\n, example'sid\n, model parameters, or anything else that would be useful to slice/dice later. - Tags. (Optional) a list of strings that you can use to filter and group records later.\nGetting started\nTo get started with evals, you need some test data. A fine starting point is to write 5-10 examples that you believe are representative. The data must have an input field (which could be complex JSON, or just a string) and should ideally have an expected output field, (although this is not required).\nOnce you have an evaluation set up end-to-end, you can always add more test cases. You'll know you need more data if your eval scores and outputs seem fine, but your production app doesn't look right. And once you have logging set up, your real application data will provide a rich source of examples to use as test cases.\nAs you scale, datasets are a great tool for managing your test cases.\nIt's a common misconception that you need a large volume of perfectly labeled evaluation data, but that's not the case. In practice, it's better to assume your data is noisy, your AI model is imperfect, and your scoring methods are a little bit wrong. The goal of evaluation is to assess each of these components and improve them over time.\nSpecifying an existing dataset in evals\nIn addition to providing inline data examples when you call the Eval()\nfunction, you can also pass an existing or newly initialized dataset.\nScorers\nA scoring function allows you to compare the expected output of a task to the actual output and produce a score\nbetween 0 and 1. You use a scoring function by referencing it in the scores\narray in your eval.\nWe recommend starting with the scorers provided by Braintrust's autoevals library. They work out of the box and will get you up and running quickly. Just like with test cases, once you begin running evaluations, you will find areas that need improvement. This will lead you create your own scorers, customized to your usecases, to get a well rounded view of your application's performance.\nDefine your own scorers\nYou can define your own score, e.g.\nScore using AI (LLM judges)\nYou can also define your own prompt-based scoring functions. For example,\nConditional scoring\nSometimes, the scoring function(s) you want to use depend on the input data. For example, if you're evaluating a chatbot, you might want to use a scoring function that measures whether calculator-style inputs are correctly answered.\nSkip scorers\nReturn null\n/None\nto skip a scorer for a particular test case.\nScores with null\n/None\nvalues will be ignored when computing the overall\nscore, improvements/regressions, and summary metrics like standard deviation.\nHandling scorers on errored test cases\nBy default, eval tasks or scorers that throw an exception will not generate score values.\nThis means you may encounter a computed overall score that shows a higher value than if there were no errored test cases. If you would like to change this behavior,\nyou can pass an unhandled score function to your Eval\ncall. We provide a default handler that logs 0% values\nto any score that doesn't complete successfully.\nList of scorers\nYou can also return a list of scorers from a scorer function. This allows you to dynamically generate scores based on the input data, or even combine scores together into a single score. When you return a list of scores, you must return a Score\nobject, which has a name\nand a score\nfield.\nScorers with additional fields\nCertain scorers, like ClosedQA,\nallow additional fields to be passed in. You can pass them in by initializing them with .partial(...)\n.\nThis approach works well if the criteria is static, but if the criteria is dynamic, you can pass them in via a wrapper function, e.g.\nComposing scorers\nSometimes, it's useful to build scorers that call other scorers. For example, if you're building a translation app,\nyou could reverse translate the output, and use EmbeddingSimilarity\nto compare it to the original input.\nTo compose scorers, call one scorer from another.\nCustom metrics\nSometimes, you need to measure counts or other numbers that cannot be normalized to [0,1]\n. In Braintrust, these are called\nmetrics, and they can be aggregated just like scores, but have less built-in semantic meaning. Braintrust automatically collects\nseveral metrics, like token usage, duration, and error counts, but you can also add your own.\nFor example, to log a metric corresponding to the number of docs retrieved, you can write:\nAggregation\nMetrics can be aggregated within a trace (for example, to report in the experiment table) and across traces (for example, to report their\nperformance at the experiment level). For the most part, metrics are aggregated by sum, for example token counts, but there are some exceptions,\nlike duration\nwhich is the max of metrics.end-metrics.start\nacross spans within a trace.\nAny custom metrics you log will be summed.\nAdditional metadata\nWhile executing the task\nAlthough you can provide metadata\nabout each test case in the data\nfunction, it can be helpful to add additional\nmetadata while your task\nis executing. The second argument to task\nis a hooks\nobject, which allows you to read\nand update metadata on the test case.\nAdding metadata to a scoring function\nTo make it easier to debug logs that do not produce a good score, you may want to log additional values in addition to the output of a scoring function. To do this, you can add a metadata\nfield to the return value of your function, for example:\nExperiment-level metadata\nIt can be useful to add custom metadata to your experiments, for example, to store information about the model or other\nparameters that you use. To set custom metadata, pass a metadata\nfield to your Eval\nblock:\nOnce you set metadata, you can view and filter by it on the Experiments page:\nYou can also construct complex analyses across experiments. See Analyze across experiments for more details.\nUsing custom prompts/functions from Braintrust\nIn addition to writing code directly in your evals, you can also use custom prompts and functions that you host in Braintrust in your code. Use cases include:\n- Running a code-based eval on a prompt that lives in Braintrust.\n- Using a hosted scorer in your evals.\n- Using a scorer written in a different language than your eval code (e.g. calling a Python scorer from a TypeScript eval).\nYou can reference a hosted prompt or scorer by using the initFunction\n/init_function\nfunction.\nTrials\nIt is often useful to run each input in an evaluation multiple times, to get a sense of the variance in\nresponses and get a more robust overall score. Braintrust supports trials as a first-class concept, allowing\nyou to run each input multiple times. Behind the scenes, Braintrust will intelligently aggregate the results\nby bucketing test cases with the same input\nvalue and computing summary statistics for each bucket.\nTo enable trials, add a trialCount\n/trial_count\nproperty to your evaluation:\nHill climbing\nSometimes you do not have expected outputs, and instead want to use a previous experiment as a baseline. Hill climbing is inspired by, but not exactly the same as, the term used in numerical optimization. In the context of Braintrust, hill climbing is a way to iteratively improve a model's performance by comparing new experiments to previous ones. This is especially useful when you don't have a pre-existing benchmark to evaluate against.\nBraintrust supports hill climbing as a first-class concept, allowing you to use a previous experiment's output\nfield as the expected\nfield for the current experiment. Autoevals also includes a number of scoreres, like\nSummary\nand Battle\n, that are designed to work well with hill climbing.\nTo enable hill climbing, use BaseExperiment()\nin the data\nfield of an eval:\nThat's it! Braintrust will automatically pick the best base experiment, either using git metadata if available or\ntimestamps otherwise, and then populate the expected\nfield by merging the expected\nand output\nfield of the base experiment. This means that if you set expected\n, e.g. through the UI while reviewing results,\nit will be used as the expected\nfield for the next experiment.\nUsing a specific experiment\nIf you want to use a specific experiment as the base experiment, you can pass the name\nfield to BaseExperiment()\n:\nScoring considerations\nOften while hill climbing, you want to use two different types of scoring functions:\n- Methods that do not require an expected output, e.g.\nClosedQA\n, so that you can judge the quality of the output purely based on the input and output. This measure is useful to track across experiments, and it can be used to compare any two experiments, even if they are not sequentially related. - Comparative methods, e.g.\nBattle\norSummary\n, that accept anexpected\noutput but do not treat it as a ground truth. Generally speaking, if you score > 50% on a comparative method, it means you're doing better than the base on average. To learn more about howBattle\nandSummary\nwork, check out their prompts.\nCustom reporters\nWhen you run an experiment, Braintrust logs the results to your terminal, and braintrust eval\nreturns a non-zero exit code if any eval throws an exception. However, it's often useful to customize this behavior, e.g. in your CI/CD pipeline to precisely define what constitutes a failure, or to report results to a different system.\nBraintrust allows you to define custom reporters that can be used to process and log results anywhere you'd like. You can define a reporter by adding a Reporter(...)\nblock. A Reporter has two functions:\nAny Reporter\nincluded among your evaluated files will be automatically picked up by the braintrust eval\ncommand.\n- If no reporters are defined, the default reporter will be used which logs the results to the console.\n- If you define one reporter, it'll be used for all\nEval\nblocks. - If you define multiple\nReporter\ns, you have to specify the reporter name as an optional 3rd argument toEval()\n.\nExample: the default reporter\nAs an example, here's the default reporter that Braintrust uses:\nAttachments\nBraintrust allows you to log arbitrary binary data, like images, audio, and\nPDFs, as attachments. The easiest way\nto use attachments in your evals is to initialize an Attachment\nobject in your\ndata.\nYou can also store attachments in a dataset for reuse across multiple experiments. After creating the dataset, you can use it by name in an eval. Upon access, the attachment data will be automatically downloaded from Braintrust.\nYou can also obtain a signed URL for the attachment for forwarding to other services, such as OpenAI.\nTracing\nBraintrust allows you to trace detailed debug information and metrics about your application that you can use to measure performance and debug issues. The trace is a tree of spans, where each span represents an expensive task, e.g. an LLM call, vector database lookup, or API request.\nIf you are using the OpenAI API, Braintrust includes a wrapper function that\nautomatically logs your requests. To use it, call\nwrapOpenAI/wrap_openai\non your OpenAI instance. See Wrapping\nOpenAI\nfor more info.\nEach call to experiment.log()\ncreates its own trace, starting at the time of\nthe previous log statement and ending at the completion of the current. Do not\nmix experiment.log()\nwith tracing. It will result in extra traces that are\nnot correctly parented.\nFor more detailed tracing, you can wrap existing code with the\nbraintrust.traced\nfunction. Inside the wrapped function, you can log\nincrementally to braintrust.currentSpan()\n. For example, you can progressively\nlog the input, output, and expected output of a task, and then log a score at the\nend:\nThis results in a span tree you can visualize in the UI by clicking on each test case in the experiment:\nLogging SDK\nThe SDK allows you to report evaluation results directly from your code, without using the Eval()\nor .traced()\nfunctions.\nThis is useful if you want to structure your own complex evaluation logic, or integrate Braintrust with an\nexisting testing or evaluation framework.\nRefer to the tracing guide for examples of how to trace evaluations using the low-level SDK. For more details on how to use the low level SDK, see the Python or Node.js documentation.\nTroubleshooting\nException when mixing log\nwith traced\nThere are two ways to log to Braintrust: Experiment.log\nand\nExperiment.traced\n. Experiment.log\nis for non-traced logging, while\nExperiment.traced\nis for tracing. This exception is thrown when you mix both\nmethods on the same object, for instance:\nMost of the time, you should use either Experiment.log\nor Experiment.traced\n,\nbut not both, so the SDK throws an error to prevent accidentally mixing them\ntogether. For the above example, you most likely want to write:\nIn rare cases, if you are certain you want to mix traced and\nnon-traced logging on the same object, you may pass the argument\nallowConcurrentWithSpans: true\n/allow_concurrent_with_spans=True\nto\nExperiment.log\n.\nLocal evaluation without sending logs to Braintrust\nYou can also run evaluations locally without creating experiments or sending data to Braintrust. In TypeScript, use the noSendLogs\nparameter. In Python, use the no_send_logs\nparameter.\nWhen you set the parameter to true, the evaluation will:\n- Run all tasks and scorers locally\n- Generate a local summary of results\n- Not create an experiment in Braintrust\n- Not send any data to the Braintrust servers\nAccessing results from local evaluation\nWhen running locally, you can access the detailed results and summary from the returned object:\nThis is equivalent to passing the --no-send-logs\nflag when using the CLI command braintrust eval\n.\nOnline evaluation\nAlthough you can log scores from your application, it can be awkward and computationally intensive to run evals code in your production environment. To solve this, Braintrust supports server-side online evaluations that are automatically run asynchronously as you upload logs. You can pick from the pre-built autoevals functions or your custom scorers, and define a sampling rate along with more granular filters to control which logs get evaluated.\nConfiguring online evaluation\nTo create an online evaluation, navigate to the Configuration tab in a project and create an online scoring rule.\nThe score will now automatically run at the specified sampling rate for all logs in the project.\nNote that online scoring will only be activated once a span has been fully\nlogged. We detect this by checking for the existence of a metrics.end\ntimestamp on the span, which is written automatically by the SDK when the span\nis finished.\nIf you are logging through a different means, such as the REST API or any of our\nAPI wrappers, you will have to explicitly\ninclude metrics.end\nas a Unix timestamp (we also suggest metrics.start\n) in\norder to activate online scoring.\nDefining custom scoring logic\nIn addition to the pre-built autoevals, you can define your own custom scoring logic by creating custom scorers. Currently, you can do that by visiting the Playground and creating custom scorers.", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "Write evals"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/run", "anchor": "Run evals"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/interpret", "anchor": "Interpret evals"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "Write evals"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/Evaluator", "anchor": "SDK docs"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "Data"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "Getting started"}, {"href": "https://www.braintrust.dev/docs/guides/logging", "anchor": "logging"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "datasets"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "Specifying an existing dataset in evals"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "pass an existing or newly initialized dataset"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "Scorers"}, {"href": "https://www.braintrust.dev/docs/autoevals", "anchor": "autoevals library"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "Define your own scorers"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "Score using AI (LLM judges)"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "Conditional scoring"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "Skip scorers"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "Handling scorers on errored test cases"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "List of scorers"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "Scorers with additional fields"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "Composing scorers"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "Custom metrics"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "Aggregation"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "Additional metadata"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "While executing the task"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "Adding metadata to a scoring function"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "Experiment-level metadata"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/interpret", "anchor": "Analyze across experiments"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "Using custom prompts/functions from Braintrust"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "Trials"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "Hill climbing"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "Custom reporters"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "attachments"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "store attachments in a dataset"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "Tracing"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "Logging SDK"}, {"href": "https://www.braintrust.dev/docs/guides/tracing", "anchor": "tracing"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "Python"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Node.js"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "Troubleshooting"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "Exception when mixing log with traced"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "Local evaluation without sending logs to Braintrust"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "Accessing results from local evaluation"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "Online evaluation"}, {"href": "https://www.braintrust.dev/docs/reference/autoevals", "anchor": "autoevals"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "Configuring online evaluation"}, {"href": "https://www.braintrust.dev/docs/reference/api", "anchor": "API wrappers"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "Defining custom scoring logic"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playground"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "Write evals"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "Data"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "Getting started"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "Specifying an existing dataset in evals"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "Scorers"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "Define your own scorers"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "Score using AI (LLM judges)"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "Conditional scoring"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "Skip scorers"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "Handling scorers on errored test cases"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "List of scorers"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "Scorers with additional fields"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "Composing scorers"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "Custom metrics"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "Aggregation"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "Additional metadata"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "While executing the task"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "Adding metadata to a scoring function"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "Experiment-level metadata"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "Using custom prompts/functions from Braintrust"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "Trials"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "Hill climbing"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "Custom reporters"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "Tracing"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "Logging SDK"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "Troubleshooting"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "Exception when mixing log with traced"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "Local evaluation without sending logs to Braintrust"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "Accessing results from local evaluation"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "Online evaluation"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "Configuring online evaluation"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "Defining custom scoring logic"}], "depth": 3}, "https://www.braintrust.dev/docs/guides/experiments/run": {"url": "https://www.braintrust.dev/docs/guides/experiments/run", "title": "Run evaluations - Docs - Guides - Experiments - Braintrust", "text": "Run evals\nBraintrust allows you to create evaluations directly in your code, and run them in your development workflow\nor CI/CD pipeline. Once you have defined one or more evaluations, you can run them using the braintrust eval\ncommand. This command will run all evaluations in the specified files and directories. As they run, they will automatically\ncreate experiments in Braintrust and display a summary in your terminal.\nThe braintrust eval\ncommand uses the Next.js convention to load environment variables from:\nenv.development.local\n.env.local\nenv.development\n.env\nWatch mode\nYou can run evaluations in watch-mode by passing the --watch\nflag. This will re-run evaluations whenever any of\nthe files they depend on change.\nDev mode\nYou can expose an Eval\nrunning at a remote URL or your local machine by passing the --dev\nflag. For more information, check out the remote evals guide.\nLocal testing mode\nPass the --no-send-logs\nflag to run evaluations locally without sending logs to Braintrust. This is useful for testing scorers during development without uploading results to your Braintrust project.\nGithub action\nOnce you get the hang of running evaluations, you can integrate them into your CI/CD pipeline to automatically run them on every pull request or commit. This workflow allows you to catch eval regressions early and often.\nThe braintrustdata/eval-action\naction allows you to run\nevaluations directly in your Github workflow. Each time you run an evaluation, the action automatically posts\na comment:\nTo use the action, include it in a workflow yaml file (.github/workflows\n):\nYou must specify permissions\nfor the action to leave comments on your PR.\nWithout these permissions, you'll see Github API errors.\nFor more information, see the braintrustdata/eval-action\nREADME, or check\nout full workflow files in the examples directory.\nThe braintrustdata/eval-action\nGitHub action does not currently support\ncustom reporters. If you use custom reporters, you'll need to run the\nbraintrust eval\ncommand directly in your CI/CD pipeline.\nRun code directly\nAlthough you can invoke Eval()\nfunctions via the braintrust eval\ncommand, you can also call them directly in your code.\nIn TypeScript, Eval()\nis an async function that returns a Promise\n. You can run Eval()\ns concurrently\nand wait for all of them to finish using Promise.all()\n.\nLimiting concurrency\nIf you are writing asynchronous code (TypeScript or asynchronous Python), then Braintrust will automatically run each dataset row concurrently. This optimizes for speed, but you can run into errors if your LLM's rate limits are too low. In this case, maxConcurrency\n/max_concurrency\nallows you to constrain concurrency and avoid rate limits.\nIf you're using synchronous Python, Braintrust runs tasks on a thread pool, whose size is defaulted to the number of CPU cores. The max_concurrency\nparameter will still be respected, but global max concurrency will be bounded by the size of this thread pool. You can use the set_thread_pool_max_workers\nfunction to adjust the thread pool size and achieve more parallelism.\nBoth the task function and scoring functions respect the max concurrency limit.\nConcurrency performance and costs\nConcurrency can significantly improve evaluation speed, especially when your tasks involve:\n- API calls to language models or other external services\n- Network requests or database queries\n- I/O operations like file reading\nBy running multiple test cases concurrently, you can reduce total evaluation time since tasks can execute in parallel while waiting for external responses.\nHowever, higher concurrency can increase costs in several ways:\n- Rate limits: Many API providers (like OpenAI and Anthropic) have rate limits. Exceeding these can result in throttling, errors, or additional charges.\n- Resource usage: More concurrent operations consume more memory, CPU, and network bandwidth\n- External service costs: Some services charge based on concurrent connections or have tiered pricing for higher throughput\nTroubleshooting\nStack traces\nBy default, the evaluation framework swallows errors in individual tasks, reports them to Braintrust,\nand prints a single line per error to the console. If you want to see the full stack trace for each\nerror, you can pass the --verbose\nflag.\nWhy are my scores getting averaged?\nBraintrust organizes your data into traces, each of which is a row in the experiments table. Within a trace, if you log the same score multiple times, it will be averaged in the table. This is a useful way to collect an overall measurement, e.g. if you compute the relevance of each retrieved document in a RAG use case, and want to see the overall relevance. However, if you want to see each score individually, you have a few options:\n- Split the input into multiple independent traces, and log each score in a separate trace. The trials feature will naturally average the results at the top-level, but allow you to view each individual output as a separate test case.\n- Compute a separate score for each instance. For example, if you have exactly 3 documents you retrieve every time, you may want to compute a separate score for the 1st, 2nd, and 3rd position.\n- Create separate experiments for each thing you're trying to score. For example, you may want to try out two different models and compute a score for each. In this case, if you split into separate experiments, you'll be able to diff across experiments and compare outputs side-by-side.\nNode bundling errors (e.g. \"cannot be marked as external\")\nThe .eval.ts\nfiles are bundled in a somewhat limiting way, via esbuild\nand a special set of\nbuild options that work in most cases, but not all. For example, if you have any export\nstatements\nin them, you may see errors like \"cannot be marked as external\".\nYou can usually fix this specific error by removing export\nstatements. However, if that does not work,\nor you want more control over how the files are bundled, you can also just run the files directly.\nEval\nis an async function, so you can just call it directly in a script:", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "Write evals"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/run", "anchor": "Run evals"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/interpret", "anchor": "Interpret evals"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/run", "anchor": "Run evals"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/run", "anchor": "Watch mode"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/run", "anchor": "Dev mode"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "remote evals guide"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/run", "anchor": "Local testing mode"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/run", "anchor": "Github action"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/run", "anchor": "Full example"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/run", "anchor": "Run code directly"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/run", "anchor": "Limiting concurrency"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/run", "anchor": "Concurrency performance and costs"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/run", "anchor": "Troubleshooting"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/run", "anchor": "Stack traces"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/run", "anchor": "Why are my scores getting averaged?"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/run", "anchor": "trials"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/run", "anchor": "Node bundling errors (e.g. \"cannot be marked as external\")"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/run", "anchor": "Run evals"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/run", "anchor": "Watch mode"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/run", "anchor": "Dev mode"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/run", "anchor": "Local testing mode"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/run", "anchor": "Github action"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/run", "anchor": "Full example"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/run", "anchor": "Full example"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/run", "anchor": "Run code directly"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/run", "anchor": "Limiting concurrency"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/run", "anchor": "Concurrency performance and costs"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/run", "anchor": "Troubleshooting"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/run", "anchor": "Stack traces"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/run", "anchor": "Why are my scores getting averaged?"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/run", "anchor": "Node bundling errors (e.g. \"cannot be marked as external\")"}], "depth": 3}, "https://www.braintrust.dev/docs/guides/experiments/interpret": {"url": "https://www.braintrust.dev/docs/guides/experiments/interpret", "title": "Visualize and interpret evaluations - Docs - Guides - Experiments - Braintrust", "text": "Visualize and interpret eval results\nView results in the UI\nRunning an eval from the API or SDK will return a link to the corresponding experiment in Braintrust's UI. When you open the link, you'll land on a detailed view of the eval run that you selected. The detailed view includes:\n- Diff mode toggle - Allows you to compare eval runs to each other. If you click the toggle, you will see the results of your current eval compared to the results of the baseline.\n- Filter bar - Allows you to focus in on a subset of test cases. You can filter by typing natural language or BTQL.\n- Column visibility - Allows you to toggle column visibility. You can also order columns by regressions to hone in on problematic areas.\n- Table - Shows the data for every test case in your eval run.\nExperiment summaries\nWhen you select an experiment, you'll get a summary of the comparisons, scorers, datasets, and metadata.\nYou can also view and copy the experiment ID from the bottom of the summary pane.\nTable header summaries\nSummaries will appear for score and metric columns. To find test cases to focus on, use column header summaries to filter by improvements or regressions (test cases that decreased in score). This allows you to see the scorers with the biggest issues. You can also group the table to view summaries across metadata fields or inputs. For example, if you use separate datasets for distinct types of usecases, you can group by dataset to see which usecases are having the biggest issues.\nGroup summaries\nBy default, group rows will show one experiment's summary data, and you can switch between them by selecting your desired aggregation.\nIf you would like to view the summary data for all experiments, select Include comparisons in group.\nWithin a grouped table, you can also sort rows by regressions of a specific score relative to a comparison experiment.\nNow that you've narrowed your test cases, you can view a test case in detail by selecting a row.\nTrace view\nSelecting a row will open the trace view. Here you can see all of the data for the trace for this test case, including input, output, metadata, and metrics for each span inside the trace.\nLook at the scores and the output and decide whether the scores seem \"right\". Do good scores correspond to a good output? If not, you'll want to improve your evals by updating scorers or test cases.\nCreate custom columns\nYou can create custom columns to extract values from the root span. To do this, use the Add custom column option at the bottom of the Columns dropdown or select the + icon at the end of the table headers.\nAfter naming your custom column, you can either choose from the inferred fields in the dropdown or enter a custom BTQL statement.\nOnce created, you can filter and sort the table using your custom columns.\nInterpreting results\nHow metrics are calculated\nAlong with the scores you track, Braintrust tracks a number of metrics about your LLM calls that help you assess and understand performance. For example, if you're trying to figure out why the average duration increased substantially when you change a model, it's useful to look at both duration and token metrics to diagnose the underlying issue.\nWherever possible, metrics are computed on the task\nsubspan, so that LLM-as-a-judge calls are excluded. Specifically:\nDuration\nis the duration of the\"task\"\nspan.Offset\nis the time elapsed since the trace start time.Prompt tokens\n,Completion tokens\n,Total tokens\n,LLM duration\n, andEstimated LLM cost\nare averaged over every span that is not marked withspan_attributes.purpose = \"scorer\"\n, which is set automatically in autoevals.\nIf you are using the logging SDK, or API, you will need to follow these conventions to ensure that metrics are computed correctly.\nTo compute LLM metrics (like token counts), make sure you wrap your LLM calls.\nDiff mode\nWhen you run multiple experiments, Braintrust will automatically compare the results of experiments to each other. This allows you to quickly see which test cases improved or regressed across experiments.\nYou can also select any individual row in an experiment to see diffs for each field in a span.\nHow rows are matched\nBy default, Braintrust considers two test cases to be the same if they have the same input\nfield. This is used both to match test cases across experiments\nand to bucket equivalent cases together in a trial.\nViewing data across trials\nTo group by trials, or multiple rows with the same input\nvalue, select Input from the Group dropdown menu.\nThis will consolidate each trial for a given input and display aggregate data, showing comparisons for each unique input across all experiments.\nIf Braintrust detects that any rows have the same input\nvalue within the same experiment, diff mode will show a Trials column where you can select matching trials in your comparison experiments.\nYou can also step through the relevant trial rows in your comparison experiment by selecting a specific trace.\nCustomizing the comparison key\nHowever, sometimes your input\nmay include additional data, and you need to use a different\nexpression to match test cases. You can configure the comparison key in your project's Configuration page.\nExperiment view layouts\nGrid layout\nWhen you run multiple experiments, you can also compare experiment outputs side-by-side in the table by selecting the Grid layout. In the grid layout, select which fields to display in cells by selecting from the Fields dropdown menu.\nSummary layout\nThe Summary layout summarizes scores and metrics across the base experiment and all comparison experiments, in a reporting-friendly format with large type. Both summary and grid layouts respect all view filters.\nAggregate (weighted) scores\nIt's often useful to compute many, even hundreds, of scores in your experiments, but when reporting on an experiment, or comparing experiments over time, it's often useful to have a single score that represents the experiment as a whole.\nBraintrust allows you to do this with aggregate scores, which are formulas that combine multiple scores. To create an aggregate score, go to your project's Configuration page, and select Add aggregate score.\nBraintrust currently supports three types of aggregate scores:\n- Weighted average - A weighted average of selected scores.\n- Minimum - The minimum value among the selected scores.\n- Maximum - The maximum value among the selected scores.\nAnalyze across experiments\nBraintrust allows you to analyze data across experiments to, for example, compare the performance of different models.\nBar chart\nOn the Experiments page, you can view your scores as a bar chart by selecting Score comparison from the X axis selector:\nYou can also select the metadata fields you want to group by to create bar charts:\nScatter plot\nSelect a metric on the x-axis to construct a scatter plot. Here's an example comparing the relationship between accuracy and duration.\nExport experiments\nUI\nTo export an experiment's results, open the menu next to the experiment name. You can export as CSV\nor JSON\n, and choose if you'd like to download all fields.\nAPI\nTo fetch the events in an experiment via the API, see Fetch experiment (POST form) or Fetch experiment (GET form).\nSDK\nIf you need to access the data from a previous experiment, you can pass the open\nflag into\ninit()\nand then just iterate through the experiment object:\nYou can use the the asDataset()\n/as_dataset()\nfunction to automatically convert the experiment into the same\nfields you'd use in a dataset (input\n, expected\n, and metadata\n).\nFor a more advanced overview of how to reuse experiments as datasets, see Hill climbing.", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "Write evals"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/run", "anchor": "Run evals"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/interpret", "anchor": "Interpret evals"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/interpret", "anchor": "Visualize and interpret eval results"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/interpret", "anchor": "View results in the UI"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "BTQL"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/interpret", "anchor": "Experiment summaries"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/interpret", "anchor": "Table header summaries"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/interpret", "anchor": "Group summaries"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/interpret", "anchor": "Trace view"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "scorers"}, {"href": "https://www.braintrust.dev/blog/eval-feedback-loops", "anchor": "test cases"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/interpret", "anchor": "Create custom columns"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "BTQL"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/interpret", "anchor": "Interpreting results"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/interpret", "anchor": "How metrics are calculated"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "wrap your LLM calls"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/interpret", "anchor": "Diff mode"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/interpret", "anchor": "How rows are matched"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "trial"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/interpret", "anchor": "Viewing data across trials"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "trials"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/interpret", "anchor": "Customizing the comparison key"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/interpret", "anchor": "Experiment view layouts"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/interpret", "anchor": "Grid layout"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/interpret", "anchor": "Summary layout"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/interpret", "anchor": "Aggregate (weighted) scores"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/interpret", "anchor": "Analyze across experiments"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/interpret", "anchor": "Bar chart"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/interpret", "anchor": "Scatter plot"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/interpret", "anchor": "Export experiments"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/interpret", "anchor": "UI"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/interpret", "anchor": "API"}, {"href": "https://www.braintrust.dev/docs/api/spec", "anchor": "Fetch experiment (POST form)"}, {"href": "https://www.braintrust.dev/docs/api/spec", "anchor": "Fetch experiment (GET form)"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/interpret", "anchor": "SDK"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "Hill climbing"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/interpret", "anchor": "Visualize and interpret eval results"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/interpret", "anchor": "View results in the UI"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/interpret", "anchor": "Experiment summaries"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/interpret", "anchor": "Table header summaries"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/interpret", "anchor": "Group summaries"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/interpret", "anchor": "Trace view"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/interpret", "anchor": "Create custom columns"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/interpret", "anchor": "Interpreting results"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/interpret", "anchor": "How metrics are calculated"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/interpret", "anchor": "Diff mode"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/interpret", "anchor": "How rows are matched"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/interpret", "anchor": "Viewing data across trials"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/interpret", "anchor": "Customizing the comparison key"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/interpret", "anchor": "Experiment view layouts"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/interpret", "anchor": "Grid layout"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/interpret", "anchor": "Summary layout"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/interpret", "anchor": "Aggregate (weighted) scores"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/interpret", "anchor": "Analyze across experiments"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/interpret", "anchor": "Bar chart"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/interpret", "anchor": "Scatter plot"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/interpret", "anchor": "Export experiments"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/interpret", "anchor": "UI"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/interpret", "anchor": "API"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/interpret", "anchor": "SDK"}], "depth": 3}, "https://www.braintrust.dev/docs/reference/api/ServiceTokens": {"url": "https://www.braintrust.dev/docs/reference/api/ServiceTokens", "title": "Service tokens - Docs - Reference - Braintrust", "text": "List service_tokens\nList out all service_tokens. The service_tokens are sorted by creation date, with the most recently-created service_tokens coming first\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nQuery Parameters\nlimit\nintegerLimit the number of objects to return\n0\nstarting_after\nstringPagination cursor id.\nFor example, if the final item in the last page you fetched had an id of foo\n, pass starting_after=foo\nto fetch the next page. Note: you may only pass one of starting_after\nand ending_before\n\"uuid\"\nending_before\nstringPagination cursor id.\nFor example, if the initial item in the last page you fetched had an id of foo\n, pass ending_before=foo\nto fetch the previous page. Note: you may only pass one of starting_after\nand ending_before\n\"uuid\"\nids\nAny properties in string, array<string>Filter search results to a particular set of object IDs. To specify a list of IDs, include the query param multiple times\nservice_token_name\nstringName of the service_token to search for\norg_name\nstringFilter search results to within a particular organization\nReturns a list of service_token objects\nCreate service_token\nCreate a new service_token. It is possible to have multiple API keys with the same name. There is no de-duplication\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nRequest Body\napplication/json\nOptionalAny desired information about the new service_token object\nname\nName of the service token. Does not have to be unique\norg_name\nstringFor nearly all users, this parameter should be unnecessary. But in the rare case that your API key belongs to multiple organizations, you may specify the name of the organization the Service token belongs in.\nservice_account_id\nThe service account ID this service token should belong to.\nReturns an object containing the raw service token. This is the only time the raw API key will be exposed\nGet service_token\nGet a service_token object by its id\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nPath Parameters\nservice_token_id\nServiceToken id\n\"uuid\"\nReturns the service_token object\nDelete service_token\nDelete a service_token object by its id\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nPath Parameters\nservice_token_id\nServiceToken id\n\"uuid\"\nReturns the deleted service_token object", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/reference/api", "anchor": "API"}, {"href": "https://www.braintrust.dev/docs/reference/api/Acls", "anchor": "Acls"}, {"href": "https://www.braintrust.dev/docs/reference/api/AiSecrets", "anchor": "Ai Secrets"}, {"href": "https://www.braintrust.dev/docs/reference/api/ApiKeys", "anchor": "Api Keys"}, {"href": "https://www.braintrust.dev/docs/reference/api/CrossObject", "anchor": "Cross Object"}, {"href": "https://www.braintrust.dev/docs/reference/api/Datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/reference/api/EnvVars", "anchor": "Env Vars"}, {"href": "https://www.braintrust.dev/docs/reference/api/Evals", "anchor": "Evals"}, {"href": "https://www.braintrust.dev/docs/reference/api/Experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/reference/api/Functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/reference/api/Groups", "anchor": "Groups"}, {"href": "https://www.braintrust.dev/docs/reference/api/Logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/reference/api/Organizations", "anchor": "Organizations"}, {"href": "https://www.braintrust.dev/docs/reference/api/Other", "anchor": "Other"}, {"href": "https://www.braintrust.dev/docs/reference/api/Projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectScores", "anchor": "Project Scores"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectTags", "anchor": "Project Tags"}, {"href": "https://www.braintrust.dev/docs/reference/api/Prompts", "anchor": "Prompts"}, {"href": "https://www.braintrust.dev/docs/reference/api/Proxy", "anchor": "Proxy"}, {"href": "https://www.braintrust.dev/docs/reference/api/Roles", "anchor": "Roles"}, {"href": "https://www.braintrust.dev/docs/reference/api/ServiceTokens", "anchor": "Service tokens"}, {"href": "https://www.braintrust.dev/docs/reference/api/SpanIframes", "anchor": "Span Iframes"}, {"href": "https://www.braintrust.dev/docs/reference/api/Users", "anchor": "Users"}, {"href": "https://www.braintrust.dev/docs/reference/api/Views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/reference/autoevals", "anchor": "Autoevals library"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "BTQL query syntax"}, {"href": "https://www.braintrust.dev/docs/reference/streaming", "anchor": "Streaming"}, {"href": "https://www.braintrust.dev/docs/reference/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/reference/reasoning", "anchor": "Reasoning"}, {"href": "https://www.braintrust.dev/docs/reference/organizations", "anchor": "Organizations"}, {"href": "https://www.braintrust.dev/docs/reference/mcp", "anchor": "Model Context Protocol (MCP)"}, {"href": "https://www.braintrust.dev/docs/reference/object-links", "anchor": "Sharing via URL"}, {"href": "https://www.braintrust.dev/docs/reference/glossary", "anchor": "Glossary"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/reference/api", "anchor": "API"}, {"href": "https://www.braintrust.dev/docs/reference/api/ServiceTokens", "anchor": "Service tokens"}, {"href": "https://www.braintrust.dev/docs/reference/api/ServiceTokens", "anchor": "List service_tokens"}, {"href": "https://www.braintrust.dev/docs/reference/api/ServiceTokens", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/ServiceTokens", "anchor": "Query Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/api/ServiceTokens", "anchor": "Create service_token"}, {"href": "https://www.braintrust.dev/docs/reference/api/ServiceTokens", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/ServiceTokens", "anchor": "Request Body"}, {"href": "https://www.braintrust.dev/docs/reference/api/ServiceTokens", "anchor": "Get service_token"}, {"href": "https://www.braintrust.dev/docs/reference/api/ServiceTokens", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/ServiceTokens", "anchor": "Path Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/api/ServiceTokens", "anchor": "Delete service_token"}, {"href": "https://www.braintrust.dev/docs/reference/api/ServiceTokens", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/ServiceTokens", "anchor": "Path Parameters"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}, {"href": "https://www.braintrust.dev/docs/reference/api/ServiceTokens", "anchor": "List service_tokens"}, {"href": "https://www.braintrust.dev/docs/reference/api/ServiceTokens", "anchor": "Create service_token"}, {"href": "https://www.braintrust.dev/docs/reference/api/ServiceTokens", "anchor": "Get service_token"}, {"href": "https://www.braintrust.dev/docs/reference/api/ServiceTokens", "anchor": "Delete service_token"}], "depth": 3}, "https://www.braintrust.dev/docs/guides/traces/customize": {"url": "https://www.braintrust.dev/docs/guides/traces/customize", "title": "Customize traces - Docs - Guides - Braintrust", "text": "Customize traces\nYou can customize how you trace to better understand how your application runs and make it easier to find and fix problems. By adjusting how you collect and manage trace data, you can better track complex processes, monitor systems that work across multiple services, and debug issues more effectively.\nAnnotating your code\nYou can add traces for multiple, specific functions in your code to your logs by annotating them with functional wrappers (TypeScript) or decorators and context managers (Python):\nWrapping LLM clients\nWrapping OpenAI\nFor information about how to wrap OpenAI clients, see the OpenAI provider docs.\nWrapping Anthropic\nFor information about how to wrap Anthropic clients, see the Anthropic provider docs.\nWrapping a custom LLM client\nIf you're using your own client, you can wrap it yourself using the same conventions as the OpenAI wrapper. Feel free to check out the Python and TypeScript implementations for reference.\nTo track the span as an LLM, you must:\n- Specify the\ntype\nasllm\n. You can specify anyname\nyou'd like. This enables LLM duration metrics. - Add\nprompt_tokens\n,completion_tokens\n, andtokens\nto themetrics\nfield. This enables LLM token usage metrics. - To track cached tokens, you can log\nprompt_cached_tokens\n(cache reads) andprompt_cache_creation_tokens\n(cache writes) tometrics\n. By convention,prompt_tokens\nshould include bothprompt_cached_tokens\nandprompt_cache_creation_tokens\n. So if you have a request with 10 cache read tokens, 5 cache write tokens, and 3 uncached tokens, you should logprompt_tokens: 18\n. - Format the\ninput\nas a list of messages (using the OpenAI format), and put other parameters (likemodel\n) inmetadata\n. This enables the \"Try prompt\" button in the UI.\nMultimodal content\nUploading attachments\nIn addition to text and structured data, Braintrust also supports uploading file attachments (blobs). This is especially useful when working with multimodal models, which can require logging large image, audio, or video files. You can also use attachments to log other unstructured data related to your LLM usage, such as a user-provided PDF file that your application later transforms into an LLM input.\nTo upload an attachment, create a new Attachment\nobject to represent the file\non disk or binary data in memory to be uploaded. You can place Attachment\nobjects anywhere in the event to be logged, including in arrays/lists or deeply\nnested in objects. See the TypeScript or Python SDK\nreference for usage details.\nThe SDK uploads the attachments separately from other parts of the log, so the presence of attachments doesn't affect non-attachment logging latency.\nImage, audio, video, and PDF attachments can be previewed in Braintrust. All attachments can be downloaded for viewing locally.\nUsing external files as attachments\nBraintrust also supports references to files in external object stores with\nthe ExternalAttachment\nobject. You can use this anywhere you would use an\nAttachment\n. See the Attachments guide for more\ninformation.\nLinking to external images\nTo log an external image, provide an image URL, an external object store URL, or a base64 encoded image as a string. The tree viewer will automatically render the image.\nThe tree viewer will look at the URL or string to determine if it is an image. If you want to force the viewer to treat it as an image, then nest it in an object like\nand the viewer will render it as an image. Base64 images must be rendered in URL format, just like the OpenAI API. For example:\nIf your image's URL does not have a recognized file extension, it may not get rendered as an image automatically. In this case, you can use an inline attachment to force it to be rendered as an image.\nErrors\nWhen you run:\n- Python code inside of the\n@traced\ndecorator or within astart_span()\ncontext - TypeScript code inside of\ntraced\n(or awrappedTraced\nfunction)\nBraintrust will automatically log any exceptions that occur within the span.\nUnder the hood, every span has an error\nfield which you can also log to directly.\nDeeply nested code\nOften, you want to trace functions that are deep in the call stack, without\nhaving to propagate the span\nobject throughout. Braintrust uses async-friendly\ncontext variables to make this workflow easy:\n- The\ntraced\nfunction/decorator will create a span underneath the currently-active span. - The\ncurrentSpan()\n/current_span()\nmethod returns the currently active span, in case you need to do additional logging.\nMasking sensitive data\nYou can configure a global masking function to redact sensitive information before it's sent to Braintrust. The masking function is applied to the input\n, output\n, expected\n, metadata\n, and context\nfields of each logged event.\nMasking API keys and passwords\nThis example shows how to mask common sensitive fields like API keys, passwords, and tokens in your data:\nMasking personally identifiable information (PII)\nThis example demonstrates masking PII such as email addresses, phone numbers, and social security numbers:\nCustom masking for specific data structures\nThis example shows how to handle custom data structures and implement selective masking based on context:\nConsiderations\n- The masking function is applied globally and affects all logging across your application\n- Masking is applied after events are merged but before they are sent to Braintrust\n- The masking function can modify data in place, which is often more performant\n- Only the fields\ninput\n,output\n,expected\n,metadata\n, andcontext\nare passed to the masking function - Always test your masking function thoroughly to ensure sensitive data is properly redacted\n- In the event of an error, the data will be masked with a generic message like\nERROR: Failed to mask field\nto avoid leaking sensitive information\nDistributed tracing\nSometimes it's useful to be able to start a trace in one process and continue it\nin a different one. For this purpose, Braintrust provides an export\nfunction\nwhich returns an opaque string identifier. This identifier can be passed to\nstart_span\nto resume the trace elsewhere. Consider the following example of\ntracing across separate client and server processes.\nClient code\nServer code\nUpdating spans\nSimilar to distributed tracing, it can be useful to update spans after you initially log them. For example, if you collect the output of a span asynchronously.\nThe Experiment\nand Logger\nclasses each have an updateSpan()\nmethod, which you can call with\nthe span's id to perform an update:\nYou can also use span.export()\nto export the span in a fully contained string, which is useful if you\nhave multiple loggers or perform the update from a different service.\nIt's important to make sure the update happens after the original span has been logged, otherwise they can trample on each other.\nDistributed tracing is designed specifically to prevent this edge case, and instead works by logging a new (sub) span.\nDeep-linking to spans\nThe Span.permalink\nmethod formats a permalink to the Braintrust application\nfor viewing the span. The link will open the UI to the row represented by the\nSpan\nobject.\nIf you do not have access to the original Span\nobject, the slug produced by\nSpan.export\ncontains enough information to produce the same permalink. The\nbraintrust.permalink\nfunction can be used to construct a deep link to the row\nin the UI from a given span slug.\nManually managing spans\nIn more complicated environments, it may not always be possible to wrap the entire duration of a span within a single block of code. In such cases, you can always pass spans around manually.\nConsider this hypothetical server handler, which logs to a span incrementally over several distinct callbacks:\nImporting and exporting spans\nSpans are processed in Braintrust as a simple format, consisting of input\n, output\n, expected\n, metadata\n, scores\n,\nand metrics\nfields (all optional), as well as a few system-defined fields which you usually do not need to mess with, but\nare described below for completeness. This simple format makes\nit easy to import spans captured in other systems (e.g. languages other than TypeScript/Python), or to export spans from\nBraintrust to consume in other systems.\nUnderlying format\nThe underlying span format contains a number of fields which are not exposed directly through the SDK, but are useful to understand when importing/exporting spans.\nid\nis a unique identifier for the span, within the container (e.g. an experiment, or logs for a project). You can technically set this field yourself (to overwrite a span), but it is recommended to let Braintrust generate it automatically.input\n,output\n,expected\n,scores\n,metadata\n, andmetrics\nare optional fields which describe the span and are exposed in the Braintrust UI. When you use the TypeScript or Python SDK, these fields are validated for you (e.g. scores must be a mapping from strings to numbers between 0 and 1).span_attributes\ncontains attributes about the span. Currently the recognized attributes arename\n, which is used to display the span name in the UI, andtype\n, which displays a helpful icon.type\nshould be one of\"llm\"\n,\"score\"\n,\"function\"\n,\"eval\"\n,\"task\"\n, or\"tool\"\n.- Depending on the container, e.g. an experiment, or project logs, or a dataset, fields like\nproject_id\n,experiment_id\n,dataset_id\n, andlog_id\nare set automatically, by the SDK, so the span can be later retrieved by the UI and API. You should not set these fields yourself. span_id\n,root_span_id\n, andspan_parents\nare used to construct the span tree and are automatically set by Braintrust. You should not set these fields yourself, but rather let the SDK create and manage them (even if importing from another system).\nWhen importing spans, the only fields you should need to think about are input\n, output\n, expected\n, scores\n, metadata\n, and metrics\n.\nYou can use the SDK to populate the remaining fields, which the next section covers with an example.\nHere is an example of a span in the underlying format:\nExample import/export\nThe following example walks through how to generate spans in one program and then import them to Braintrust in a script. You can use this pattern to support tracing or running experiments in environments that use programming languages other than TypeScript/Python (e.g. Kotlin, Java, Go, Ruby, Rust, C++), or codebases that cannot integrate the Braintrust SDK directly.\nGenerating spans\nThe following example runs a simple LLM app and collects logging information at each stage of the process, without using the Braintrust SDK. This could be implemented in any programming language, and you certainly do not need to collect or process information this way. All that matters is that your program generates a useful format that you can later parse and use to import the spans using the SDK.\nRunning this script produces output like:\nImporting spans\nThe following program uses the Braintrust SDK in Python to import the spans generated by the previous script. Again, you can modify this program to fit the needs of your environment, e.g. to import spans from a different source or format.\nRunning traced functions in a ThreadPoolExecutor\nThe Python SDK uses context variables to hold the span state for traces.\nThis means that if you run a traced function inside of a concurrent.futures.ThreadPoolExecutor\n,\nthe span state will be lost.\nInstead, you can use the TracedThreadPoolExecutor\nclass provided by the Braintrust SDK.\nThis class is a thin extension of concurrent.futures.ThreadPoolExecutor\nthat captures and passes context variables to its workers.\nTuning parameters\nThe SDK includes several tuning knobs that may prove useful for debugging.\nBRAINTRUST_SYNC_FLUSH\n: By default, the SDKs will log to the backend API in the background, asynchronously. Logging is automatically batched and retried upon encountering network errors. If you wish to have fine-grained control over when logs are flushed to the backend, you may setBRAINTRUST_SYNC_FLUSH=1\n. When true, flushing will only occur when you runExperiment.flush\n(or any of the other object flush methods). If the flush fails, the SDK will raise an exception which you can handle.BRAINTRUST_MAX_REQUEST_SIZE\n: The SDK logger batches requests to save on network roundtrips. The batch size is tuned for the AWS lambda gateway, but you may adjust this if your backend has a different max payload requirement.BRAINTRUST_DEFAULT_BATCH_SIZE\n: The maximum number of individual log messages that are sent to the network in one payload.BRAINTRUST_NUM_RETRIES\n: The number of times the logger will attempt to retry network requests before failing.BRAINTRUST_QUEUE_SIZE\n(Python only): The maximum number of elements in the logging queue. It must be greater than zero. This value limits the memory usage of the logger. Logging additional elements beyond this size will drop the oldest elements in the queue, as of v0.1.5. In v0.1.4 and earlier, you can choose to drop or block the calling thread with theBRAINTRUST_QUEUE_DROP_WHEN_FULL\nenv variable.BRAINTRUST_QUEUE_DROP_EXCEEDING_MAXSIZE\n(Javascript only): Essentially a combination ofBRAINTRUST_QUEUE_SIZE\nandBRAINTRUST_QUEUE_DROP_WHEN_FULL\n, which changes the behavior of the queue from storing an unlimited number of elements to capping out at the specified value. Additional elements are discarded.BRAINTRUST_FAILED_PUBLISH_PAYLOADS_DIR\n: Sometimes errors occur when writing records to the backend. To aid in debugging errors, you may set this environment variable to a directory of choice, and Braintrust will save any payloads it failed to publish to this directory.BRAINTRUST_ALL_PUBLISH_PAYLOADS_DIR\n: Analogous toBRAINTRUST_FAILED_PUBLISH_PAYLOADS_DIR\n, except that Braintrust will save all payloads to this directory.\nDisabling logging\nIf you are not running an eval or logging, then the tracing code will be a no-op with negligible performance overhead. In other words, if you do not call initLogger/init_logger/init, in your code, then the tracing annotations are a no-op.\nTrace data structures\nA trace is a directed acyclic graph (DAG) of spans. Each span can have multiple parents, but most executions are a tree of spans. Currently, the UI only supports displaying a single root span, due to the popularity of this pattern.\nBackground logging and retries\nIf the Braintrust SDK cannot log for some reason (e.g. a network issue), then your application should not be affected. All logging operations run in a background thread, including api key validation, project/experiment registration, and flushing logs.\nWhen errors occur, the SDK retries a few times before eventually giving up. You'll see loud warning messages when this occurs. And you can tune this behavior via the environment variables defined in Tuning parameters.", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Customize traces"}, {"href": "https://www.braintrust.dev/docs/guides/traces/view", "anchor": "View traces"}, {"href": "https://www.braintrust.dev/docs/guides/traces/extend", "anchor": "Extend traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Customize traces"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Annotating your code"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Wrapping LLM clients"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Wrapping OpenAI"}, {"href": "https://www.braintrust.dev/docs/providers/openai", "anchor": "OpenAI provider"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Wrapping Anthropic"}, {"href": "https://www.braintrust.dev/docs/providers/anthropic", "anchor": "Anthropic provider"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Wrapping a custom LLM client"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Multimodal content"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Uploading attachments"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/classes/Attachment", "anchor": "TypeScript"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "Python"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Using external files as attachments"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Linking to external images"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "inline attachment"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Errors"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Deeply nested code"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Masking sensitive data"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Masking API keys and passwords"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Masking personally identifiable information (PII)"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Custom masking for specific data structures"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Considerations"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Distributed tracing"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Client code"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Server code"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Updating spans"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Deep-linking to spans"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Manually managing spans"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Importing and exporting spans"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Underlying format"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Example import/export"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Generating spans"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Importing spans"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Running traced functions in a ThreadPoolExecutor"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Tuning parameters"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Disabling logging"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Trace data structures"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Background logging and retries"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Tuning parameters"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Customize traces"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Annotating your code"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Wrapping LLM clients"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Wrapping OpenAI"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Wrapping Anthropic"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Wrapping a custom LLM client"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Multimodal content"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Uploading attachments"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Using external files as attachments"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Linking to external images"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Errors"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Deeply nested code"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Masking sensitive data"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Masking API keys and passwords"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Masking personally identifiable information (PII)"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Custom masking for specific data structures"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Considerations"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Distributed tracing"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Client code"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Server code"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Updating spans"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Deep-linking to spans"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Manually managing spans"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Importing and exporting spans"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Underlying format"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Example import/export"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Generating spans"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Importing spans"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Running traced functions in a ThreadPoolExecutor"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Tuning parameters"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Disabling logging"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Trace data structures"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Background logging and retries"}], "depth": 3}, "https://www.braintrust.dev/docs/guides/integrations": {"url": "https://www.braintrust.dev/docs/guides/integrations", "title": "Frameworks - Docs - Start - Braintrust", "text": "Trace with existing frameworks\nTrace your apps using existing frameworks to quickly add observability. This guide walks you through the supported frameworks and how to configure them for maximum observability and insight.\nTrace your apps using existing frameworks to quickly add observability. This guide walks you through the supported frameworks and how to configure them for maximum observability and insight.", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/opentelemetry", "anchor": "OpenTelemetry (OTel)"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/vercel-ai-sdk", "anchor": "Vercel AI SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/openai-agents-sdk", "anchor": "OpenAI Agents SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/instructor", "anchor": "Instructor"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/langchain", "anchor": "LangChain"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/langgraph", "anchor": "LangGraph"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/google", "anchor": "Google ADK"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/guides/integrations", "anchor": "Trace with existing frameworks"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/opentelemetry", "anchor": "OpenTelemetry"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/vercel-ai-sdk", "anchor": "Vercel AI SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/openai-agents-sdk", "anchor": "Agents SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/instructor", "anchor": "Instructor"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/langchain", "anchor": "LangChain"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/langgraph", "anchor": "LangGraph"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/google", "anchor": "Google ADK"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}], "depth": 3}, "https://www.braintrust.dev/docs/guides/traces/integrations": {"url": "https://www.braintrust.dev/docs/guides/traces/integrations", "title": "Frameworks - Docs - Start - Braintrust", "text": "Trace with existing frameworks\nTrace your apps using existing frameworks to quickly add observability. This guide walks you through the supported frameworks and how to configure them for maximum observability and insight.\nTrace your apps using existing frameworks to quickly add observability. This guide walks you through the supported frameworks and how to configure them for maximum observability and insight.", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/opentelemetry", "anchor": "OpenTelemetry (OTel)"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/vercel-ai-sdk", "anchor": "Vercel AI SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/openai-agents-sdk", "anchor": "OpenAI Agents SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/instructor", "anchor": "Instructor"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/langchain", "anchor": "LangChain"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/langgraph", "anchor": "LangGraph"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/google", "anchor": "Google ADK"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/guides/traces/integrations", "anchor": "Trace with existing frameworks"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/opentelemetry", "anchor": "OpenTelemetry"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/vercel-ai-sdk", "anchor": "Vercel AI SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/openai-agents-sdk", "anchor": "Agents SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/instructor", "anchor": "Instructor"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/langchain", "anchor": "LangChain"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/langgraph", "anchor": "LangGraph"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/google", "anchor": "Google ADK"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}], "depth": 3}, "https://www.braintrust.dev/docs/reference/btql": {"url": "https://www.braintrust.dev/docs/reference/btql", "title": "BTQL query syntax - Docs - Reference - Braintrust", "text": "BTQL query syntax\nBraintrust Query Language (BTQL) is a precise, SQL-like syntax for querying your experiments, logs, and datasets. You can use BTQL to filter and run more complex queries to analyze your data.\nWhy use BTQL?\nBTQL gives you precise control over your AI application data. You can:\n- Filter and search for relevant logs and experiments\n- Create consistent, reusable queries for monitoring\n- Build automated reporting and analysis pipelines\n- Write complex queries to analyze model performance\nQuery structure\nBTQL queries follow a familiar SQL-like structure that lets you define what data you want, how you want it returned, and how to analyze it:\nEach clause serves a specific purpose:\nselect\n: choose which fields to retrievefrom\n: specify the data source - can be an identifier (likeproject_logs\n) or a function call (likeexperiment(\"id\")\n). Has an optional designator for the shape of the data:spans\n,traces\n,summary\n. If not specified, defaults tospans\n.filter\n: define conditions to filter the datasort\n: set the order of results (asc\nordesc\n)limit\nandcursor\n: control result size and enable pagination\nYou can also use dimensions\n, measures\n, and pivot\ninstead of select\nfor aggregation queries.\nData source shapes\nWhen you query traces (experiments and logs) with BTQL, you can choose between three different shapes that determine how the data is returned. To specify this explicitly, add the shape after the data source:\nUsing summary\nThe summary\nshape provides a high-level overview of your experiment results, aggregating metrics across all spans in a trace.\nIt returns one row per trace, making it ideal for analyzing overall performance and comparing results across experiments.\nsummary\nincludes aggregated metrics like total tokens, total cost, and average scores, while also providing previews of the input\n, output\n, expected\n, and metadata\nfields.\nThis allows faster loading of data and easier comparison between experiments.\nWhen using summary\n, scores are averaged across all spans in the trace, and metrics like tokens and costs are summed.\nThe duration metrics are calculated from the span timings.\nThe preview fields (input, output, expected, error) show only data from the root span and are truncated to a default length of 124 characters, although you can change the length with the preview_length\nclause.\nSet preview_length: -1\nto disable truncation entirely.\nAvailable operators\nHere are the operators you can use in your queries:\nAvailable functions\nHere are all the functions you can use in any context (select, filter, dimensions, measures):\nField access\nBTQL provides flexible ways to access nested data in arrays and objects:\nArray indices are 0-based, and negative indices count from the end (-1 is the last element).\nSelect clause\nThe select\nclause determines which fields appear in your results. You can select specific fields, compute values, or use *\nto get everything:\nWorking with expressions\nTransform your data directly in the select clause:\nUsing functions\nTransform values and create meaningful aliases for your results:\nDimensions and measures\nInstead of select\n, you can use dimensions\nand measures\nto group and aggregate data:\nAggregate functions\nCommon aggregate functions for measures:\nPivot results\nThe pivot\nclause transforms your results to make comparisons easier by converting rows into columns. This is especially useful when comparing metrics across different categories or time periods.\nSyntax:\nHere are some examples:\nPivot columns are automatically named by combining the dimension value and measure name. For example, if you pivot by metadata.model\nand a model named \"gpt-4\" to measure avg_score\n, the name becomes gpt-4_avg_score\n.\nUnpivot\nThe unpivot\nclause transforms columns into rows, which is useful when you need to analyze arbitrary scores and metrics without specifying each score name. This is particularly helpful when working with dynamic sets of metrics or when you need to know all possible score names in advance.\nConditional expressions\nBTQL supports conditional logic using the ternary operator (? :\n):\nTime intervals\nBTQL supports intervals for time-based operations:\nFilter clause\nThe filter\nclause lets you specify conditions to narrow down results. It supports a wide range of operators and functions:\nNote: Negative filters on tags (e.g., NOT tags includes \"resolved\"\n) may not work as expected. Since tags are only applied to the root span of a trace, and queries return complete traces, negative tag filters will match child spans (which don't have tags) and return the entire trace. We recommend using positive tag filters instead.\nSort clause\nThe sort\nclause determines the order of results:\nLimit and cursor\nControl result size and implement pagination:\nCursors are automatically returned in BTQL responses. A default limit is applied in a query without a limit clause, and the number of returned results can be overridden by using an explicit limit\n. In order to implement pagination, after an initial query, provide the subsequent cursor token returned in the results in the cursor\nclause in follow-on queries. When a cursor has reached the end of the result set, the data\narray will be empty, and no cursor token will be returned by the query.\nCursors can only be used for pagination when no sort\nclause is specified. If you need sorted results, you'll need to implement offset-based pagination by using the last value from your sort field as a filter in the next query, as shown in the example above.\nAPI access\nAccess BTQL programmatically through our API:\nThe API accepts these parameters:\nquery\n(required): your BTQL query stringfmt\n: response format (json\norparquet\n, defaults tojson\n)tz_offset\n: timezone offset in minutes for time-based operationsaudit_log\n: include audit log data\nFor correct day boundaries, set tz_offset\nto match your timezone. For example, use 480\nfor US Pacific Standard Time.\nExamples\nLet's look at some real-world examples:\nTracking token usage\nThis query helps you monitor token consumption across your application:\nThe response shows daily token usage:\nModel quality monitoring\nTrack model performance across different versions and configurations:\nError analysis\nIdentify and investigate errors in your application:\nLatency analysis\nMonitor and optimize response times:\nPrompt analysis\nAnalyze prompt effectiveness and patterns:\nTag-based analysis\nUse tags to track and analyze specific behaviors:\nBTQL sandbox\nTo test BTQL with autocomplete, validation, and a table of results, try the BTQL sandbox in the dashboard.", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/reference/api", "anchor": "API"}, {"href": "https://www.braintrust.dev/docs/reference/autoevals", "anchor": "Autoevals library"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "BTQL query syntax"}, {"href": "https://www.braintrust.dev/docs/reference/streaming", "anchor": "Streaming"}, {"href": "https://www.braintrust.dev/docs/reference/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/reference/reasoning", "anchor": "Reasoning"}, {"href": "https://www.braintrust.dev/docs/reference/organizations", "anchor": "Organizations"}, {"href": "https://www.braintrust.dev/docs/reference/mcp", "anchor": "Model Context Protocol (MCP)"}, {"href": "https://www.braintrust.dev/docs/reference/object-links", "anchor": "Sharing via URL"}, {"href": "https://www.braintrust.dev/docs/reference/glossary", "anchor": "Glossary"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "BTQL query syntax"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "Why use BTQL?"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "Query structure"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "Data source shapes"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "Using summary"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "Available operators"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "Available functions"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "Field access"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "Select clause"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "Working with expressions"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "Using functions"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "Dimensions and measures"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "Aggregate functions"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "Pivot results"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "Unpivot"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "Conditional expressions"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "Time intervals"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "Filter clause"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "Sort clause"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "Limit and cursor"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "API access"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "Examples"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "Tracking token usage"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "Model quality monitoring"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "Error analysis"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "Latency analysis"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "Prompt analysis"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "Tag-based analysis"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "BTQL sandbox"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "BTQL query syntax"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "Why use BTQL?"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "Query structure"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "Data source shapes"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "Using summary"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "Available operators"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "Available functions"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "Field access"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "Select clause"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "Working with expressions"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "Using functions"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "Dimensions and measures"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "Aggregate functions"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "Pivot results"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "Unpivot"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "Conditional expressions"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "Time intervals"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "Filter clause"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "Sort clause"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "Limit and cursor"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "API access"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "Examples"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "Tracking token usage"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "Model quality monitoring"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "Error analysis"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "Latency analysis"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "Prompt analysis"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "Tag-based analysis"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "BTQL sandbox"}], "depth": 3}, "https://www.braintrust.dev/docs/guides/functions/prompts": {"url": "https://www.braintrust.dev/docs/guides/functions/prompts", "title": "Prompts - Docs - Guides - Functions - Braintrust", "text": "Prompts\nPrompt engineering is a core activity in AI engineering. Braintrust allows you to create prompts, test them out in the playground, use them in your code, update them, and track their performance over time. Our goal is to provide a world-class authoring experience in Braintrust, seamlessly, securely, and reliably integrate them into your code, and debug issues as they arise.\nCreating a prompt\nTo create a prompt, navigate to your Library in the top menu bar and select Prompts, then Create prompt. Pick a name and unique slug for your prompt. The slug is an identifier that you can use to reference it in your code. As you change the prompt's name, description, or contents, its slug stays constant.\nPrompts can use mustache templating syntax to refer to variables. These variables are substituted\nautomatically in the API, playground, and using the .build()\nfunction in your code. More on that below.\nIn code\nTo create a prompt in code, you can write a script and push\nit to Braintrust:\nEach prompt change is versioned, e.g. 5878bd218351fb8e\n. You can use this identifier to pin a specific\nversion of the prompt in your code.\nYou can use this identifier to refer to a specific version of the prompt in your code.\nAdding few-shot examples to a prompt\nYou can also use mustache syntax to add few-shot examples to your prompt. For example:\nTesting in the playground\nWhile developing a prompt, it can be useful to test it out on real-world data in the Playground. You can open a prompt in the playground, tweak it, and save a new version once you're ready.\nStructured outputs\nWhen using prompts in the playground, you can also define the JSON schema for the structured output of the prompt. Like tool calls, the returned value is parsed as a JSON object automatically.\nFor example:\nThis JSON object corresponds to the response_format.json_schema\nargument in the OpenAI API, so this feature currently only works for OpenAI models.\nStructured outputs can also be defined using the visual schema builder UI.\nUsing tools\nYou can use any custom tools you've created during prompt execution. To reference a tool when creating a prompt via the SDK, add the names of the tools you want to use to the tools\nparameter:\nIn Python, the prompt and the tool need to be defined in the same file and pushed to Braintrust together. In TypeScript, they can be defined and pushed separately.\nTo add a tool to a prompt via the UI, select the Tools dropdown in the prompt creation window and select a tool from your library, then save the prompt.\nFor more information about creating and using tools, check out the Tools guide.\nUsing prompts in your code\nExecuting directly\nIn Braintrust, a prompt is a simple function that can be invoked directly through the SDK and REST API. When invoked, prompt functions leverage the proxy to access a wide range of providers and models with managed secrets, and are automatically traced and logged to your Braintrust project. All functions are fully managed and versioned via the UI and API.\nFunctions are a broad concept that encompass prompts, code snippets, HTTP endpoints, and more. When using the functions API, you can use a prompt's slug or ID as the function's slug or ID, respectively. To learn more about functions, see the functions reference.\nThe return value, result\n, is a string unless you have tool calls, in which case it returns the arguments\nof the first tool call. In TypeScript, you can assert this by using the schema\nargument, which ensures your\ncode matches a particular zod schema:\nAdding extra messages\nIf you're building a chat app, it's often useful to send back additional messages of context as you gather them. You can provide\nOpenAI-style messages to the invoke\nfunction by adding messages\n, which are appended to the end of the built-in messages:\nStreaming\nYou can also stream results in an easy-to-parse format.\nVercel AI SDK\nIf you're using Next.js and the Vercel AI SDK, you can use the Braintrust\nadapter by installing the @braintrust/vercel-ai-sdk\npackage and converting the stream to Vercel's format:\nYou can also use streamText\nto leverage the Vercel AI SDK directly. Be sure to have the OpenTelemetry environment variables configured to log these requests to Braintrust.\nLogging\ninvoke\nuses the active logging state of your application, just like any function decorated with @traced\nor wrapTraced\n.\nThis means that if you initialize a logger while calling invoke\n, it will automatically log spans to Braintrust. By default, invoke\nrequests will log to a root span, but you can customize the name of a span using the name\nargument. For example:\nwill generate a log like this:\nYou can also pass in the parent\nargument, which is a string that you can\nderive from span.export()\nwhile doing distributed tracing.\nFetching in code\nIf you'd like to run prompts directly, you can fetch them using the Braintrust SDK. The loadPrompt()\n/load_prompt()\nfunction loads a prompt into a simple format that you can pass along to the OpenAI client.\nloadPrompt\nalso caches prompts with a two-layered cache,\nand attempts to use this cache if the prompt cannot be fetched from the Braintrust server:\n- A memory cache, which stores up to\nBRAINTRUST_PROMPT_CACHE_MEMORY_MAX\nprompts in memory. This defaults to 1024. - A disk cache, which stores up to\nBRAINTRUST_PROMPT_CACHE_DISK_MAX\nprompts on disk. This defaults to 1048576.\nYou can also configure the directory used by disk cache\nby setting the BRAINTRUST_PROMPT_CACHE_DIR\nenvironment variable.\nIf you need to use another model provider, then you can use the Braintrust\nproxy to access a wide range of models using the OpenAI\nformat. You can also grab the messages\nand other parameters directly from\nthe returned object to use a model library of your choice.\nPinning a specific version\nTo pin a specific version of a prompt, use the loadPrompt()\n/load_prompt()\nfunction with the version identifier.\nPulling prompts locally\nYou can also download prompts to your local filesystem and ensure a specific version is used via version control. You should\nuse the pull\ncommand to:\n- Download prompts to public projects so others can use them\n- Pin your production environment to a specific version without running them through Braintrust on the request path\n- Review changes to prompts in pull requests\nCurrently, braintrust pull\nonly supports TypeScript.\nWhen you run braintrust pull\n, you can specify a project name, prompt slug, or version to pull. If you don't specify\nany of these, all prompts across projects will be pulled into a separate file per project. For example, if you have a\nproject named Summary\nwill generate the following file:\nTo pin your production environment to a specific version, you can run braintrust pull\nwith the --version\nflag.\nUsing a pulled prompt\nThe prompts.create\nfunction generates the same Prompt\nobject as the loadPrompt\nfunction.\nThis means you can use a pulled prompt in the same way you would use a normal prompt, e.g. by\nrunning prompt.build()\nand passing the result to client.chat.completions.create()\ncall.\nPushing prompts\nJust like with tools, you can push prompts to Braintrust using the push\ncommand by changing\nthe prompt definition and running braintrust push\nfrom the command line. Braintrust automatically generates\na new version for each pushed prompt.\nWhen you run npx braintrust push\n, you can specify one or more files or directories to push. If you specify a directory, all .ts\nfiles under that directory are pushed.\nSee the example in the guide to tools for more details.\nPushing from code\nSometimes it's more convenient to write a script that pushes prompts, rather than using the command line. You can do this by\nrunning project.publish()\ndirectly from your code.\nDeployment strategies\nIt is often useful to use different versions of a prompt in different environments. For example, you might want to use the latest\nversion locally and in staging, but pin a specific version in production. This is simple to setup by conditionally passing a version\nto loadPrompt()\n/load_prompt()\nbased on the environment.\nGet prompt versions\nUse the getPromptVersions()\nfunction to get a list of available versions for a specific prompt. This function returns transaction IDs that can be used with the version\nparameter when loading prompts.\nChat vs. completion format\nIn Python, prompt.build()\nreturns a dictionary with chat or completion parameters, depending on the prompt type. In TypeScript, however,\nprompt.build()\naccepts an additional parameter (flavor\n) to specify the format. This allows prompt.build\nto be used in a more type-safe\nmanner. When you specify a flavor, the SDK also validates that the parameters are correct for that format.\nOpening from traces\nWhen you use a prompt in your code, Braintrust automatically links spans to the prompt used to generate them. This allows you to click to open a span in the playground, and see the prompt that generated it alongside the input variables. You can even test and save a new version of the prompt directly from the playground.\nThis workflow is very powerful. It effectively allows you to debug, iterate, and publish changes to your prompts directly within Braintrust. And because Braintrust flexibly allows you to load the latest prompt, a specific version, or even a version controlled artifact, you have a lot of control over how these updates propagate into your production systems.\nUsing the API\nThe full lifecycle of prompts - creating, retrieving, modifying, etc. - can be managed through the REST API. See the API docs for more details.", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/functions/prompts", "anchor": "Prompts"}, {"href": "https://www.braintrust.dev/docs/guides/functions/tools", "anchor": "Tools"}, {"href": "https://www.braintrust.dev/docs/guides/functions/scorers", "anchor": "Scorers"}, {"href": "https://www.braintrust.dev/docs/guides/functions/agents", "anchor": "Agents"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/functions/prompts", "anchor": "Prompts"}, {"href": "https://www.braintrust.dev/docs/guides/functions/prompts", "anchor": "Creating a prompt"}, {"href": "https://www.braintrust.dev/docs/guides/functions/prompts", "anchor": "In code"}, {"href": "https://www.braintrust.dev/docs/guides/functions/prompts", "anchor": "Adding few-shot examples to a prompt"}, {"href": "https://www.braintrust.dev/docs/guides/functions/prompts", "anchor": "Testing in the playground"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playground"}, {"href": "https://www.braintrust.dev/docs/guides/functions/prompts", "anchor": "Structured outputs"}, {"href": "https://www.braintrust.dev/docs/guides/functions/prompts", "anchor": "Using tools"}, {"href": "https://www.braintrust.dev/docs/guides/functions/tools", "anchor": "Tools guide"}, {"href": "https://www.braintrust.dev/docs/guides/functions/prompts", "anchor": "Using prompts in your code"}, {"href": "https://www.braintrust.dev/docs/guides/functions/prompts", "anchor": "Executing directly"}, {"href": "https://www.braintrust.dev/docs/reference/api/Functions", "anchor": "REST API"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "proxy"}, {"href": "https://www.braintrust.dev/docs/reference/functions", "anchor": "functions reference"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/InvokeFunctionArgs", "anchor": "schema"}, {"href": "https://www.braintrust.dev/docs/guides/functions/prompts", "anchor": "Adding extra messages"}, {"href": "https://www.braintrust.dev/docs/guides/functions/prompts", "anchor": "Streaming"}, {"href": "https://www.braintrust.dev/docs/guides/functions/prompts", "anchor": "Vercel AI SDK"}, {"href": "https://www.braintrust.dev/docs/guides/traces/integrations", "anchor": "OpenTelemetry environment variables"}, {"href": "https://www.braintrust.dev/docs/guides/functions/prompts", "anchor": "Logging"}, {"href": "https://www.braintrust.dev/docs/guides/functions/prompts", "anchor": "initialize a logger"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "distributed tracing"}, {"href": "https://www.braintrust.dev/docs/guides/functions/prompts", "anchor": "Fetching in code"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "loadPrompt()"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "load_prompt()"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "Braintrust proxy"}, {"href": "https://www.braintrust.dev/docs/guides/functions/prompts", "anchor": "Pinning a specific version"}, {"href": "https://www.braintrust.dev/docs/guides/functions/prompts", "anchor": "Pulling prompts locally"}, {"href": "https://www.braintrust.dev/docs/guides/functions/prompts", "anchor": "Using a pulled prompt"}, {"href": "https://www.braintrust.dev/docs/guides/functions/prompts", "anchor": "Pushing prompts"}, {"href": "https://www.braintrust.dev/docs/guides/functions/tools", "anchor": "guide to tools"}, {"href": "https://www.braintrust.dev/docs/guides/functions/prompts", "anchor": "Pushing from code"}, {"href": "https://www.braintrust.dev/docs/guides/functions/prompts", "anchor": "Deployment strategies"}, {"href": "https://www.braintrust.dev/docs/guides/functions/prompts", "anchor": "Get prompt versions"}, {"href": "https://www.braintrust.dev/docs/guides/functions/prompts", "anchor": "Chat vs. completion format"}, {"href": "https://www.braintrust.dev/docs/guides/functions/prompts", "anchor": "Opening from traces"}, {"href": "https://www.braintrust.dev/docs/guides/functions/prompts", "anchor": "Using the API"}, {"href": "https://www.braintrust.dev/docs/api/spec", "anchor": "API docs"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}, {"href": "https://www.braintrust.dev/docs/guides/functions/prompts", "anchor": "Prompts"}, {"href": "https://www.braintrust.dev/docs/guides/functions/prompts", "anchor": "Creating a prompt"}, {"href": "https://www.braintrust.dev/docs/guides/functions/prompts", "anchor": "In code"}, {"href": "https://www.braintrust.dev/docs/guides/functions/prompts", "anchor": "Adding few-shot examples to a prompt"}, {"href": "https://www.braintrust.dev/docs/guides/functions/prompts", "anchor": "Testing in the playground"}, {"href": "https://www.braintrust.dev/docs/guides/functions/prompts", "anchor": "Structured outputs"}, {"href": "https://www.braintrust.dev/docs/guides/functions/prompts", "anchor": "Using tools"}, {"href": "https://www.braintrust.dev/docs/guides/functions/prompts", "anchor": "Using prompts in your code"}, {"href": "https://www.braintrust.dev/docs/guides/functions/prompts", "anchor": "Executing directly"}, {"href": "https://www.braintrust.dev/docs/guides/functions/prompts", "anchor": "Adding extra messages"}, {"href": "https://www.braintrust.dev/docs/guides/functions/prompts", "anchor": "Streaming"}, {"href": "https://www.braintrust.dev/docs/guides/functions/prompts", "anchor": "Vercel AI SDK"}, {"href": "https://www.braintrust.dev/docs/guides/functions/prompts", "anchor": "Logging"}, {"href": "https://www.braintrust.dev/docs/guides/functions/prompts", "anchor": "Fetching in code"}, {"href": "https://www.braintrust.dev/docs/guides/functions/prompts", "anchor": "Pinning a specific version"}, {"href": "https://www.braintrust.dev/docs/guides/functions/prompts", "anchor": "Pulling prompts locally"}, {"href": "https://www.braintrust.dev/docs/guides/functions/prompts", "anchor": "Using a pulled prompt"}, {"href": "https://www.braintrust.dev/docs/guides/functions/prompts", "anchor": "Pushing prompts"}, {"href": "https://www.braintrust.dev/docs/guides/functions/prompts", "anchor": "Pushing from code"}, {"href": "https://www.braintrust.dev/docs/guides/functions/prompts", "anchor": "Deployment strategies"}, {"href": "https://www.braintrust.dev/docs/guides/functions/prompts", "anchor": "Get prompt versions"}, {"href": "https://www.braintrust.dev/docs/guides/functions/prompts", "anchor": "Chat vs. completion format"}, {"href": "https://www.braintrust.dev/docs/guides/functions/prompts", "anchor": "Opening from traces"}, {"href": "https://www.braintrust.dev/docs/guides/functions/prompts", "anchor": "Using the API"}], "depth": 3}, "https://www.braintrust.dev/docs/guides/functions/tools": {"url": "https://www.braintrust.dev/docs/guides/functions/tools", "title": "Tools - Docs - Guides - Functions - Braintrust", "text": "Tools\nTool functions in Braintrust allow you to define general-purpose code that can be invoked by LLMs to add complex logic or external operations to your workflows. Tools are reusable and composable, making it easy to iterate on assistant-style agents and more advanced applications. You can create tools in TypeScript or Python and deploy them across the UI and API via prompts.\nCreating a tool\nCurrently, you must define tools via code and push them to Braintrust with braintrust push\n. To define a tool,\nuse project.tool.create\nand pick a name and\nunique slug:\nPushing to Braintrust\nOnce you define a tool, you can push it to Braintrust with braintrust push\n:\nDependencies\nBraintrust will take care of bundling the dependencies your tool needs.\nIn TypeScript, we use esbuild to bundle your code and its dependencies together. This works for most dependencies, but it does not support native (compiled) libraries like SQLite.\nIf you have trouble bundling your dependencies, let us know by filing an issue.\nUsing tools in the UI\nOnce you define a tool in Braintrust, you can access it through the UI and API. However, the real advantage lies in calling a tool from an LLM. Most models support tool calling, which allows them to select a tool from a list of available options. Normally, it's up to you to execute the tool, retrieve its results, and re-run the model with the updated context.\nBraintrust simplifies this process dramatically by:\n- Automatically passing the tool's definition to the model\n- Running the tool securely in a sandbox environment when called\n- Re-running the model with the tool's output\n- Streaming the whole output along with intermediate progress to the client\nViewing tools in the UI\nIf you visit a project in the UI, you'll see the available tools listed on the Tools page in the Library:\nYou can run single datapoints right inside the tool to test its functionality.\nAdding tools to a prompt\nTo add a tool to a prompt, select it in the Tools dropdown in your Prompt window. Braintrust will automatically:\n- Include it in the list of available tools to the model\n- Invoke the tool if the model calls it, and append the result to the message history\n- Call the model again with the tool's result as context\n- Continue for up to (default) 5 iterations or until the model produces a non-tool result\nAs an example, let's define a tool that looks up information about the most recent commit in a GitHub repository:\nIf you save this file locally to github.ts\nor github.py\n, you can run\nto push the function to Braintrust. Once the command completes, you should see the function listed in the Library's Tools tab.\nThen, you can add the tool to your prompt and run it.\nEmbedding tool calls into a prompt\nIn addition to selecting from the tool menu to add a tool to a prompt, you can also add a tool call directly from the Assistant or Tool messages within a prompt.\nTo add a tool call to an Assistant prompt, select Assistant from the dropdown menu. Then, select the Swiss army knife icon to Toggle tool calls. You'll be able to add the tool code directly into the prompt editor. For example:\nIn this example, input.2.function_call.0.id\nis pulled from the input data and refers to the third message's first tool call.\nYou can also select Tool from the dropdown menu to enter a tool call ID, such as {{input.3.function_responses.0.id}}\n.\nStructured outputs\nAnother use case for tool calling is to coerce a model into producing structured outputs that match a given JSON schema. You can do this without creating a tool function, and instead use the Raw tab in the Tools dropdown.\nEnter an array of tool definitions following the OpenAI tool format:\nBraintrust supports two different modes for executing raw tools:\nauto\nwill return the arguments of the first tool call as a JSON object. This is the default mode.parallel\nwill return an array of all tool calls including both function names and arguments.\nresponse_format: { type: \"json_object\" }\ndoes not get parsed as a JSON object and will be returned as a string.\nUsing tools in code\nYou can also attach a tool to a prompt defined in code. For example:\nIf you run braintrust push\non this file, Braintrust will push both the tool and the prompt.\nYou can also define the tool and prompt in separate files and push them together with braintrust push\n:\nIf you run braintrust push\non the prompt file, Braintrust will push both the tool and the prompt.\nNote that the Python interpreter only supports relative imports from within a package,\nso you must either define the tool in the same file as the prompt, or use a package structure.", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/functions/prompts", "anchor": "Prompts"}, {"href": "https://www.braintrust.dev/docs/guides/functions/tools", "anchor": "Tools"}, {"href": "https://www.braintrust.dev/docs/guides/functions/scorers", "anchor": "Scorers"}, {"href": "https://www.braintrust.dev/docs/guides/functions/agents", "anchor": "Agents"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/functions/tools", "anchor": "Tools"}, {"href": "https://www.braintrust.dev/docs/guides/functions/tools", "anchor": "Creating a tool"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/classes/ToolBuilder", "anchor": "project.tool.create"}, {"href": "https://www.braintrust.dev/docs/guides/functions/tools", "anchor": "Pushing to Braintrust"}, {"href": "https://www.braintrust.dev/docs/guides/functions/tools", "anchor": "Dependencies"}, {"href": "https://www.braintrust.dev/docs/guides/functions/tools", "anchor": "Using tools in the UI"}, {"href": "https://www.braintrust.dev/docs/reference/api/Functions", "anchor": "API"}, {"href": "https://www.braintrust.dev/docs/guides/functions/tools", "anchor": "Viewing tools in the UI"}, {"href": "https://www.braintrust.dev/docs/guides/functions/tools", "anchor": "Adding tools to a prompt"}, {"href": "https://www.braintrust.dev/docs/guides/functions/tools", "anchor": "Embedding tool calls into a prompt"}, {"href": "https://www.braintrust.dev/docs/guides/functions/tools", "anchor": "Structured outputs"}, {"href": "https://www.braintrust.dev/docs/guides/functions/tools", "anchor": "Using tools in code"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}, {"href": "https://www.braintrust.dev/docs/guides/functions/tools", "anchor": "Tools"}, {"href": "https://www.braintrust.dev/docs/guides/functions/tools", "anchor": "Creating a tool"}, {"href": "https://www.braintrust.dev/docs/guides/functions/tools", "anchor": "Pushing to Braintrust"}, {"href": "https://www.braintrust.dev/docs/guides/functions/tools", "anchor": "Dependencies"}, {"href": "https://www.braintrust.dev/docs/guides/functions/tools", "anchor": "Using tools in the UI"}, {"href": "https://www.braintrust.dev/docs/guides/functions/tools", "anchor": "Viewing tools in the UI"}, {"href": "https://www.braintrust.dev/docs/guides/functions/tools", "anchor": "Adding tools to a prompt"}, {"href": "https://www.braintrust.dev/docs/guides/functions/tools", "anchor": "Embedding tool calls into a prompt"}, {"href": "https://www.braintrust.dev/docs/guides/functions/tools", "anchor": "Structured outputs"}, {"href": "https://www.braintrust.dev/docs/guides/functions/tools", "anchor": "Using tools in code"}], "depth": 3}, "https://www.braintrust.dev/docs/guides/functions/scorers": {"url": "https://www.braintrust.dev/docs/guides/functions/scorers", "title": "Scorers - Docs - Guides - Functions - Braintrust", "text": "Scorers\nScorers in Braintrust allow you to evaluate the output of LLMs based on a set of criteria. These can include both heuristics (expressed as code) or prompts (expressed as LLM-as-a-judge). Scorers help you assign a performance score between 0 and 100% to assess how well the AI outputs match expected results. While many scorers are available out of the box in Braintrust, you can also create your own custom scorers directly in the UI or upload them via the command line. Scorers that you define in the UI can also be used as functions.\nAutoevals\nThere are several pre-built scorers available via the open-source autoevals library, which offers standard evaluation methods that you can start using immediately.\nAutoeval scorers offer a strong starting point for a variety of evaluation tasks. Some autoeval scorers require configuration before they can be used effectively. For example, you might need to define expected outputs or certain parameters for specific tasks. To edit an autoeval scorer, you must copy it first.\nWhile autoevals are a great way to get started, you may eventually need to create your own custom scorers for more advanced use cases.\nCustom scorers\nYou can create custom scorers in TypeScript, Python, or as an LLM-as-a-judge through the UI by navigating to Library > Scorers and selecting Create scorer. These scorers will be available to use as functions throughout your project. You can also upload custom scorers from the command line.\nTypeScript and Python scorers\nFor more specialized evals, you can create custom scorers in either TypeScript or Python. These code-based scorers are highly customizable and can return scores based on your exact requirements. Add your custom code to the TypeScript\nor Python\ntabs, and it will run in a sandboxed environment.\nThis command will bundle and upload your custom scorer functions, making them accessible across your Braintrust projects.\nLLM-as-a-judge scorers\nIn addition to code-based scorers, you can also create LLM-as-a-judge scorers through the UI. For an LLM-as-a-judge scorer, you define a prompt that evaluates the AI's output and maps its choices to specific scores. You can also configure whether to use techniques like chain-of-thought (CoT) reasoning for more complex evaluations.\nUsing a scorer in the UI\nYou can use both autoevals and custom scorers in the Braintrust Playground. In your playground, navigate to Scorers and select from the list of available scorers. You can also create a new custom scorer from this menu.\nThe Playground allows you to iterate quickly on prompts while running evaluations, making it the perfect tool for testing and refining your AI models and prompts.\nPushing scorers via the CLI\nAs with tools, when writing custom scorers in the UI, there may be restrictions on certain imports or functionality, but you can always write your scorers in your own environment and upload them for use in Braintrust. This works for both code-based scorers and LLM-as-a-judge scorers.\nPushing to Braintrust\nOnce you define a scorer, you can push it to Braintrust with braintrust push\n:\nDependencies\nBraintrust will take care of bundling the dependencies your scorer needs.\nIn TypeScript, we use esbuild to bundle your code and its dependencies together. This works for most dependencies, but it does not support native (compiled) libraries like SQLite.\nIf you have trouble bundling your dependencies, let us know by filing an issue.\nUsing scorers in your evals\nThe scorers that you create in Braintrust are available throughout the UI, e.g. in the playground, but you can also use them in your code-based evals. See Using custom prompts/functions from Braintrust for more details.", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/functions/prompts", "anchor": "Prompts"}, {"href": "https://www.braintrust.dev/docs/guides/functions/tools", "anchor": "Tools"}, {"href": "https://www.braintrust.dev/docs/guides/functions/scorers", "anchor": "Scorers"}, {"href": "https://www.braintrust.dev/docs/guides/functions/agents", "anchor": "Agents"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/functions/scorers", "anchor": "Scorers"}, {"href": "https://www.braintrust.dev/docs/guides/functions/scorers", "anchor": "Autoevals"}, {"href": "https://www.braintrust.dev/docs/guides/functions/scorers", "anchor": "Custom scorers"}, {"href": "https://www.braintrust.dev/docs/guides/functions/scorers", "anchor": "TypeScript and Python scorers"}, {"href": "https://www.braintrust.dev/docs/guides/functions/scorers", "anchor": "LLM-as-a-judge scorers"}, {"href": "https://www.braintrust.dev/docs/guides/functions/scorers", "anchor": "Using a scorer in the UI"}, {"href": "https://www.braintrust.dev/docs/guides/functions/scorers", "anchor": "Pushing scorers via the CLI"}, {"href": "https://www.braintrust.dev/docs/guides/functions/scorers", "anchor": "Pushing to Braintrust"}, {"href": "https://www.braintrust.dev/docs/guides/functions/scorers", "anchor": "Dependencies"}, {"href": "https://www.braintrust.dev/docs/guides/functions/scorers", "anchor": "Using scorers in your evals"}, {"href": "https://www.braintrust.dev/docs/guides/experiments/write", "anchor": "Using custom prompts/functions from Braintrust"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}, {"href": "https://www.braintrust.dev/docs/guides/functions/scorers", "anchor": "Scorers"}, {"href": "https://www.braintrust.dev/docs/guides/functions/scorers", "anchor": "Autoevals"}, {"href": "https://www.braintrust.dev/docs/guides/functions/scorers", "anchor": "Custom scorers"}, {"href": "https://www.braintrust.dev/docs/guides/functions/scorers", "anchor": "TypeScript and Python scorers"}, {"href": "https://www.braintrust.dev/docs/guides/functions/scorers", "anchor": "LLM-as-a-judge scorers"}, {"href": "https://www.braintrust.dev/docs/guides/functions/scorers", "anchor": "Using a scorer in the UI"}, {"href": "https://www.braintrust.dev/docs/guides/functions/scorers", "anchor": "Pushing scorers via the CLI"}, {"href": "https://www.braintrust.dev/docs/guides/functions/scorers", "anchor": "Pushing to Braintrust"}, {"href": "https://www.braintrust.dev/docs/guides/functions/scorers", "anchor": "Dependencies"}, {"href": "https://www.braintrust.dev/docs/guides/functions/scorers", "anchor": "Using scorers in your evals"}], "depth": 3}, "https://www.braintrust.dev/docs/guides/functions/agents": {"url": "https://www.braintrust.dev/docs/guides/functions/agents", "title": "Agents - Docs - Guides - Functions - Braintrust", "text": "Agents\nAgents in Braintrust allow you to chain together two or more prompts. You can create or edit agents in the playground, and view and execute them from the library.\nAgents are in beta. They currently only work in the playground UI, and are limited to prompt chaining functionality. If you are on a hybrid deployment, agents are available starting with v0.0.66\n.\nControl flow with loops is coming soon, along with full SDK support.\nCreating an agent in the playground\nTo create an agent, navigate to a playground and select +Agent. Start by creating the base prompt or selecting one from your library. Then, create or select another prompt by selecting the + icon in the comparison agent pane.\nThe prompts will chain together, and they will automatically run consecutively.\nVariables\nWhen building agents, the first prompt node and subsequent prompt nodes have slightly different templating behavior. To refer to variables, you can use mustache templating syntax.\nUsing dataset\nIf you are on a hybrid deployment, the dataset\nvariable is available starting with v1.1.1\n.\nThe dataset\nvariable is globally available. You can use it in any agent prompt node to access input\n, expected\n, and metadata\n. For example, to access metadata.foo\n, use {{dataset.metadata.foo}}\n.\nIn the first agent prompt\nThe first prompt node can access dataset variables directly by using {{input}}\n, {{expected}}\n, and {{metadata}}\n, for convenience. Using {{dataset}}\nis useful for consistent access patterns with later prompts, or if you use this prompt in a later position in the future, since all agent prompts can access {{dataset}}\n.\nLater prompts\nSubsequent prompts can access the output of the previous node by using {{input}}\n.\n- If the previous node outputs structured data, use dot notation. For example,\n{{input.bar}}\n. - If the previous node outputs text or unschematized JSON, you can only use\n{{input}}\n. If you're using JSON outputs, consider switching to structured outputs to enable accessing nested output variables with linting and autocomplete.\nViewing and executing agents\nYou can view and execute single runs of agents from your agent library, but you will not be able to edit them or see them run.", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/functions/prompts", "anchor": "Prompts"}, {"href": "https://www.braintrust.dev/docs/guides/functions/tools", "anchor": "Tools"}, {"href": "https://www.braintrust.dev/docs/guides/functions/scorers", "anchor": "Scorers"}, {"href": "https://www.braintrust.dev/docs/guides/functions/agents", "anchor": "Agents"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/functions/agents", "anchor": "Agents"}, {"href": "https://www.braintrust.dev/docs/guides/functions/agents", "anchor": "Creating an agent in the playground"}, {"href": "https://www.braintrust.dev/docs/guides/functions/agents", "anchor": "Variables"}, {"href": "https://www.braintrust.dev/docs/guides/functions/agents", "anchor": "Using dataset"}, {"href": "https://www.braintrust.dev/docs/guides/functions/agents", "anchor": "In the first agent prompt"}, {"href": "https://www.braintrust.dev/docs/guides/functions/agents", "anchor": "Later prompts"}, {"href": "https://www.braintrust.dev/docs/guides/functions/prompts", "anchor": "outputs structured data"}, {"href": "https://www.braintrust.dev/docs/guides/functions/agents", "anchor": "Viewing and executing agents"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}, {"href": "https://www.braintrust.dev/docs/guides/functions/agents", "anchor": "Agents"}, {"href": "https://www.braintrust.dev/docs/guides/functions/agents", "anchor": "Creating an agent in the playground"}, {"href": "https://www.braintrust.dev/docs/guides/functions/agents", "anchor": "Variables"}, {"href": "https://www.braintrust.dev/docs/guides/functions/agents", "anchor": "Using dataset"}, {"href": "https://www.braintrust.dev/docs/guides/functions/agents", "anchor": "In the first agent prompt"}, {"href": "https://www.braintrust.dev/docs/guides/functions/agents", "anchor": "Later prompts"}, {"href": "https://www.braintrust.dev/docs/guides/functions/agents", "anchor": "Viewing and executing agents"}], "depth": 3}, "https://www.braintrust.dev/docs/reference/api/Functions": {"url": "https://www.braintrust.dev/docs/reference/api/Functions", "title": "Functions - Docs - Reference - Braintrust", "text": "List functions\nList out all functions. The functions are sorted by creation date, with the most recently-created functions coming first\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nQuery Parameters\nlimit\nintegerLimit the number of objects to return\n0\nstarting_after\nstringPagination cursor id.\nFor example, if the final item in the last page you fetched had an id of foo\n, pass starting_after=foo\nto fetch the next page. Note: you may only pass one of starting_after\nand ending_before\n\"uuid\"\nending_before\nstringPagination cursor id.\nFor example, if the initial item in the last page you fetched had an id of foo\n, pass ending_before=foo\nto fetch the previous page. Note: you may only pass one of starting_after\nand ending_before\n\"uuid\"\nids\nAny properties in string, array<string>Filter search results to a particular set of object IDs. To specify a list of IDs, include the query param multiple times\nfunction_name\nstringName of the function to search for\nproject_name\nstringName of the project to search for\nproject_id\nstringProject id\n\"uuid\"\nslug\nstringRetrieve prompt with a specific slug\nversion\nstringRetrieve prompt at a specific version.\nThe version id can either be a transaction id (e.g. '1000192656880881099') or a version identifier (e.g. '81cd05ee665fdfb3').\norg_name\nstringFilter search results to within a particular organization\nReturns a list of function objects\nCreate function\nCreate a new function. If there is an existing function in the project with the same slug as the one specified in the request, will return the existing function unmodified\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nRequest Body\napplication/json\nOptionalAny desired information about the new function object\nproject_id\nUnique identifier for the project that the prompt belongs under\n\"uuid\"\nname\nName of the prompt\n1\nslug\nUnique identifier for the prompt\n1\ndescription\nstringTextual description of the prompt\nprompt_data\nobjectThe prompt, model, and its parameters\ntags\narray<string>A list of tags for the prompt\nfunction_type\nstring\"llm\" | \"scorer\" | \"task\" | \"tool\" | null\nfunction_data\norigin\nobjectfunction_schema\nobjectJSON schema for the function's parameters and return type\nReturns the new function object\nCreate or replace function\nCreate or replace function. If there is an existing function in the project with the same slug as the one specified in the request, will replace the existing function with the provided fields\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nRequest Body\napplication/json\nOptionalAny desired information about the new function object\nproject_id\nUnique identifier for the project that the prompt belongs under\n\"uuid\"\nname\nName of the prompt\n1\nslug\nUnique identifier for the prompt\n1\ndescription\nstringTextual description of the prompt\nprompt_data\nobjectThe prompt, model, and its parameters\ntags\narray<string>A list of tags for the prompt\nfunction_type\nstring\"llm\" | \"scorer\" | \"task\" | \"tool\" | null\nfunction_data\norigin\nobjectfunction_schema\nobjectJSON schema for the function's parameters and return type\nReturns the new function object\nGet function\nGet a function object by its id\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nPath Parameters\nfunction_id\nFunction id\n\"uuid\"\nQuery Parameters\nversion\nstringRetrieve prompt at a specific version.\nThe version id can either be a transaction id (e.g. '1000192656880881099') or a version identifier (e.g. '81cd05ee665fdfb3').\nReturns the function object\nPartially update function\nPartially update a function object. Specify the fields to update in the payload. Any object-type fields will be deep-merged with existing content. Currently we do not support removing fields or setting them to null.\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nRequest Body\napplication/json\nOptionalFields to update\nname\nstringName of the prompt\ndescription\nstringTextual description of the prompt\nprompt_data\nobjectThe prompt, model, and its parameters\nfunction_data\nAny properties in prompt, code, graph, remote_eval, globaltags\narray<string>A list of tags for the prompt\nPath Parameters\nfunction_id\nFunction id\n\"uuid\"\nReturns the function object\nDelete function\nDelete a function object by its id\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nPath Parameters\nfunction_id\nFunction id\n\"uuid\"\nReturns the deleted function object\nInvoke function\nInvoke a function.\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nRequest Body\napplication/json\nOptionalFunction invocation parameters\ninput\nunknownArgument to the function, which can be any JSON serializable value\nexpected\nunknownThe expected output of the function\nmetadata\nobjectAny relevant metadata. This will be logged and available as the metadata\nargument.\ntags\narray<string>Any relevant tags to log on the span.\nmessages\narray<Any properties in system, user, assistant, tool, function, developer, fallback>If the function is an LLM, additional messages to pass along to it\nparent\nAny properties in span_parent_struct, stringOptions for tracing the function call\nstream\nbooleanWhether to stream the response. If true, results will be returned in the Braintrust SSE format.\nmode\nstringThe mode format of the returned value (defaults to 'auto')\n\"auto\" | \"parallel\" | null\nstrict\nbooleanIf true, throw an error if one of the variables in the prompt is not present in the input\nversion\nstringThe version of the function\nPath Parameters\nfunction_id\nFunction id\n\"uuid\"\nFunction invocation response", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/reference/api", "anchor": "API"}, {"href": "https://www.braintrust.dev/docs/reference/api/Acls", "anchor": "Acls"}, {"href": "https://www.braintrust.dev/docs/reference/api/AiSecrets", "anchor": "Ai Secrets"}, {"href": "https://www.braintrust.dev/docs/reference/api/ApiKeys", "anchor": "Api Keys"}, {"href": "https://www.braintrust.dev/docs/reference/api/CrossObject", "anchor": "Cross Object"}, {"href": "https://www.braintrust.dev/docs/reference/api/Datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/reference/api/EnvVars", "anchor": "Env Vars"}, {"href": "https://www.braintrust.dev/docs/reference/api/Evals", "anchor": "Evals"}, {"href": "https://www.braintrust.dev/docs/reference/api/Experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/reference/api/Functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/reference/api/Groups", "anchor": "Groups"}, {"href": "https://www.braintrust.dev/docs/reference/api/Logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/reference/api/Organizations", "anchor": "Organizations"}, {"href": "https://www.braintrust.dev/docs/reference/api/Other", "anchor": "Other"}, {"href": "https://www.braintrust.dev/docs/reference/api/Projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectScores", "anchor": "Project Scores"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectTags", "anchor": "Project Tags"}, {"href": "https://www.braintrust.dev/docs/reference/api/Prompts", "anchor": "Prompts"}, {"href": "https://www.braintrust.dev/docs/reference/api/Proxy", "anchor": "Proxy"}, {"href": "https://www.braintrust.dev/docs/reference/api/Roles", "anchor": "Roles"}, {"href": "https://www.braintrust.dev/docs/reference/api/ServiceTokens", "anchor": "Service tokens"}, {"href": "https://www.braintrust.dev/docs/reference/api/SpanIframes", "anchor": "Span Iframes"}, {"href": "https://www.braintrust.dev/docs/reference/api/Users", "anchor": "Users"}, {"href": "https://www.braintrust.dev/docs/reference/api/Views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/reference/autoevals", "anchor": "Autoevals library"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "BTQL query syntax"}, {"href": "https://www.braintrust.dev/docs/reference/streaming", "anchor": "Streaming"}, {"href": "https://www.braintrust.dev/docs/reference/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/reference/reasoning", "anchor": "Reasoning"}, {"href": "https://www.braintrust.dev/docs/reference/organizations", "anchor": "Organizations"}, {"href": "https://www.braintrust.dev/docs/reference/mcp", "anchor": "Model Context Protocol (MCP)"}, {"href": "https://www.braintrust.dev/docs/reference/object-links", "anchor": "Sharing via URL"}, {"href": "https://www.braintrust.dev/docs/reference/glossary", "anchor": "Glossary"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/reference/api", "anchor": "API"}, {"href": "https://www.braintrust.dev/docs/reference/api/Functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/reference/api/Functions", "anchor": "List functions"}, {"href": "https://www.braintrust.dev/docs/reference/api/Functions", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/Functions", "anchor": "Query Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/api/Functions", "anchor": "Create function"}, {"href": "https://www.braintrust.dev/docs/reference/api/Functions", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/Functions", "anchor": "Request Body"}, {"href": "https://www.braintrust.dev/docs/reference/api/Functions", "anchor": "Create or replace function"}, {"href": "https://www.braintrust.dev/docs/reference/api/Functions", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/Functions", "anchor": "Request Body"}, {"href": "https://www.braintrust.dev/docs/reference/api/Functions", "anchor": "Get function"}, {"href": "https://www.braintrust.dev/docs/reference/api/Functions", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/Functions", "anchor": "Path Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/api/Functions", "anchor": "Query Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/api/Functions", "anchor": "Partially update function"}, {"href": "https://www.braintrust.dev/docs/reference/api/Functions", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/Functions", "anchor": "Request Body"}, {"href": "https://www.braintrust.dev/docs/reference/api/Functions", "anchor": "Path Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/api/Functions", "anchor": "Delete function"}, {"href": "https://www.braintrust.dev/docs/reference/api/Functions", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/Functions", "anchor": "Path Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/api/Functions", "anchor": "Invoke function"}, {"href": "https://www.braintrust.dev/docs/reference/api/Functions", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/Functions", "anchor": "Request Body"}, {"href": "https://www.braintrust.dev/docs/reference/api/Functions", "anchor": "Path Parameters"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}, {"href": "https://www.braintrust.dev/docs/reference/api/Functions", "anchor": "List functions"}, {"href": "https://www.braintrust.dev/docs/reference/api/Functions", "anchor": "Create function"}, {"href": "https://www.braintrust.dev/docs/reference/api/Functions", "anchor": "Create or replace function"}, {"href": "https://www.braintrust.dev/docs/reference/api/Functions", "anchor": "Get function"}, {"href": "https://www.braintrust.dev/docs/reference/api/Functions", "anchor": "Partially update function"}, {"href": "https://www.braintrust.dev/docs/reference/api/Functions", "anchor": "Delete function"}, {"href": "https://www.braintrust.dev/docs/reference/api/Functions", "anchor": "Invoke function"}], "depth": 3}, "https://www.braintrust.dev/docs/reference/functions": {"url": "https://www.braintrust.dev/docs/reference/functions", "title": "Functions - Docs - Reference - Functions - Braintrust", "text": "Functions\nMany of the advanced capabilities of Braintrust involve defining and calling custom code functions. Currently, Braintrust supports defining functions in JavaScript/TypeScript and Python, which you can use as custom scorers or callable tools.\nThis guide serves as a reference for functions, how they work, and some security considerations when working with them.\nAccessing functions\nSeveral places in the UI, for example the custom scorer menu in the playground, allow you to define functions. You can also\nbundle them in your code and push them to Braintrust with braintrust push\nand braintrust eval --push\n. Technically speaking,\nfunctions are a generalization of prompts and code functions, so when you define a custom prompt, you are technically defining\na \"prompt function\".\nEvery function supports a number of common features:\n- Well-defined parameters and return types\n- Streaming and non-streaming invocation\n- Automatic tracing and logging in Braintrust\n- Prompts can be loaded into your code in the OpenAI argument format\n- Prompts and code can be easily saved and uploaded from your codebase\nSee the API docs for more information on how to create and invoke functions.\nSandbox\nFunctions are executed in a secure sandbox environment. If you are self-hosting Braintrust, then you must:\n- Set\nEnableQuarantine\ntotrue\nin the Cloudformation stack - Set\nALLOW_CODE_FUNCTION_EXECUTION\nto1\nin the Docker configuration\nIf you use our managed AWS stack, custom code runs in a quarantined-VPC in lambda functions which are sandboxed and isolated from your other AWS resources. If you run via Docker, then the code runs in a sandbox but not a virtual machine, so it is your responsibility to ensure that malicious code is not uploaded to Braintrust.\nFor more information on the security architecture underlying code execution, please reach out to us.", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/reference/api", "anchor": "API"}, {"href": "https://www.braintrust.dev/docs/reference/autoevals", "anchor": "Autoevals library"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "BTQL query syntax"}, {"href": "https://www.braintrust.dev/docs/reference/streaming", "anchor": "Streaming"}, {"href": "https://www.braintrust.dev/docs/reference/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/reference/reasoning", "anchor": "Reasoning"}, {"href": "https://www.braintrust.dev/docs/reference/organizations", "anchor": "Organizations"}, {"href": "https://www.braintrust.dev/docs/reference/mcp", "anchor": "Model Context Protocol (MCP)"}, {"href": "https://www.braintrust.dev/docs/reference/object-links", "anchor": "Sharing via URL"}, {"href": "https://www.braintrust.dev/docs/reference/glossary", "anchor": "Glossary"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/reference/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/reference/functions", "anchor": "Accessing functions"}, {"href": "https://www.braintrust.dev/docs/reference/api/Functions", "anchor": "API docs"}, {"href": "https://www.braintrust.dev/docs/reference/functions", "anchor": "Sandbox"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting/aws", "anchor": "Cloudformation stack"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting/docker", "anchor": "Docker configuration"}, {"href": "mailto:support@braintrust.dev", "anchor": "reach out to us"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}, {"href": "https://www.braintrust.dev/docs/reference/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/reference/functions", "anchor": "Accessing functions"}, {"href": "https://www.braintrust.dev/docs/reference/functions", "anchor": "Sandbox"}], "depth": 3}, "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/InvokeFunctionArgs": {"url": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/InvokeFunctionArgs", "title": "InvokeFunctionArgs - Docs - Reference - Braintrust", "text": "Interface: InvokeFunctionArgs<Input, Output, Stream>\nArguments for the invoke\nfunction.\nType parameters\nProperties\nfunction_id\n\u2022 Optional\nfunction_id: string\nThe ID of the function to invoke.\nglobalFunction\n\u2022 Optional\nglobalFunction: string\nThe name of the global function to invoke.\ninput\n\u2022 input: Input\nThe input to the function. This will be logged as the input\nfield in the span.\nmessages\n\u2022 Optional\nmessages: ({ content\n: {} ; name?\n: string\n; role\n: \"system\"\n} | { content\n: {} ; name?\n: string\n; role\n: \"user\"\n} | { content?\n: null\n| string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text\n: string\n; type\n: \"text\"\n}[] ; function_call?\n: { arguments\n: string\n; name\n: string\n} ; name?\n: string\n; reasoning?\n: { content?\n: string\n; id?\n: string\n}[] ; role\n: \"assistant\"\n; tool_calls?\n: { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[] } | { content\n: {} ; role\n: \"tool\"\n; tool_call_id\n: string\n} | { content\n: null\n| string\n; name\n: string\n; role\n: \"function\"\n} | { content?\n: null\n| string\n; role\n: \"model\"\n})[]\nAdditional OpenAI-style messages to add to the prompt (only works for llm functions).\nmetadata\n\u2022 Optional\nmetadata: Record\n<string\n, unknown\n>\nAdditional metadata to add to the span. This will be logged as the metadata\nfield in the span.\nIt will also be available as the {{metadata}} field in the prompt and as the metadata\nargument\nto the function.\nmode\n\u2022 Optional\nmode: \"auto\"\n| \"parallel\"\nThe mode of the function. If \"auto\", will return a string if the function returns a string, and a JSON object otherwise. If \"parallel\", will return an array of JSON objects with one object per tool call.\nparent\n\u2022 Optional\nparent: string\n| Exportable\nThe parent of the function. This can be an existing span, logger, or experiment, or\nthe output of .export()\nif you are distributed tracing. If unspecified, will use\nthe same semantics as traced()\nto determine the parent and no-op if not in a tracing\ncontext.\nprojectName\n\u2022 Optional\nprojectName: string\nThe name of the project containing the function to invoke.\npromptSessionFunctionId\n\u2022 Optional\npromptSessionFunctionId: string\nThe ID of the function in the prompt session to invoke.\npromptSessionId\n\u2022 Optional\npromptSessionId: string\nThe ID of the prompt session to invoke the function from.\nschema\n\u2022 Optional\nschema: Stream\nextends true\n? never\n: ZodType\n<Output\n, ZodTypeDef\n, Output\n>\nA Zod schema to validate the output of the function and return a typed value. This\nis only used if stream\nis false.\nslug\n\u2022 Optional\nslug: string\nThe slug of the function to invoke.\nstate\n\u2022 Optional\nstate: BraintrustState\n(Advanced) This parameter allows you to pass in a custom login state. This is useful for multi-tenant environments where you are running functions from different Braintrust organizations.\nstream\n\u2022 Optional\nstream: Stream\nWhether to stream the function's output. If true, the function will return a\nBraintrustStream\n, otherwise it will return the output of the function as a JSON\nobject.\nstrict\n\u2022 Optional\nstrict: boolean\nWhether to use strict mode for the function. If true, the function will throw an error if the variable names in the prompt do not match the input keys.\ntags\n\u2022 Optional\ntags: string\n[]\nTags to add to the span. This will be logged as the tags\nfield in the span.\nversion\n\u2022 Optional\nversion: string\nThe version of the function to invoke.", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "TypeScript"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/AttachmentParams", "anchor": "AttachmentParams"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/BackgroundLoggerOpts", "anchor": "BackgroundLoggerOpts"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/DatasetSummary", "anchor": "DatasetSummary"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/DataSummary", "anchor": "DataSummary"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/EvalHooks", "anchor": "EvalHooks"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/Evaluator", "anchor": "Evaluator"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/ExperimentSummary", "anchor": "ExperimentSummary"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/Exportable", "anchor": "Exportable"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/ExternalAttachmentParams", "anchor": "ExternalAttachmentParams"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/InvokeFunctionArgs", "anchor": "InvokeFunctionArgs"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/LoginOptions", "anchor": "LoginOptions"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/LogOptions", "anchor": "LogOptions"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/MetricSummary", "anchor": "MetricSummary"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/ObjectMetadata", "anchor": "ObjectMetadata"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/ParentExperimentIds", "anchor": "ParentExperimentIds"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/ParentProjectLogIds", "anchor": "ParentProjectLogIds"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/ReporterBody", "anchor": "ReporterBody"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/ScoreSummary", "anchor": "ScoreSummary"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/Span", "anchor": "Span"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "Python"}, {"href": "https://www.braintrust.dev/docs/reference/api", "anchor": "API"}, {"href": "https://www.braintrust.dev/docs/reference/autoevals", "anchor": "Autoevals library"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "BTQL query syntax"}, {"href": "https://www.braintrust.dev/docs/reference/streaming", "anchor": "Streaming"}, {"href": "https://www.braintrust.dev/docs/reference/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/reference/reasoning", "anchor": "Reasoning"}, {"href": "https://www.braintrust.dev/docs/reference/organizations", "anchor": "Organizations"}, {"href": "https://www.braintrust.dev/docs/reference/mcp", "anchor": "Model Context Protocol (MCP)"}, {"href": "https://www.braintrust.dev/docs/reference/object-links", "anchor": "Sharing via URL"}, {"href": "https://www.braintrust.dev/docs/reference/glossary", "anchor": "Glossary"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "TypeScript"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/InvokeFunctionArgs", "anchor": "Interface: InvokeFunctionArgs<Input, Output, Stream>"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/InvokeFunctionArgs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/InvokeFunctionArgs", "anchor": "Properties"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/InvokeFunctionArgs", "anchor": "function_id"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/InvokeFunctionArgs", "anchor": "globalFunction"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/InvokeFunctionArgs", "anchor": "input"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/InvokeFunctionArgs", "anchor": "messages"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/InvokeFunctionArgs", "anchor": "metadata"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/InvokeFunctionArgs", "anchor": "mode"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/InvokeFunctionArgs", "anchor": "parent"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/Exportable.md", "anchor": "Exportable"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/InvokeFunctionArgs", "anchor": "projectName"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/InvokeFunctionArgs", "anchor": "promptSessionFunctionId"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/InvokeFunctionArgs", "anchor": "promptSessionId"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/InvokeFunctionArgs", "anchor": "schema"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/InvokeFunctionArgs", "anchor": "slug"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/InvokeFunctionArgs", "anchor": "state"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/BraintrustState.md", "anchor": "BraintrustState"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/InvokeFunctionArgs", "anchor": "stream"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/InvokeFunctionArgs", "anchor": "strict"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/InvokeFunctionArgs", "anchor": "tags"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/InvokeFunctionArgs", "anchor": "version"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/InvokeFunctionArgs", "anchor": "Interface: InvokeFunctionArgs<Input, Output, Stream>"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/InvokeFunctionArgs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/InvokeFunctionArgs", "anchor": "Properties"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/InvokeFunctionArgs", "anchor": "function_id"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/InvokeFunctionArgs", "anchor": "globalFunction"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/InvokeFunctionArgs", "anchor": "input"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/InvokeFunctionArgs", "anchor": "messages"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/InvokeFunctionArgs", "anchor": "metadata"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/InvokeFunctionArgs", "anchor": "mode"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/InvokeFunctionArgs", "anchor": "parent"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/InvokeFunctionArgs", "anchor": "projectName"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/InvokeFunctionArgs", "anchor": "promptSessionFunctionId"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/InvokeFunctionArgs", "anchor": "promptSessionId"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/InvokeFunctionArgs", "anchor": "schema"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/InvokeFunctionArgs", "anchor": "slug"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/InvokeFunctionArgs", "anchor": "state"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/InvokeFunctionArgs", "anchor": "stream"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/InvokeFunctionArgs", "anchor": "strict"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/InvokeFunctionArgs", "anchor": "tags"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/InvokeFunctionArgs", "anchor": "version"}], "depth": 3}, "https://www.braintrust.dev/docs/reference/libs/nodejs": {"url": "https://www.braintrust.dev/docs/reference/libs/nodejs", "title": "TypeScript - Docs - Reference - Braintrust", "text": "braintrust\nAn isomorphic JS library for working with Braintrust. This library contains functionality for running evaluations, logging completions, loading and invoking functions, and more.\nbraintrust\nis distributed as a library on NPM.\nIt is also open source and available on GitHub.\nQuickstart\nInstall the library with npm (or yarn).\nThen, create a file like hello.eval.ts\nwith the following content:\nFinally, run the script with npx braintrust eval hello.eval.ts\n.\nClasses\n- Attachment\n- BaseAttachment\n- BraintrustState\n- BraintrustStream\n- CodeFunction\n- CodePrompt\n- Dataset\n- EvalResultWithSummary\n- Experiment\n- ExternalAttachment\n- FailedHTTPResponse\n- LazyValue\n- Logger\n- NoopSpan\n- Project\n- Prompt\n- PromptBuilder\n- ReadonlyAttachment\n- ReadonlyExperiment\n- ScorerBuilder\n- SpanImpl\n- TestBackgroundLogger\n- ToolBuilder\nInterfaces\n- AttachmentParams\n- BackgroundLoggerOpts\n- DataSummary\n- DatasetSummary\n- EvalHooks\n- Evaluator\n- ExperimentSummary\n- Exportable\n- ExternalAttachmentParams\n- InvokeFunctionArgs\n- LogOptions\n- LoginOptions\n- MetricSummary\n- ObjectMetadata\n- ParentExperimentIds\n- ParentProjectLogIds\n- ReporterBody\n- ScoreSummary\n- Span\nNamespaces\nFunctions\nBaseExperiment\n\u25b8 BaseExperiment<Input\n, Expected\n, Metadata\n>(options?\n): BaseExperiment\n<Input\n, Expected\n, Metadata\n>\nUse this to specify that the dataset should actually be the data from a previous (base) experiment. If you do not specify a name, Braintrust will automatically figure out the best base experiment to use based on your git history (or fall back to timestamps).\nType parameters\nParameters\nReturns\nBaseExperiment\n<Input\n, Expected\n, Metadata\n>\nEval\n\u25b8 Eval<Input\n, Output\n, Expected\n, Metadata\n, EvalReport\n, Parameters\n>(name\n, evaluator\n, reporterOrOpts?\n): Promise\n<EvalResultWithSummary\n<Input\n, Output\n, Expected\n, Metadata\n>>\nType parameters\nParameters\nReturns\nPromise\n<EvalResultWithSummary\n<Input\n, Output\n, Expected\n, Metadata\n>>\nReporter\n\u25b8 Reporter<EvalReport\n>(name\n, reporter\n): ReporterDef\n<EvalReport\n>\nType parameters\nParameters\nReturns\nReporterDef\n<EvalReport\n>\nbuildLocalSummary\n\u25b8 buildLocalSummary(evaluator\n, results\n): ExperimentSummary\nParameters\nReturns\ncreateFinalValuePassThroughStream\n\u25b8 createFinalValuePassThroughStream<T\n>(onFinal\n, onError\n): TransformStream\n<T\n, BraintrustStreamChunk\n>\nCreate a stream that passes through the final value of the stream. This is\nused to implement BraintrustStream.finalValue()\n.\nType parameters\nParameters\nReturns\nTransformStream\n<T\n, BraintrustStreamChunk\n>\nA new stream that passes through the final value of the stream.\ncurrentExperiment\n\u25b8 currentExperiment(options?\n): Experiment\n| undefined\nReturns the currently-active experiment (set by init). Returns undefined if no current experiment has been set.\nParameters\nReturns\nExperiment\n| undefined\ncurrentLogger\n\u25b8 currentLogger<IsAsyncFlush\n>(options?\n): Logger\n<IsAsyncFlush\n> | undefined\nReturns the currently-active logger (set by initLogger). Returns undefined if no current logger has been set.\nType parameters\nParameters\nReturns\nLogger\n<IsAsyncFlush\n> | undefined\ncurrentSpan\n\u25b8 currentSpan(options?\n): Span\nReturn the currently-active span for logging (set by one of the traced\nmethods). If there is no active span, returns a no-op span object, which supports the same interface as spans but does no logging.\nSee Span for full details.\nParameters\nReturns\ndefaultErrorScoreHandler\n\u25b8 defaultErrorScoreHandler(args\n): undefined\n| void\n| Record\n<string\n, number\n>\nParameters\nReturns\nundefined\n| void\n| Record\n<string\n, number\n>\ndeserializePlainStringAsJSON\n\u25b8 deserializePlainStringAsJSON(s\n): { error\n: undefined\n= undefined; value\n: any\n} | { error\n: unknown\n= e; value\n: string\n= s }\nParameters\nReturns\n{ error\n: undefined\n= undefined; value\n: any\n} | { error\n: unknown\n= e; value\n: string\n= s }\ndevNullWritableStream\n\u25b8 devNullWritableStream(): WritableStream\nReturns\nWritableStream\nflush\n\u25b8 flush(options?\n): Promise\n<void\n>\nFlush any pending rows to the server.\nParameters\nReturns\nPromise\n<void\n>\ngetSpanParentObject\n\u25b8 getSpanParentObject<IsAsyncFlush\n>(options?\n): Span\n| Experiment\n| Logger\n<IsAsyncFlush\n>\nMainly for internal use. Return the parent object for starting a span in a global context.\nType parameters\nParameters\nReturns\nSpan\n| Experiment\n| Logger\n<IsAsyncFlush\n>\ninit\n\u25b8 init<IsOpen\n>(options\n): InitializedExperiment\n<IsOpen\n>\nLog in, and then initialize a new experiment in a specified project. If the project does not exist, it will be created.\nType parameters\nParameters\nReturns\nInitializedExperiment\n<IsOpen\n>\nThe newly created Experiment.\n\u25b8 init<IsOpen\n>(project\n, options?\n): InitializedExperiment\n<IsOpen\n>\nLegacy form of init\nwhich accepts the project name as the first parameter,\nseparately from the remaining options. See init(options)\nfor full details.\nType parameters\nParameters\nReturns\nInitializedExperiment\n<IsOpen\n>\ninitDataset\n\u25b8 initDataset<IsLegacyDataset\n>(options\n): Dataset\n<IsLegacyDataset\n>\nCreate a new dataset in a specified project. If the project does not exist, it will be created.\nType parameters\nParameters\nReturns\nDataset\n<IsLegacyDataset\n>\nThe newly created Dataset.\n\u25b8 initDataset<IsLegacyDataset\n>(project\n, options?\n): Dataset\n<IsLegacyDataset\n>\nLegacy form of initDataset\nwhich accepts the project name as the first\nparameter, separately from the remaining options.\nSee initDataset(options)\nfor full details.\nType parameters\nParameters\nReturns\nDataset\n<IsLegacyDataset\n>\ninitExperiment\n\u25b8 initExperiment<IsOpen\n>(options\n): InitializedExperiment\n<IsOpen\n>\nAlias for init(options).\nType parameters\nParameters\nReturns\nInitializedExperiment\n<IsOpen\n>\n\u25b8 initExperiment<IsOpen\n>(project\n, options?\n): InitializedExperiment\n<IsOpen\n>\nAlias for init(project, options).\nType parameters\nParameters\nReturns\nInitializedExperiment\n<IsOpen\n>\ninitFunction\n\u25b8 initFunction(options\n): (input\n: any\n) => Promise\n<any\n>\nCreates a function that can be used as a task or scorer in the Braintrust evaluation framework. The returned function wraps a Braintrust function and can be passed directly to Eval().\nWhen used as a task:\nWhen used as a scorer:\nParameters\nReturns\nfn\nA function that can be used as a task or scorer in Eval().\n\u25b8 (input\n): Promise\n<any\n>\nParameters\nReturns\nPromise\n<any\n>\ninitLogger\n\u25b8 initLogger<IsAsyncFlush\n>(options?\n): Logger\n<IsAsyncFlush\n>\nCreate a new logger in a specified project. If the project does not exist, it will be created.\nType parameters\nParameters\nReturns\nLogger\n<IsAsyncFlush\n>\nThe newly created Logger.\ninvoke\n\u25b8 invoke<Input\n, Output\n, Stream\n>(args\n): Promise\n<InvokeReturn\n<Stream\n, Output\n>>\nInvoke a Braintrust function, returning a BraintrustStream\nor the value as a plain\nJavascript object.\nType parameters\nParameters\nReturns\nPromise\n<InvokeReturn\n<Stream\n, Output\n>>\nThe output of the function.\nloadPrompt\n\u25b8 loadPrompt(options\n): Promise\n<Prompt\n<true\n, true\n>>\nLoad a prompt from the specified project.\nParameters\nReturns\nPromise\n<Prompt\n<true\n, true\n>>\nThe prompt object.\nThrows\nIf the prompt is not found.\nThrows\nIf multiple prompts are found with the same slug in the same project (this should never happen).\nExample\nlog\n\u25b8 log(event\n): string\nLog a single event to the current experiment. The event will be batched and uploaded behind the scenes.\nParameters\nReturns\nstring\nThe id\nof the logged event.\nlogError\n\u25b8 logError(span\n, error\n): void\nParameters\nReturns\nvoid\nlogin\n\u25b8 login(options?\n): Promise\n<BraintrustState\n>\nLog into Braintrust. This will prompt you for your API token, which you can find at\nhttps://www.braintrust.dev/app/token. This method is called automatically by init()\n.\nParameters\nReturns\nPromise\n<BraintrustState\n>\nloginToState\n\u25b8 loginToState(options?\n): Promise\n<BraintrustState\n>\nParameters\nReturns\nPromise\n<BraintrustState\n>\nnewId\n\u25b8 newId(): string\nReturns\nstring\nparseCachedHeader\n\u25b8 parseCachedHeader(value\n): number\n| undefined\nParameters\nReturns\nnumber\n| undefined\npermalink\n\u25b8 permalink(slug\n, opts?\n): Promise\n<string\n>\nFormat a permalink to the Braintrust application for viewing the span\nrepresented by the provided slug\n.\nLinks can be generated at any time, but they will only become viewable after the span and its root have been flushed to the server and ingested.\nIf you have a Span\nobject, use Span.link instead.\nParameters\nReturns\nPromise\n<string\n>\nA permalink to the exported span.\npromptDefinitionToPromptData\n\u25b8 promptDefinitionToPromptData(promptDefinition\n, rawTools?\n): PromptData\nParameters\nReturns\nPromptData\nrenderMessage\n\u25b8 renderMessage<T\n>(render\n, message\n): T\nType parameters\nParameters\nReturns\nT\nrenderPromptParams\n\u25b8 renderPromptParams(params\n, args\n, options\n): ModelParams\n| undefined\nParameters\nReturns\nModelParams\n| undefined\nreportFailures\n\u25b8 reportFailures<Input\n, Output\n, Expected\n, Metadata\n>(evaluator\n, failingResults\n, \u00abdestructured\u00bb\n): void\nType parameters\nParameters\nReturns\nvoid\nrunEvaluator\n\u25b8 runEvaluator(experiment\n, evaluator\n, progressReporter\n, filters\n, stream\n, parameters?\n): Promise\n<EvalResultWithSummary\n<any\n, any\n, any\n, any\n>>\nParameters\nReturns\nPromise\n<EvalResultWithSummary\n<any\n, any\n, any\n, any\n>>\nsetFetch\n\u25b8 setFetch(fetch\n): void\nSet the fetch implementation to use for requests. You can specify it here,\nor when you call login\n.\nParameters\nReturns\nvoid\nspanComponentsToObjectId\n\u25b8 spanComponentsToObjectId(\u00abdestructured\u00bb\n): Promise\n<string\n>\nParameters\nReturns\nPromise\n<string\n>\nstartSpan\n\u25b8 startSpan<IsAsyncFlush\n>(args?\n): Span\nLower-level alternative to traced\n. This allows you to start a span yourself, and can be useful in situations\nwhere you cannot use callbacks. However, spans started with startSpan\nwill not be marked as the \"current span\",\nso currentSpan()\nand traced()\nwill be no-ops. If you want to mark a span as current, use traced\ninstead.\nSee traced for full details.\nType parameters\nParameters\nReturns\nsummarize\n\u25b8 summarize(options?\n): Promise\n<ExperimentSummary\n>\nSummarize the current experiment, including the scores (compared to the closest reference experiment) and metadata.\nParameters\nReturns\nPromise\n<ExperimentSummary\n>\nA summary of the experiment, including the scores (compared to the closest reference experiment) and metadata.\ntraceable\n\u25b8 traceable<F\n, IsAsyncFlush\n>(fn\n, args?\n): IsAsyncFlush\nextends false\n? (...args\n: Parameters\n<F\n>) => Promise\n<Awaited\n<ReturnType\n<F\n>>> : F\nA synonym for wrapTraced\n. If you're porting from systems that use traceable\n, you can use this to\nmake your codebase more consistent.\nType parameters\nParameters\nReturns\nIsAsyncFlush\nextends false\n? (...args\n: Parameters\n<F\n>) => Promise\n<Awaited\n<ReturnType\n<F\n>>> : F\ntraced\n\u25b8 traced<IsAsyncFlush\n, R\n>(callback\n, args?\n): PromiseUnless\n<IsAsyncFlush\n, R\n>\nToplevel function for starting a span. It checks the following (in precedence order):\n- Currently-active span\n- Currently-active experiment\n- Currently-active logger\nand creates a span under the first one that is active. Alternatively, if parent\nis specified, it creates a span under the specified parent row. If none of these are active, it returns a no-op span object.\nSee Span.traced for full details.\nType parameters\nParameters\nReturns\nPromiseUnless\n<IsAsyncFlush\n, R\n>\nupdateSpan\n\u25b8 updateSpan(\u00abdestructured\u00bb\n): void\nUpdate a span using the output of span.export()\n. It is important that you only resume updating\nto a span once the original span has been fully written and flushed, since otherwise updates to\nthe span may conflict with the original span.\nParameters\nReturns\nvoid\nwithCurrent\n\u25b8 withCurrent<R\n>(span\n, callback\n, state?\n): R\nRuns the provided callback with the span as the current span.\nType parameters\nParameters\nReturns\nR\nwithDataset\n\u25b8 withDataset<R\n, IsLegacyDataset\n>(project\n, callback\n, options?\n): R\nType parameters\nParameters\nReturns\nR\nDeprecated\nUse initDataset instead.\nwithExperiment\n\u25b8 withExperiment<R\n>(project\n, callback\n, options?\n): R\nType parameters\nParameters\nReturns\nR\nDeprecated\nUse init instead.\nwithLogger\n\u25b8 withLogger<IsAsyncFlush\n, R\n>(callback\n, options?\n): R\nType parameters\nParameters\nReturns\nR\nDeprecated\nUse initLogger instead.\nwithParent\n\u25b8 withParent<R\n>(parent\n, callback\n, state?\n): R\nType parameters\nParameters\nReturns\nR\nwrapAISDKModel\n\u25b8 wrapAISDKModel<T\n>(model\n): T\nWrap an ai-sdk model (created with .chat()\n, .completion()\n, etc.) to add tracing. If Braintrust is\nnot configured, this is a no-op\nType parameters\nParameters\nReturns\nT\nThe wrapped object.\nwrapAnthropic\n\u25b8 wrapAnthropic<T\n>(anthropic\n): T\nWrap an Anthropic\nobject (created with new Anthropic(...)\n) to add tracing. If Braintrust is\nnot configured, nothing will be traced. If this is not an Anthropic\nobject, this function is\na no-op.\nCurrently, this only supports the v4\nAPI.\nType parameters\nParameters\nReturns\nT\nThe wrapped Anthropic\nobject.\nwrapOpenAI\n\u25b8 wrapOpenAI<T\n>(openai\n): T\nWrap an OpenAI\nobject (created with new OpenAI(...)\n) to add tracing. If Braintrust is\nnot configured, nothing will be traced. If this is not an OpenAI\nobject, this function is\na no-op.\nCurrently, this only supports the v4\nAPI.\nType parameters\nParameters\nReturns\nT\nThe wrapped OpenAI\nobject.\nwrapOpenAIv4\n\u25b8 wrapOpenAIv4<T\n>(openai\n): T\nType parameters\nParameters\nReturns\nT\nwrapTraced\n\u25b8 wrapTraced<F\n, IsAsyncFlush\n>(fn\n, args?\n): IsAsyncFlush\nextends false\n? (...args\n: Parameters\n<F\n>) => Promise\n<Awaited\n<ReturnType\n<F\n>>> : F\nWrap a function with traced\n, using the arguments as input\nand return value as output\n.\nAny functions wrapped this way will automatically be traced, similar to the @traced\ndecorator\nin Python. If you want to correctly propagate the function's name and define it in one go, then\nyou can do so like this:\nNow, any calls to myFunc\nwill be traced, and the input and output will be logged automatically.\nIf tracing is inactive, i.e. there is no active logger or experiment, it's just a no-op.\nIf you're using NextJS or another framework that minifies your code before deployment, the function name will be obfuscated when deployed. To trace the name properly, you can specify it in the span-level arguments for wrapTraced\nlike so.\nType parameters\nParameters\nReturns\nIsAsyncFlush\nextends false\n? (...args\n: Parameters\n<F\n>) => Promise\n<Awaited\n<ReturnType\n<F\n>>> : F\nThe wrapped function.\nType Aliases\nAnyDataset\n\u01ac AnyDataset: Dataset\n<boolean\n>\nBaseExperiment\n\u01ac BaseExperiment<Input\n, Expected\n, Metadata\n>: Object\nType parameters\nType declaration\nBaseMetadata\n\u01ac BaseMetadata: Record\n<string\n, unknown\n> | void\nBraintrustStreamChunk\n\u01ac BraintrustStreamChunk: z.infer\n<typeof braintrustStreamChunkSchema\n>\nA chunk of data from a Braintrust stream. Each chunk type matches an SSE event type.\nChatPrompt\n\u01ac ChatPrompt: Object\nType declaration\nCodeOpts\n\u01ac CodeOpts<Params\n, Returns\n, Fn\n>: Partial\n<BaseFnOpts\n> & { handler\n: Fn\n} & Schema\n<Params\n, Returns\n>\nType parameters\nCommentEvent\n\u01ac CommentEvent: IdField\n& { _audit_metadata?\n: Record\n<string\n, unknown\n> ; _audit_source\n: Source\n; comment\n: { text\n: string\n} ; created\n: string\n; origin\n: { id\n: string\n} } & ParentExperimentIds\n| ParentProjectLogIds\n| ParentPlaygroundLogIds\nCompiledPrompt\n\u01ac CompiledPrompt<Flavor\n>: CompiledPromptParams\n& { span_info?\n: { metadata\n: { prompt\n: { id\n: string\n; project_id\n: string\n; variables\n: Record\n<string\n, unknown\n> ; version\n: string\n} } ; name?\n: string\n; spanAttributes?\n: Record\n<any\n, any\n> } } & Flavor\nextends \"chat\"\n? ChatPrompt\n: Flavor\nextends \"completion\"\n? CompletionPrompt\n: {}\nType parameters\nCompiledPromptParams\n\u01ac CompiledPromptParams: Omit\n<NonNullable\n<PromptData\n[\"options\"\n]>[\"params\"\n], \"use_cache\"\n> & { model\n: NonNullable\n<NonNullable\n<PromptData\n[\"options\"\n]>[\"model\"\n]> }\nCompletionPrompt\n\u01ac CompletionPrompt: Object\nType declaration\nCreateProjectOpts\n\u01ac CreateProjectOpts: NameOrId\nDatasetRecord\n\u01ac DatasetRecord<IsLegacyDataset\n>: IsLegacyDataset\nextends true\n? LegacyDatasetRecord\n: NewDatasetRecord\nType parameters\nDefaultMetadataType\n\u01ac DefaultMetadataType: void\nDefaultPromptArgs\n\u01ac DefaultPromptArgs: Partial\n<CompiledPromptParams\n& AnyModelParam\n& ChatPrompt\n& CompletionPrompt\n>\nEndSpanArgs\n\u01ac EndSpanArgs: Object\nType declaration\nEvalCase\n\u01ac EvalCase<Input\n, Expected\n, Metadata\n>: { _xact_id?\n: TransactionId\n; created?\n: string\n| null\n; id?\n: string\n; input\n: Input\n; tags?\n: string\n[] ; upsert_id?\n: string\n} & Expected\nextends void\n? object\n: { expected\n: Expected\n} & Metadata\nextends void\n? object\n: { metadata\n: Metadata\n}\nType parameters\nEvalParameterSerializedSchema\n\u01ac EvalParameterSerializedSchema: z.infer\n<typeof evalParametersSerializedSchema\n>\nEvalParameters\n\u01ac EvalParameters: z.infer\n<typeof evalParametersSchema\n>\nEvalResult\n\u01ac EvalResult<Input\n, Output\n, Expected\n, Metadata\n>: EvalCase\n<Input\n, Expected\n, Metadata\n> & { error\n: unknown\n; origin?\n: ObjectReference\n; output\n: Output\n; scores\n: Record\n<string\n, number\n| null\n> }\nType parameters\nEvalScorer\n\u01ac EvalScorer<Input\n, Output\n, Expected\n, Metadata\n>: (args\n: EvalScorerArgs\n<Input\n, Output\n, Expected\n, Metadata\n>) => OneOrMoreScores\n| Promise\n<OneOrMoreScores\n>\nType parameters\nType declaration\n\u25b8 (args\n): OneOrMoreScores\n| Promise\n<OneOrMoreScores\n>\nParameters\nReturns\nOneOrMoreScores\n| Promise\n<OneOrMoreScores\n>\nEvalScorerArgs\n\u01ac EvalScorerArgs<Input\n, Output\n, Expected\n, Metadata\n>: EvalCase\n<Input\n, Expected\n, Metadata\n> & { output\n: Output\n}\nType parameters\nEvalTask\n\u01ac EvalTask<Input\n, Output\n, Expected\n, Metadata\n, Parameters\n>: (input\n: Input\n, hooks\n: EvalHooks\n<Expected\n, Metadata\n, Parameters\n>) => Promise\n<Output\n> | (input\n: Input\n, hooks\n: EvalHooks\n<Expected\n, Metadata\n, Parameters\n>) => Output\nType parameters\nEvaluatorDef\n\u01ac EvaluatorDef<Input\n, Output\n, Expected\n, Metadata\n, Parameters\n>: { evalName\n: string\n; projectName\n: string\n} & Evaluator\n<Input\n, Output\n, Expected\n, Metadata\n, Parameters\n>\nType parameters\nEvaluatorDefinition\n\u01ac EvaluatorDefinition: z.infer\n<typeof evaluatorDefinitionSchema\n>\nEvaluatorDefinitions\n\u01ac EvaluatorDefinitions: z.infer\n<typeof evaluatorDefinitionsSchema\n>\nEvaluatorFile\n\u01ac EvaluatorFile: Object\nType declaration\nEvaluatorManifest\n\u01ac EvaluatorManifest: Record\n<string\n, EvaluatorDef\n<unknown\n, unknown\n, unknown\n, BaseMetadata\n>>\nExperimentLogFullArgs\n\u01ac ExperimentLogFullArgs: Partial\n<Omit\n<OtherExperimentLogFields\n, \"output\"\n| \"scores\"\n>> & Required\n<Pick\n<OtherExperimentLogFields\n, \"output\"\n| \"scores\"\n>> & Partial\n<InputField\n> & Partial\n<IdField\n>\nExperimentLogPartialArgs\n\u01ac ExperimentLogPartialArgs: Partial\n<OtherExperimentLogFields\n> & Partial\n<InputField\n>\nFullInitOptions\n\u01ac FullInitOptions<IsOpen\n>: { project?\n: string\n} & InitOptions\n<IsOpen\n>\nType parameters\nFullLoginOptions\n\u01ac FullLoginOptions: LoginOptions\n& { forceLogin?\n: boolean\n}\nIdField\n\u01ac IdField: Object\nType declaration\nInitOptions\n\u01ac InitOptions<IsOpen\n>: FullLoginOptions\n& { baseExperiment?\n: string\n; baseExperimentId?\n: string\n; dataset?\n: AnyDataset\n; description?\n: string\n; experiment?\n: string\n; gitMetadataSettings?\n: GitMetadataSettings\n; isPublic?\n: boolean\n; metadata?\n: Record\n<string\n, unknown\n> ; projectId?\n: string\n; repoInfo?\n: RepoInfo\n; setCurrent?\n: boolean\n; state?\n: BraintrustState\n; update?\n: boolean\n} & InitOpenOption\n<IsOpen\n>\nType parameters\nInputField\n\u01ac InputField: Object\nType declaration\nInvokeReturn\n\u01ac InvokeReturn<Stream\n, Output\n>: Stream\nextends true\n? BraintrustStream\n: Output\nThe return type of the invoke\nfunction. Conditionally returns a BraintrustStream\nif stream\nis true, otherwise returns the output of the function using the Zod schema's\ntype if present.\nType parameters\nLogCommentFullArgs\n\u01ac LogCommentFullArgs: IdField\n& { _audit_metadata?\n: Record\n<string\n, unknown\n> ; _audit_source\n: Source\n; comment\n: { text\n: string\n} ; created\n: string\n; origin\n: { id\n: string\n} } & ParentExperimentIds\n| ParentProjectLogIds\nLogFeedbackFullArgs\n\u01ac LogFeedbackFullArgs: IdField\n& Partial\n<Omit\n<OtherExperimentLogFields\n, \"output\"\n| \"metrics\"\n| \"datasetRecordId\"\n> & { comment\n: string\n; source\n: Source\n}>\nOtherExperimentLogFields\n\u01ac OtherExperimentLogFields: Object\nType declaration\nPromiseUnless\n\u01ac PromiseUnless<B\n, R\n>: B\nextends true\n? R\n: Promise\n<Awaited\n<R\n>>\nType parameters\nPromptContents\n\u01ac PromptContents: z.infer\n<typeof promptContentsSchema\n>\nPromptDefinition\n\u01ac PromptDefinition: z.infer\n<typeof promptDefinitionSchema\n>\nPromptDefinitionWithTools\n\u01ac PromptDefinitionWithTools: z.infer\n<typeof promptDefinitionWithToolsSchema\n>\nPromptOpts\n\u01ac PromptOpts<HasId\n, HasVersion\n, HasTools\n, HasNoTrace\n>: Partial\n<Omit\n<BaseFnOpts\n, \"name\"\n>> & { name\n: string\n} & HasId\nextends true\n? PromptId\n: Partial\n<PromptId\n> & HasVersion\nextends true\n? PromptVersion\n: Partial\n<PromptVersion\n> & HasTools\nextends true\n? Partial\n<PromptTools\n> : {} & HasNoTrace\nextends true\n? Partial\n<PromptNoTrace\n> : {} & PromptDefinition\nType parameters\nPromptRowWithId\n\u01ac PromptRowWithId<HasId\n, HasVersion\n>: Omit\n<PromptRow\n, \"log_id\"\n| \"org_id\"\n| \"project_id\"\n| \"id\"\n| \"_xact_id\"\n> & Partial\n<Pick\n<PromptRow\n, \"project_id\"\n>> & HasId\nextends true\n? Pick\n<PromptRow\n, \"id\"\n> : Partial\n<Pick\n<PromptRow\n, \"id\"\n>> & HasVersion\nextends true\n? Pick\n<PromptRow\n, \"_xact_id\"\n> : Partial\n<Pick\n<PromptRow\n, \"_xact_id\"\n>>\nType parameters\nScorerOpts\n\u01ac ScorerOpts<Output\n, Input\n, Params\n, Returns\n, Fn\n>: CodeOpts\n<Exact\n<Params\n, ScorerArgs\n<Output\n, Input\n>>, Returns\n, Fn\n> | ScorerPromptOpts\nType parameters\nSerializedBraintrustState\n\u01ac SerializedBraintrustState: z.infer\n<typeof loginSchema\n>\nSetCurrentArg\n\u01ac SetCurrentArg: Object\nType declaration\nSpanContext\n\u01ac SpanContext: Object\nType declaration\nStartSpanArgs\n\u01ac StartSpanArgs: Object\nType declaration\nToolFunctionDefinition\n\u01ac ToolFunctionDefinition: z.infer\n<typeof toolFunctionDefinitionSchema\n>\nWithTransactionId\n\u01ac WithTransactionId<R\n>: R\n& { _xact_id\n: TransactionId\n}\nType parameters\nVariables\nERR_PERMALINK\n\u2022 Const\nERR_PERMALINK: \"https://braintrust.dev/error-generating-link\"\nINTERNAL_BTQL_LIMIT\n\u2022 Const\nINTERNAL_BTQL_LIMIT: 1000\nLEGACY_CACHED_HEADER\n\u2022 Const\nLEGACY_CACHED_HEADER: \"x-cached\"\nNOOP_SPAN\n\u2022 Const\nNOOP_SPAN: NoopSpan\nNOOP_SPAN_PERMALINK\n\u2022 Const\nNOOP_SPAN_PERMALINK: \"https://braintrust.dev/noop-span\"\nX_CACHED_HEADER\n\u2022 Const\nX_CACHED_HEADER: \"x-bt-cached\"\n_exportsForTestingOnly\n\u2022 Const\n_exportsForTestingOnly: Object\nType declaration\nbraintrustStreamChunkSchema\n\u2022 Const\nbraintrustStreamChunkSchema: ZodUnion\n<[ZodObject\n<{ data\n: ZodString\n; type\n: ZodLiteral\n<\"text_delta\"\n> }, \"strip\"\n, ZodTypeAny\n, { data\n: string\n; type\n: \"text_delta\"\n}, { data\n: string\n; type\n: \"text_delta\"\n}>, ZodObject\n<{ data\n: ZodString\n; type\n: ZodLiteral\n<\"reasoning_delta\"\n> }, \"strip\"\n, ZodTypeAny\n, { data\n: string\n; type\n: \"reasoning_delta\"\n}, { data\n: string\n; type\n: \"reasoning_delta\"\n}>, ZodObject\n<{ data\n: ZodString\n; type\n: ZodLiteral\n<\"json_delta\"\n> }, \"strip\"\n, ZodTypeAny\n, { data\n: string\n; type\n: \"json_delta\"\n}, { data\n: string\n; type\n: \"json_delta\"\n}>]>\nevaluatorDefinitionSchema\n\u2022 Const\nevaluatorDefinitionSchema: ZodObject\n<{ parameters\n: ZodOptional\n<ZodRecord\n<ZodString\n, ZodUnion\n<[ZodObject\n<{ default\n: ZodOptional\n<ZodObject\n<{ options\n: ZodOptional\n<ZodNullable\n<ZodObject\n<{ model\n: ZodOptional\n<ZodString\n> ; params\n: ZodOptional\n<ZodUnion\n<[ZodObject\n<{ frequency_penalty\n: ZodOptional\n<ZodNumber\n> ; function_call\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}>]>> ; max_completion_tokens\n: ZodOptional\n<ZodNumber\n> ; max_tokens\n: ZodOptional\n<ZodNumber\n> ; n\n: ZodOptional\n<ZodNumber\n> ; presence_penalty\n: ZodOptional\n<ZodNumber\n> ; reasoning_effort\n: ZodOptional\n<ZodEnum\n<[\"low\"\n, \"medium\"\n, \"high\"\n]>> ; response_format\n: ZodOptional\n<ZodNullable\n<ZodUnion\n<[ZodObject\n<{ type\n: ZodLiteral\n<\"json_object\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"json_object\"\n}, { type\n: \"json_object\"\n}>, ZodObject\n<{ json_schema\n: ZodObject\n<{ description\n: ZodOptional\n<ZodString\n> ; name\n: ZodString\n; schema\n: ZodOptional\n<ZodUnion\n<[ZodRecord\n<ZodString\n, ZodUnknown\n>, ZodString\n]>> ; strict\n: ZodOptional\n<ZodNullable\n<ZodBoolean\n>> }, \"strip\"\n, ZodTypeAny\n, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}> ; type\n: ZodLiteral\n<\"json_schema\"\n> }, \"strip\"\n, ZodTypeAny\n, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}>, ZodObject\n<{ type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"text\"\n}, { type\n: \"text\"\n}>]>>> ; stop\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodOptional\n<ZodNumber\n> ; tool_choice\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodLiteral\n<\"required\"\n>, ZodObject\n<{ function\n: ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}> ; type\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { function\n: { name\n: string\n} ; type\n: \"function\"\n}, { function\n: { name\n: string\n} ; type\n: \"function\"\n}>]>> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, \"passthrough\"\n, ZodTypeAny\n, objectOutputType\n<{ frequency_penalty\n: ZodOptional\n<ZodNumber\n> ; function_call\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}>]>> ; max_completion_tokens\n: ZodOptional\n<ZodNumber\n> ; max_tokens\n: ZodOptional\n<ZodNumber\n> ; n\n: ZodOptional\n<ZodNumber\n> ; presence_penalty\n: ZodOptional\n<ZodNumber\n> ; reasoning_effort\n: ZodOptional\n<ZodEnum\n<[\"low\"\n, \"medium\"\n, \"high\"\n]>> ; response_format\n: ZodOptional\n<ZodNullable\n<ZodUnion\n<[ZodObject\n<{ type\n: ZodLiteral\n<\"json_object\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"json_object\"\n}, { type\n: \"json_object\"\n}>, ZodObject\n<{ json_schema\n: ZodObject\n<{ description\n: ZodOptional\n<ZodString\n> ; name\n: ZodString\n; schema\n: ZodOptional\n<ZodUnion\n<[ZodRecord\n<ZodString\n, ZodUnknown\n>, ZodString\n]>> ; strict\n: ZodOptional\n<ZodNullable\n<ZodBoolean\n>> }, \"strip\"\n, ZodTypeAny\n, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}> ; type\n: ZodLiteral\n<\"json_schema\"\n> }, \"strip\"\n, ZodTypeAny\n, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}>, ZodObject\n<{ type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"text\"\n}, { type\n: \"text\"\n}>]>>> ; stop\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodOptional\n<ZodNumber\n> ; tool_choice\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodLiteral\n<\"required\"\n>, ZodObject\n<{ function\n: ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}> ; type\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { function\n: { name\n: string\n} ; type\n: \"function\"\n}, { function\n: { name\n: string\n} ; type\n: \"function\"\n}>]>> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>, objectInputType\n<{ frequency_penalty\n: ZodOptional\n<ZodNumber\n> ; function_call\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}>]>> ; max_completion_tokens\n: ZodOptional\n<ZodNumber\n> ; max_tokens\n: ZodOptional\n<ZodNumber\n> ; n\n: ZodOptional\n<ZodNumber\n> ; presence_penalty\n: ZodOptional\n<ZodNumber\n> ; reasoning_effort\n: ZodOptional\n<ZodEnum\n<[\"low\"\n, \"medium\"\n, \"high\"\n]>> ; response_format\n: ZodOptional\n<ZodNullable\n<ZodUnion\n<[ZodObject\n<{ type\n: ZodLiteral\n<\"json_object\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"json_object\"\n}, { type\n: \"json_object\"\n}>, ZodObject\n<{ json_schema\n: ZodObject\n<{ description\n: ZodOptional\n<ZodString\n> ; name\n: ZodString\n; schema\n: ZodOptional\n<ZodUnion\n<[ZodRecord\n<ZodString\n, ZodUnknown\n>, ZodString\n]>> ; strict\n: ZodOptional\n<ZodNullable\n<ZodBoolean\n>> }, \"strip\"\n, ZodTypeAny\n, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}> ; type\n: ZodLiteral\n<\"json_schema\"\n> }, \"strip\"\n, ZodTypeAny\n, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}>, ZodObject\n<{ type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"text\"\n}, { type\n: \"text\"\n}>]>>> ; stop\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodOptional\n<ZodNumber\n> ; tool_choice\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodLiteral\n<\"required\"\n>, ZodObject\n<{ function\n: ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}> ; type\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { function\n: { name\n: string\n} ; type\n: \"function\"\n}, { function\n: { name\n: string\n} ; type\n: \"function\"\n}>]>> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>>, ZodObject\n<{ max_tokens\n: ZodNumber\n; max_tokens_to_sample\n: ZodOptional\n<ZodNumber\n> ; stop_sequences\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodNumber\n; top_k\n: ZodOptional\n<ZodNumber\n> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, \"passthrough\"\n, ZodTypeAny\n, objectOutputType\n<{ max_tokens\n: ZodNumber\n; max_tokens_to_sample\n: ZodOptional\n<ZodNumber\n> ; stop_sequences\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodNumber\n; top_k\n: ZodOptional\n<ZodNumber\n> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>, objectInputType\n<{ max_tokens\n: ZodNumber\n; max_tokens_to_sample\n: ZodOptional\n<ZodNumber\n> ; stop_sequences\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodNumber\n; top_k\n: ZodOptional\n<ZodNumber\n> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>>, ZodObject\n<{ maxOutputTokens\n: ZodOptional\n<ZodNumber\n> ; temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; topP\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, \"passthrough\"\n, ZodTypeAny\n, objectOutputType\n<{ maxOutputTokens\n: ZodOptional\n<ZodNumber\n> ; temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; topP\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>, objectInputType\n<{ maxOutputTokens\n: ZodOptional\n<ZodNumber\n> ; temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; topP\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>>, ZodObject\n<{ temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, \"passthrough\"\n, ZodTypeAny\n, objectOutputType\n<{ temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>, objectInputType\n<{ temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>>, ZodObject\n<{ use_cache\n: ZodOptional\n<ZodBoolean\n> }, \"passthrough\"\n, ZodTypeAny\n, objectOutputType\n<{ use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>, objectInputType\n<{ use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>>]>> ; position\n: ZodOptional\n<ZodString\n> }, \"strip\"\n, ZodTypeAny\n, { model?\n: string\n; params?\n: objectOutputType\n<{ frequency_penalty\n: ZodOptional\n<ZodNumber\n> ; function_call\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}>]>> ; max_completion_tokens\n: ZodOptional\n<ZodNumber\n> ; max_tokens\n: ZodOptional\n<ZodNumber\n> ; n\n: ZodOptional\n<ZodNumber\n> ; presence_penalty\n: ZodOptional\n<ZodNumber\n> ; reasoning_effort\n: ZodOptional\n<ZodEnum\n<[\"low\"\n, \"medium\"\n, \"high\"\n]>> ; response_format\n: ZodOptional\n<ZodNullable\n<ZodUnion\n<[ZodObject\n<{ type\n: ZodLiteral\n<\"json_object\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"json_object\"\n}, { type\n: \"json_object\"\n}>, ZodObject\n<{ json_schema\n: ZodObject\n<{ description\n: ZodOptional\n<ZodString\n> ; name\n: ZodString\n; schema\n: ZodOptional\n<ZodUnion\n<[ZodRecord\n<ZodString\n, ZodUnknown\n>, ZodString\n]>> ; strict\n: ZodOptional\n<ZodNullable\n<ZodBoolean\n>> }, \"strip\"\n, ZodTypeAny\n, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}> ; type\n: ZodLiteral\n<\"json_schema\"\n> }, \"strip\"\n, ZodTypeAny\n, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}>, ZodObject\n<{ type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"text\"\n}, { type\n: \"text\"\n}>]>>> ; stop\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodOptional\n<ZodNumber\n> ; tool_choice\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodLiteral\n<\"required\"\n>, ZodObject\n<{ function\n: ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}> ; type\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { function\n: { name\n: string\n} ; type\n: \"function\"\n}, { function\n: { name\n: string\n} ; type\n: \"function\"\n}>]>> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectOutputType\n<{ max_tokens\n: ZodNumber\n; max_tokens_to_sample\n: ZodOptional\n<ZodNumber\n> ; stop_sequences\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodNumber\n; top_k\n: ZodOptional\n<ZodNumber\n> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectOutputType\n<{ maxOutputTokens\n: ZodOptional\n<ZodNumber\n> ; temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; topP\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectOutputType\n<{ temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectOutputType\n<{ use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> ; position?\n: string\n}, { model?\n: string\n; params?\n: objectInputType\n<{ frequency_penalty\n: ZodOptional\n<ZodNumber\n> ; function_call\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}>]>> ; max_completion_tokens\n: ZodOptional\n<ZodNumber\n> ; max_tokens\n: ZodOptional\n<ZodNumber\n> ; n\n: ZodOptional\n<ZodNumber\n> ; presence_penalty\n: ZodOptional\n<ZodNumber\n> ; reasoning_effort\n: ZodOptional\n<ZodEnum\n<[\"low\"\n, \"medium\"\n, \"high\"\n]>> ; response_format\n: ZodOptional\n<ZodNullable\n<ZodUnion\n<[ZodObject\n<{ type\n: ZodLiteral\n<\"json_object\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"json_object\"\n}, { type\n: \"json_object\"\n}>, ZodObject\n<{ json_schema\n: ZodObject\n<{ description\n: ZodOptional\n<ZodString\n> ; name\n: ZodString\n; schema\n: ZodOptional\n<ZodUnion\n<[ZodRecord\n<ZodString\n, ZodUnknown\n>, ZodString\n]>> ; strict\n: ZodOptional\n<ZodNullable\n<ZodBoolean\n>> }, \"strip\"\n, ZodTypeAny\n, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}> ; type\n: ZodLiteral\n<\"json_schema\"\n> }, \"strip\"\n, ZodTypeAny\n, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}>, ZodObject\n<{ type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"text\"\n}, { type\n: \"text\"\n}>]>>> ; stop\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodOptional\n<ZodNumber\n> ; tool_choice\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodLiteral\n<\"required\"\n>, ZodObject\n<{ function\n: ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}> ; type\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { function\n: { name\n: string\n} ; type\n: \"function\"\n}, { function\n: { name\n: string\n} ; type\n: \"function\"\n}>]>> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectInputType\n<{ max_tokens\n: ZodNumber\n; max_tokens_to_sample\n: ZodOptional\n<ZodNumber\n> ; stop_sequences\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodNumber\n; top_k\n: ZodOptional\n<ZodNumber\n> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectInputType\n<{ maxOutputTokens\n: ZodOptional\n<ZodNumber\n> ; temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; topP\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectInputType\n<{ temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectInputType\n<{ use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> ; position?\n: string\n}>>> ; origin\n: ZodOptional\n<ZodNullable\n<ZodObject\n<{ project_id\n: ZodOptional\n<ZodString\n> ; prompt_id\n: ZodOptional\n<ZodString\n> ; prompt_version\n: ZodOptional\n<ZodString\n> }, \"strip\"\n, ZodTypeAny\n, { project_id?\n: string\n; prompt_id?\n: string\n; prompt_version?\n: string\n}, { project_id?\n: string\n; prompt_id?\n: string\n; prompt_version?\n: string\n}>>> ; parser\n: ZodOptional\n<ZodNullable\n<ZodObject\n<{ choice_scores\n: ZodRecord\n<ZodString\n, ZodNumber\n> ; type\n: ZodLiteral\n<\"llm_classifier\"\n> ; use_cot\n: ZodBoolean\n}, \"strip\"\n, ZodTypeAny\n, { choice_scores\n: Record\n<string\n, number\n> ; type\n: \"llm_classifier\"\n; use_cot\n: boolean\n}, { choice_scores\n: Record\n<string\n, number\n> ; type\n: \"llm_classifier\"\n; use_cot\n: boolean\n}>>> ; prompt\n: ZodOptional\n<ZodNullable\n<ZodUnion\n<[ZodObject\n<{ content\n: ZodString\n; type\n: ZodLiteral\n<\"completion\"\n> }, \"strip\"\n, ZodTypeAny\n, { content\n: string\n; type\n: \"completion\"\n}, { content\n: string\n; type\n: \"completion\"\n}>, ZodObject\n<{ messages\n: ZodArray\n<ZodUnion\n<[ZodUnion\n<[ZodObject\n<{ content\n: ZodUnion\n<[ZodDefault\n<ZodString\n>, ZodArray\n<ZodObject\n<{ cache_control\n: ZodOptional\n<ZodObject\n<{ type\n: ZodEnum\n<[\"ephemeral\"\n]> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"ephemeral\"\n}, { type\n: \"ephemeral\"\n}>> ; text\n: ZodDefault\n<ZodString\n> ; type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text\n: string\n; type\n: \"text\"\n}, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}>, \"many\"\n>]> ; name\n: ZodOptional\n<ZodString\n> ; role\n: ZodLiteral\n<\"system\"\n> }, \"strip\"\n, ZodTypeAny\n, { content\n: {} ; name?\n: string\n; role\n: \"system\"\n}, { content?\n: string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; name?\n: string\n; role\n: \"system\"\n}>, ZodObject\n<{ content\n: ZodUnion\n<[ZodDefault\n<ZodString\n>, ZodArray\n<ZodUnion\n<[ZodObject\n<{ cache_control\n: ZodOptional\n<ZodObject\n<{ type\n: ZodEnum\n<[\"ephemeral\"\n]> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"ephemeral\"\n}, { type\n: \"ephemeral\"\n}>> ; text\n: ZodDefault\n<ZodString\n> ; type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text\n: string\n; type\n: \"text\"\n}, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}>, ZodObject\n<{ image_url\n: ZodObject\n<{ detail\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"low\"\n>, ZodLiteral\n<\"high\"\n>]>> ; url\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { detail?\n: \"auto\"\n| \"high\"\n| \"low\"\n; url\n: string\n}, { detail?\n: \"auto\"\n| \"high\"\n| \"low\"\n; url\n: string\n}> ; type\n: ZodLiteral\n<\"image_url\"\n> }, \"strip\"\n, ZodTypeAny\n, { image_url\n: { detail?\n: \"auto\"\n| \"high\"\n| \"low\"\n; url\n: string\n} ; type\n: \"image_url\"\n}, { image_url\n: { detail?\n: \"auto\"\n| \"high\"\n| \"low\"\n; url\n: string\n} ; type\n: \"image_url\"\n}>]>, \"many\"\n>]> ; name\n: ZodOptional\n<ZodString\n> ; role\n: ZodLiteral\n<\"user\"\n> }, \"strip\"\n, ZodTypeAny\n, { content\n: {} ; name?\n: string\n; role\n: \"user\"\n}, { content?\n: string\n| ({ cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n} | { image_url\n: { detail?\n: \"auto\"\n| \"high\"\n| \"low\"\n; url\n: string\n} ; type\n: \"image_url\"\n})[] ; name?\n: string\n; role\n: \"user\"\n}>, ZodObject\n<{ content\n: ZodOptional\n<ZodNullable\n<ZodUnion\n<[ZodString\n, ZodArray\n<ZodObject\n<{ cache_control\n: ZodOptional\n<ZodObject\n<{ type\n: ZodEnum\n<[\"ephemeral\"\n]> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"ephemeral\"\n}, { type\n: \"ephemeral\"\n}>> ; text\n: ZodDefault\n<ZodString\n> ; type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text\n: string\n; type\n: \"text\"\n}, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}>, \"many\"\n>]>>> ; function_call\n: ZodEffects\n<ZodOptional\n<ZodNullable\n<ZodObject\n<{ arguments\n: ZodString\n; name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { arguments\n: string\n; name\n: string\n}, { arguments\n: string\n; name\n: string\n}>>>, undefined\n| { arguments\n: string\n; name\n: string\n}, undefined\n| null\n| { arguments\n: string\n; name\n: string\n}> ; name\n: ZodEffects\n<ZodOptional\n<ZodNullable\n<ZodString\n>>, undefined\n| string\n, undefined\n| null\n| string\n> ; reasoning\n: ZodEffects\n<ZodOptional\n<ZodNullable\n<ZodArray\n<ZodObject\n<{ content\n: ZodEffects\n<ZodOptional\n<ZodNullable\n<ZodString\n>>, undefined\n| string\n, undefined\n| null\n| string\n> ; id\n: ZodEffects\n<ZodOptional\n<ZodNullable\n<ZodString\n>>, undefined\n| string\n, undefined\n| null\n| string\n> }, \"strip\"\n, ZodTypeAny\n, { content?\n: string\n; id?\n: string\n}, { content?\n: null\n| string\n; id?\n: null\n| string\n}>, \"many\"\n>>>, undefined\n| { content?\n: string\n; id?\n: string\n}[], undefined\n| null\n| { content?\n: null\n| string\n; id?\n: null\n| string\n}[]> ; role\n: ZodLiteral\n<\"assistant\"\n> ; tool_calls\n: ZodEffects\n<ZodOptional\n<ZodNullable\n<ZodArray\n<ZodObject\n<{ function\n: ZodObject\n<{ arguments\n: ZodString\n; name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { arguments\n: string\n; name\n: string\n}, { arguments\n: string\n; name\n: string\n}> ; id\n: ZodString\n; type\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}, { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}>, \"many\"\n>>>, undefined\n| { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[], undefined\n| null\n| { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[]> }, \"strip\"\n, ZodTypeAny\n, { content?\n: null\n| string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text\n: string\n; type\n: \"text\"\n}[] ; function_call?\n: { arguments\n: string\n; name\n: string\n} ; name?\n: string\n; reasoning?\n: { content?\n: string\n; id?\n: string\n}[] ; role\n: \"assistant\"\n; tool_calls?\n: { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[] }, { content?\n: null\n| string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; function_call?\n: null\n| { arguments\n: string\n; name\n: string\n} ; name?\n: null\n| string\n; reasoning?\n: null\n| { content?\n: null\n| string\n; id?\n: null\n| string\n}[] ; role\n: \"assistant\"\n; tool_calls?\n: null\n| { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[] }>, ZodObject\n<{ content\n: ZodUnion\n<[ZodDefault\n<ZodString\n>, ZodArray\n<ZodObject\n<{ cache_control\n: ZodOptional\n<ZodObject\n<{ type\n: ZodEnum\n<[\"ephemeral\"\n]> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"ephemeral\"\n}, { type\n: \"ephemeral\"\n}>> ; text\n: ZodDefault\n<ZodString\n> ; type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text\n: string\n; type\n: \"text\"\n}, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}>, \"many\"\n>]> ; role\n: ZodLiteral\n<\"tool\"\n> ; tool_call_id\n: ZodDefault\n<ZodString\n> }, \"strip\"\n, ZodTypeAny\n, { content\n: {} ; role\n: \"tool\"\n; tool_call_id\n: string\n}, { content?\n: string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; role\n: \"tool\"\n; tool_call_id?\n: string\n}>, ZodObject\n<{ content\n: ZodNullable\n<ZodString\n> ; name\n: ZodString\n; role\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { content\n: null\n| string\n; name\n: string\n; role\n: \"function\"\n}, { content\n: null\n| string\n; name\n: string\n; role\n: \"function\"\n}>]>, ZodObject\n<{ content\n: ZodOptional\n<ZodNullable\n<ZodString\n>> ; role\n: ZodEnum\n<[\"model\"\n]> }, \"strip\"\n, ZodTypeAny\n, { content?\n: null\n| string\n; role\n: \"model\"\n}, { content?\n: null\n| string\n; role\n: \"model\"\n}>]>, \"many\"\n> ; tools\n: ZodOptional\n<ZodString\n> ; type\n: ZodLiteral\n<\"chat\"\n> }, \"strip\"\n, ZodTypeAny\n, { messages\n: ({ content\n: {} ; name?\n: string\n; role\n: \"system\"\n} | { content\n: {} ; name?\n: string\n; role\n: \"user\"\n} | { content\n: {} ; role\n: \"tool\"\n; tool_call_id\n: string\n} | { content\n: null\n| string\n; name\n: string\n; role\n: \"function\"\n} | { content?\n: null\n| string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text\n: string\n; type\n: \"text\"\n}[] ; function_call?\n: { arguments\n: string\n; name\n: string\n} ; name?\n: string\n; reasoning?\n: { content?\n: string\n; id?\n: string\n}[] ; role\n: \"assistant\"\n; tool_calls?\n: { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[] } | { content?\n: null\n| string\n; role\n: \"model\"\n})[] ; tools?\n: string\n; type\n: \"chat\"\n}, { messages\n: ({ content?\n: string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; name?\n: string\n; role\n: \"system\"\n} | { content?\n: string\n| ({ cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n} | { image_url\n: { detail?\n: \"auto\"\n| \"high\"\n| \"low\"\n; url\n: string\n} ; type\n: \"image_url\"\n})[] ; name?\n: string\n; role\n: \"user\"\n} | { content?\n: string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; role\n: \"tool\"\n; tool_call_id?\n: string\n} | { content\n: null\n| string\n; name\n: string\n; role\n: \"function\"\n} | { content?\n: null\n| string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; function_call?\n: null\n| { arguments\n: string\n; name\n: string\n} ; name?\n: null\n| string\n; reasoning?\n: null\n| { content?\n: null\n| string\n; id?\n: null\n| string\n}[] ; role\n: \"assistant\"\n; tool_calls?\n: null\n| { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[] } | { content?\n: null\n| string\n; role\n: \"model\"\n})[] ; tools?\n: string\n; type\n: \"chat\"\n}>]>>> ; tool_functions\n: ZodOptional\n<ZodNullable\n<ZodArray\n<ZodUnion\n<[ZodObject\n<{ id\n: ZodString\n; type\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { id\n: string\n; type\n: \"function\"\n}, { id\n: string\n; type\n: \"function\"\n}>, ZodObject\n<{ name\n: ZodString\n; type\n: ZodLiteral\n<\"global\"\n> }, \"strip\"\n, ZodTypeAny\n, { name\n: string\n; type\n: \"global\"\n}, { name\n: string\n; type\n: \"global\"\n}>]>, \"many\"\n>>> }, \"strip\"\n, ZodTypeAny\n, { options?\n: null\n| { model?\n: string\n; params?\n: objectOutputType\n<{ frequency_penalty\n: ZodOptional\n<ZodNumber\n> ; function_call\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}>]>> ; max_completion_tokens\n: ZodOptional\n<ZodNumber\n> ; max_tokens\n: ZodOptional\n<ZodNumber\n> ; n\n: ZodOptional\n<ZodNumber\n> ; presence_penalty\n: ZodOptional\n<ZodNumber\n> ; reasoning_effort\n: ZodOptional\n<ZodEnum\n<[\"low\"\n, \"medium\"\n, \"high\"\n]>> ; response_format\n: ZodOptional\n<ZodNullable\n<ZodUnion\n<[ZodObject\n<{ type\n: ZodLiteral\n<\"json_object\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"json_object\"\n}, { type\n: \"json_object\"\n}>, ZodObject\n<{ json_schema\n: ZodObject\n<{ description\n: ZodOptional\n<ZodString\n> ; name\n: ZodString\n; schema\n: ZodOptional\n<ZodUnion\n<[ZodRecord\n<ZodString\n, ZodUnknown\n>, ZodString\n]>> ; strict\n: ZodOptional\n<ZodNullable\n<ZodBoolean\n>> }, \"strip\"\n, ZodTypeAny\n, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}> ; type\n: ZodLiteral\n<\"json_schema\"\n> }, \"strip\"\n, ZodTypeAny\n, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}>, ZodObject\n<{ type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"text\"\n}, { type\n: \"text\"\n}>]>>> ; stop\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodOptional\n<ZodNumber\n> ; tool_choice\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodLiteral\n<\"required\"\n>, ZodObject\n<{ function\n: ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}> ; type\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { function\n: { name\n: string\n} ; type\n: \"function\"\n}, { function\n: { name\n: string\n} ; type\n: \"function\"\n}>]>> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectOutputType\n<{ max_tokens\n: ZodNumber\n; max_tokens_to_sample\n: ZodOptional\n<ZodNumber\n> ; stop_sequences\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodNumber\n; top_k\n: ZodOptional\n<ZodNumber\n> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectOutputType\n<{ maxOutputTokens\n: ZodOptional\n<ZodNumber\n> ; temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; topP\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectOutputType\n<{ temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectOutputType\n<{ use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> ; position?\n: string\n} ; origin?\n: null\n| { project_id?\n: string\n; prompt_id?\n: string\n; prompt_version?\n: string\n} ; parser?\n: null\n| { choice_scores\n: Record\n<string\n, number\n> ; type\n: \"llm_classifier\"\n; use_cot\n: boolean\n} ; prompt?\n: null\n| { content\n: string\n; type\n: \"completion\"\n} | { messages\n: ({ content\n: {} ; name?\n: string\n; role\n: \"system\"\n} | { content\n: {} ; name?\n: string\n; role\n: \"user\"\n} | { content\n: {} ; role\n: \"tool\"\n; tool_call_id\n: string\n} | { content\n: null\n| string\n; name\n: string\n; role\n: \"function\"\n} | { content?\n: null\n| string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text\n: string\n; type\n: \"text\"\n}[] ; function_call?\n: { arguments\n: string\n; name\n: string\n} ; name?\n: string\n; reasoning?\n: { content?\n: string\n; id?\n: string\n}[] ; role\n: \"assistant\"\n; tool_calls?\n: { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[] } | { content?\n: null\n| string\n; role\n: \"model\"\n})[] ; tools?\n: string\n; type\n: \"chat\"\n} ; tool_functions?\n: null\n| ({ id\n: string\n; type\n: \"function\"\n} | { name\n: string\n; type\n: \"global\"\n})[] }, { options?\n: null\n| { model?\n: string\n; params?\n: objectInputType\n<{ frequency_penalty\n: ZodOptional\n<ZodNumber\n> ; function_call\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}>]>> ; max_completion_tokens\n: ZodOptional\n<ZodNumber\n> ; max_tokens\n: ZodOptional\n<ZodNumber\n> ; n\n: ZodOptional\n<ZodNumber\n> ; presence_penalty\n: ZodOptional\n<ZodNumber\n> ; reasoning_effort\n: ZodOptional\n<ZodEnum\n<[\"low\"\n, \"medium\"\n, \"high\"\n]>> ; response_format\n: ZodOptional\n<ZodNullable\n<ZodUnion\n<[ZodObject\n<{ type\n: ZodLiteral\n<\"json_object\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"json_object\"\n}, { type\n: \"json_object\"\n}>, ZodObject\n<{ json_schema\n: ZodObject\n<{ description\n: ZodOptional\n<ZodString\n> ; name\n: ZodString\n; schema\n: ZodOptional\n<ZodUnion\n<[ZodRecord\n<ZodString\n, ZodUnknown\n>, ZodString\n]>> ; strict\n: ZodOptional\n<ZodNullable\n<ZodBoolean\n>> }, \"strip\"\n, ZodTypeAny\n, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}> ; type\n: ZodLiteral\n<\"json_schema\"\n> }, \"strip\"\n, ZodTypeAny\n, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}>, ZodObject\n<{ type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"text\"\n}, { type\n: \"text\"\n}>]>>> ; stop\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodOptional\n<ZodNumber\n> ; tool_choice\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodLiteral\n<\"required\"\n>, ZodObject\n<{ function\n: ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}> ; type\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { function\n: { name\n: string\n} ; type\n: \"function\"\n}, { function\n: { name\n: string\n} ; type\n: \"function\"\n}>]>> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectInputType\n<{ max_tokens\n: ZodNumber\n; max_tokens_to_sample\n: ZodOptional\n<ZodNumber\n> ; stop_sequences\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodNumber\n; top_k\n: ZodOptional\n<ZodNumber\n> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectInputType\n<{ maxOutputTokens\n: ZodOptional\n<ZodNumber\n> ; temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; topP\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectInputType\n<{ temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectInputType\n<{ use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> ; position?\n: string\n} ; origin?\n: null\n| { project_id?\n: string\n; prompt_id?\n: string\n; prompt_version?\n: string\n} ; parser?\n: null\n| { choice_scores\n: Record\n<string\n, number\n> ; type\n: \"llm_classifier\"\n; use_cot\n: boolean\n} ; prompt?\n: null\n| { content\n: string\n; type\n: \"completion\"\n} | { messages\n: ({ content?\n: string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; name?\n: string\n; role\n: \"system\"\n} | { content?\n: string\n| ({ cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n} | { image_url\n: { detail?\n: \"auto\"\n| \"high\"\n| \"low\"\n; url\n: string\n} ; type\n: \"image_url\"\n})[] ; name?\n: string\n; role\n: \"user\"\n} | { content?\n: string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; role\n: \"tool\"\n; tool_call_id?\n: string\n} | { content\n: null\n| string\n; name\n: string\n; role\n: \"function\"\n} | { content?\n: null\n| string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; function_call?\n: null\n| { arguments\n: string\n; name\n: string\n} ; name?\n: null\n| string\n; reasoning?\n: null\n| { content?\n: null\n| string\n; id?\n: null\n| string\n}[] ; role\n: \"assistant\"\n; tool_calls?\n: null\n| { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[] } | { content?\n: null\n| string\n; role\n: \"model\"\n})[] ; tools?\n: string\n; type\n: \"chat\"\n} ; tool_functions?\n: null\n| ({ id\n: string\n; type\n: \"function\"\n} | { name\n: string\n; type\n: \"global\"\n})[] }>> ; description\n: ZodOptional\n<ZodString\n> ; type\n: ZodLiteral\n<\"prompt\"\n> }, \"strip\"\n, ZodTypeAny\n, { default?\n: { options?\n: null\n| { model?\n: string\n; params?\n: objectOutputType\n<{ frequency_penalty\n: ZodOptional\n<ZodNumber\n> ; function_call\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}>]>> ; max_completion_tokens\n: ZodOptional\n<ZodNumber\n> ; max_tokens\n: ZodOptional\n<ZodNumber\n> ; n\n: ZodOptional\n<ZodNumber\n> ; presence_penalty\n: ZodOptional\n<ZodNumber\n> ; reasoning_effort\n: ZodOptional\n<ZodEnum\n<[\"low\"\n, \"medium\"\n, \"high\"\n]>> ; response_format\n: ZodOptional\n<ZodNullable\n<ZodUnion\n<[ZodObject\n<{ type\n: ZodLiteral\n<\"json_object\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"json_object\"\n}, { type\n: \"json_object\"\n}>, ZodObject\n<{ json_schema\n: ZodObject\n<{ description\n: ZodOptional\n<ZodString\n> ; name\n: ZodString\n; schema\n: ZodOptional\n<ZodUnion\n<[ZodRecord\n<ZodString\n, ZodUnknown\n>, ZodString\n]>> ; strict\n: ZodOptional\n<ZodNullable\n<ZodBoolean\n>> }, \"strip\"\n, ZodTypeAny\n, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}> ; type\n: ZodLiteral\n<\"json_schema\"\n> }, \"strip\"\n, ZodTypeAny\n, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}>, ZodObject\n<{ type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"text\"\n}, { type\n: \"text\"\n}>]>>> ; stop\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodOptional\n<ZodNumber\n> ; tool_choice\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodLiteral\n<\"required\"\n>, ZodObject\n<{ function\n: ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}> ; type\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { function\n: { name\n: string\n} ; type\n: \"function\"\n}, { function\n: { name\n: string\n} ; type\n: \"function\"\n}>]>> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectOutputType\n<{ max_tokens\n: ZodNumber\n; max_tokens_to_sample\n: ZodOptional\n<ZodNumber\n> ; stop_sequences\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodNumber\n; top_k\n: ZodOptional\n<ZodNumber\n> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectOutputType\n<{ maxOutputTokens\n: ZodOptional\n<ZodNumber\n> ; temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; topP\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectOutputType\n<{ temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectOutputType\n<{ use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> ; position?\n: string\n} ; origin?\n: null\n| { project_id?\n: string\n; prompt_id?\n: string\n; prompt_version?\n: string\n} ; parser?\n: null\n| { choice_scores\n: Record\n<string\n, number\n> ; type\n: \"llm_classifier\"\n; use_cot\n: boolean\n} ; prompt?\n: null\n| { content\n: string\n; type\n: \"completion\"\n} | { messages\n: ({ content\n: {} ; name?\n: string\n; role\n: \"system\"\n} | { content\n: {} ; name?\n: string\n; role\n: \"user\"\n} | { content\n: {} ; role\n: \"tool\"\n; tool_call_id\n: string\n} | { content\n: null\n| string\n; name\n: string\n; role\n: \"function\"\n} | { content?\n: null\n| string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text\n: string\n; type\n: \"text\"\n}[] ; function_call?\n: { arguments\n: string\n; name\n: string\n} ; name?\n: string\n; reasoning?\n: { content?\n: string\n; id?\n: string\n}[] ; role\n: \"assistant\"\n; tool_calls?\n: { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[] } | { content?\n: null\n| string\n; role\n: \"model\"\n})[] ; tools?\n: string\n; type\n: \"chat\"\n} ; tool_functions?\n: null\n| ({ id\n: string\n; type\n: \"function\"\n} | { name\n: string\n; type\n: \"global\"\n})[] } ; description?\n: string\n; type\n: \"prompt\"\n}, { default?\n: { options?\n: null\n| { model?\n: string\n; params?\n: objectInputType\n<{ frequency_penalty\n: ZodOptional\n<ZodNumber\n> ; function_call\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}>]>> ; max_completion_tokens\n: ZodOptional\n<ZodNumber\n> ; max_tokens\n: ZodOptional\n<ZodNumber\n> ; n\n: ZodOptional\n<ZodNumber\n> ; presence_penalty\n: ZodOptional\n<ZodNumber\n> ; reasoning_effort\n: ZodOptional\n<ZodEnum\n<[\"low\"\n, \"medium\"\n, \"high\"\n]>> ; response_format\n: ZodOptional\n<ZodNullable\n<ZodUnion\n<[ZodObject\n<{ type\n: ZodLiteral\n<\"json_object\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"json_object\"\n}, { type\n: \"json_object\"\n}>, ZodObject\n<{ json_schema\n: ZodObject\n<{ description\n: ZodOptional\n<ZodString\n> ; name\n: ZodString\n; schema\n: ZodOptional\n<ZodUnion\n<[ZodRecord\n<ZodString\n, ZodUnknown\n>, ZodString\n]>> ; strict\n: ZodOptional\n<ZodNullable\n<ZodBoolean\n>> }, \"strip\"\n, ZodTypeAny\n, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}> ; type\n: ZodLiteral\n<\"json_schema\"\n> }, \"strip\"\n, ZodTypeAny\n, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}>, ZodObject\n<{ type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"text\"\n}, { type\n: \"text\"\n}>]>>> ; stop\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodOptional\n<ZodNumber\n> ; tool_choice\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodLiteral\n<\"required\"\n>, ZodObject\n<{ function\n: ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}> ; type\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { function\n: { name\n: string\n} ; type\n: \"function\"\n}, { function\n: { name\n: string\n} ; type\n: \"function\"\n}>]>> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectInputType\n<{ max_tokens\n: ZodNumber\n; max_tokens_to_sample\n: ZodOptional\n<ZodNumber\n> ; stop_sequences\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodNumber\n; top_k\n: ZodOptional\n<ZodNumber\n> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectInputType\n<{ maxOutputTokens\n: ZodOptional\n<ZodNumber\n> ; temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; topP\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectInputType\n<{ temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectInputType\n<{ use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> ; position?\n: string\n} ; origin?\n: null\n| { project_id?\n: string\n; prompt_id?\n: string\n; prompt_version?\n: string\n} ; parser?\n: null\n| { choice_scores\n: Record\n<string\n, number\n> ; type\n: \"llm_classifier\"\n; use_cot\n: boolean\n} ; prompt?\n: null\n| { content\n: string\n; type\n: \"completion\"\n} | { messages\n: ({ content?\n: string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; name?\n: string\n; role\n: \"system\"\n} | { content?\n: string\n| ({ cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n} | { image_url\n: { detail?\n: \"auto\"\n| \"high\"\n| \"low\"\n; url\n: string\n} ; type\n: \"image_url\"\n})[] ; name?\n: string\n; role\n: \"user\"\n} | { content?\n: string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; role\n: \"tool\"\n; tool_call_id?\n: string\n} | { content\n: null\n| string\n; name\n: string\n; role\n: \"function\"\n} | { content?\n: null\n| string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; function_call?\n: null\n| { arguments\n: string\n; name\n: string\n} ; name?\n: null\n| string\n; reasoning?\n: null\n| { content?\n: null\n| string\n; id?\n: null\n| string\n}[] ; role\n: \"assistant\"\n; tool_calls?\n: null\n| { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[] } | { content?\n: null\n| string\n; role\n: \"model\"\n})[] ; tools?\n: string\n; type\n: \"chat\"\n} ; tool_functions?\n: null\n| ({ id\n: string\n; type\n: \"function\"\n} | { name\n: string\n; type\n: \"global\"\n})[] } ; description?\n: string\n; type\n: \"prompt\"\n}>, ZodObject\n<{ default\n: ZodOptional\n<ZodUnknown\n> ; description\n: ZodOptional\n<ZodString\n> ; schema\n: ZodRecord\n<ZodString\n, ZodUnknown\n> ; type\n: ZodLiteral\n<\"data\"\n> }, \"strip\"\n, ZodTypeAny\n, { default?\n: unknown\n; description?\n: string\n; schema\n: Record\n<string\n, unknown\n> ; type\n: \"data\"\n}, { default?\n: unknown\n; description?\n: string\n; schema\n: Record\n<string\n, unknown\n> ; type\n: \"data\"\n}>]>>> }, \"strip\"\n, ZodTypeAny\n, { parameters?\n: Record\n<string\n, { type: \"prompt\"; default?: { prompt?: { type: \"completion\"; content: string; } | { type: \"chat\"; messages: ({ content: (string | { type: \"text\"; text: string; cache_control?: { type: \"ephemeral\"; } | undefined; }[]) & (string | ... 1 more ... | undefined); role: \"system\"; name?: string | undefined; } | ... 4 more .... | { type: \"data\"; schema: Record<string, unknown>; default?: unknown; description?: string | undefined; }> }, { parameters?\n: Record\n<string\n, { type: \"prompt\"; default?: { prompt?: { type: \"completion\"; content: string; } | { type: \"chat\"; messages: ({ role: \"system\"; content?: string | { type: \"text\"; text?: string | undefined; cache_control?: { ...; } | undefined; }[] | undefined; name?: string | undefined; } | ... 4 more ... | { ...; })[]; tools?: stri... | { type: \"data\"; schema: Record<string, unknown>; default?: unknown; description?: string | undefined; }> }>\nevaluatorDefinitionsSchema\n\u2022 Const\nevaluatorDefinitionsSchema: ZodRecord\n<ZodString\n, ZodObject\n<{ parameters\n: ZodOptional\n<ZodRecord\n<ZodString\n, ZodUnion\n<[ZodObject\n<{ default\n: ZodOptional\n<ZodObject\n<{ options\n: ZodOptional\n<ZodNullable\n<ZodObject\n<{ model\n: ZodOptional\n<ZodString\n> ; params\n: ZodOptional\n<ZodUnion\n<[ZodObject\n<{ frequency_penalty\n: ZodOptional\n<ZodNumber\n> ; function_call\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}>]>> ; max_completion_tokens\n: ZodOptional\n<ZodNumber\n> ; max_tokens\n: ZodOptional\n<ZodNumber\n> ; n\n: ZodOptional\n<ZodNumber\n> ; presence_penalty\n: ZodOptional\n<ZodNumber\n> ; reasoning_effort\n: ZodOptional\n<ZodEnum\n<[\"low\"\n, \"medium\"\n, \"high\"\n]>> ; response_format\n: ZodOptional\n<ZodNullable\n<ZodUnion\n<[ZodObject\n<{ type\n: ZodLiteral\n<\"json_object\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"json_object\"\n}, { type\n: \"json_object\"\n}>, ZodObject\n<{ json_schema\n: ZodObject\n<{ description\n: ZodOptional\n<ZodString\n> ; name\n: ZodString\n; schema\n: ZodOptional\n<ZodUnion\n<[ZodRecord\n<ZodString\n, ZodUnknown\n>, ZodString\n]>> ; strict\n: ZodOptional\n<ZodNullable\n<ZodBoolean\n>> }, \"strip\"\n, ZodTypeAny\n, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}> ; type\n: ZodLiteral\n<\"json_schema\"\n> }, \"strip\"\n, ZodTypeAny\n, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}>, ZodObject\n<{ type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"text\"\n}, { type\n: \"text\"\n}>]>>> ; stop\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodOptional\n<ZodNumber\n> ; tool_choice\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodLiteral\n<\"required\"\n>, ZodObject\n<{ function\n: ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}> ; type\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { function\n: { name\n: string\n} ; type\n: \"function\"\n}, { function\n: { name\n: string\n} ; type\n: \"function\"\n}>]>> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, \"passthrough\"\n, ZodTypeAny\n, objectOutputType\n<{ frequency_penalty\n: ZodOptional\n<ZodNumber\n> ; function_call\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}>]>> ; max_completion_tokens\n: ZodOptional\n<ZodNumber\n> ; max_tokens\n: ZodOptional\n<ZodNumber\n> ; n\n: ZodOptional\n<ZodNumber\n> ; presence_penalty\n: ZodOptional\n<ZodNumber\n> ; reasoning_effort\n: ZodOptional\n<ZodEnum\n<[\"low\"\n, \"medium\"\n, \"high\"\n]>> ; response_format\n: ZodOptional\n<ZodNullable\n<ZodUnion\n<[ZodObject\n<{ type\n: ZodLiteral\n<\"json_object\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"json_object\"\n}, { type\n: \"json_object\"\n}>, ZodObject\n<{ json_schema\n: ZodObject\n<{ description\n: ZodOptional\n<ZodString\n> ; name\n: ZodString\n; schema\n: ZodOptional\n<ZodUnion\n<[ZodRecord\n<ZodString\n, ZodUnknown\n>, ZodString\n]>> ; strict\n: ZodOptional\n<ZodNullable\n<ZodBoolean\n>> }, \"strip\"\n, ZodTypeAny\n, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}> ; type\n: ZodLiteral\n<\"json_schema\"\n> }, \"strip\"\n, ZodTypeAny\n, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}>, ZodObject\n<{ type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"text\"\n}, { type\n: \"text\"\n}>]>>> ; stop\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodOptional\n<ZodNumber\n> ; tool_choice\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodLiteral\n<\"required\"\n>, ZodObject\n<{ function\n: ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}> ; type\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { function\n: { name\n: string\n} ; type\n: \"function\"\n}, { function\n: { name\n: string\n} ; type\n: \"function\"\n}>]>> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>, objectInputType\n<{ frequency_penalty\n: ZodOptional\n<ZodNumber\n> ; function_call\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}>]>> ; max_completion_tokens\n: ZodOptional\n<ZodNumber\n> ; max_tokens\n: ZodOptional\n<ZodNumber\n> ; n\n: ZodOptional\n<ZodNumber\n> ; presence_penalty\n: ZodOptional\n<ZodNumber\n> ; reasoning_effort\n: ZodOptional\n<ZodEnum\n<[\"low\"\n, \"medium\"\n, \"high\"\n]>> ; response_format\n: ZodOptional\n<ZodNullable\n<ZodUnion\n<[ZodObject\n<{ type\n: ZodLiteral\n<\"json_object\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"json_object\"\n}, { type\n: \"json_object\"\n}>, ZodObject\n<{ json_schema\n: ZodObject\n<{ description\n: ZodOptional\n<ZodString\n> ; name\n: ZodString\n; schema\n: ZodOptional\n<ZodUnion\n<[ZodRecord\n<ZodString\n, ZodUnknown\n>, ZodString\n]>> ; strict\n: ZodOptional\n<ZodNullable\n<ZodBoolean\n>> }, \"strip\"\n, ZodTypeAny\n, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}> ; type\n: ZodLiteral\n<\"json_schema\"\n> }, \"strip\"\n, ZodTypeAny\n, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}>, ZodObject\n<{ type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"text\"\n}, { type\n: \"text\"\n}>]>>> ; stop\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodOptional\n<ZodNumber\n> ; tool_choice\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodLiteral\n<\"required\"\n>, ZodObject\n<{ function\n: ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}> ; type\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { function\n: { name\n: string\n} ; type\n: \"function\"\n}, { function\n: { name\n: string\n} ; type\n: \"function\"\n}>]>> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>>, ZodObject\n<{ max_tokens\n: ZodNumber\n; max_tokens_to_sample\n: ZodOptional\n<ZodNumber\n> ; stop_sequences\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodNumber\n; top_k\n: ZodOptional\n<ZodNumber\n> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, \"passthrough\"\n, ZodTypeAny\n, objectOutputType\n<{ max_tokens\n: ZodNumber\n; max_tokens_to_sample\n: ZodOptional\n<ZodNumber\n> ; stop_sequences\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodNumber\n; top_k\n: ZodOptional\n<ZodNumber\n> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>, objectInputType\n<{ max_tokens\n: ZodNumber\n; max_tokens_to_sample\n: ZodOptional\n<ZodNumber\n> ; stop_sequences\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodNumber\n; top_k\n: ZodOptional\n<ZodNumber\n> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>>, ZodObject\n<{ maxOutputTokens\n: ZodOptional\n<ZodNumber\n> ; temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; topP\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, \"passthrough\"\n, ZodTypeAny\n, objectOutputType\n<{ maxOutputTokens\n: ZodOptional\n<ZodNumber\n> ; temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; topP\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>, objectInputType\n<{ maxOutputTokens\n: ZodOptional\n<ZodNumber\n> ; temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; topP\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>>, ZodObject\n<{ temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, \"passthrough\"\n, ZodTypeAny\n, objectOutputType\n<{ temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>, objectInputType\n<{ temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>>, ZodObject\n<{ use_cache\n: ZodOptional\n<ZodBoolean\n> }, \"passthrough\"\n, ZodTypeAny\n, objectOutputType\n<{ use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>, objectInputType\n<{ use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>>]>> ; position\n: ZodOptional\n<ZodString\n> }, \"strip\"\n, ZodTypeAny\n, { model?\n: string\n; params?\n: objectOutputType\n<{ frequency_penalty\n: ZodOptional\n<ZodNumber\n> ; function_call\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}>]>> ; max_completion_tokens\n: ZodOptional\n<ZodNumber\n> ; max_tokens\n: ZodOptional\n<ZodNumber\n> ; n\n: ZodOptional\n<ZodNumber\n> ; presence_penalty\n: ZodOptional\n<ZodNumber\n> ; reasoning_effort\n: ZodOptional\n<ZodEnum\n<[\"low\"\n, \"medium\"\n, \"high\"\n]>> ; response_format\n: ZodOptional\n<ZodNullable\n<ZodUnion\n<[ZodObject\n<{ type\n: ZodLiteral\n<\"json_object\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"json_object\"\n}, { type\n: \"json_object\"\n}>, ZodObject\n<{ json_schema\n: ZodObject\n<{ description\n: ZodOptional\n<ZodString\n> ; name\n: ZodString\n; schema\n: ZodOptional\n<ZodUnion\n<[ZodRecord\n<ZodString\n, ZodUnknown\n>, ZodString\n]>> ; strict\n: ZodOptional\n<ZodNullable\n<ZodBoolean\n>> }, \"strip\"\n, ZodTypeAny\n, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}> ; type\n: ZodLiteral\n<\"json_schema\"\n> }, \"strip\"\n, ZodTypeAny\n, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}>, ZodObject\n<{ type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"text\"\n}, { type\n: \"text\"\n}>]>>> ; stop\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodOptional\n<ZodNumber\n> ; tool_choice\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodLiteral\n<\"required\"\n>, ZodObject\n<{ function\n: ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}> ; type\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { function\n: { name\n: string\n} ; type\n: \"function\"\n}, { function\n: { name\n: string\n} ; type\n: \"function\"\n}>]>> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectOutputType\n<{ max_tokens\n: ZodNumber\n; max_tokens_to_sample\n: ZodOptional\n<ZodNumber\n> ; stop_sequences\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodNumber\n; top_k\n: ZodOptional\n<ZodNumber\n> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectOutputType\n<{ maxOutputTokens\n: ZodOptional\n<ZodNumber\n> ; temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; topP\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectOutputType\n<{ temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectOutputType\n<{ use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> ; position?\n: string\n}, { model?\n: string\n; params?\n: objectInputType\n<{ frequency_penalty\n: ZodOptional\n<ZodNumber\n> ; function_call\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}>]>> ; max_completion_tokens\n: ZodOptional\n<ZodNumber\n> ; max_tokens\n: ZodOptional\n<ZodNumber\n> ; n\n: ZodOptional\n<ZodNumber\n> ; presence_penalty\n: ZodOptional\n<ZodNumber\n> ; reasoning_effort\n: ZodOptional\n<ZodEnum\n<[\"low\"\n, \"medium\"\n, \"high\"\n]>> ; response_format\n: ZodOptional\n<ZodNullable\n<ZodUnion\n<[ZodObject\n<{ type\n: ZodLiteral\n<\"json_object\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"json_object\"\n}, { type\n: \"json_object\"\n}>, ZodObject\n<{ json_schema\n: ZodObject\n<{ description\n: ZodOptional\n<ZodString\n> ; name\n: ZodString\n; schema\n: ZodOptional\n<ZodUnion\n<[ZodRecord\n<ZodString\n, ZodUnknown\n>, ZodString\n]>> ; strict\n: ZodOptional\n<ZodNullable\n<ZodBoolean\n>> }, \"strip\"\n, ZodTypeAny\n, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}> ; type\n: ZodLiteral\n<\"json_schema\"\n> }, \"strip\"\n, ZodTypeAny\n, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}>, ZodObject\n<{ type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"text\"\n}, { type\n: \"text\"\n}>]>>> ; stop\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodOptional\n<ZodNumber\n> ; tool_choice\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodLiteral\n<\"required\"\n>, ZodObject\n<{ function\n: ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}> ; type\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { function\n: { name\n: string\n} ; type\n: \"function\"\n}, { function\n: { name\n: string\n} ; type\n: \"function\"\n}>]>> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectInputType\n<{ max_tokens\n: ZodNumber\n; max_tokens_to_sample\n: ZodOptional\n<ZodNumber\n> ; stop_sequences\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodNumber\n; top_k\n: ZodOptional\n<ZodNumber\n> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectInputType\n<{ maxOutputTokens\n: ZodOptional\n<ZodNumber\n> ; temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; topP\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectInputType\n<{ temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectInputType\n<{ use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> ; position?\n: string\n}>>> ; origin\n: ZodOptional\n<ZodNullable\n<ZodObject\n<{ project_id\n: ZodOptional\n<ZodString\n> ; prompt_id\n: ZodOptional\n<ZodString\n> ; prompt_version\n: ZodOptional\n<ZodString\n> }, \"strip\"\n, ZodTypeAny\n, { project_id?\n: string\n; prompt_id?\n: string\n; prompt_version?\n: string\n}, { project_id?\n: string\n; prompt_id?\n: string\n; prompt_version?\n: string\n}>>> ; parser\n: ZodOptional\n<ZodNullable\n<ZodObject\n<{ choice_scores\n: ZodRecord\n<ZodString\n, ZodNumber\n> ; type\n: ZodLiteral\n<\"llm_classifier\"\n> ; use_cot\n: ZodBoolean\n}, \"strip\"\n, ZodTypeAny\n, { choice_scores\n: Record\n<string\n, number\n> ; type\n: \"llm_classifier\"\n; use_cot\n: boolean\n}, { choice_scores\n: Record\n<string\n, number\n> ; type\n: \"llm_classifier\"\n; use_cot\n: boolean\n}>>> ; prompt\n: ZodOptional\n<ZodNullable\n<ZodUnion\n<[ZodObject\n<{ content\n: ZodString\n; type\n: ZodLiteral\n<\"completion\"\n> }, \"strip\"\n, ZodTypeAny\n, { content\n: string\n; type\n: \"completion\"\n}, { content\n: string\n; type\n: \"completion\"\n}>, ZodObject\n<{ messages\n: ZodArray\n<ZodUnion\n<[ZodUnion\n<[ZodObject\n<{ content\n: ZodUnion\n<[ZodDefault\n<ZodString\n>, ZodArray\n<ZodObject\n<{ cache_control\n: ZodOptional\n<ZodObject\n<{ type\n: ZodEnum\n<[\"ephemeral\"\n]> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"ephemeral\"\n}, { type\n: \"ephemeral\"\n}>> ; text\n: ZodDefault\n<ZodString\n> ; type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text\n: string\n; type\n: \"text\"\n}, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}>, \"many\"\n>]> ; name\n: ZodOptional\n<ZodString\n> ; role\n: ZodLiteral\n<\"system\"\n> }, \"strip\"\n, ZodTypeAny\n, { content\n: {} ; name?\n: string\n; role\n: \"system\"\n}, { content?\n: string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; name?\n: string\n; role\n: \"system\"\n}>, ZodObject\n<{ content\n: ZodUnion\n<[ZodDefault\n<ZodString\n>, ZodArray\n<ZodUnion\n<[ZodObject\n<{ cache_control\n: ZodOptional\n<ZodObject\n<{ type\n: ZodEnum\n<[\"ephemeral\"\n]> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"ephemeral\"\n}, { type\n: \"ephemeral\"\n}>> ; text\n: ZodDefault\n<ZodString\n> ; type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text\n: string\n; type\n: \"text\"\n}, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}>, ZodObject\n<{ image_url\n: ZodObject\n<{ detail\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"low\"\n>, ZodLiteral\n<\"high\"\n>]>> ; url\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { detail?\n: \"auto\"\n| \"high\"\n| \"low\"\n; url\n: string\n}, { detail?\n: \"auto\"\n| \"high\"\n| \"low\"\n; url\n: string\n}> ; type\n: ZodLiteral\n<\"image_url\"\n> }, \"strip\"\n, ZodTypeAny\n, { image_url\n: { detail?\n: \"auto\"\n| \"high\"\n| \"low\"\n; url\n: string\n} ; type\n: \"image_url\"\n}, { image_url\n: { detail?\n: \"auto\"\n| \"high\"\n| \"low\"\n; url\n: string\n} ; type\n: \"image_url\"\n}>]>, \"many\"\n>]> ; name\n: ZodOptional\n<ZodString\n> ; role\n: ZodLiteral\n<\"user\"\n> }, \"strip\"\n, ZodTypeAny\n, { content\n: {} ; name?\n: string\n; role\n: \"user\"\n}, { content?\n: string\n| ({ cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n} | { image_url\n: { detail?\n: \"auto\"\n| \"high\"\n| \"low\"\n; url\n: string\n} ; type\n: \"image_url\"\n})[] ; name?\n: string\n; role\n: \"user\"\n}>, ZodObject\n<{ content\n: ZodOptional\n<ZodNullable\n<ZodUnion\n<[ZodString\n, ZodArray\n<ZodObject\n<{ cache_control\n: ZodOptional\n<ZodObject\n<{ type\n: ZodEnum\n<[\"ephemeral\"\n]> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"ephemeral\"\n}, { type\n: \"ephemeral\"\n}>> ; text\n: ZodDefault\n<ZodString\n> ; type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text\n: string\n; type\n: \"text\"\n}, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}>, \"many\"\n>]>>> ; function_call\n: ZodEffects\n<ZodOptional\n<ZodNullable\n<ZodObject\n<{ arguments\n: ZodString\n; name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { arguments\n: string\n; name\n: string\n}, { arguments\n: string\n; name\n: string\n}>>>, undefined\n| { arguments\n: string\n; name\n: string\n}, undefined\n| null\n| { arguments\n: string\n; name\n: string\n}> ; name\n: ZodEffects\n<ZodOptional\n<ZodNullable\n<ZodString\n>>, undefined\n| string\n, undefined\n| null\n| string\n> ; reasoning\n: ZodEffects\n<ZodOptional\n<ZodNullable\n<ZodArray\n<ZodObject\n<{ content\n: ZodEffects\n<ZodOptional\n<ZodNullable\n<ZodString\n>>, undefined\n| string\n, undefined\n| null\n| string\n> ; id\n: ZodEffects\n<ZodOptional\n<ZodNullable\n<ZodString\n>>, undefined\n| string\n, undefined\n| null\n| string\n> }, \"strip\"\n, ZodTypeAny\n, { content?\n: string\n; id?\n: string\n}, { content?\n: null\n| string\n; id?\n: null\n| string\n}>, \"many\"\n>>>, undefined\n| { content?\n: string\n; id?\n: string\n}[], undefined\n| null\n| { content?\n: null\n| string\n; id?\n: null\n| string\n}[]> ; role\n: ZodLiteral\n<\"assistant\"\n> ; tool_calls\n: ZodEffects\n<ZodOptional\n<ZodNullable\n<ZodArray\n<ZodObject\n<{ function\n: ZodObject\n<{ arguments\n: ZodString\n; name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { arguments\n: string\n; name\n: string\n}, { arguments\n: string\n; name\n: string\n}> ; id\n: ZodString\n; type\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}, { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}>, \"many\"\n>>>, undefined\n| { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[], undefined\n| null\n| { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[]> }, \"strip\"\n, ZodTypeAny\n, { content?\n: null\n| string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text\n: string\n; type\n: \"text\"\n}[] ; function_call?\n: { arguments\n: string\n; name\n: string\n} ; name?\n: string\n; reasoning?\n: { content?\n: string\n; id?\n: string\n}[] ; role\n: \"assistant\"\n; tool_calls?\n: { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[] }, { content?\n: null\n| string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; function_call?\n: null\n| { arguments\n: string\n; name\n: string\n} ; name?\n: null\n| string\n; reasoning?\n: null\n| { content?\n: null\n| string\n; id?\n: null\n| string\n}[] ; role\n: \"assistant\"\n; tool_calls?\n: null\n| { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[] }>, ZodObject\n<{ content\n: ZodUnion\n<[ZodDefault\n<ZodString\n>, ZodArray\n<ZodObject\n<{ cache_control\n: ZodOptional\n<ZodObject\n<{ type\n: ZodEnum\n<[\"ephemeral\"\n]> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"ephemeral\"\n}, { type\n: \"ephemeral\"\n}>> ; text\n: ZodDefault\n<ZodString\n> ; type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text\n: string\n; type\n: \"text\"\n}, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}>, \"many\"\n>]> ; role\n: ZodLiteral\n<\"tool\"\n> ; tool_call_id\n: ZodDefault\n<ZodString\n> }, \"strip\"\n, ZodTypeAny\n, { content\n: {} ; role\n: \"tool\"\n; tool_call_id\n: string\n}, { content?\n: string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; role\n: \"tool\"\n; tool_call_id?\n: string\n}>, ZodObject\n<{ content\n: ZodNullable\n<ZodString\n> ; name\n: ZodString\n; role\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { content\n: null\n| string\n; name\n: string\n; role\n: \"function\"\n}, { content\n: null\n| string\n; name\n: string\n; role\n: \"function\"\n}>]>, ZodObject\n<{ content\n: ZodOptional\n<ZodNullable\n<ZodString\n>> ; role\n: ZodEnum\n<[\"model\"\n]> }, \"strip\"\n, ZodTypeAny\n, { content?\n: null\n| string\n; role\n: \"model\"\n}, { content?\n: null\n| string\n; role\n: \"model\"\n}>]>, \"many\"\n> ; tools\n: ZodOptional\n<ZodString\n> ; type\n: ZodLiteral\n<\"chat\"\n> }, \"strip\"\n, ZodTypeAny\n, { messages\n: ({ content\n: {} ; name?\n: string\n; role\n: \"system\"\n} | { content\n: {} ; name?\n: string\n; role\n: \"user\"\n} | { content\n: {} ; role\n: \"tool\"\n; tool_call_id\n: string\n} | { content\n: null\n| string\n; name\n: string\n; role\n: \"function\"\n} | { content?\n: null\n| string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text\n: string\n; type\n: \"text\"\n}[] ; function_call?\n: { arguments\n: string\n; name\n: string\n} ; name?\n: string\n; reasoning?\n: { content?\n: string\n; id?\n: string\n}[] ; role\n: \"assistant\"\n; tool_calls?\n: { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[] } | { content?\n: null\n| string\n; role\n: \"model\"\n})[] ; tools?\n: string\n; type\n: \"chat\"\n}, { messages\n: ({ content?\n: string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; name?\n: string\n; role\n: \"system\"\n} | { content?\n: string\n| ({ cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n} | { image_url\n: { detail?\n: \"auto\"\n| \"high\"\n| \"low\"\n; url\n: string\n} ; type\n: \"image_url\"\n})[] ; name?\n: string\n; role\n: \"user\"\n} | { content?\n: string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; role\n: \"tool\"\n; tool_call_id?\n: string\n} | { content\n: null\n| string\n; name\n: string\n; role\n: \"function\"\n} | { content?\n: null\n| string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; function_call?\n: null\n| { arguments\n: string\n; name\n: string\n} ; name?\n: null\n| string\n; reasoning?\n: null\n| { content?\n: null\n| string\n; id?\n: null\n| string\n}[] ; role\n: \"assistant\"\n; tool_calls?\n: null\n| { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[] } | { content?\n: null\n| string\n; role\n: \"model\"\n})[] ; tools?\n: string\n; type\n: \"chat\"\n}>]>>> ; tool_functions\n: ZodOptional\n<ZodNullable\n<ZodArray\n<ZodUnion\n<[ZodObject\n<{ id\n: ZodString\n; type\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { id\n: string\n; type\n: \"function\"\n}, { id\n: string\n; type\n: \"function\"\n}>, ZodObject\n<{ name\n: ZodString\n; type\n: ZodLiteral\n<\"global\"\n> }, \"strip\"\n, ZodTypeAny\n, { name\n: string\n; type\n: \"global\"\n}, { name\n: string\n; type\n: \"global\"\n}>]>, \"many\"\n>>> }, \"strip\"\n, ZodTypeAny\n, { options?\n: null\n| { model?\n: string\n; params?\n: objectOutputType\n<{ frequency_penalty\n: ZodOptional\n<ZodNumber\n> ; function_call\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}>]>> ; max_completion_tokens\n: ZodOptional\n<ZodNumber\n> ; max_tokens\n: ZodOptional\n<ZodNumber\n> ; n\n: ZodOptional\n<ZodNumber\n> ; presence_penalty\n: ZodOptional\n<ZodNumber\n> ; reasoning_effort\n: ZodOptional\n<ZodEnum\n<[\"low\"\n, \"medium\"\n, \"high\"\n]>> ; response_format\n: ZodOptional\n<ZodNullable\n<ZodUnion\n<[ZodObject\n<{ type\n: ZodLiteral\n<\"json_object\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"json_object\"\n}, { type\n: \"json_object\"\n}>, ZodObject\n<{ json_schema\n: ZodObject\n<{ description\n: ZodOptional\n<ZodString\n> ; name\n: ZodString\n; schema\n: ZodOptional\n<ZodUnion\n<[ZodRecord\n<ZodString\n, ZodUnknown\n>, ZodString\n]>> ; strict\n: ZodOptional\n<ZodNullable\n<ZodBoolean\n>> }, \"strip\"\n, ZodTypeAny\n, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}> ; type\n: ZodLiteral\n<\"json_schema\"\n> }, \"strip\"\n, ZodTypeAny\n, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}>, ZodObject\n<{ type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"text\"\n}, { type\n: \"text\"\n}>]>>> ; stop\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodOptional\n<ZodNumber\n> ; tool_choice\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodLiteral\n<\"required\"\n>, ZodObject\n<{ function\n: ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}> ; type\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { function\n: { name\n: string\n} ; type\n: \"function\"\n}, { function\n: { name\n: string\n} ; type\n: \"function\"\n}>]>> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectOutputType\n<{ max_tokens\n: ZodNumber\n; max_tokens_to_sample\n: ZodOptional\n<ZodNumber\n> ; stop_sequences\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodNumber\n; top_k\n: ZodOptional\n<ZodNumber\n> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectOutputType\n<{ maxOutputTokens\n: ZodOptional\n<ZodNumber\n> ; temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; topP\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectOutputType\n<{ temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectOutputType\n<{ use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> ; position?\n: string\n} ; origin?\n: null\n| { project_id?\n: string\n; prompt_id?\n: string\n; prompt_version?\n: string\n} ; parser?\n: null\n| { choice_scores\n: Record\n<string\n, number\n> ; type\n: \"llm_classifier\"\n; use_cot\n: boolean\n} ; prompt?\n: null\n| { content\n: string\n; type\n: \"completion\"\n} | { messages\n: ({ content\n: {} ; name?\n: string\n; role\n: \"system\"\n} | { content\n: {} ; name?\n: string\n; role\n: \"user\"\n} | { content\n: {} ; role\n: \"tool\"\n; tool_call_id\n: string\n} | { content\n: null\n| string\n; name\n: string\n; role\n: \"function\"\n} | { content?\n: null\n| string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text\n: string\n; type\n: \"text\"\n}[] ; function_call?\n: { arguments\n: string\n; name\n: string\n} ; name?\n: string\n; reasoning?\n: { content?\n: string\n; id?\n: string\n}[] ; role\n: \"assistant\"\n; tool_calls?\n: { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[] } | { content?\n: null\n| string\n; role\n: \"model\"\n})[] ; tools?\n: string\n; type\n: \"chat\"\n} ; tool_functions?\n: null\n| ({ id\n: string\n; type\n: \"function\"\n} | { name\n: string\n; type\n: \"global\"\n})[] }, { options?\n: null\n| { model?\n: string\n; params?\n: objectInputType\n<{ frequency_penalty\n: ZodOptional\n<ZodNumber\n> ; function_call\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}>]>> ; max_completion_tokens\n: ZodOptional\n<ZodNumber\n> ; max_tokens\n: ZodOptional\n<ZodNumber\n> ; n\n: ZodOptional\n<ZodNumber\n> ; presence_penalty\n: ZodOptional\n<ZodNumber\n> ; reasoning_effort\n: ZodOptional\n<ZodEnum\n<[\"low\"\n, \"medium\"\n, \"high\"\n]>> ; response_format\n: ZodOptional\n<ZodNullable\n<ZodUnion\n<[ZodObject\n<{ type\n: ZodLiteral\n<\"json_object\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"json_object\"\n}, { type\n: \"json_object\"\n}>, ZodObject\n<{ json_schema\n: ZodObject\n<{ description\n: ZodOptional\n<ZodString\n> ; name\n: ZodString\n; schema\n: ZodOptional\n<ZodUnion\n<[ZodRecord\n<ZodString\n, ZodUnknown\n>, ZodString\n]>> ; strict\n: ZodOptional\n<ZodNullable\n<ZodBoolean\n>> }, \"strip\"\n, ZodTypeAny\n, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}> ; type\n: ZodLiteral\n<\"json_schema\"\n> }, \"strip\"\n, ZodTypeAny\n, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}>, ZodObject\n<{ type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"text\"\n}, { type\n: \"text\"\n}>]>>> ; stop\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodOptional\n<ZodNumber\n> ; tool_choice\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodLiteral\n<\"required\"\n>, ZodObject\n<{ function\n: ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}> ; type\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { function\n: { name\n: string\n} ; type\n: \"function\"\n}, { function\n: { name\n: string\n} ; type\n: \"function\"\n}>]>> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectInputType\n<{ max_tokens\n: ZodNumber\n; max_tokens_to_sample\n: ZodOptional\n<ZodNumber\n> ; stop_sequences\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodNumber\n; top_k\n: ZodOptional\n<ZodNumber\n> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectInputType\n<{ maxOutputTokens\n: ZodOptional\n<ZodNumber\n> ; temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; topP\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectInputType\n<{ temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectInputType\n<{ use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> ; position?\n: string\n} ; origin?\n: null\n| { project_id?\n: string\n; prompt_id?\n: string\n; prompt_version?\n: string\n} ; parser?\n: null\n| { choice_scores\n: Record\n<string\n, number\n> ; type\n: \"llm_classifier\"\n; use_cot\n: boolean\n} ; prompt?\n: null\n| { content\n: string\n; type\n: \"completion\"\n} | { messages\n: ({ content?\n: string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; name?\n: string\n; role\n: \"system\"\n} | { content?\n: string\n| ({ cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n} | { image_url\n: { detail?\n: \"auto\"\n| \"high\"\n| \"low\"\n; url\n: string\n} ; type\n: \"image_url\"\n})[] ; name?\n: string\n; role\n: \"user\"\n} | { content?\n: string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; role\n: \"tool\"\n; tool_call_id?\n: string\n} | { content\n: null\n| string\n; name\n: string\n; role\n: \"function\"\n} | { content?\n: null\n| string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; function_call?\n: null\n| { arguments\n: string\n; name\n: string\n} ; name?\n: null\n| string\n; reasoning?\n: null\n| { content?\n: null\n| string\n; id?\n: null\n| string\n}[] ; role\n: \"assistant\"\n; tool_calls?\n: null\n| { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[] } | { content?\n: null\n| string\n; role\n: \"model\"\n})[] ; tools?\n: string\n; type\n: \"chat\"\n} ; tool_functions?\n: null\n| ({ id\n: string\n; type\n: \"function\"\n} | { name\n: string\n; type\n: \"global\"\n})[] }>> ; description\n: ZodOptional\n<ZodString\n> ; type\n: ZodLiteral\n<\"prompt\"\n> }, \"strip\"\n, ZodTypeAny\n, { default?\n: { options?\n: null\n| { model?\n: string\n; params?\n: objectOutputType\n<{ frequency_penalty\n: ZodOptional\n<ZodNumber\n> ; function_call\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}>]>> ; max_completion_tokens\n: ZodOptional\n<ZodNumber\n> ; max_tokens\n: ZodOptional\n<ZodNumber\n> ; n\n: ZodOptional\n<ZodNumber\n> ; presence_penalty\n: ZodOptional\n<ZodNumber\n> ; reasoning_effort\n: ZodOptional\n<ZodEnum\n<[\"low\"\n, \"medium\"\n, \"high\"\n]>> ; response_format\n: ZodOptional\n<ZodNullable\n<ZodUnion\n<[ZodObject\n<{ type\n: ZodLiteral\n<\"json_object\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"json_object\"\n}, { type\n: \"json_object\"\n}>, ZodObject\n<{ json_schema\n: ZodObject\n<{ description\n: ZodOptional\n<ZodString\n> ; name\n: ZodString\n; schema\n: ZodOptional\n<ZodUnion\n<[ZodRecord\n<ZodString\n, ZodUnknown\n>, ZodString\n]>> ; strict\n: ZodOptional\n<ZodNullable\n<ZodBoolean\n>> }, \"strip\"\n, ZodTypeAny\n, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}> ; type\n: ZodLiteral\n<\"json_schema\"\n> }, \"strip\"\n, ZodTypeAny\n, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}>, ZodObject\n<{ type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"text\"\n}, { type\n: \"text\"\n}>]>>> ; stop\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodOptional\n<ZodNumber\n> ; tool_choice\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodLiteral\n<\"required\"\n>, ZodObject\n<{ function\n: ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}> ; type\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { function\n: { name\n: string\n} ; type\n: \"function\"\n}, { function\n: { name\n: string\n} ; type\n: \"function\"\n}>]>> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectOutputType\n<{ max_tokens\n: ZodNumber\n; max_tokens_to_sample\n: ZodOptional\n<ZodNumber\n> ; stop_sequences\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodNumber\n; top_k\n: ZodOptional\n<ZodNumber\n> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectOutputType\n<{ maxOutputTokens\n: ZodOptional\n<ZodNumber\n> ; temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; topP\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectOutputType\n<{ temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectOutputType\n<{ use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> ; position?\n: string\n} ; origin?\n: null\n| { project_id?\n: string\n; prompt_id?\n: string\n; prompt_version?\n: string\n} ; parser?\n: null\n| { choice_scores\n: Record\n<string\n, number\n> ; type\n: \"llm_classifier\"\n; use_cot\n: boolean\n} ; prompt?\n: null\n| { content\n: string\n; type\n: \"completion\"\n} | { messages\n: ({ content\n: {} ; name?\n: string\n; role\n: \"system\"\n} | { content\n: {} ; name?\n: string\n; role\n: \"user\"\n} | { content\n: {} ; role\n: \"tool\"\n; tool_call_id\n: string\n} | { content\n: null\n| string\n; name\n: string\n; role\n: \"function\"\n} | { content?\n: null\n| string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text\n: string\n; type\n: \"text\"\n}[] ; function_call?\n: { arguments\n: string\n; name\n: string\n} ; name?\n: string\n; reasoning?\n: { content?\n: string\n; id?\n: string\n}[] ; role\n: \"assistant\"\n; tool_calls?\n: { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[] } | { content?\n: null\n| string\n; role\n: \"model\"\n})[] ; tools?\n: string\n; type\n: \"chat\"\n} ; tool_functions?\n: null\n| ({ id\n: string\n; type\n: \"function\"\n} | { name\n: string\n; type\n: \"global\"\n})[] } ; description?\n: string\n; type\n: \"prompt\"\n}, { default?\n: { options?\n: null\n| { model?\n: string\n; params?\n: objectInputType\n<{ frequency_penalty\n: ZodOptional\n<ZodNumber\n> ; function_call\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}>]>> ; max_completion_tokens\n: ZodOptional\n<ZodNumber\n> ; max_tokens\n: ZodOptional\n<ZodNumber\n> ; n\n: ZodOptional\n<ZodNumber\n> ; presence_penalty\n: ZodOptional\n<ZodNumber\n> ; reasoning_effort\n: ZodOptional\n<ZodEnum\n<[\"low\"\n, \"medium\"\n, \"high\"\n]>> ; response_format\n: ZodOptional\n<ZodNullable\n<ZodUnion\n<[ZodObject\n<{ type\n: ZodLiteral\n<\"json_object\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"json_object\"\n}, { type\n: \"json_object\"\n}>, ZodObject\n<{ json_schema\n: ZodObject\n<{ description\n: ZodOptional\n<ZodString\n> ; name\n: ZodString\n; schema\n: ZodOptional\n<ZodUnion\n<[ZodRecord\n<ZodString\n, ZodUnknown\n>, ZodString\n]>> ; strict\n: ZodOptional\n<ZodNullable\n<ZodBoolean\n>> }, \"strip\"\n, ZodTypeAny\n, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}> ; type\n: ZodLiteral\n<\"json_schema\"\n> }, \"strip\"\n, ZodTypeAny\n, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}>, ZodObject\n<{ type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"text\"\n}, { type\n: \"text\"\n}>]>>> ; stop\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodOptional\n<ZodNumber\n> ; tool_choice\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodLiteral\n<\"required\"\n>, ZodObject\n<{ function\n: ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}> ; type\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { function\n: { name\n: string\n} ; type\n: \"function\"\n}, { function\n: { name\n: string\n} ; type\n: \"function\"\n}>]>> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectInputType\n<{ max_tokens\n: ZodNumber\n; max_tokens_to_sample\n: ZodOptional\n<ZodNumber\n> ; stop_sequences\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodNumber\n; top_k\n: ZodOptional\n<ZodNumber\n> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectInputType\n<{ maxOutputTokens\n: ZodOptional\n<ZodNumber\n> ; temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; topP\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectInputType\n<{ temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectInputType\n<{ use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> ; position?\n: string\n} ; origin?\n: null\n| { project_id?\n: string\n; prompt_id?\n: string\n; prompt_version?\n: string\n} ; parser?\n: null\n| { choice_scores\n: Record\n<string\n, number\n> ; type\n: \"llm_classifier\"\n; use_cot\n: boolean\n} ; prompt?\n: null\n| { content\n: string\n; type\n: \"completion\"\n} | { messages\n: ({ content?\n: string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; name?\n: string\n; role\n: \"system\"\n} | { content?\n: string\n| ({ cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n} | { image_url\n: { detail?\n: \"auto\"\n| \"high\"\n| \"low\"\n; url\n: string\n} ; type\n: \"image_url\"\n})[] ; name?\n: string\n; role\n: \"user\"\n} | { content?\n: string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; role\n: \"tool\"\n; tool_call_id?\n: string\n} | { content\n: null\n| string\n; name\n: string\n; role\n: \"function\"\n} | { content?\n: null\n| string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; function_call?\n: null\n| { arguments\n: string\n; name\n: string\n} ; name?\n: null\n| string\n; reasoning?\n: null\n| { content?\n: null\n| string\n; id?\n: null\n| string\n}[] ; role\n: \"assistant\"\n; tool_calls?\n: null\n| { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[] } | { content?\n: null\n| string\n; role\n: \"model\"\n})[] ; tools?\n: string\n; type\n: \"chat\"\n} ; tool_functions?\n: null\n| ({ id\n: string\n; type\n: \"function\"\n} | { name\n: string\n; type\n: \"global\"\n})[] } ; description?\n: string\n; type\n: \"prompt\"\n}>, ZodObject\n<{ default\n: ZodOptional\n<ZodUnknown\n> ; description\n: ZodOptional\n<ZodString\n> ; schema\n: ZodRecord\n<ZodString\n, ZodUnknown\n> ; type\n: ZodLiteral\n<\"data\"\n> }, \"strip\"\n, ZodTypeAny\n, { default?\n: unknown\n; description?\n: string\n; schema\n: Record\n<string\n, unknown\n> ; type\n: \"data\"\n}, { default?\n: unknown\n; description?\n: string\n; schema\n: Record\n<string\n, unknown\n> ; type\n: \"data\"\n}>]>>> }, \"strip\"\n, ZodTypeAny\n, { parameters?\n: Record\n<string\n, { type: \"prompt\"; default?: { prompt?: { type: \"completion\"; content: string; } | { type: \"chat\"; messages: ({ content: (string | { type: \"text\"; text: string; cache_control?: { type: \"ephemeral\"; } | undefined; }[]) & (string | ... 1 more ... | undefined); role: \"system\"; name?: string | undefined; } | ... 4 more .... | { type: \"data\"; schema: Record<string, unknown>; default?: unknown; description?: string | undefined; }> }, { parameters?\n: Record\n<string\n, { type: \"prompt\"; default?: { prompt?: { type: \"completion\"; content: string; } | { type: \"chat\"; messages: ({ role: \"system\"; content?: string | { type: \"text\"; text?: string | undefined; cache_control?: { ...; } | undefined; }[] | undefined; name?: string | undefined; } | ... 4 more ... | { ...; })[]; tools?: stri... | { type: \"data\"; schema: Record<string, unknown>; default?: unknown; description?: string | undefined; }> }>>\nprojects\n\u2022 Const\nprojects: ProjectBuilder\npromptContentsSchema\n\u2022 Const\npromptContentsSchema: ZodUnion\n<[ZodObject\n<{ prompt\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { prompt\n: string\n}, { prompt\n: string\n}>, ZodObject\n<{ messages\n: ZodArray\n<ZodUnion\n<[ZodUnion\n<[ZodObject\n<{ content\n: ZodUnion\n<[ZodDefault\n<ZodString\n>, ZodArray\n<ZodObject\n<{ cache_control\n: ZodOptional\n<ZodObject\n<{ type\n: ZodEnum\n<[\"ephemeral\"\n]> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"ephemeral\"\n}, { type\n: \"ephemeral\"\n}>> ; text\n: ZodDefault\n<ZodString\n> ; type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text\n: string\n; type\n: \"text\"\n}, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}>, \"many\"\n>]> ; name\n: ZodOptional\n<ZodString\n> ; role\n: ZodLiteral\n<\"system\"\n> }, \"strip\"\n, ZodTypeAny\n, { content\n: {} ; name?\n: string\n; role\n: \"system\"\n}, { content?\n: string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; name?\n: string\n; role\n: \"system\"\n}>, ZodObject\n<{ content\n: ZodUnion\n<[ZodDefault\n<ZodString\n>, ZodArray\n<ZodUnion\n<[ZodObject\n<{ cache_control\n: ZodOptional\n<ZodObject\n<{ type\n: ZodEnum\n<[\"ephemeral\"\n]> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"ephemeral\"\n}, { type\n: \"ephemeral\"\n}>> ; text\n: ZodDefault\n<ZodString\n> ; type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text\n: string\n; type\n: \"text\"\n}, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}>, ZodObject\n<{ image_url\n: ZodObject\n<{ detail\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"low\"\n>, ZodLiteral\n<\"high\"\n>]>> ; url\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { detail?\n: \"auto\"\n| \"high\"\n| \"low\"\n; url\n: string\n}, { detail?\n: \"auto\"\n| \"high\"\n| \"low\"\n; url\n: string\n}> ; type\n: ZodLiteral\n<\"image_url\"\n> }, \"strip\"\n, ZodTypeAny\n, { image_url\n: { detail?\n: \"auto\"\n| \"high\"\n| \"low\"\n; url\n: string\n} ; type\n: \"image_url\"\n}, { image_url\n: { detail?\n: \"auto\"\n| \"high\"\n| \"low\"\n; url\n: string\n} ; type\n: \"image_url\"\n}>]>, \"many\"\n>]> ; name\n: ZodOptional\n<ZodString\n> ; role\n: ZodLiteral\n<\"user\"\n> }, \"strip\"\n, ZodTypeAny\n, { content\n: {} ; name?\n: string\n; role\n: \"user\"\n}, { content?\n: string\n| ({ cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n} | { image_url\n: { detail?\n: \"auto\"\n| \"high\"\n| \"low\"\n; url\n: string\n} ; type\n: \"image_url\"\n})[] ; name?\n: string\n; role\n: \"user\"\n}>, ZodObject\n<{ content\n: ZodOptional\n<ZodNullable\n<ZodUnion\n<[ZodString\n, ZodArray\n<ZodObject\n<{ cache_control\n: ZodOptional\n<ZodObject\n<{ type\n: ZodEnum\n<[\"ephemeral\"\n]> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"ephemeral\"\n}, { type\n: \"ephemeral\"\n}>> ; text\n: ZodDefault\n<ZodString\n> ; type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text\n: string\n; type\n: \"text\"\n}, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}>, \"many\"\n>]>>> ; function_call\n: ZodEffects\n<ZodOptional\n<ZodNullable\n<ZodObject\n<{ arguments\n: ZodString\n; name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { arguments\n: string\n; name\n: string\n}, { arguments\n: string\n; name\n: string\n}>>>, undefined\n| { arguments\n: string\n; name\n: string\n}, undefined\n| null\n| { arguments\n: string\n; name\n: string\n}> ; name\n: ZodEffects\n<ZodOptional\n<ZodNullable\n<ZodString\n>>, undefined\n| string\n, undefined\n| null\n| string\n> ; reasoning\n: ZodEffects\n<ZodOptional\n<ZodNullable\n<ZodArray\n<ZodObject\n<{ content\n: ZodEffects\n<ZodOptional\n<ZodNullable\n<ZodString\n>>, undefined\n| string\n, undefined\n| null\n| string\n> ; id\n: ZodEffects\n<ZodOptional\n<ZodNullable\n<ZodString\n>>, undefined\n| string\n, undefined\n| null\n| string\n> }, \"strip\"\n, ZodTypeAny\n, { content?\n: string\n; id?\n: string\n}, { content?\n: null\n| string\n; id?\n: null\n| string\n}>, \"many\"\n>>>, undefined\n| { content?\n: string\n; id?\n: string\n}[], undefined\n| null\n| { content?\n: null\n| string\n; id?\n: null\n| string\n}[]> ; role\n: ZodLiteral\n<\"assistant\"\n> ; tool_calls\n: ZodEffects\n<ZodOptional\n<ZodNullable\n<ZodArray\n<ZodObject\n<{ function\n: ZodObject\n<{ arguments\n: ZodString\n; name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { arguments\n: string\n; name\n: string\n}, { arguments\n: string\n; name\n: string\n}> ; id\n: ZodString\n; type\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}, { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}>, \"many\"\n>>>, undefined\n| { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[], undefined\n| null\n| { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[]> }, \"strip\"\n, ZodTypeAny\n, { content?\n: null\n| string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text\n: string\n; type\n: \"text\"\n}[] ; function_call?\n: { arguments\n: string\n; name\n: string\n} ; name?\n: string\n; reasoning?\n: { content?\n: string\n; id?\n: string\n}[] ; role\n: \"assistant\"\n; tool_calls?\n: { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[] }, { content?\n: null\n| string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; function_call?\n: null\n| { arguments\n: string\n; name\n: string\n} ; name?\n: null\n| string\n; reasoning?\n: null\n| { content?\n: null\n| string\n; id?\n: null\n| string\n}[] ; role\n: \"assistant\"\n; tool_calls?\n: null\n| { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[] }>, ZodObject\n<{ content\n: ZodUnion\n<[ZodDefault\n<ZodString\n>, ZodArray\n<ZodObject\n<{ cache_control\n: ZodOptional\n<ZodObject\n<{ type\n: ZodEnum\n<[\"ephemeral\"\n]> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"ephemeral\"\n}, { type\n: \"ephemeral\"\n}>> ; text\n: ZodDefault\n<ZodString\n> ; type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text\n: string\n; type\n: \"text\"\n}, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}>, \"many\"\n>]> ; role\n: ZodLiteral\n<\"tool\"\n> ; tool_call_id\n: ZodDefault\n<ZodString\n> }, \"strip\"\n, ZodTypeAny\n, { content\n: {} ; role\n: \"tool\"\n; tool_call_id\n: string\n}, { content?\n: string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; role\n: \"tool\"\n; tool_call_id?\n: string\n}>, ZodObject\n<{ content\n: ZodNullable\n<ZodString\n> ; name\n: ZodString\n; role\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { content\n: null\n| string\n; name\n: string\n; role\n: \"function\"\n}, { content\n: null\n| string\n; name\n: string\n; role\n: \"function\"\n}>]>, ZodObject\n<{ content\n: ZodOptional\n<ZodNullable\n<ZodString\n>> ; role\n: ZodEnum\n<[\"model\"\n]> }, \"strip\"\n, ZodTypeAny\n, { content?\n: null\n| string\n; role\n: \"model\"\n}, { content?\n: null\n| string\n; role\n: \"model\"\n}>]>, \"many\"\n> }, \"strip\"\n, ZodTypeAny\n, { messages\n: ({ content\n: {} ; name?\n: string\n; role\n: \"system\"\n} | { content\n: {} ; name?\n: string\n; role\n: \"user\"\n} | { content?\n: null\n| string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text\n: string\n; type\n: \"text\"\n}[] ; function_call?\n: { arguments\n: string\n; name\n: string\n} ; name?\n: string\n; reasoning?\n: { content?\n: string\n; id?\n: string\n}[] ; role\n: \"assistant\"\n; tool_calls?\n: { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[] } | { content\n: {} ; role\n: \"tool\"\n; tool_call_id\n: string\n} | { content\n: null\n| string\n; name\n: string\n; role\n: \"function\"\n} | { content?\n: null\n| string\n; role\n: \"model\"\n})[] }, { messages\n: ({ content?\n: string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; name?\n: string\n; role\n: \"system\"\n} | { content?\n: string\n| ({ cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n} | { image_url\n: { detail?\n: \"auto\"\n| \"high\"\n| \"low\"\n; url\n: string\n} ; type\n: \"image_url\"\n})[] ; name?\n: string\n; role\n: \"user\"\n} | { content?\n: null\n| string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; function_call?\n: null\n| { arguments\n: string\n; name\n: string\n} ; name?\n: null\n| string\n; reasoning?\n: null\n| { content?\n: null\n| string\n; id?\n: null\n| string\n}[] ; role\n: \"assistant\"\n; tool_calls?\n: null\n| { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[] } | { content?\n: string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; role\n: \"tool\"\n; tool_call_id?\n: string\n} | { content\n: null\n| string\n; name\n: string\n; role\n: \"function\"\n} | { content?\n: null\n| string\n; role\n: \"model\"\n})[] }>]>\npromptDefinitionSchema\n\u2022 Const\npromptDefinitionSchema: ZodIntersection\n<ZodUnion\n<[ZodObject\n<{ prompt\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { prompt\n: string\n}, { prompt\n: string\n}>, ZodObject\n<{ messages\n: ZodArray\n<ZodUnion\n<[ZodUnion\n<[ZodObject\n<{ content\n: ZodUnion\n<[ZodDefault\n<ZodString\n>, ZodArray\n<ZodObject\n<{ cache_control\n: ZodOptional\n<ZodObject\n<{ type\n: ZodEnum\n<[\"ephemeral\"\n]> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"ephemeral\"\n}, { type\n: \"ephemeral\"\n}>> ; text\n: ZodDefault\n<ZodString\n> ; type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text\n: string\n; type\n: \"text\"\n}, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}>, \"many\"\n>]> ; name\n: ZodOptional\n<ZodString\n> ; role\n: ZodLiteral\n<\"system\"\n> }, \"strip\"\n, ZodTypeAny\n, { content\n: {} ; name?\n: string\n; role\n: \"system\"\n}, { content?\n: string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; name?\n: string\n; role\n: \"system\"\n}>, ZodObject\n<{ content\n: ZodUnion\n<[ZodDefault\n<ZodString\n>, ZodArray\n<ZodUnion\n<[ZodObject\n<{ cache_control\n: ZodOptional\n<ZodObject\n<{ type\n: ZodEnum\n<[\"ephemeral\"\n]> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"ephemeral\"\n}, { type\n: \"ephemeral\"\n}>> ; text\n: ZodDefault\n<ZodString\n> ; type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text\n: string\n; type\n: \"text\"\n}, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}>, ZodObject\n<{ image_url\n: ZodObject\n<{ detail\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"low\"\n>, ZodLiteral\n<\"high\"\n>]>> ; url\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { detail?\n: \"auto\"\n| \"high\"\n| \"low\"\n; url\n: string\n}, { detail?\n: \"auto\"\n| \"high\"\n| \"low\"\n; url\n: string\n}> ; type\n: ZodLiteral\n<\"image_url\"\n> }, \"strip\"\n, ZodTypeAny\n, { image_url\n: { detail?\n: \"auto\"\n| \"high\"\n| \"low\"\n; url\n: string\n} ; type\n: \"image_url\"\n}, { image_url\n: { detail?\n: \"auto\"\n| \"high\"\n| \"low\"\n; url\n: string\n} ; type\n: \"image_url\"\n}>]>, \"many\"\n>]> ; name\n: ZodOptional\n<ZodString\n> ; role\n: ZodLiteral\n<\"user\"\n> }, \"strip\"\n, ZodTypeAny\n, { content\n: {} ; name?\n: string\n; role\n: \"user\"\n}, { content?\n: string\n| ({ cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n} | { image_url\n: { detail?\n: \"auto\"\n| \"high\"\n| \"low\"\n; url\n: string\n} ; type\n: \"image_url\"\n})[] ; name?\n: string\n; role\n: \"user\"\n}>, ZodObject\n<{ content\n: ZodOptional\n<ZodNullable\n<ZodUnion\n<[ZodString\n, ZodArray\n<ZodObject\n<{ cache_control\n: ZodOptional\n<ZodObject\n<{ type\n: ZodEnum\n<[\"ephemeral\"\n]> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"ephemeral\"\n}, { type\n: \"ephemeral\"\n}>> ; text\n: ZodDefault\n<ZodString\n> ; type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text\n: string\n; type\n: \"text\"\n}, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}>, \"many\"\n>]>>> ; function_call\n: ZodEffects\n<ZodOptional\n<ZodNullable\n<ZodObject\n<{ arguments\n: ZodString\n; name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { arguments\n: string\n; name\n: string\n}, { arguments\n: string\n; name\n: string\n}>>>, undefined\n| { arguments\n: string\n; name\n: string\n}, undefined\n| null\n| { arguments\n: string\n; name\n: string\n}> ; name\n: ZodEffects\n<ZodOptional\n<ZodNullable\n<ZodString\n>>, undefined\n| string\n, undefined\n| null\n| string\n> ; reasoning\n: ZodEffects\n<ZodOptional\n<ZodNullable\n<ZodArray\n<ZodObject\n<{ content\n: ZodEffects\n<ZodOptional\n<ZodNullable\n<ZodString\n>>, undefined\n| string\n, undefined\n| null\n| string\n> ; id\n: ZodEffects\n<ZodOptional\n<ZodNullable\n<ZodString\n>>, undefined\n| string\n, undefined\n| null\n| string\n> }, \"strip\"\n, ZodTypeAny\n, { content?\n: string\n; id?\n: string\n}, { content?\n: null\n| string\n; id?\n: null\n| string\n}>, \"many\"\n>>>, undefined\n| { content?\n: string\n; id?\n: string\n}[], undefined\n| null\n| { content?\n: null\n| string\n; id?\n: null\n| string\n}[]> ; role\n: ZodLiteral\n<\"assistant\"\n> ; tool_calls\n: ZodEffects\n<ZodOptional\n<ZodNullable\n<ZodArray\n<ZodObject\n<{ function\n: ZodObject\n<{ arguments\n: ZodString\n; name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { arguments\n: string\n; name\n: string\n}, { arguments\n: string\n; name\n: string\n}> ; id\n: ZodString\n; type\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}, { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}>, \"many\"\n>>>, undefined\n| { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[], undefined\n| null\n| { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[]> }, \"strip\"\n, ZodTypeAny\n, { content?\n: null\n| string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text\n: string\n; type\n: \"text\"\n}[] ; function_call?\n: { arguments\n: string\n; name\n: string\n} ; name?\n: string\n; reasoning?\n: { content?\n: string\n; id?\n: string\n}[] ; role\n: \"assistant\"\n; tool_calls?\n: { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[] }, { content?\n: null\n| string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; function_call?\n: null\n| { arguments\n: string\n; name\n: string\n} ; name?\n: null\n| string\n; reasoning?\n: null\n| { content?\n: null\n| string\n; id?\n: null\n| string\n}[] ; role\n: \"assistant\"\n; tool_calls?\n: null\n| { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[] }>, ZodObject\n<{ content\n: ZodUnion\n<[ZodDefault\n<ZodString\n>, ZodArray\n<ZodObject\n<{ cache_control\n: ZodOptional\n<ZodObject\n<{ type\n: ZodEnum\n<[\"ephemeral\"\n]> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"ephemeral\"\n}, { type\n: \"ephemeral\"\n}>> ; text\n: ZodDefault\n<ZodString\n> ; type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text\n: string\n; type\n: \"text\"\n}, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}>, \"many\"\n>]> ; role\n: ZodLiteral\n<\"tool\"\n> ; tool_call_id\n: ZodDefault\n<ZodString\n> }, \"strip\"\n, ZodTypeAny\n, { content\n: {} ; role\n: \"tool\"\n; tool_call_id\n: string\n}, { content?\n: string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; role\n: \"tool\"\n; tool_call_id?\n: string\n}>, ZodObject\n<{ content\n: ZodNullable\n<ZodString\n> ; name\n: ZodString\n; role\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { content\n: null\n| string\n; name\n: string\n; role\n: \"function\"\n}, { content\n: null\n| string\n; name\n: string\n; role\n: \"function\"\n}>]>, ZodObject\n<{ content\n: ZodOptional\n<ZodNullable\n<ZodString\n>> ; role\n: ZodEnum\n<[\"model\"\n]> }, \"strip\"\n, ZodTypeAny\n, { content?\n: null\n| string\n; role\n: \"model\"\n}, { content?\n: null\n| string\n; role\n: \"model\"\n}>]>, \"many\"\n> }, \"strip\"\n, ZodTypeAny\n, { messages\n: ({ content\n: {} ; name?\n: string\n; role\n: \"system\"\n} | { content\n: {} ; name?\n: string\n; role\n: \"user\"\n} | { content?\n: null\n| string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text\n: string\n; type\n: \"text\"\n}[] ; function_call?\n: { arguments\n: string\n; name\n: string\n} ; name?\n: string\n; reasoning?\n: { content?\n: string\n; id?\n: string\n}[] ; role\n: \"assistant\"\n; tool_calls?\n: { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[] } | { content\n: {} ; role\n: \"tool\"\n; tool_call_id\n: string\n} | { content\n: null\n| string\n; name\n: string\n; role\n: \"function\"\n} | { content?\n: null\n| string\n; role\n: \"model\"\n})[] }, { messages\n: ({ content?\n: string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; name?\n: string\n; role\n: \"system\"\n} | { content?\n: string\n| ({ cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n} | { image_url\n: { detail?\n: \"auto\"\n| \"high\"\n| \"low\"\n; url\n: string\n} ; type\n: \"image_url\"\n})[] ; name?\n: string\n; role\n: \"user\"\n} | { content?\n: null\n| string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; function_call?\n: null\n| { arguments\n: string\n; name\n: string\n} ; name?\n: null\n| string\n; reasoning?\n: null\n| { content?\n: null\n| string\n; id?\n: null\n| string\n}[] ; role\n: \"assistant\"\n; tool_calls?\n: null\n| { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[] } | { content?\n: string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; role\n: \"tool\"\n; tool_call_id?\n: string\n} | { content\n: null\n| string\n; name\n: string\n; role\n: \"function\"\n} | { content?\n: null\n| string\n; role\n: \"model\"\n})[] }>]>, ZodObject\n<{ model\n: ZodString\n; params\n: ZodOptional\n<ZodUnion\n<[ZodObject\n<{ frequency_penalty\n: ZodOptional\n<ZodNumber\n> ; function_call\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}>]>> ; max_completion_tokens\n: ZodOptional\n<ZodNumber\n> ; max_tokens\n: ZodOptional\n<ZodNumber\n> ; n\n: ZodOptional\n<ZodNumber\n> ; presence_penalty\n: ZodOptional\n<ZodNumber\n> ; reasoning_effort\n: ZodOptional\n<ZodEnum\n<[\"low\"\n, \"medium\"\n, \"high\"\n]>> ; response_format\n: ZodOptional\n<ZodNullable\n<ZodUnion\n<[ZodObject\n<{ type\n: ZodLiteral\n<\"json_object\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"json_object\"\n}, { type\n: \"json_object\"\n}>, ZodObject\n<{ json_schema\n: ZodObject\n<{ description\n: ZodOptional\n<ZodString\n> ; name\n: ZodString\n; schema\n: ZodOptional\n<ZodUnion\n<[ZodRecord\n<ZodString\n, ZodUnknown\n>, ZodString\n]>> ; strict\n: ZodOptional\n<ZodNullable\n<ZodBoolean\n>> }, \"strip\"\n, ZodTypeAny\n, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}> ; type\n: ZodLiteral\n<\"json_schema\"\n> }, \"strip\"\n, ZodTypeAny\n, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}>, ZodObject\n<{ type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"text\"\n}, { type\n: \"text\"\n}>]>>> ; stop\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodOptional\n<ZodNumber\n> ; tool_choice\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodLiteral\n<\"required\"\n>, ZodObject\n<{ function\n: ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}> ; type\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { function\n: { name\n: string\n} ; type\n: \"function\"\n}, { function\n: { name\n: string\n} ; type\n: \"function\"\n}>]>> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, \"passthrough\"\n, ZodTypeAny\n, objectOutputType\n<{ frequency_penalty\n: ZodOptional\n<ZodNumber\n> ; function_call\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}>]>> ; max_completion_tokens\n: ZodOptional\n<ZodNumber\n> ; max_tokens\n: ZodOptional\n<ZodNumber\n> ; n\n: ZodOptional\n<ZodNumber\n> ; presence_penalty\n: ZodOptional\n<ZodNumber\n> ; reasoning_effort\n: ZodOptional\n<ZodEnum\n<[\"low\"\n, \"medium\"\n, \"high\"\n]>> ; response_format\n: ZodOptional\n<ZodNullable\n<ZodUnion\n<[ZodObject\n<{ type\n: ZodLiteral\n<\"json_object\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"json_object\"\n}, { type\n: \"json_object\"\n}>, ZodObject\n<{ json_schema\n: ZodObject\n<{ description\n: ZodOptional\n<ZodString\n> ; name\n: ZodString\n; schema\n: ZodOptional\n<ZodUnion\n<[ZodRecord\n<ZodString\n, ZodUnknown\n>, ZodString\n]>> ; strict\n: ZodOptional\n<ZodNullable\n<ZodBoolean\n>> }, \"strip\"\n, ZodTypeAny\n, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}> ; type\n: ZodLiteral\n<\"json_schema\"\n> }, \"strip\"\n, ZodTypeAny\n, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}>, ZodObject\n<{ type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"text\"\n}, { type\n: \"text\"\n}>]>>> ; stop\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodOptional\n<ZodNumber\n> ; tool_choice\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodLiteral\n<\"required\"\n>, ZodObject\n<{ function\n: ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}> ; type\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { function\n: { name\n: string\n} ; type\n: \"function\"\n}, { function\n: { name\n: string\n} ; type\n: \"function\"\n}>]>> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>, objectInputType\n<{ frequency_penalty\n: ZodOptional\n<ZodNumber\n> ; function_call\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}>]>> ; max_completion_tokens\n: ZodOptional\n<ZodNumber\n> ; max_tokens\n: ZodOptional\n<ZodNumber\n> ; n\n: ZodOptional\n<ZodNumber\n> ; presence_penalty\n: ZodOptional\n<ZodNumber\n> ; reasoning_effort\n: ZodOptional\n<ZodEnum\n<[\"low\"\n, \"medium\"\n, \"high\"\n]>> ; response_format\n: ZodOptional\n<ZodNullable\n<ZodUnion\n<[ZodObject\n<{ type\n: ZodLiteral\n<\"json_object\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"json_object\"\n}, { type\n: \"json_object\"\n}>, ZodObject\n<{ json_schema\n: ZodObject\n<{ description\n: ZodOptional\n<ZodString\n> ; name\n: ZodString\n; schema\n: ZodOptional\n<ZodUnion\n<[ZodRecord\n<ZodString\n, ZodUnknown\n>, ZodString\n]>> ; strict\n: ZodOptional\n<ZodNullable\n<ZodBoolean\n>> }, \"strip\"\n, ZodTypeAny\n, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}> ; type\n: ZodLiteral\n<\"json_schema\"\n> }, \"strip\"\n, ZodTypeAny\n, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}>, ZodObject\n<{ type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"text\"\n}, { type\n: \"text\"\n}>]>>> ; stop\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodOptional\n<ZodNumber\n> ; tool_choice\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodLiteral\n<\"required\"\n>, ZodObject\n<{ function\n: ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}> ; type\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { function\n: { name\n: string\n} ; type\n: \"function\"\n}, { function\n: { name\n: string\n} ; type\n: \"function\"\n}>]>> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>>, ZodObject\n<{ max_tokens\n: ZodNumber\n; max_tokens_to_sample\n: ZodOptional\n<ZodNumber\n> ; stop_sequences\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodNumber\n; top_k\n: ZodOptional\n<ZodNumber\n> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, \"passthrough\"\n, ZodTypeAny\n, objectOutputType\n<{ max_tokens\n: ZodNumber\n; max_tokens_to_sample\n: ZodOptional\n<ZodNumber\n> ; stop_sequences\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodNumber\n; top_k\n: ZodOptional\n<ZodNumber\n> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>, objectInputType\n<{ max_tokens\n: ZodNumber\n; max_tokens_to_sample\n: ZodOptional\n<ZodNumber\n> ; stop_sequences\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodNumber\n; top_k\n: ZodOptional\n<ZodNumber\n> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>>, ZodObject\n<{ maxOutputTokens\n: ZodOptional\n<ZodNumber\n> ; temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; topP\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, \"passthrough\"\n, ZodTypeAny\n, objectOutputType\n<{ maxOutputTokens\n: ZodOptional\n<ZodNumber\n> ; temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; topP\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>, objectInputType\n<{ maxOutputTokens\n: ZodOptional\n<ZodNumber\n> ; temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; topP\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>>, ZodObject\n<{ temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, \"passthrough\"\n, ZodTypeAny\n, objectOutputType\n<{ temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>, objectInputType\n<{ temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>>, ZodObject\n<{ use_cache\n: ZodOptional\n<ZodBoolean\n> }, \"passthrough\"\n, ZodTypeAny\n, objectOutputType\n<{ use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>, objectInputType\n<{ use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>>]>> }, \"strip\"\n, ZodTypeAny\n, { model\n: string\n; params?\n: objectOutputType\n<{ frequency_penalty\n: ZodOptional\n<ZodNumber\n> ; function_call\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}>]>> ; max_completion_tokens\n: ZodOptional\n<ZodNumber\n> ; max_tokens\n: ZodOptional\n<ZodNumber\n> ; n\n: ZodOptional\n<ZodNumber\n> ; presence_penalty\n: ZodOptional\n<ZodNumber\n> ; reasoning_effort\n: ZodOptional\n<ZodEnum\n<[\"low\"\n, \"medium\"\n, \"high\"\n]>> ; response_format\n: ZodOptional\n<ZodNullable\n<ZodUnion\n<[ZodObject\n<{ type\n: ZodLiteral\n<\"json_object\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"json_object\"\n}, { type\n: \"json_object\"\n}>, ZodObject\n<{ json_schema\n: ZodObject\n<{ description\n: ZodOptional\n<ZodString\n> ; name\n: ZodString\n; schema\n: ZodOptional\n<ZodUnion\n<[ZodRecord\n<ZodString\n, ZodUnknown\n>, ZodString\n]>> ; strict\n: ZodOptional\n<ZodNullable\n<ZodBoolean\n>> }, \"strip\"\n, ZodTypeAny\n, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}> ; type\n: ZodLiteral\n<\"json_schema\"\n> }, \"strip\"\n, ZodTypeAny\n, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}>, ZodObject\n<{ type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"text\"\n}, { type\n: \"text\"\n}>]>>> ; stop\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodOptional\n<ZodNumber\n> ; tool_choice\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodLiteral\n<\"required\"\n>, ZodObject\n<{ function\n: ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}> ; type\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { function\n: { name\n: string\n} ; type\n: \"function\"\n}, { function\n: { name\n: string\n} ; type\n: \"function\"\n}>]>> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectOutputType\n<{ max_tokens\n: ZodNumber\n; max_tokens_to_sample\n: ZodOptional\n<ZodNumber\n> ; stop_sequences\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodNumber\n; top_k\n: ZodOptional\n<ZodNumber\n> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectOutputType\n<{ maxOutputTokens\n: ZodOptional\n<ZodNumber\n> ; temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; topP\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectOutputType\n<{ temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectOutputType\n<{ use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> }, { model\n: string\n; params?\n: objectInputType\n<{ frequency_penalty\n: ZodOptional\n<ZodNumber\n> ; function_call\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}>]>> ; max_completion_tokens\n: ZodOptional\n<ZodNumber\n> ; max_tokens\n: ZodOptional\n<ZodNumber\n> ; n\n: ZodOptional\n<ZodNumber\n> ; presence_penalty\n: ZodOptional\n<ZodNumber\n> ; reasoning_effort\n: ZodOptional\n<ZodEnum\n<[\"low\"\n, \"medium\"\n, \"high\"\n]>> ; response_format\n: ZodOptional\n<ZodNullable\n<ZodUnion\n<[ZodObject\n<{ type\n: ZodLiteral\n<\"json_object\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"json_object\"\n}, { type\n: \"json_object\"\n}>, ZodObject\n<{ json_schema\n: ZodObject\n<{ description\n: ZodOptional\n<ZodString\n> ; name\n: ZodString\n; schema\n: ZodOptional\n<ZodUnion\n<[ZodRecord\n<ZodString\n, ZodUnknown\n>, ZodString\n]>> ; strict\n: ZodOptional\n<ZodNullable\n<ZodBoolean\n>> }, \"strip\"\n, ZodTypeAny\n, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}> ; type\n: ZodLiteral\n<\"json_schema\"\n> }, \"strip\"\n, ZodTypeAny\n, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}>, ZodObject\n<{ type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"text\"\n}, { type\n: \"text\"\n}>]>>> ; stop\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodOptional\n<ZodNumber\n> ; tool_choice\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodLiteral\n<\"required\"\n>, ZodObject\n<{ function\n: ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}> ; type\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { function\n: { name\n: string\n} ; type\n: \"function\"\n}, { function\n: { name\n: string\n} ; type\n: \"function\"\n}>]>> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectInputType\n<{ max_tokens\n: ZodNumber\n; max_tokens_to_sample\n: ZodOptional\n<ZodNumber\n> ; stop_sequences\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodNumber\n; top_k\n: ZodOptional\n<ZodNumber\n> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectInputType\n<{ maxOutputTokens\n: ZodOptional\n<ZodNumber\n> ; temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; topP\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectInputType\n<{ temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectInputType\n<{ use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> }>>\npromptDefinitionWithToolsSchema\n\u2022 Const\npromptDefinitionWithToolsSchema: ZodIntersection\n<ZodIntersection\n<ZodUnion\n<[ZodObject\n<{ prompt\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { prompt\n: string\n}, { prompt\n: string\n}>, ZodObject\n<{ messages\n: ZodArray\n<ZodUnion\n<[ZodUnion\n<[ZodObject\n<{ content\n: ZodUnion\n<[ZodDefault\n<ZodString\n>, ZodArray\n<ZodObject\n<{ cache_control\n: ZodOptional\n<ZodObject\n<{ type\n: ZodEnum\n<[\"ephemeral\"\n]> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"ephemeral\"\n}, { type\n: \"ephemeral\"\n}>> ; text\n: ZodDefault\n<ZodString\n> ; type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text\n: string\n; type\n: \"text\"\n}, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}>, \"many\"\n>]> ; name\n: ZodOptional\n<ZodString\n> ; role\n: ZodLiteral\n<\"system\"\n> }, \"strip\"\n, ZodTypeAny\n, { content\n: {} ; name?\n: string\n; role\n: \"system\"\n}, { content?\n: string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; name?\n: string\n; role\n: \"system\"\n}>, ZodObject\n<{ content\n: ZodUnion\n<[ZodDefault\n<ZodString\n>, ZodArray\n<ZodUnion\n<[ZodObject\n<{ cache_control\n: ZodOptional\n<ZodObject\n<{ type\n: ZodEnum\n<[\"ephemeral\"\n]> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"ephemeral\"\n}, { type\n: \"ephemeral\"\n}>> ; text\n: ZodDefault\n<ZodString\n> ; type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text\n: string\n; type\n: \"text\"\n}, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}>, ZodObject\n<{ image_url\n: ZodObject\n<{ detail\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"low\"\n>, ZodLiteral\n<\"high\"\n>]>> ; url\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { detail?\n: \"auto\"\n| \"high\"\n| \"low\"\n; url\n: string\n}, { detail?\n: \"auto\"\n| \"high\"\n| \"low\"\n; url\n: string\n}> ; type\n: ZodLiteral\n<\"image_url\"\n> }, \"strip\"\n, ZodTypeAny\n, { image_url\n: { detail?\n: \"auto\"\n| \"high\"\n| \"low\"\n; url\n: string\n} ; type\n: \"image_url\"\n}, { image_url\n: { detail?\n: \"auto\"\n| \"high\"\n| \"low\"\n; url\n: string\n} ; type\n: \"image_url\"\n}>]>, \"many\"\n>]> ; name\n: ZodOptional\n<ZodString\n> ; role\n: ZodLiteral\n<\"user\"\n> }, \"strip\"\n, ZodTypeAny\n, { content\n: {} ; name?\n: string\n; role\n: \"user\"\n}, { content?\n: string\n| ({ cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n} | { image_url\n: { detail?\n: \"auto\"\n| \"high\"\n| \"low\"\n; url\n: string\n} ; type\n: \"image_url\"\n})[] ; name?\n: string\n; role\n: \"user\"\n}>, ZodObject\n<{ content\n: ZodOptional\n<ZodNullable\n<ZodUnion\n<[ZodString\n, ZodArray\n<ZodObject\n<{ cache_control\n: ZodOptional\n<ZodObject\n<{ type\n: ZodEnum\n<[\"ephemeral\"\n]> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"ephemeral\"\n}, { type\n: \"ephemeral\"\n}>> ; text\n: ZodDefault\n<ZodString\n> ; type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text\n: string\n; type\n: \"text\"\n}, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}>, \"many\"\n>]>>> ; function_call\n: ZodEffects\n<ZodOptional\n<ZodNullable\n<ZodObject\n<{ arguments\n: ZodString\n; name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { arguments\n: string\n; name\n: string\n}, { arguments\n: string\n; name\n: string\n}>>>, undefined\n| { arguments\n: string\n; name\n: string\n}, undefined\n| null\n| { arguments\n: string\n; name\n: string\n}> ; name\n: ZodEffects\n<ZodOptional\n<ZodNullable\n<ZodString\n>>, undefined\n| string\n, undefined\n| null\n| string\n> ; reasoning\n: ZodEffects\n<ZodOptional\n<ZodNullable\n<ZodArray\n<ZodObject\n<{ content\n: ZodEffects\n<ZodOptional\n<ZodNullable\n<ZodString\n>>, undefined\n| string\n, undefined\n| null\n| string\n> ; id\n: ZodEffects\n<ZodOptional\n<ZodNullable\n<ZodString\n>>, undefined\n| string\n, undefined\n| null\n| string\n> }, \"strip\"\n, ZodTypeAny\n, { content?\n: string\n; id?\n: string\n}, { content?\n: null\n| string\n; id?\n: null\n| string\n}>, \"many\"\n>>>, undefined\n| { content?\n: string\n; id?\n: string\n}[], undefined\n| null\n| { content?\n: null\n| string\n; id?\n: null\n| string\n}[]> ; role\n: ZodLiteral\n<\"assistant\"\n> ; tool_calls\n: ZodEffects\n<ZodOptional\n<ZodNullable\n<ZodArray\n<ZodObject\n<{ function\n: ZodObject\n<{ arguments\n: ZodString\n; name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { arguments\n: string\n; name\n: string\n}, { arguments\n: string\n; name\n: string\n}> ; id\n: ZodString\n; type\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}, { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}>, \"many\"\n>>>, undefined\n| { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[], undefined\n| null\n| { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[]> }, \"strip\"\n, ZodTypeAny\n, { content?\n: null\n| string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text\n: string\n; type\n: \"text\"\n}[] ; function_call?\n: { arguments\n: string\n; name\n: string\n} ; name?\n: string\n; reasoning?\n: { content?\n: string\n; id?\n: string\n}[] ; role\n: \"assistant\"\n; tool_calls?\n: { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[] }, { content?\n: null\n| string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; function_call?\n: null\n| { arguments\n: string\n; name\n: string\n} ; name?\n: null\n| string\n; reasoning?\n: null\n| { content?\n: null\n| string\n; id?\n: null\n| string\n}[] ; role\n: \"assistant\"\n; tool_calls?\n: null\n| { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[] }>, ZodObject\n<{ content\n: ZodUnion\n<[ZodDefault\n<ZodString\n>, ZodArray\n<ZodObject\n<{ cache_control\n: ZodOptional\n<ZodObject\n<{ type\n: ZodEnum\n<[\"ephemeral\"\n]> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"ephemeral\"\n}, { type\n: \"ephemeral\"\n}>> ; text\n: ZodDefault\n<ZodString\n> ; type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text\n: string\n; type\n: \"text\"\n}, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}>, \"many\"\n>]> ; role\n: ZodLiteral\n<\"tool\"\n> ; tool_call_id\n: ZodDefault\n<ZodString\n> }, \"strip\"\n, ZodTypeAny\n, { content\n: {} ; role\n: \"tool\"\n; tool_call_id\n: string\n}, { content?\n: string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; role\n: \"tool\"\n; tool_call_id?\n: string\n}>, ZodObject\n<{ content\n: ZodNullable\n<ZodString\n> ; name\n: ZodString\n; role\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { content\n: null\n| string\n; name\n: string\n; role\n: \"function\"\n}, { content\n: null\n| string\n; name\n: string\n; role\n: \"function\"\n}>]>, ZodObject\n<{ content\n: ZodOptional\n<ZodNullable\n<ZodString\n>> ; role\n: ZodEnum\n<[\"model\"\n]> }, \"strip\"\n, ZodTypeAny\n, { content?\n: null\n| string\n; role\n: \"model\"\n}, { content?\n: null\n| string\n; role\n: \"model\"\n}>]>, \"many\"\n> }, \"strip\"\n, ZodTypeAny\n, { messages\n: ({ content\n: {} ; name?\n: string\n; role\n: \"system\"\n} | { content\n: {} ; name?\n: string\n; role\n: \"user\"\n} | { content?\n: null\n| string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text\n: string\n; type\n: \"text\"\n}[] ; function_call?\n: { arguments\n: string\n; name\n: string\n} ; name?\n: string\n; reasoning?\n: { content?\n: string\n; id?\n: string\n}[] ; role\n: \"assistant\"\n; tool_calls?\n: { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[] } | { content\n: {} ; role\n: \"tool\"\n; tool_call_id\n: string\n} | { content\n: null\n| string\n; name\n: string\n; role\n: \"function\"\n} | { content?\n: null\n| string\n; role\n: \"model\"\n})[] }, { messages\n: ({ content?\n: string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; name?\n: string\n; role\n: \"system\"\n} | { content?\n: string\n| ({ cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n} | { image_url\n: { detail?\n: \"auto\"\n| \"high\"\n| \"low\"\n; url\n: string\n} ; type\n: \"image_url\"\n})[] ; name?\n: string\n; role\n: \"user\"\n} | { content?\n: null\n| string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; function_call?\n: null\n| { arguments\n: string\n; name\n: string\n} ; name?\n: null\n| string\n; reasoning?\n: null\n| { content?\n: null\n| string\n; id?\n: null\n| string\n}[] ; role\n: \"assistant\"\n; tool_calls?\n: null\n| { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[] } | { content?\n: string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; role\n: \"tool\"\n; tool_call_id?\n: string\n} | { content\n: null\n| string\n; name\n: string\n; role\n: \"function\"\n} | { content?\n: null\n| string\n; role\n: \"model\"\n})[] }>]>, ZodObject\n<{ model\n: ZodString\n; params\n: ZodOptional\n<ZodUnion\n<[ZodObject\n<{ frequency_penalty\n: ZodOptional\n<ZodNumber\n> ; function_call\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}>]>> ; max_completion_tokens\n: ZodOptional\n<ZodNumber\n> ; max_tokens\n: ZodOptional\n<ZodNumber\n> ; n\n: ZodOptional\n<ZodNumber\n> ; presence_penalty\n: ZodOptional\n<ZodNumber\n> ; reasoning_effort\n: ZodOptional\n<ZodEnum\n<[\"low\"\n, \"medium\"\n, \"high\"\n]>> ; response_format\n: ZodOptional\n<ZodNullable\n<ZodUnion\n<[ZodObject\n<{ type\n: ZodLiteral\n<\"json_object\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"json_object\"\n}, { type\n: \"json_object\"\n}>, ZodObject\n<{ json_schema\n: ZodObject\n<{ description\n: ZodOptional\n<ZodString\n> ; name\n: ZodString\n; schema\n: ZodOptional\n<ZodUnion\n<[ZodRecord\n<ZodString\n, ZodUnknown\n>, ZodString\n]>> ; strict\n: ZodOptional\n<ZodNullable\n<ZodBoolean\n>> }, \"strip\"\n, ZodTypeAny\n, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}> ; type\n: ZodLiteral\n<\"json_schema\"\n> }, \"strip\"\n, ZodTypeAny\n, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}>, ZodObject\n<{ type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"text\"\n}, { type\n: \"text\"\n}>]>>> ; stop\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodOptional\n<ZodNumber\n> ; tool_choice\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodLiteral\n<\"required\"\n>, ZodObject\n<{ function\n: ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}> ; type\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { function\n: { name\n: string\n} ; type\n: \"function\"\n}, { function\n: { name\n: string\n} ; type\n: \"function\"\n}>]>> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, \"passthrough\"\n, ZodTypeAny\n, objectOutputType\n<{ frequency_penalty\n: ZodOptional\n<ZodNumber\n> ; function_call\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}>]>> ; max_completion_tokens\n: ZodOptional\n<ZodNumber\n> ; max_tokens\n: ZodOptional\n<ZodNumber\n> ; n\n: ZodOptional\n<ZodNumber\n> ; presence_penalty\n: ZodOptional\n<ZodNumber\n> ; reasoning_effort\n: ZodOptional\n<ZodEnum\n<[\"low\"\n, \"medium\"\n, \"high\"\n]>> ; response_format\n: ZodOptional\n<ZodNullable\n<ZodUnion\n<[ZodObject\n<{ type\n: ZodLiteral\n<\"json_object\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"json_object\"\n}, { type\n: \"json_object\"\n}>, ZodObject\n<{ json_schema\n: ZodObject\n<{ description\n: ZodOptional\n<ZodString\n> ; name\n: ZodString\n; schema\n: ZodOptional\n<ZodUnion\n<[ZodRecord\n<ZodString\n, ZodUnknown\n>, ZodString\n]>> ; strict\n: ZodOptional\n<ZodNullable\n<ZodBoolean\n>> }, \"strip\"\n, ZodTypeAny\n, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}> ; type\n: ZodLiteral\n<\"json_schema\"\n> }, \"strip\"\n, ZodTypeAny\n, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}>, ZodObject\n<{ type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"text\"\n}, { type\n: \"text\"\n}>]>>> ; stop\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodOptional\n<ZodNumber\n> ; tool_choice\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodLiteral\n<\"required\"\n>, ZodObject\n<{ function\n: ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}> ; type\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { function\n: { name\n: string\n} ; type\n: \"function\"\n}, { function\n: { name\n: string\n} ; type\n: \"function\"\n}>]>> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>, objectInputType\n<{ frequency_penalty\n: ZodOptional\n<ZodNumber\n> ; function_call\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}>]>> ; max_completion_tokens\n: ZodOptional\n<ZodNumber\n> ; max_tokens\n: ZodOptional\n<ZodNumber\n> ; n\n: ZodOptional\n<ZodNumber\n> ; presence_penalty\n: ZodOptional\n<ZodNumber\n> ; reasoning_effort\n: ZodOptional\n<ZodEnum\n<[\"low\"\n, \"medium\"\n, \"high\"\n]>> ; response_format\n: ZodOptional\n<ZodNullable\n<ZodUnion\n<[ZodObject\n<{ type\n: ZodLiteral\n<\"json_object\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"json_object\"\n}, { type\n: \"json_object\"\n}>, ZodObject\n<{ json_schema\n: ZodObject\n<{ description\n: ZodOptional\n<ZodString\n> ; name\n: ZodString\n; schema\n: ZodOptional\n<ZodUnion\n<[ZodRecord\n<ZodString\n, ZodUnknown\n>, ZodString\n]>> ; strict\n: ZodOptional\n<ZodNullable\n<ZodBoolean\n>> }, \"strip\"\n, ZodTypeAny\n, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}> ; type\n: ZodLiteral\n<\"json_schema\"\n> }, \"strip\"\n, ZodTypeAny\n, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}>, ZodObject\n<{ type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"text\"\n}, { type\n: \"text\"\n}>]>>> ; stop\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodOptional\n<ZodNumber\n> ; tool_choice\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodLiteral\n<\"required\"\n>, ZodObject\n<{ function\n: ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}> ; type\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { function\n: { name\n: string\n} ; type\n: \"function\"\n}, { function\n: { name\n: string\n} ; type\n: \"function\"\n}>]>> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>>, ZodObject\n<{ max_tokens\n: ZodNumber\n; max_tokens_to_sample\n: ZodOptional\n<ZodNumber\n> ; stop_sequences\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodNumber\n; top_k\n: ZodOptional\n<ZodNumber\n> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, \"passthrough\"\n, ZodTypeAny\n, objectOutputType\n<{ max_tokens\n: ZodNumber\n; max_tokens_to_sample\n: ZodOptional\n<ZodNumber\n> ; stop_sequences\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodNumber\n; top_k\n: ZodOptional\n<ZodNumber\n> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>, objectInputType\n<{ max_tokens\n: ZodNumber\n; max_tokens_to_sample\n: ZodOptional\n<ZodNumber\n> ; stop_sequences\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodNumber\n; top_k\n: ZodOptional\n<ZodNumber\n> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>>, ZodObject\n<{ maxOutputTokens\n: ZodOptional\n<ZodNumber\n> ; temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; topP\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, \"passthrough\"\n, ZodTypeAny\n, objectOutputType\n<{ maxOutputTokens\n: ZodOptional\n<ZodNumber\n> ; temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; topP\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>, objectInputType\n<{ maxOutputTokens\n: ZodOptional\n<ZodNumber\n> ; temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; topP\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>>, ZodObject\n<{ temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, \"passthrough\"\n, ZodTypeAny\n, objectOutputType\n<{ temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>, objectInputType\n<{ temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>>, ZodObject\n<{ use_cache\n: ZodOptional\n<ZodBoolean\n> }, \"passthrough\"\n, ZodTypeAny\n, objectOutputType\n<{ use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>, objectInputType\n<{ use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>>]>> }, \"strip\"\n, ZodTypeAny\n, { model\n: string\n; params?\n: objectOutputType\n<{ frequency_penalty\n: ZodOptional\n<ZodNumber\n> ; function_call\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}>]>> ; max_completion_tokens\n: ZodOptional\n<ZodNumber\n> ; max_tokens\n: ZodOptional\n<ZodNumber\n> ; n\n: ZodOptional\n<ZodNumber\n> ; presence_penalty\n: ZodOptional\n<ZodNumber\n> ; reasoning_effort\n: ZodOptional\n<ZodEnum\n<[\"low\"\n, \"medium\"\n, \"high\"\n]>> ; response_format\n: ZodOptional\n<ZodNullable\n<ZodUnion\n<[ZodObject\n<{ type\n: ZodLiteral\n<\"json_object\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"json_object\"\n}, { type\n: \"json_object\"\n}>, ZodObject\n<{ json_schema\n: ZodObject\n<{ description\n: ZodOptional\n<ZodString\n> ; name\n: ZodString\n; schema\n: ZodOptional\n<ZodUnion\n<[ZodRecord\n<ZodString\n, ZodUnknown\n>, ZodString\n]>> ; strict\n: ZodOptional\n<ZodNullable\n<ZodBoolean\n>> }, \"strip\"\n, ZodTypeAny\n, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}> ; type\n: ZodLiteral\n<\"json_schema\"\n> }, \"strip\"\n, ZodTypeAny\n, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}>, ZodObject\n<{ type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"text\"\n}, { type\n: \"text\"\n}>]>>> ; stop\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodOptional\n<ZodNumber\n> ; tool_choice\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodLiteral\n<\"required\"\n>, ZodObject\n<{ function\n: ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}> ; type\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { function\n: { name\n: string\n} ; type\n: \"function\"\n}, { function\n: { name\n: string\n} ; type\n: \"function\"\n}>]>> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectOutputType\n<{ max_tokens\n: ZodNumber\n; max_tokens_to_sample\n: ZodOptional\n<ZodNumber\n> ; stop_sequences\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodNumber\n; top_k\n: ZodOptional\n<ZodNumber\n> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectOutputType\n<{ maxOutputTokens\n: ZodOptional\n<ZodNumber\n> ; temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; topP\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectOutputType\n<{ temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectOutputType\n<{ use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> }, { model\n: string\n; params?\n: objectInputType\n<{ frequency_penalty\n: ZodOptional\n<ZodNumber\n> ; function_call\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}>]>> ; max_completion_tokens\n: ZodOptional\n<ZodNumber\n> ; max_tokens\n: ZodOptional\n<ZodNumber\n> ; n\n: ZodOptional\n<ZodNumber\n> ; presence_penalty\n: ZodOptional\n<ZodNumber\n> ; reasoning_effort\n: ZodOptional\n<ZodEnum\n<[\"low\"\n, \"medium\"\n, \"high\"\n]>> ; response_format\n: ZodOptional\n<ZodNullable\n<ZodUnion\n<[ZodObject\n<{ type\n: ZodLiteral\n<\"json_object\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"json_object\"\n}, { type\n: \"json_object\"\n}>, ZodObject\n<{ json_schema\n: ZodObject\n<{ description\n: ZodOptional\n<ZodString\n> ; name\n: ZodString\n; schema\n: ZodOptional\n<ZodUnion\n<[ZodRecord\n<ZodString\n, ZodUnknown\n>, ZodString\n]>> ; strict\n: ZodOptional\n<ZodNullable\n<ZodBoolean\n>> }, \"strip\"\n, ZodTypeAny\n, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}> ; type\n: ZodLiteral\n<\"json_schema\"\n> }, \"strip\"\n, ZodTypeAny\n, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}>, ZodObject\n<{ type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"text\"\n}, { type\n: \"text\"\n}>]>>> ; stop\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodOptional\n<ZodNumber\n> ; tool_choice\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodLiteral\n<\"required\"\n>, ZodObject\n<{ function\n: ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}> ; type\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { function\n: { name\n: string\n} ; type\n: \"function\"\n}, { function\n: { name\n: string\n} ; type\n: \"function\"\n}>]>> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectInputType\n<{ max_tokens\n: ZodNumber\n; max_tokens_to_sample\n: ZodOptional\n<ZodNumber\n> ; stop_sequences\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodNumber\n; top_k\n: ZodOptional\n<ZodNumber\n> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectInputType\n<{ maxOutputTokens\n: ZodOptional\n<ZodNumber\n> ; temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; topP\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectInputType\n<{ temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectInputType\n<{ use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> }>>, ZodObject\n<{ tools\n: ZodOptional\n<ZodArray\n<ZodObject\n<{ function\n: ZodObject\n<{ description\n: ZodOptional\n<ZodString\n> ; name\n: ZodString\n; parameters\n: ZodOptional\n<ZodRecord\n<ZodString\n, ZodUnknown\n>> ; strict\n: ZodOptional\n<ZodNullable\n<ZodBoolean\n>> }, \"strip\"\n, ZodTypeAny\n, { description?\n: string\n; name\n: string\n; parameters?\n: Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}, { description?\n: string\n; name\n: string\n; parameters?\n: Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}> ; type\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { function\n: { description?\n: string\n; name\n: string\n; parameters?\n: Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"function\"\n}, { function\n: { description?\n: string\n; name\n: string\n; parameters?\n: Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"function\"\n}>, \"many\"\n>> }, \"strip\"\n, ZodTypeAny\n, { tools?\n: { function\n: { description?\n: string\n; name\n: string\n; parameters?\n: Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"function\"\n}[] }, { tools?\n: { function\n: { description?\n: string\n; name\n: string\n; parameters?\n: Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"function\"\n}[] }>>\ntoolFunctionDefinitionSchema\n\u2022 Const\ntoolFunctionDefinitionSchema: z.ZodObject\n<{ function\n: z.ZodObject\n<{ description\n: z.ZodOptional\n<z.ZodString\n> ; name\n: z.ZodString\n; parameters\n: z.ZodOptional\n<z.ZodRecord\n<z.ZodString\n, z.ZodUnknown\n>> ; strict\n: z.ZodOptional\n<z.ZodNullable\n<z.ZodBoolean\n>> }, \"strip\"\n, z.ZodTypeAny\n, { description?\n: string\n; name\n: string\n; parameters?\n: Record\n<string\n, unknown\n> ; strict?\n: boolean\n| null\n}, { description?\n: string\n; name\n: string\n; parameters?\n: Record\n<string\n, unknown\n> ; strict?\n: boolean\n| null\n}> ; type\n: z.ZodLiteral\n<\"function\"\n> }, \"strip\"\n, z.ZodTypeAny\n, { function\n: { description?\n: string\n; name\n: string\n; parameters?\n: Record\n<string\n, unknown\n> ; strict?\n: boolean\n| null\n} ; type\n: \"function\"\n}, { function\n: { description?\n: string\n; name\n: string\n; parameters?\n: Record\n<string\n, unknown\n> ; strict?\n: boolean\n| null\n} ; type\n: \"function\"\n}>", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "TypeScript"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "Python"}, {"href": "https://www.braintrust.dev/docs/reference/api", "anchor": "API"}, {"href": "https://www.braintrust.dev/docs/reference/autoevals", "anchor": "Autoevals library"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "BTQL query syntax"}, {"href": "https://www.braintrust.dev/docs/reference/streaming", "anchor": "Streaming"}, {"href": "https://www.braintrust.dev/docs/reference/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/reference/reasoning", "anchor": "Reasoning"}, {"href": "https://www.braintrust.dev/docs/reference/organizations", "anchor": "Organizations"}, {"href": "https://www.braintrust.dev/docs/reference/mcp", "anchor": "Model Context Protocol (MCP)"}, {"href": "https://www.braintrust.dev/docs/reference/object-links", "anchor": "Sharing via URL"}, {"href": "https://www.braintrust.dev/docs/reference/glossary", "anchor": "Glossary"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "braintrust"}, {"href": "https://braintrust.dev/", "anchor": "Braintrust"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Quickstart"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Classes"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/Attachment.md", "anchor": "Attachment"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/BaseAttachment.md", "anchor": "BaseAttachment"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/BraintrustState.md", "anchor": "BraintrustState"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/BraintrustStream.md", "anchor": "BraintrustStream"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/CodeFunction.md", "anchor": "CodeFunction"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/CodePrompt.md", "anchor": "CodePrompt"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/Dataset.md", "anchor": "Dataset"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/EvalResultWithSummary.md", "anchor": "EvalResultWithSummary"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/Experiment.md", "anchor": "Experiment"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/ExternalAttachment.md", "anchor": "ExternalAttachment"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/FailedHTTPResponse.md", "anchor": "FailedHTTPResponse"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/LazyValue.md", "anchor": "LazyValue"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/Logger.md", "anchor": "Logger"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/NoopSpan.md", "anchor": "NoopSpan"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/Project.md", "anchor": "Project"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/Prompt.md", "anchor": "Prompt"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/PromptBuilder.md", "anchor": "PromptBuilder"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/ReadonlyAttachment.md", "anchor": "ReadonlyAttachment"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/ReadonlyExperiment.md", "anchor": "ReadonlyExperiment"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/ScorerBuilder.md", "anchor": "ScorerBuilder"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/SpanImpl.md", "anchor": "SpanImpl"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/TestBackgroundLogger.md", "anchor": "TestBackgroundLogger"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/ToolBuilder.md", "anchor": "ToolBuilder"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Interfaces"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/AttachmentParams.md", "anchor": "AttachmentParams"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/BackgroundLoggerOpts.md", "anchor": "BackgroundLoggerOpts"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/DataSummary.md", "anchor": "DataSummary"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/DatasetSummary.md", "anchor": "DatasetSummary"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/EvalHooks.md", "anchor": "EvalHooks"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/Evaluator.md", "anchor": "Evaluator"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/ExperimentSummary.md", "anchor": "ExperimentSummary"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/Exportable.md", "anchor": "Exportable"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/ExternalAttachmentParams.md", "anchor": "ExternalAttachmentParams"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/InvokeFunctionArgs.md", "anchor": "InvokeFunctionArgs"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/LogOptions.md", "anchor": "LogOptions"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/LoginOptions.md", "anchor": "LoginOptions"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/MetricSummary.md", "anchor": "MetricSummary"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/ObjectMetadata.md", "anchor": "ObjectMetadata"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/ParentExperimentIds.md", "anchor": "ParentExperimentIds"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/ParentProjectLogIds.md", "anchor": "ParentProjectLogIds"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/ReporterBody.md", "anchor": "ReporterBody"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/ScoreSummary.md", "anchor": "ScoreSummary"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/Span.md", "anchor": "Span"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Namespaces"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/modules/default.md", "anchor": "default"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/modules/graph.md", "anchor": "graph"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "BaseExperiment"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "BaseExperiment"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "BaseMetadata"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "BaseExperiment"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Eval"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/EvalResultWithSummary.md", "anchor": "EvalResultWithSummary"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "BaseMetadata"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/Evaluator.md", "anchor": "Evaluator"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/EvalResultWithSummary.md", "anchor": "EvalResultWithSummary"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Reporter"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/ReporterBody.md", "anchor": "ReporterBody"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "buildLocalSummary"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/ExperimentSummary.md", "anchor": "ExperimentSummary"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "EvaluatorDef"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "EvalResult"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/ExperimentSummary.md", "anchor": "ExperimentSummary"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "createFinalValuePassThroughStream"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "BraintrustStreamChunk"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "BraintrustStreamChunk"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "currentExperiment"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/Experiment.md", "anchor": "Experiment"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "init"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/Experiment.md", "anchor": "Experiment"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "currentLogger"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/Logger.md", "anchor": "Logger"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "initLogger"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/Logger.md", "anchor": "Logger"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "currentSpan"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/Span.md", "anchor": "Span"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/Span.md", "anchor": "Span"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/Span.md", "anchor": "Span"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "defaultErrorScoreHandler"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "EvalCase"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/Span.md", "anchor": "Span"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "deserializePlainStringAsJSON"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "devNullWritableStream"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "flush"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "getSpanParentObject"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/Span.md", "anchor": "Span"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/Experiment.md", "anchor": "Experiment"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/Logger.md", "anchor": "Logger"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/Span.md", "anchor": "Span"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/Experiment.md", "anchor": "Experiment"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/Logger.md", "anchor": "Logger"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "init"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "FullInitOptions"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "InitOptions"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "initDataset"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/Dataset.md", "anchor": "Dataset"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/Dataset.md", "anchor": "Dataset"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/Dataset.md", "anchor": "Dataset"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/Dataset.md", "anchor": "Dataset"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "initExperiment"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "InitOptions"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "InitOptions"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "initFunction"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "initLogger"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/Logger.md", "anchor": "Logger"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/Logger.md", "anchor": "Logger"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "invoke"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "InvokeReturn"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/InvokeFunctionArgs.md", "anchor": "InvokeFunctionArgs"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/LoginOptions.md", "anchor": "LoginOptions"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/InvokeFunctionArgs.md", "anchor": "InvokeFunctionArgs"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "InvokeReturn"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "loadPrompt"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/Prompt.md", "anchor": "Prompt"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/Prompt.md", "anchor": "Prompt"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "log"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "ExperimentLogFullArgs"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/Experiment.md", "anchor": "Experiment.log"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "logError"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/Span.md", "anchor": "Span"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "login"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/BraintrustState.md", "anchor": "BraintrustState"}, {"href": "https://www.braintrust.dev/app/token", "anchor": "https://www.braintrust.dev/app/token"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/LoginOptions.md", "anchor": "LoginOptions"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/BraintrustState.md", "anchor": "BraintrustState"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "loginToState"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/BraintrustState.md", "anchor": "BraintrustState"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/LoginOptions.md", "anchor": "LoginOptions"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/BraintrustState.md", "anchor": "BraintrustState"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "newId"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "parseCachedHeader"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "permalink"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/Span.md", "anchor": "Span.link"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/Span.md", "anchor": "Span.export"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/BraintrustState.md", "anchor": "BraintrustState"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "promptDefinitionToPromptData"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "renderMessage"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "renderPromptParams"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "reportFailures"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "BaseMetadata"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "EvaluatorDef"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "EvalResult"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "runEvaluator"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/EvalResultWithSummary.md", "anchor": "EvalResultWithSummary"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/Experiment.md", "anchor": "Experiment"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "EvaluatorDef"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/EvalResultWithSummary.md", "anchor": "EvalResultWithSummary"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "setFetch"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "spanComponentsToObjectId"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/BraintrustState.md", "anchor": "BraintrustState"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "startSpan"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/Span.md", "anchor": "Span"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "traced"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "StartSpanArgs"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/Span.md", "anchor": "Span"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "summarize"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/ExperimentSummary.md", "anchor": "ExperimentSummary"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/ExperimentSummary.md", "anchor": "ExperimentSummary"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "traceable"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "StartSpanArgs"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "SetCurrentArg"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "traced"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "PromiseUnless"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/Span.md", "anchor": "Span.traced"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/Span.md", "anchor": "Span"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "StartSpanArgs"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "SetCurrentArg"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "PromiseUnless"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "updateSpan"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "withCurrent"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/Span.md", "anchor": "Span"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/Span.md", "anchor": "Span"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/BraintrustState.md", "anchor": "BraintrustState"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "withDataset"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/Dataset.md", "anchor": "Dataset"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "initDataset"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "withExperiment"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/Experiment.md", "anchor": "Experiment"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/LoginOptions.md", "anchor": "LoginOptions"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "AnyDataset"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/BraintrustState.md", "anchor": "BraintrustState"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "SetCurrentArg"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "init"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "withLogger"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/Logger.md", "anchor": "Logger"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/LoginOptions.md", "anchor": "LoginOptions"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/BraintrustState.md", "anchor": "BraintrustState"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "SetCurrentArg"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "initLogger"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "withParent"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/BraintrustState.md", "anchor": "BraintrustState"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "wrapAISDKModel"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "wrapAnthropic"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "wrapOpenAI"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "wrapOpenAIv4"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "wrapTraced"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "StartSpanArgs"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "SetCurrentArg"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Type Aliases"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "AnyDataset"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/Dataset.md", "anchor": "Dataset"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "BaseExperiment"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "BaseMetadata"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "DefaultMetadataType"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Type declaration"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "BaseMetadata"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "BraintrustStreamChunk"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "braintrustStreamChunkSchema"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "ChatPrompt"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Type declaration"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "CodeOpts"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "CommentEvent"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "IdField"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/ParentExperimentIds.md", "anchor": "ParentExperimentIds"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/ParentProjectLogIds.md", "anchor": "ParentProjectLogIds"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "CompiledPrompt"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "CompiledPromptParams"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "ChatPrompt"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "CompletionPrompt"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "CompiledPromptParams"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "CompletionPrompt"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Type declaration"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "CreateProjectOpts"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "DatasetRecord"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "DefaultMetadataType"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "DefaultPromptArgs"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "CompiledPromptParams"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "ChatPrompt"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "CompletionPrompt"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "EndSpanArgs"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Type declaration"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "EvalCase"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "EvalParameterSerializedSchema"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "EvalParameters"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "EvalResult"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "EvalCase"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "BaseMetadata"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "DefaultMetadataType"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "EvalScorer"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "EvalScorerArgs"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "BaseMetadata"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "DefaultMetadataType"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Type declaration"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "EvalScorerArgs"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "EvalScorerArgs"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "EvalCase"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "BaseMetadata"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "DefaultMetadataType"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "EvalTask"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/EvalHooks.md", "anchor": "EvalHooks"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/EvalHooks.md", "anchor": "EvalHooks"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "BaseMetadata"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "EvalParameters"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "EvaluatorDef"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/Evaluator.md", "anchor": "Evaluator"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "BaseMetadata"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "DefaultMetadataType"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "EvalParameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "EvalParameters"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "EvaluatorDefinition"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "evaluatorDefinitionSchema"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "EvaluatorDefinitions"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "evaluatorDefinitionsSchema"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "EvaluatorFile"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Type declaration"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "EvaluatorDef"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "BaseMetadata"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "EvalParameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/CodeFunction.md", "anchor": "CodeFunction"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/CodePrompt.md", "anchor": "CodePrompt"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "EvaluatorManifest"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "EvaluatorDef"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "BaseMetadata"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "ExperimentLogFullArgs"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "OtherExperimentLogFields"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "OtherExperimentLogFields"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "InputField"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "IdField"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "ExperimentLogPartialArgs"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "OtherExperimentLogFields"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "InputField"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "FullInitOptions"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "InitOptions"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "FullLoginOptions"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/LoginOptions.md", "anchor": "LoginOptions"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "IdField"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Type declaration"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "InitOptions"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "FullLoginOptions"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "AnyDataset"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/BraintrustState.md", "anchor": "BraintrustState"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "InputField"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Type declaration"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "InvokeReturn"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/BraintrustStream.md", "anchor": "BraintrustStream"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "LogCommentFullArgs"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "IdField"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/ParentExperimentIds.md", "anchor": "ParentExperimentIds"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/ParentProjectLogIds.md", "anchor": "ParentProjectLogIds"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "LogFeedbackFullArgs"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "IdField"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "OtherExperimentLogFields"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "OtherExperimentLogFields"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Type declaration"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "PromiseUnless"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "PromptContents"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "promptContentsSchema"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "PromptDefinition"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "promptDefinitionSchema"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "PromptDefinitionWithTools"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "promptDefinitionWithToolsSchema"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "PromptOpts"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "PromptDefinition"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "PromptRowWithId"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "ScorerOpts"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "CodeOpts"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "SerializedBraintrustState"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "SetCurrentArg"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Type declaration"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "SpanContext"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Type declaration"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "NOOP_SPAN"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "currentSpan"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "startSpan"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "withCurrent"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "StartSpanArgs"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Type declaration"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "ToolFunctionDefinition"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "toolFunctionDefinitionSchema"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "WithTransactionId"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Variables"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "ERR_PERMALINK"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "INTERNAL_BTQL_LIMIT"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "LEGACY_CACHED_HEADER"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "NOOP_SPAN"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/NoopSpan.md", "anchor": "NoopSpan"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "NOOP_SPAN_PERMALINK"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "X_CACHED_HEADER"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "_exportsForTestingOnly"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "Type declaration"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/BaseAttachment.md", "anchor": "BaseAttachment"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/BraintrustState.md", "anchor": "BraintrustState"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/BraintrustState.md", "anchor": "BraintrustState"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/TestBackgroundLogger.md", "anchor": "TestBackgroundLogger"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "braintrustStreamChunkSchema"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "evaluatorDefinitionSchema"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "evaluatorDefinitionsSchema"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "projects"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "promptContentsSchema"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "promptDefinitionSchema"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "promptDefinitionWithToolsSchema"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "toolFunctionDefinitionSchema"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}], "depth": 3}, "https://www.braintrust.dev/docs/reference/libs/python": {"url": "https://www.braintrust.dev/docs/reference/libs/python", "title": "Python - Docs - Reference - Braintrust", "text": "braintrust\nA Python library for interacting with Braintrust. This library contains functionality for running evaluations, logging completions, loading and invoking functions, and more.\nbraintrust\nis distributed as a library on PyPI. It is open source and\navailable on GitHub.\nQuickstart\nInstall the library with pip.\nThen, create a file like eval_hello.py\nwith the following content:\nFinally, run the script with braintrust eval eval_hello.py\n.\nAPI Reference\nbraintrust.logger\nExportable Objects\nexport\nReturn a serialized representation of the object that can be used to start subspans in other places. See Span.start_span\nfor more details.\nSpan Objects\nA Span encapsulates logged data and metrics for a unit of work. This interface is shared by all span implementations.\nWe suggest using one of the various start_span\nmethods, instead of creating Spans directly. See Span.start_span\nfor full details.\nid\nRow ID of the span.\nlog\nIncrementally update the current span with new data. The event will be batched and uploaded behind the scenes.\nArguments:\n**event\n: Data to be logged. SeeExperiment.log\nfor full details.\nlog_feedback\nAdd feedback to the current span. Unlike Experiment.log_feedback\nand Logger.log_feedback\n, this method does not accept an id parameter, because it logs feedback to the current span.\nArguments:\n**event\n: Data to be logged. SeeExperiment.log_feedback\nfor full details.\nstart_span\nCreate a new span. This is useful if you want to log more detailed trace information beyond the scope of a single log event. Data logged over several calls to Span.log\nwill be merged into one logical row.\nWe recommend running spans within context managers (with start_span(...) as span\n) to automatically mark them as current and ensure they are ended. Only spans run within a context manager will be marked current, so they can be accessed using braintrust.current_span()\n. If you wish to start a span outside a context manager, be sure to end it with span.end()\n.\nArguments:\nname\n: Optional name of the span. If not provided, a name will be inferred from the call stack.type\n: Optional type of the span. Use theSpanTypeAttribute\nenum or just provide a string directly. If not provided, the type will be unset.span_attributes\n: Optional additional attributes to attach to the span, such as a type name.start_time\n: Optional start time of the span, as a timestamp in seconds.set_current\n: If true (the default), the span will be marked as the currently-active span for the duration of the context manager.parent\n: Optional parent info string for the span. The string can be generated from[Span,Experiment,Logger].export\n. If not provided, the current span will be used (depending on context). This is useful for adding spans to an existing trace.**event\n: Data to be logged. SeeExperiment.log\nfor full details.\nReturns:\nThe newly-created Span\nexport\nSerialize the identifiers of this span. The return value can be used to identify this span when starting a subspan elsewhere, such as another process or service, without needing to access this Span\nobject. See the parameters of Span.start_span\nfor usage details.\nCallers should treat the return value as opaque. The serialization format may change from time to time. If parsing is needed, use SpanComponentsV3.from_str\n.\nReturns:\nSerialized representation of this span's identifiers.\nlink\nFormat a link to the Braintrust application for viewing this span.\nLinks can be generated at any time, but they will only become viewable after the span and its root have been flushed to the server and ingested.\nThere are some conditions when a Span doesn't have enough information to return a stable link (e.g. during an unresolved experiment). In this case or if there's an error generating link, we'll return a placeholder link.\nReturns:\nA link to the span.\npermalink\nFormat a permalink to the Braintrust application for viewing this span.\nLinks can be generated at any time, but they will only become viewable after the span and its root have been flushed to the server and ingested.\nThis function can block resolving data with the server. For production\napplications it's preferable to call Span.link\ninstead.\nReturns:\nA permalink to the span.\nend\nLog an end time to the span (defaults to the current time). Returns the logged time.\nWill be invoked automatically if the span is bound to a context manager.\nArguments:\nend_time\n: Optional end time of the span, as a timestamp in seconds.\nReturns:\nThe end time logged to the span metrics.\nflush\nFlush any pending rows to the server.\nclose\nAlias for end\n.\nset_attributes\nSet the span's name, type, or other attributes. These attributes will be attached to all log events within the span.\nThe attributes are equivalent to the arguments to start_span.\nArguments:\nname\n: Optional name of the span. If not provided, a name will be inferred from the call stack.type\n: Optional type of the span. Use theSpanTypeAttribute\nenum or just provide a string directly. If not provided, the type will be unset.span_attributes\n: Optional additional attributes to attach to the span, such as a type name.\nset_current\nSet the span as the current span. This is used to mark the span as the active span for the current thread.\nset_http_adapter\nSpecify a custom HTTP adapter to use for all network requests. This is useful for setting custom retry policies, timeouts, etc.\nBraintrust uses the requests\nlibrary, so the adapter should be an instance of requests.adapters.HTTPAdapter\n. Alternatively, consider\nsub-classing our RetryRequestExceptionsAdapter\nto get automatic retries on network-related exceptions.\nArguments:\nadapter\n: The adapter to use.\nRetryRequestExceptionsAdapter Objects\nAn HTTP adapter that automatically retries requests on connection exceptions.\nThis adapter extends requests' HTTPAdapter to add retry logic for common network-related exceptions including connection errors, timeouts, and other HTTP errors. It implements an exponential backoff strategy between retries to avoid overwhelming servers during intermittent connectivity issues.\nAttributes:\nbase_num_retries\n- Maximum number of retries before giving up and re-raising the exception.backoff_factor\n- A multiplier used to determine the time to wait between retries. The actual wait time is calculated as: backoff_factor * (2 ** retry_count).\ninit\nLog in, and then initialize a new experiment in a specified project. If the project does not exist, it will be created.\nArguments:\nproject\n: The name of the project to create the experiment in. Must specify at least one ofproject\norproject_id\n.experiment\n: The name of the experiment to create. If not specified, a name will be generated automatically.description\n: (Optional) An optional description of the experiment.dataset\n: (Optional) A dataset to associate with the experiment. The dataset must be initialized withbraintrust.init_dataset\nbefore passing it into the experiment.update\n: If the experiment already exists, continue logging to it. If it does not exist, creates the experiment with the specified arguments.base_experiment\n: An optional experiment name to use as a base. If specified, the new experiment will be summarized and compared to this experiment. Otherwise, it will pick an experiment by finding the closest ancestor on the default (e.g. main) branch.is_public\n: An optional parameter to control whether the experiment is publicly visible to anybody with the link or privately visible to only members of the organization. Defaults to private.app_url\n: The URL of the Braintrust App. Defaults to https://www.braintrust.dev.api_key\n: The API key to use. If the parameter is not specified, will try to use theBRAINTRUST_API_KEY\nenvironment variable. If no API key is specified, will prompt the user to login.org_name\n: (Optional) The name of a specific organization to connect to. This is useful if you belong to multiple.metadata\n: (Optional) a dictionary with additional data about the test example, model outputs, or just about anything else that's relevant, that you can use to help find and analyze examples later. For example, you could log theprompt\n, example'sid\n, or anything else that would be useful to slice/dice later. The values inmetadata\ncan be any JSON-serializable type, but its keys must be strings.git_metadata_settings\n: (Optional) Settings for collecting git metadata. By default, will collect all git metadata fields allowed in org-level settings.set_current\n: If true (the default), set the global current-experiment to the newly-created one.open\n: If the experiment already exists, open it in read-only mode. Throws an error if the experiment does not already exist.project_id\n: The id of the project to create the experiment in. This takes precedence overproject\nif specified.base_experiment_id\n: An optional experiment id to use as a base. If specified, the new experiment will be summarized and compared to this. This takes precedence overbase_experiment\nif specified.repo_info\n: (Optional) Explicitly specify the git metadata for this experiment. This takes precedence overgit_metadata_settings\nif specified.\nReturns:\nThe experiment object.\ninit_experiment\nAlias for init\ninit_dataset\nCreate a new dataset in a specified project. If the project does not exist, it will be created.\nArguments:\nproject_name\n: The name of the project to create the dataset in. Must specify at least one ofproject_name\norproject_id\n.name\n: The name of the dataset to create. If not specified, a name will be generated automatically.description\n: An optional description of the dataset.version\n: An optional version of the dataset (to read). If not specified, the latest version will be used.app_url\n: The URL of the Braintrust App. Defaults to https://www.braintrust.dev.api_key\n: The API key to use. If the parameter is not specified, will try to use theBRAINTRUST_API_KEY\nenvironment variable. If no API key is specified, will prompt the user to login.org_name\n: (Optional) The name of a specific organization to connect to. This is useful if you belong to multiple.project_id\n: The id of the project to create the dataset in. This takes precedence overproject\nif specified.metadata\n: (Optional) a dictionary with additional data about the dataset. The values inmetadata\ncan be any JSON-serializable type, but its keys must be strings.use_output\n: (Deprecated) If True, records will be fetched from this dataset in the legacy format, with the \"expected\" field renamed to \"output\". This option will be removed in a future version of Braintrust.\nReturns:\nThe dataset object.\ninit_logger\nCreate a new logger in a specified project. If the project does not exist, it will be created.\nArguments:\nproject\n: The name of the project to log into. If unspecified, will default to the Global project.project_id\n: The id of the project to log into. This takes precedence over project if specified.async_flush\n: If true (the default), log events will be batched and sent asynchronously in a background thread. If false, log events will be sent synchronously. Set to false in serverless environments.app_url\n: The URL of the Braintrust API. Defaults to https://www.braintrust.dev.api_key\n: The API key to use. If the parameter is not specified, will try to use theBRAINTRUST_API_KEY\nenvironment variable. If no API key is specified, will prompt the user to login.org_name\n: (Optional) The name of a specific organization to connect to. This is useful if you belong to multiple.force_login\n: Login again, even if you have already logged in (by default, the logger will not login if you are already logged in)set_current\n: If true (the default), set the global current-experiment to the newly-created one.\nReturns:\nThe newly created Logger.\nload_prompt\nLoads a prompt from the specified project.\nArguments:\nproject\n: The name of the project to load the prompt from. Must specify at least one ofproject\norproject_id\n.slug\n: The slug of the prompt to load.version\n: An optional version of the prompt (to read). If not specified, the latest version will be used.project_id\n: The id of the project to load the prompt from. This takes precedence overproject\nif specified.id\n: The id of a specific prompt to load. If specified, this takes precedence over all other parameters (project, slug, version).defaults\n: (Optional) A dictionary of default values to use when rendering the prompt. Prompt values will override these defaults.no_trace\n: If true, do not include logging metadata for this prompt when build() is called.environment\n: The environment to load the prompt from. Cannot be used together with version.app_url\n: The URL of the Braintrust App. Defaults to https://www.braintrust.dev.api_key\n: The API key to use. If the parameter is not specified, will try to use theBRAINTRUST_API_KEY\nenvironment variable. If no API key is specified, will prompt the user to login.org_name\n: (Optional) The name of a specific organization to connect to. This is useful if you belong to multiple.\nReturns:\nThe prompt object.\nlogin\nLog into Braintrust. This will prompt you for your API token, which you can find at\nhttps://www.braintrust.dev/app/token. This method is called automatically by init()\n.\nArguments:\napp_url\n: The URL of the Braintrust App. Defaults to https://www.braintrust.dev.api_key\n: The API key to use. If the parameter is not specified, will try to use theBRAINTRUST_API_KEY\nenvironment variable. If no API key is specified, will prompt the user to login.org_name\n: (Optional) The name of a specific organization to connect to. This is useful if you belong to multiple.force_login\n: Login again, even if you have already logged in (by default, this function will exit quickly if you have already logged in)\nlog\nLog a single event to the current experiment. The event will be batched and uploaded behind the scenes.\nArguments:\n**event\n: Data to be logged. SeeExperiment.log\nfor full details.\nReturns:\nThe id\nof the logged event.\nsummarize\nSummarize the current experiment, including the scores (compared to the closest reference experiment) and metadata.\nArguments:\nsummarize_scores\n: Whether to summarize the scores. If False, only the metadata will be returned.comparison_experiment_id\n: The experiment to compare against. If None, the most recent experiment on the comparison_commit will be used.\nReturns:\nExperimentSummary\ncurrent_experiment\nReturns the currently-active experiment (set by braintrust.init(...)\n). Returns None if no current experiment has been set.\ncurrent_logger\nReturns the currently-active logger (set by braintrust.init_logger(...)\n). Returns None if no current logger has been set.\ncurrent_span\nReturn the currently-active span for logging (set by running a span under a context manager). If there is no active span, returns a no-op span object, which supports the same interface as spans but does no logging.\nSee Span\nfor full details.\nget_span_parent_object\nMainly for internal use. Return the parent object for starting a span in a global context.\ntraced\nDecorator to trace the wrapped function when used without parentheses.\ntraced\nDecorator to trace the wrapped function when used with arguments.\ntraced\nDecorator to trace the wrapped function. Can either be applied bare (@traced\n) or by providing arguments (@traced(*span_args, **span_kwargs)\n), which will be forwarded to the created span. See Span.start_span\nfor full details on the span arguments.\nIt checks the following (in precedence order):\nand creates a span in the first one that is active. If none of these are active, it returns a no-op span object.\nThe decorator will automatically log the input and output of the wrapped function to the corresponding fields of the created span. Pass the kwarg notrace_io=True\nto the decorator to prevent this.\nUnless a name is explicitly provided in span_args\nor span_kwargs\n, the name of the span will be the name of the decorated function.\nstart_span\nLower-level alternative to @traced\nfor starting a span at the toplevel. It creates a span under the first active object (using the same precedence order as @traced\n), or if parent\nis specified, under the specified parent row, or returns a no-op span object.\nWe recommend running spans bound to a context manager (with start_span\n) to automatically mark them as current and ensure they are terminated. If you wish to start a span outside a context manager, be sure to terminate it with span.end()\n.\nSee Span.start_span\nfor full details.\nflush\nFlush any pending rows to the server.\nObjectFetcher Objects\nfetch\nFetch all records.\nReturns:\nAn iterator over the records.\nAttachment Objects\nRepresents an attachment to be uploaded and the associated metadata.\nAttachment\nobjects can be inserted anywhere in an event, allowing you to\nlog arbitrary file data. The SDK will asynchronously upload the file to\nobject storage and replace the Attachment\nobject with an\nAttachmentReference\n.\n__init__\nConstruct an attachment.\nArguments:\ndata\n: A string representing the path of the file on disk, or abytes\n/bytearray\nwith the file's contents. The caller is responsible for ensuring the file on disk or mutablebytearray\nis not modified until upload is complete.filename\n: The desired name of the file in Braintrust after uploading. This parameter is for visualization purposes only and has no effect on attachment storage.content_type\n: The MIME type of the file.\nreference\nThe object that replaces this Attachment\nat upload time.\ndata\nThe attachment contents. This is a lazy value that will read the attachment contents from disk or memory on first access.\nupload\nOn first access, (1) reads the attachment from disk if needed, (2) authenticates with the data plane to request a signed URL, (3) uploads to object store, and (4) updates the attachment.\nReturns:\nThe attachment status.\ndebug_info\nA human-readable description for logging and debugging.\nReturns:\nThe debug object. The return type is not stable and may change in a future release.\nExternalAttachment Objects\nRepresents an attachment that resides in an external object store and the associated metadata.\nExternalAttachment\nobjects can be inserted anywhere in an event, similar to\nAttachment\nobjects, but they reference files that already exist in an external\nobject store rather than requiring upload. The SDK will replace the ExternalAttachment\nobject with an AttachmentReference\nduring logging.\n__init__\nConstruct an external attachment reference.\nArguments:\nurl\n: A fully qualified URL to the object in the external object store.filename\n: The desired name of the file in Braintrust. This parameter is for visualization purposes only and has no effect on attachment storage.content_type\n: The MIME type of the file.\nreference\nThe object that replaces this Attachment\nat upload time.\ndata\nThe attachment contents. This is a lazy value that will read the attachment contents from the external object store on first access.\nupload\nFor ExternalAttachment, this is a no-op since the data already resides\nin the external object store. It marks the attachment as already uploaded.\nReturns:\nThe attachment status, which will always indicate success.\ndebug_info\nA human-readable description for logging and debugging.\nReturns:\nThe debug object. The return type is not stable and may change in a future release.\nReadonlyAttachment Objects\nA readonly alternative to Attachment\n, which can be used for fetching\nalready-uploaded Attachments.\ndata\nThe attachment contents. This is a lazy value that will read the attachment contents from the object store on first access.\nmetadata\nFetch the attachment metadata, which includes a downloadUrl and a status. This will re-fetch the status each time in case it changes over time.\nstatus\nFetch the attachment upload status. This will re-fetch the status each time in case it changes over time.\nupdate_span\nUpdate a span using the output of span.export()\n. It is important that you only resume updating\nto a span once the original span has been fully written and flushed, since otherwise updates to the span may conflict with the original span.\nArguments:\nexported\n: The output ofspan.export()\n.**event\n: Data to update. SeeExperiment.log\nfor a full list of valid fields.\nspan_components_to_object_id\nUtility function to resolve the object ID of a SpanComponentsV3 object. This function may trigger a login to braintrust if the object ID is encoded lazily.\npermalink\nFormat a permalink to the Braintrust application for viewing the span represented by the provided slug\n.\nLinks can be generated at any time, but they will only become viewable after the span and its root have been flushed to the server and ingested.\nIf you have a Span\nobject, use Span.permalink\ninstead.\nArguments:\nslug\n: The identifier generated fromSpan.export\n.org_name\n: The org name to use. If not provided, the org name will be inferred from the global login state.app_url\n: The app URL to use. If not provided, the app URL will be inferred from the global login state.\nReturns:\nA permalink to the exported span.\nExperiment Objects\nAn experiment is a collection of logged events, such as model inputs and outputs, which represent a snapshot of your application at a particular point in time. An experiment is meant to capture more than just the model you use, and includes the data you use to test, pre- and post- processing code, comparison metrics (scores), and any other metadata you want to include.\nExperiments are associated with a project, and two experiments are meant to be easily comparable via\ntheir input\n. You can change the attributes of the experiments in a project (e.g. scoring functions)\nover time, simply by changing what you log.\nYou should not create Experiment\nobjects directly. Instead, use the braintrust.init()\nmethod.\nlog\nLog a single event to the experiment. The event will be batched and uploaded behind the scenes.\nArguments:\ninput\n: The arguments that uniquely define a test case (an arbitrary, JSON serializable object). Later on, Braintrust will use theinput\nto know whether two test cases are the same between experiments, so they should not contain experiment-specific state. A simple rule of thumb is that if you run the same experiment twice, theinput\nshould be identical.output\n: The output of your application, including post-processing (an arbitrary, JSON serializable object), that allows you to determine whether the result is correct or not. For example, in an app that generates SQL queries, theoutput\nshould be the result of the SQL query generated by the model, not the query itself, because there may be multiple valid queries that answer a single question.expected\n: (Optional) the ground truth value (an arbitrary, JSON serializable object) that you'd compare tooutput\nto determine if youroutput\nvalue is correct or not. Braintrust currently does not compareoutput\ntoexpected\nfor you, since there are so many different ways to do that correctly. Instead, these values are just used to help you navigate your experiments while digging into analyses. However, we may later use these values to re-score outputs or fine-tune your models.error\n: (Optional) The error that occurred, if any. If you use tracing to run an experiment, errors are automatically logged when your code throws an exception.scores\n: A dictionary of numeric values (between 0 and 1) to log. The scores should give you a variety of signals that help you determine how accurate the outputs are compared to what you expect and diagnose failures. For example, a summarization app might have one score that tells you how accurate the summary is, and another that measures the word similarity between the generated and grouth truth summary. The word similarity score could help you determine whether the summarization was covering similar concepts or not. You can use these scores to help you sort, filter, and compare experiments.metadata\n: (Optional) a dictionary with additional data about the test example, model outputs, or just about anything else that's relevant, that you can use to help find and analyze examples later. For example, you could log theprompt\n, example'sid\n, or anything else that would be useful to slice/dice later. The values inmetadata\ncan be any JSON-serializable type, but its keys must be strings.tags\n: (Optional) a list of strings that you can use to filter and group records later.metrics\n: (Optional) a dictionary of metrics to log. The following keys are populated automatically: \"start\", \"end\".id\n: (Optional) a unique identifier for the event. If you don't provide one, BrainTrust will generate one for you.allow_concurrent_with_spans\n: (Optional) in rare cases where you need to log at the top level separately from using spans on the experiment elsewhere, set this to True.dataset_record_id\n: (Deprecated) the id of the dataset record that this event is associated with. This field is required if and only if the experiment is associated with a dataset. This field is unused and will be removed in a future version.\nReturns:\nThe id\nof the logged event.\nlog_feedback\nLog feedback to an event in the experiment. Feedback is used to save feedback scores, set an expected value, or add a comment.\nArguments:\nid\n: The id of the event to log feedback for. This is theid\nreturned bylog\nor accessible as theid\nfield of a span.scores\n: (Optional) a dictionary of numeric values (between 0 and 1) to log. These scores will be merged into the existing scores for the event.expected\n: (Optional) the ground truth value (an arbitrary, JSON serializable object) that you'd compare tooutput\nto determine if youroutput\nvalue is correct or not.tags\n: (Optional) a list of strings that you can use to filter and group records later.comment\n: (Optional) an optional comment string to log about the event.metadata\n: (Optional) a dictionary with additional data about the feedback. If you have auser_id\n, you can log it here and access it in the Braintrust UI. Note, this metadata does not correspond to the main event itself, but rather the audit log attached to the event.source\n: (Optional) the source of the feedback. Must be one of \"external\" (default), \"app\", or \"api\".\nstart_span\nCreate a new toplevel span underneath the experiment. The name defaults to \"root\" and the span type to \"eval\".\nSee Span.start_span\nfor full details\nupdate_span\nUpdate a span in the experiment using its id. It is important that you only update a span once the original span has been fully written and flushed,\nsince otherwise updates to the span may conflict with the original span.\nArguments:\nid\n: The id of the span to update.**event\n: Data to update. SeeExperiment.log\nfor a full list of valid fields.\nsummarize\nSummarize the experiment, including the scores (compared to the closest reference experiment) and metadata.\nArguments:\nsummarize_scores\n: Whether to summarize the scores. If False, only the metadata will be returned.comparison_experiment_id\n: The experiment to compare against. If None, the most recent experiment on the origin's main branch will be used.\nReturns:\nExperimentSummary\nclose\nThis function is deprecated. You can simply remove it from your code.\nflush\nFlush any pending rows to the server.\nReadonlyExperiment Objects\nA read-only view of an experiment, initialized by passing open=True\nto init()\n.\nSpanImpl Objects\nPrimary implementation of the Span\ninterface. See the Span\ninterface for full details on each method.\nWe suggest using one of the various start_span\nmethods, instead of creating Spans directly. See Span.start_span\nfor full details.\nflush\nFlush any pending rows to the server.\nDataset Objects\nA dataset is a collection of records, such as model inputs and outputs, which represent data you can use to evaluate and fine-tune models. You can log production data to datasets, curate them with interesting examples, edit/delete records, and run evaluations against them.\nYou should not create Dataset\nobjects directly. Instead, use the braintrust.init_dataset()\nmethod.\ninsert\nInsert a single record to the dataset. The record will be batched and uploaded behind the scenes. If you pass in an id\n,\nand a record with that id\nalready exists, it will be overwritten (upsert).\nArguments:\ninput\n: The argument that uniquely define an input case (an arbitrary, JSON serializable object).expected\n: The output of your application, including post-processing (an arbitrary, JSON serializable object).tags\n: (Optional) a list of strings that you can use to filter and group records later.metadata\n: (Optional) a dictionary with additional data about the test example, model outputs, or just about anything else that's relevant, that you can use to help find and analyze examples later. For example, you could log theprompt\n, example'sid\n, or anything else that would be useful to slice/dice later. The values inmetadata\ncan be any JSON-serializable type, but its keys must be strings.id\n: (Optional) a unique identifier for the event. If you don't provide one, Braintrust will generate one for you.output\n: (Deprecated) The output of your application. Useexpected\ninstead.\nReturns:\nThe id\nof the logged record.\nupdate\nUpdate fields of a single record in the dataset. The updated fields will be batched and uploaded behind the scenes.\nYou must pass in an id\nof the record to update. Only the fields provided will be updated; other fields will remain unchanged.\nArguments:\nid\n: The unique identifier of the record to update.input\n: (Optional) The new input value for the record (an arbitrary, JSON serializable object).expected\n: (Optional) The new expected output value for the record (an arbitrary, JSON serializable object).tags\n: (Optional) A list of strings to update the tags of the record.metadata\n: (Optional) A dictionary to update the metadata of the record. The values inmetadata\ncan be any JSON-serializable type, but its keys must be strings.\nReturns:\nThe id\nof the updated record.\ndelete\nDelete a record from the dataset.\nArguments:\nid\n: Theid\nof the record to delete.\nsummarize\nSummarize the dataset, including high level metrics about its size and other metadata.\nArguments:\nsummarize_data\n: Whether to summarize the data. If False, only the metadata will be returned.\nReturns:\nDatasetSummary\nclose\nThis function is deprecated. You can simply remove it from your code.\nflush\nFlush any pending rows to the server.\nPrompt Objects\nA prompt object consists of prompt text, a model, and model parameters (such as temperature), which\ncan be used to generate completions or chat messages. The prompt object supports calling .build()\nwhich uses mustache templating to build the prompt with the given formatting options and returns a\nplain dictionary that includes the built prompt and arguments. The dictionary can be passed as\nkwargs to the OpenAI client or modified as you see fit.\nYou should not create Prompt\nobjects directly. Instead, use the braintrust.load_prompt()\nmethod.\nbuild\nBuild the prompt with the given formatting options. The args you pass in will\nbe forwarded to the mustache template that defines the prompt and rendered with\nthe chevron\nlibrary.\nReturns:\nA dictionary that includes the rendered prompt and arguments, that can be passed as kwargs to the OpenAI client.\nLogger Objects\nlog\nLog a single event. The event will be batched and uploaded behind the scenes.\nArguments:\ninput\n: (Optional) the arguments that uniquely define a user input (an arbitrary, JSON serializable object).output\n: (Optional) the output of your application, including post-processing (an arbitrary, JSON serializable object), that allows you to determine whether the result is correct or not. For example, in an app that generates SQL queries, theoutput\nshould be the result of the SQL query generated by the model, not the query itself, because there may be multiple valid queries that answer a single question.expected\n: (Optional) the ground truth value (an arbitrary, JSON serializable object) that you'd compare tooutput\nto determine if youroutput\nvalue is correct or not. Braintrust currently does not compareoutput\ntoexpected\nfor you, since there are so many different ways to do that correctly. Instead, these values are just used to help you navigate while digging into analyses. However, we may later use these values to re-score outputs or fine-tune your models.error\n: (Optional) The error that occurred, if any. If you use tracing to run an experiment, errors are automatically logged when your code throws an exception.tags\n: (Optional) a list of strings that you can use to filter and group records later.scores\n: (Optional) a dictionary of numeric values (between 0 and 1) to log. The scores should give you a variety of signals that help you determine how accurate the outputs are compared to what you expect and diagnose failures. For example, a summarization app might have one score that tells you how accurate the summary is, and another that measures the word similarity between the generated and grouth truth summary. The word similarity score could help you determine whether the summarization was covering similar concepts or not. You can use these scores to help you sort, filter, and compare logs.metadata\n: (Optional) a dictionary with additional data about the test example, model outputs, or just about anything else that's relevant, that you can use to help find and analyze examples later. For example, you could log theprompt\n, example'sid\n, or anything else that would be useful to slice/dice later. The values inmetadata\ncan be any JSON-serializable type, but its keys must be strings.metrics\n: (Optional) a dictionary of metrics to log. The following keys are populated automatically: \"start\", \"end\".id\n: (Optional) a unique identifier for the event. If you don't provide one, BrainTrust will generate one for you.allow_concurrent_with_spans\n: (Optional) in rare cases where you need to log at the top level separately from using spans on the logger elsewhere, set this to True.\nlog_feedback\nLog feedback to an event. Feedback is used to save feedback scores, set an expected value, or add a comment.\nArguments:\nid\n: The id of the event to log feedback for. This is theid\nreturned bylog\nor accessible as theid\nfield of a span.scores\n: (Optional) a dictionary of numeric values (between 0 and 1) to log. These scores will be merged into the existing scores for the event.expected\n: (Optional) the ground truth value (an arbitrary, JSON serializable object) that you'd compare tooutput\nto determine if youroutput\nvalue is correct or not.tags\n: (Optional) a list of strings that you can use to filter and group records later.comment\n: (Optional) an optional comment string to log about the event.metadata\n: (Optional) a dictionary with additional data about the feedback. If you have auser_id\n, you can log it here and access it in the Braintrust UI. Note, this metadata does not correspond to the main event itself, but rather the audit log attached to the event.source\n: (Optional) the source of the feedback. Must be one of \"external\" (default), \"app\", or \"api\".\nstart_span\nCreate a new toplevel span underneath the logger. The name defaults to \"root\" and the span type to \"task\".\nSee Span.start_span\nfor full details\nupdate_span\nUpdate a span in the experiment using its id. It is important that you only update a span once the original span\nhas been fully written and flushed, since otherwise updates to the span may conflict with the original span.\nArguments:\nid\n: The id of the span to update.**event\n: Data to update. SeeExperiment.log\nfor a full list of valid fields.\nexport\nReturn a serialized representation of the logger that can be used to start subspans in other places. See Span.start_span\nfor more details.\nflush\nFlush any pending logs to the server.\nScoreSummary Objects\nSummary of a score's performance.\nname\nName of the score.\nscore\nAverage score across all examples.\nimprovements\nNumber of improvements in the score.\nregressions\nNumber of regressions in the score.\ndiff\nDifference in score between the current and reference experiment.\nMetricSummary Objects\nSummary of a metric's performance.\nname\nName of the metric.\nmetric\nAverage metric across all examples.\nunit\nUnit label for the metric.\nimprovements\nNumber of improvements in the metric.\nregressions\nNumber of regressions in the metric.\ndiff\nDifference in metric between the current and reference experiment.\nExperimentSummary Objects\nSummary of an experiment's scores and metadata.\nproject_name\nName of the project that the experiment belongs to.\nproject_id\nID of the project. May be None\nif the eval was run locally.\nexperiment_id\nID of the experiment. May be None\nif the eval was run locally.\nexperiment_name\nName of the experiment.\nproject_url\nURL to the project's page in the Braintrust app.\nexperiment_url\nURL to the experiment's page in the Braintrust app.\ncomparison_experiment_name\nThe experiment scores are baselined against.\nscores\nSummary of the experiment's scores.\nmetrics\nSummary of the experiment's metrics.\nDataSummary Objects\nSummary of a dataset's data.\nnew_records\nNew or updated records added in this session.\ntotal_records\nTotal records in the dataset.\nDatasetSummary Objects\nSummary of a dataset's scores and metadata.\nproject_name\nName of the project that the dataset belongs to.\ndataset_name\nName of the dataset.\nproject_url\nURL to the project's page in the Braintrust app.\ndataset_url\nURL to the experiment's page in the Braintrust app.\ndata_summary\nSummary of the dataset's data.\nget_prompt_versions\nGet the versions for a specific prompt.\nArguments:\nproject_id\n- The ID of the project to queryprompt_id\n- The ID of the prompt to get versions for\nReturns:\nList of transaction IDs (_xact_id) for entries where audit_data.action is \"upsert\"\nbraintrust.framework\nEvalCase Objects\nAn evaluation case. This is a single input to the evaluation task, along with an optional expected output, metadata, and tags.\nEvalResult Objects\nThe result of an evaluation. This includes the input, expected output, actual output, and metadata.\nEvalHooks Objects\nAn object that can be used to add metadata to an evaluation. This is passed to the task\nfunction.\nmetadata\nThe metadata object for the current evaluation. You can mutate this object to add or remove metadata.\nexpected\nThe expected output for the current evaluation.\nspan\nAccess the span under which the task is run. Also accessible via braintrust.current_span()\ntrial_index\nThe index of the current trial (0-based). This is useful when trial_count > 1.\ntags\nThe tags for the current evaluation. You can mutate this object to add or remove tags.\nmeta\nDEPRECATED: Use the metadata field on the hook directly.\nAdds metadata to the evaluation. This metadata will be logged to the Braintrust. You can pass in metadaa\nas keyword arguments, e.g. hooks.meta(foo=\"bar\")\n.\nEvalScorerArgs Objects\nArguments passed to an evaluator scorer. This includes the input, expected output, actual output, and metadata.\nSyncScorerLike Objects\nProtocol for synchronous scorers that implement the callable interface. This is the most common interface and is used when no async version is available.\nAsyncScorerLike Objects\nProtocol for asynchronous scorers that implement the eval_async interface. The framework will prefer this interface if available.\nBaseExperiment Objects\nUse this to specify that the dataset should actually be the data from a previous (base) experiment. If you do not specify a name, Braintrust will automatically figure out the best base experiment to use based on your git history (or fall back to timestamps).\nname\nThe name of the base experiment to use. If unspecified, Braintrust will automatically figure out the best base using your git history (or fall back to timestamps).\nEvaluator Objects\nAn evaluator is an abstraction that defines an evaluation dataset, a task to run on the dataset, and a set of scorers to evaluate the results of the task. Each method attribute can be synchronous or asynchronous (for optimal performance, it is recommended to provide asynchronous implementations).\nYou should not create Evaluators directly if you plan to use the Braintrust eval framework. Instead, you should\ncreate them using the Eval()\nmethod, which will register them so that braintrust eval ...\ncan find them.\nproject_name\nThe name of the project the eval falls under.\neval_name\nA name that describes the experiment. You do not need to change it each time the experiment runs.\ndata\nReturns an iterator over the evaluation dataset. Each element of the iterator should be an EvalCase\nor a dict\nwith the same fields as an EvalCase\n(input\n, expected\n, metadata\n).\ntask\nRuns the evaluation task on a single input. The hooks\nobject can be used to add metadata to the evaluation.\nscores\nA list of scorers to evaluate the results of the task. Each scorer can be a Scorer object or a function\nthat takes input\n, output\n, and expected\narguments and returns a Score\nobject. The function can be async.\nexperiment_name\nOptional experiment name. If not specified, a name will be generated automatically.\nmetadata\nA dictionary with additional data about the test example, model outputs, or just about anything else that's\nrelevant, that you can use to help find and analyze examples later. For example, you could log the prompt\n,\nexample's id\n, or anything else that would be useful to slice/dice later. The values in metadata\ncan be any\nJSON-serializable type, but its keys must be strings.\ntrial_count\nThe number of times to run the evaluator per input. This is useful for evaluating applications that have non-deterministic behavior and gives you both a stronger aggregate measure and a sense of the variance in the results.\nis_public\nWhether the experiment should be public. Defaults to false.\nupdate\nWhether to update an existing experiment with experiment_name\nif one exists. Defaults to false.\ntimeout\nThe duration, in seconds, after which to time out the evaluation. Defaults to None, in which case there is no timeout.\nmax_concurrency\nThe maximum number of tasks/scorers that will be run concurrently. Defaults to None, in which case there is no max concurrency.\nproject_id\nIf specified, uses the given project ID instead of the evaluator's name to identify the project.\nbase_experiment_name\nAn optional experiment name to use as a base. If specified, the new experiment will be summarized and compared to this experiment.\nbase_experiment_id\nAn optional experiment id to use as a base. If specified, the new experiment will be summarized and\ncompared to this experiment. This takes precedence over base_experiment_name\nif specified.\ngit_metadata_settings\nOptional settings for collecting git metadata. By default, will collect all git metadata fields allowed in org-level settings.\nrepo_info\nOptionally explicitly specify the git metadata for this experiment. This\ntakes precedence over git_metadata_settings\nif specified.\nerror_score_handler\nOptionally supply a custom function to specifically handle score values when tasks or scoring functions have errored.\nA default implementation is exported as default_error_score_handler\nwhich will log a 0 score to the root span for any scorer that was not run.\ndescription\nAn optional description for the experiment.\nsummarize_scores\nWhether to summarize the scores of the experiment after it has run.\nReporterDef Objects\nA reporter takes an evaluator and its result and returns a report.\nname\nThe name of the reporter.\nreport_eval\nA function that takes an evaluator and its result and returns a report.\nreport_run\nA function that takes all evaluator results and returns a boolean indicating whether the run was successful.\nIf you return false, the braintrust eval\ncommand will exit with a non-zero status code.\nEvalAsync\nA function you can use to define an evaluator. This is a convenience wrapper around the Evaluator\nclass.\nUse this function over Eval()\nwhen you are running in an async context, including in a Jupyter notebook.\nExample:\nArguments:\nname\n: The name of the evaluator. This corresponds to a project name in Braintrust.data\n: Returns an iterator over the evaluation dataset. Each element of the iterator should be aEvalCase\n.task\n: Runs the evaluation task on a single input. Thehooks\nobject can be used to add metadata to the evaluation.scores\n: A list of scorers to evaluate the results of the task. Each scorer can be a Scorer object or a function that takes anEvalScorerArgs\nobject and returns aScore\nobject.experiment_name\n: (Optional) Experiment name. If not specified, a name will be generated automatically.trial_count\n: The number of times to run the evaluator per input. This is useful for evaluating applications that have non-deterministic behavior and gives you both a stronger aggregate measure and a sense of the variance in the results.metadata\n: (Optional) A dictionary with additional data about the test example, model outputs, or just about anything else that's relevant, that you can use to help find and analyze examples later. For example, you could log theprompt\n, example'sid\n, or anything else that would be useful to slice/dice later. The values inmetadata\ncan be any JSON-serializable type, but its keys must be strings.is_public\n: (Optional) Whether the experiment should be public. Defaults to false.reporter\n: (Optional) A reporter that takes an evaluator and its result and returns a report.timeout\n: (Optional) The duration, in seconds, after which to time out the evaluation. Defaults to None, in which case there is no timeout.project_id\n: (Optional) If specified, uses the given project ID instead of the evaluator's name to identify the project.base_experiment_name\n: An optional experiment name to use as a base. If specified, the new experiment will be summarized and compared to this experiment.base_experiment_id\n: An optional experiment id to use as a base. If specified, the new experiment will be summarized and compared to this experiment. This takes precedence overbase_experiment_name\nif specified.git_metadata_settings\n: Optional settings for collecting git metadata. By default, will collect all git metadata fields allowed in org-level settings.repo_info\n: Optionally explicitly specify the git metadata for this experiment. This takes precedence overgit_metadata_settings\nif specified.error_score_handler\n: Optionally supply a custom function to specifically handle score values when tasks or scoring functions have errored.description\n: An optional description for the experiment.summarize_scores\n: Whether to summarize the scores of the experiment after it has run.no_send_logs\n: Do not send logs to Braintrust. When True, the evaluation runs locally and builds a local summary instead of creating an experiment. Defaults to False.\nReturns:\nAn EvalResultWithSummary\nobject, which contains all results and a summary.\nEval\nA function you can use to define an evaluator. This is a convenience wrapper around the Evaluator\nclass.\nFor callers running in an async context, use EvalAsync()\ninstead.\nExample:\nArguments:\nname\n: The name of the evaluator. This corresponds to a project name in Braintrust.data\n: Returns an iterator over the evaluation dataset. Each element of the iterator should be aEvalCase\n.task\n: Runs the evaluation task on a single input. Thehooks\nobject can be used to add metadata to the evaluation.scores\n: A list of scorers to evaluate the results of the task. Each scorer can be a Scorer object or a function that takes anEvalScorerArgs\nobject and returns aScore\nobject.experiment_name\n: (Optional) Experiment name. If not specified, a name will be generated automatically.trial_count\n: The number of times to run the evaluator per input. This is useful for evaluating applications that have non-deterministic behavior and gives you both a stronger aggregate measure and a sense of the variance in the results.metadata\n: (Optional) A dictionary with additional data about the test example, model outputs, or just about anything else that's relevant, that you can use to help find and analyze examples later. For example, you could log theprompt\n, example'sid\n, or anything else that would be useful to slice/dice later. The values inmetadata\ncan be any JSON-serializable type, but its keys must be strings.is_public\n: (Optional) Whether the experiment should be public. Defaults to false.reporter\n: (Optional) A reporter that takes an evaluator and its result and returns a report.timeout\n: (Optional) The duration, in seconds, after which to time out the evaluation. Defaults to None, in which case there is no timeout.project_id\n: (Optional) If specified, uses the given project ID instead of the evaluator's name to identify the project.base_experiment_name\n: An optional experiment name to use as a base. If specified, the new experiment will be summarized and compared to this experiment.base_experiment_id\n: An optional experiment id to use as a base. If specified, the new experiment will be summarized and compared to this experiment. This takes precedence overbase_experiment_name\nif specified.git_metadata_settings\n: Optional settings for collecting git metadata. By default, will collect all git metadata fields allowed in org-level settings.repo_info\n: Optionally explicitly specify the git metadata for this experiment. This takes precedence overgit_metadata_settings\nif specified.error_score_handler\n: Optionally supply a custom function to specifically handle score values when tasks or scoring functions have errored.description\n: An optional description for the experiment.summarize_scores\n: Whether to summarize the scores of the experiment after it has run.no_send_logs\n: Do not send logs to Braintrust. When True, the evaluation runs locally and builds a local summary instead of creating an experiment. Defaults to False.\nReturns:\nAn EvalResultWithSummary\nobject, which contains all results and a summary.\nReporter\nA function you can use to define a reporter. This is a convenience wrapper around the ReporterDef\nclass.\nExample:\nArguments:\nname\n: The name of the reporter.report_eval\n: A function that takes an evaluator and its result and returns a report.report_run\n: A function that takes all evaluator results and returns a boolean indicating whether the run was successful.\nset_thread_pool_max_workers\nSet the maximum number of threads to use for running evaluators. By default, this is the number of CPUs on the machine.\nrun_evaluator\nWrapper on _run_evaluator_internal that times out execution after evaluator.timeout.\nbraintrust.functions.stream\nThis module provides classes and functions for handling Braintrust streams.\nA Braintrust stream is a wrapper around a generator of BraintrustStreamChunk\n,\nwith utility methods to make them easy to log and convert into various formats.\nBraintrustTextChunk Objects\nA chunk of text data from a Braintrust stream.\nBraintrustJsonChunk Objects\nA chunk of JSON data from a Braintrust stream.\nBraintrustErrorChunk Objects\nAn error chunk from a Braintrust stream.\nBraintrustConsoleChunk Objects\nA console chunk from a Braintrust stream.\nBraintrustProgressChunk Objects\nA progress chunk from a Braintrust stream.\nBraintrustInvokeError Objects\nAn error that occurs during a Braintrust stream.\nBraintrustStream Objects\nA Braintrust stream. This is a wrapper around a generator of BraintrustStreamChunk\n,\nwith utility methods to make them easy to log and convert into various formats.\n__init__\nInitialize a BraintrustStream.\nArguments:\nbase_stream\n- Either an SSEClient or a list of BraintrustStreamChunks.\ncopy\nCopy the stream. This returns a new stream that shares the same underlying\ngenerator (via tee\n). Since generators are consumed in Python, use copy()\nif you\nneed to use the stream multiple times.\nReturns:\nBraintrustStream\n- A new stream that you can independently consume.\nfinal_value\nGet the final value of the stream. The final value is the concatenation of all the chunks in the stream, deserialized into a string or object, depending on the value's type.\nThis function consumes the stream, so if you need to use the stream multiple\ntimes, you should call copy()\nfirst.\nReturns:\nThe final value of the stream.\n__iter__\nIterate over the stream chunks.\nYields:\nBraintrustStreamChunk\n- The next chunk in the stream.\nparse_stream\nParse a BraintrustStream into its final value.\nArguments:\nstream\n- The BraintrustStream to parse.\nReturns:\nThe final value of the stream.\nbraintrust.functions.invoke\ninvoke\nInvoke a Braintrust function, returning a BraintrustStream\nor the value as a plain\nPython object.\nArguments:\ninput\n- The input to the function. This will be logged as theinput\nfield in the span.messages\n- Additional OpenAI-style messages to add to the prompt (only works for llm functions).metadata\n- Additional metadata to add to the span. This will be logged as themetadata\nfield in the span. It will also be available as the {{metadata}} field in the prompt and as themetadata\nargument to the function.tags\n- Tags to add to the span. This will be logged as thetags\nfield in the span.parent\n- The parent of the function. This can be an existing span, logger, or experiment, or the output of.export()\nif you are distributed tracing. If unspecified, will use the same semantics astraced()\nto determine the parent and no-op if not in a tracing context.stream\n- Whether to stream the function's output. If True, the function will return aBraintrustStream\n, otherwise it will return the output of the function as a JSON object.mode\n- The response shape of the function if returning tool calls. If \"auto\", will return a string if the function returns a string, and a JSON object otherwise. If \"parallel\", will return an array of JSON objects with one object per tool call.strict\n- Whether to use strict mode for the function. If true, the function will throw an error if the variable names in the prompt do not match the input keys.org_name\n- The name of the Braintrust organization to use.api_key\n- The API key to use for authentication.app_url\n- The URL of the Braintrust application.force_login\n- Whether to force a new login even if already logged in.function_id\n- The ID of the function to invoke.version\n- The version of the function to invoke.prompt_session_id\n- The ID of the prompt session to invoke the function from.prompt_session_function_id\n- The ID of the function in the prompt session to invoke.project_name\n- The name of the project containing the function to invoke.slug\n- The slug of the function to invoke.global_function\n- The name of the global function to invoke.\nReturns:\nThe output of the function. If stream\nis True, returns a BraintrustStream\n,\notherwise returns the output as a Python object.\ninit_function\nCreates a function that can be used as either a task or scorer in the Eval framework.\nWhen used as a task, it will invoke the specified Braintrust function with the input. When used as a scorer, it will invoke the function with the scorer arguments.\nExample:\nArguments:\nproject_name\n: The name of the project containing the function.slug\n: The slug of the function to invoke.version\n: Optional version of the function to use. Defaults to latest.\nReturns:\nA function that can be used as a task or scorer.", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "TypeScript"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "Python"}, {"href": "https://www.braintrust.dev/docs/reference/api", "anchor": "API"}, {"href": "https://www.braintrust.dev/docs/reference/autoevals", "anchor": "Autoevals library"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "BTQL query syntax"}, {"href": "https://www.braintrust.dev/docs/reference/streaming", "anchor": "Streaming"}, {"href": "https://www.braintrust.dev/docs/reference/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/reference/reasoning", "anchor": "Reasoning"}, {"href": "https://www.braintrust.dev/docs/reference/organizations", "anchor": "Organizations"}, {"href": "https://www.braintrust.dev/docs/reference/mcp", "anchor": "Model Context Protocol (MCP)"}, {"href": "https://www.braintrust.dev/docs/reference/object-links", "anchor": "Sharing via URL"}, {"href": "https://www.braintrust.dev/docs/reference/glossary", "anchor": "Glossary"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "braintrust"}, {"href": "https://braintrust.dev/", "anchor": "Braintrust"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "Quickstart"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "API Reference"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "braintrust.logger"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "Exportable Objects"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "export"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "Span Objects"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "id"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "log"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "log_feedback"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "start_span"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "export"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "link"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "permalink"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "end"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "flush"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "close"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "set_attributes"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "set_current"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "set_http_adapter"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "RetryRequestExceptionsAdapter Objects"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "init"}, {"href": "https://www.braintrust.dev", "anchor": "https://www.braintrust.dev"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "init_experiment"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "init_dataset"}, {"href": "https://www.braintrust.dev", "anchor": "https://www.braintrust.dev"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "init_logger"}, {"href": "https://www.braintrust.dev", "anchor": "https://www.braintrust.dev"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "load_prompt"}, {"href": "https://www.braintrust.dev", "anchor": "https://www.braintrust.dev"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "login"}, {"href": "https://www.braintrust.dev/app/token", "anchor": "https://www.braintrust.dev/app/token"}, {"href": "https://www.braintrust.dev", "anchor": "https://www.braintrust.dev"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "log"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "summarize"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "current_experiment"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "current_logger"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "current_span"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "get_span_parent_object"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "traced"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "traced"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "traced"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "start_span"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "flush"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "ObjectFetcher Objects"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "fetch"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "Attachment Objects"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "__init__"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "reference"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "data"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "upload"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "debug_info"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "ExternalAttachment Objects"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "__init__"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "reference"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "data"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "upload"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "debug_info"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "ReadonlyAttachment Objects"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "data"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "metadata"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "status"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "update_span"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "span_components_to_object_id"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "permalink"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "Experiment Objects"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "log"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "log_feedback"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "start_span"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "update_span"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "summarize"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "close"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "flush"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "ReadonlyExperiment Objects"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "SpanImpl Objects"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "flush"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "Dataset Objects"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "insert"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "update"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "delete"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "summarize"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "close"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "flush"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "Prompt Objects"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "build"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "Logger Objects"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "log"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "log_feedback"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "start_span"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "update_span"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "export"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "flush"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "ScoreSummary Objects"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "name"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "score"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "improvements"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "regressions"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "diff"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "MetricSummary Objects"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "name"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "metric"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "unit"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "improvements"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "regressions"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "diff"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "ExperimentSummary Objects"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "project_name"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "project_id"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "experiment_id"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "experiment_name"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "project_url"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "experiment_url"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "comparison_experiment_name"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "scores"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "metrics"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "DataSummary Objects"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "new_records"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "total_records"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "DatasetSummary Objects"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "project_name"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "dataset_name"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "project_url"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "dataset_url"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "data_summary"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "get_prompt_versions"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "braintrust.framework"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "EvalCase Objects"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "EvalResult Objects"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "EvalHooks Objects"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "metadata"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "expected"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "span"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "trial_index"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "tags"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "meta"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "EvalScorerArgs Objects"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "SyncScorerLike Objects"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "AsyncScorerLike Objects"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "BaseExperiment Objects"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "name"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "Evaluator Objects"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "project_name"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "eval_name"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "data"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "task"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "scores"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "experiment_name"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "metadata"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "trial_count"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "is_public"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "update"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "timeout"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "max_concurrency"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "project_id"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "base_experiment_name"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "base_experiment_id"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "git_metadata_settings"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "repo_info"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "error_score_handler"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "description"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "summarize_scores"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "ReporterDef Objects"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "name"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "report_eval"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "report_run"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "EvalAsync"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "Eval"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "Reporter"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "set_thread_pool_max_workers"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "run_evaluator"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "braintrust.functions.stream"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "BraintrustTextChunk Objects"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "BraintrustJsonChunk Objects"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "BraintrustErrorChunk Objects"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "BraintrustConsoleChunk Objects"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "BraintrustProgressChunk Objects"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "BraintrustInvokeError Objects"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "BraintrustStream Objects"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "__init__"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "copy"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "final_value"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "__iter__"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "parse_stream"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "braintrust.functions.invoke"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "invoke"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "init_function"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "braintrust"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "Quickstart"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "API Reference"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "braintrust.logger"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "Exportable Objects"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "export"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "Span Objects"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "id"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "log"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "log_feedback"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "start_span"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "export"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "link"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "permalink"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "end"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "flush"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "close"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "set_attributes"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "set_current"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "set_http_adapter"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "RetryRequestExceptionsAdapter Objects"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "init"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "init_experiment"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "init_dataset"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "init_logger"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "load_prompt"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "login"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "log"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "summarize"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "current_experiment"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "current_logger"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "current_span"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "get_span_parent_object"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "traced"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "traced"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "traced"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "start_span"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "flush"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "ObjectFetcher Objects"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "fetch"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "Attachment Objects"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "__init__"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "reference"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "data"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "upload"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "debug_info"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "ExternalAttachment Objects"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "__init__"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "reference"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "data"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "upload"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "debug_info"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "ReadonlyAttachment Objects"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "data"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "metadata"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "status"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "update_span"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "span_components_to_object_id"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "permalink"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "Experiment Objects"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "log"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "log_feedback"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "start_span"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "update_span"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "summarize"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "close"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "flush"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "ReadonlyExperiment Objects"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "SpanImpl Objects"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "flush"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "Dataset Objects"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "insert"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "update"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "delete"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "summarize"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "close"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "flush"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "Prompt Objects"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "build"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "Logger Objects"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "log"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "log_feedback"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "start_span"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "update_span"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "export"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "flush"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "ScoreSummary Objects"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "name"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "score"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "improvements"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "regressions"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "diff"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "MetricSummary Objects"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "name"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "metric"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "unit"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "improvements"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "regressions"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "diff"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "ExperimentSummary Objects"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "project_name"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "project_id"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "experiment_id"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "experiment_name"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "project_url"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "experiment_url"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "comparison_experiment_name"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "scores"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "metrics"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "DataSummary Objects"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "new_records"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "total_records"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "DatasetSummary Objects"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "project_name"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "dataset_name"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "project_url"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "dataset_url"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "data_summary"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "get_prompt_versions"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "braintrust.framework"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "EvalCase Objects"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "EvalResult Objects"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "EvalHooks Objects"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "metadata"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "expected"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "span"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "trial_index"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "tags"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "meta"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "EvalScorerArgs Objects"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "SyncScorerLike Objects"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "AsyncScorerLike Objects"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "BaseExperiment Objects"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "name"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "Evaluator Objects"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "project_name"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "eval_name"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "data"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "task"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "scores"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "experiment_name"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "metadata"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "trial_count"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "is_public"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "update"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "timeout"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "max_concurrency"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "project_id"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "base_experiment_name"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "base_experiment_id"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "git_metadata_settings"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "repo_info"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "error_score_handler"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "description"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "summarize_scores"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "ReporterDef Objects"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "name"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "report_eval"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "report_run"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "EvalAsync"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "Eval"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "Reporter"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "set_thread_pool_max_workers"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "run_evaluator"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "braintrust.functions.stream"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "BraintrustTextChunk Objects"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "BraintrustJsonChunk Objects"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "BraintrustErrorChunk Objects"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "BraintrustConsoleChunk Objects"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "BraintrustProgressChunk Objects"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "BraintrustInvokeError Objects"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "BraintrustStream Objects"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "__init__"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "copy"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "final_value"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "__iter__"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "parse_stream"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "braintrust.functions.invoke"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "invoke"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "init_function"}], "depth": 3}, "https://www.braintrust.dev/docs/api/spec": {"url": "https://www.braintrust.dev/docs/api/spec", "title": "API - Docs - Reference - Braintrust", "text": "API reference\nThis section contains the full API specification for Braintrust's data plane. The API is hosted globally at https://api.braintrust.dev or in your own environment. The API allows you to access all of the core objects in Braintrust, including experiments, datasets, prompts, users, groups, roles, and more. It also enables you to access Braintrust from languages other than TypeScript and Python.\nYou can access the full OpenAPI spec for this API at https://github.com/braintrustdata/braintrust-openapi.\nAcls\nManage role-based access controls for the organization\nAiSecrets\nManage external AI provider secrets scoped to the organization\nApiKeys\nManagement of API keys for Braintrust users\nCrossObject\nEvents and feedback across object types\nDatasets\nCollections of data used to run evaluations and track improvements over time\nEnvVars\nEnvironment variables that your scoring functions and tools can access\nEvals\nRun evaluations through the API\nExperiments\nA set of traces run to test the behavior of code\nFunctions\nInvokable custom logic for LLM applications\nGroups\nPermission management for collections of users\nLogs\nLog real-world interactions in your application\nOrganizations\nThe billing unit of Braintrust. Can represent a company or team\nOther\n'Hello world' endpoint for testing the API\nProjectScores\nScores scoped to a project to be used in experiment code and human review\nProjectTags\nTags used to track various kinds of data across your application, and track how they change over time\nProjects\nA single AI feature. The container for experiments, logs, datasets, prompts, and playgrounds\nPrompts\nVersioned prompt that can be referenced in your code\nProxy\nAn OpenAI-protocol compatible proxy that supports multiple model formats, caching, and secret management\nRoles\nCollections of permissions that can be assigned to users or groups\nServiceTokens\nManagement of automated Service tokens\nSpanIframes\nCustom iframe URLs that can be rendered in spans\nUsers\nBraintrust users\nViews\nTable configurations that can be saved for quick access\nAPI wrappers\nThrough Stainless, we have language-specific wrappers over the bare REST API for a variety of languages. Note that unlike our custom-built Python or TypeScript SDKs, these libraries map essentially 1:1 with the REST API:", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/reference/api", "anchor": "API"}, {"href": "https://www.braintrust.dev/docs/reference/api/Acls", "anchor": "Acls"}, {"href": "https://www.braintrust.dev/docs/reference/api/AiSecrets", "anchor": "Ai Secrets"}, {"href": "https://www.braintrust.dev/docs/reference/api/ApiKeys", "anchor": "Api Keys"}, {"href": "https://www.braintrust.dev/docs/reference/api/CrossObject", "anchor": "Cross Object"}, {"href": "https://www.braintrust.dev/docs/reference/api/Datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/reference/api/EnvVars", "anchor": "Env Vars"}, {"href": "https://www.braintrust.dev/docs/reference/api/Evals", "anchor": "Evals"}, {"href": "https://www.braintrust.dev/docs/reference/api/Experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/reference/api/Functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/reference/api/Groups", "anchor": "Groups"}, {"href": "https://www.braintrust.dev/docs/reference/api/Logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/reference/api/Organizations", "anchor": "Organizations"}, {"href": "https://www.braintrust.dev/docs/reference/api/Other", "anchor": "Other"}, {"href": "https://www.braintrust.dev/docs/reference/api/Projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectScores", "anchor": "Project Scores"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectTags", "anchor": "Project Tags"}, {"href": "https://www.braintrust.dev/docs/reference/api/Prompts", "anchor": "Prompts"}, {"href": "https://www.braintrust.dev/docs/reference/api/Proxy", "anchor": "Proxy"}, {"href": "https://www.braintrust.dev/docs/reference/api/Roles", "anchor": "Roles"}, {"href": "https://www.braintrust.dev/docs/reference/api/ServiceTokens", "anchor": "Service tokens"}, {"href": "https://www.braintrust.dev/docs/reference/api/SpanIframes", "anchor": "Span Iframes"}, {"href": "https://www.braintrust.dev/docs/reference/api/Users", "anchor": "Users"}, {"href": "https://www.braintrust.dev/docs/reference/api/Views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/reference/autoevals", "anchor": "Autoevals library"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "BTQL query syntax"}, {"href": "https://www.braintrust.dev/docs/reference/streaming", "anchor": "Streaming"}, {"href": "https://www.braintrust.dev/docs/reference/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/reference/reasoning", "anchor": "Reasoning"}, {"href": "https://www.braintrust.dev/docs/reference/organizations", "anchor": "Organizations"}, {"href": "https://www.braintrust.dev/docs/reference/mcp", "anchor": "Model Context Protocol (MCP)"}, {"href": "https://www.braintrust.dev/docs/reference/object-links", "anchor": "Sharing via URL"}, {"href": "https://www.braintrust.dev/docs/reference/glossary", "anchor": "Glossary"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/api/spec", "anchor": "API reference"}, {"href": "https://api.braintrust.dev", "anchor": "https://api.braintrust.dev"}, {"href": "https://www.braintrust.dev/docs/reference/api/Acls", "anchor": "Acls Manage role-based access controls for the organization"}, {"href": "https://www.braintrust.dev/docs/reference/api/AiSecrets", "anchor": "AiSecrets Manage external AI provider secrets scoped to the organization"}, {"href": "https://www.braintrust.dev/docs/reference/api/ApiKeys", "anchor": "ApiKeys Management of API keys for Braintrust users"}, {"href": "https://www.braintrust.dev/docs/reference/api/CrossObject", "anchor": "CrossObject Events and feedback across object types"}, {"href": "https://www.braintrust.dev/docs/reference/api/Datasets", "anchor": "Datasets Collections of data used to run evaluations and track improvements over time"}, {"href": "https://www.braintrust.dev/docs/reference/api/EnvVars", "anchor": "EnvVars Environment variables that your scoring functions and tools can access"}, {"href": "https://www.braintrust.dev/docs/reference/api/Evals", "anchor": "Evals Run evaluations through the API"}, {"href": "https://www.braintrust.dev/docs/reference/api/Experiments", "anchor": "Experiments A set of traces run to test the behavior of code"}, {"href": "https://www.braintrust.dev/docs/reference/api/Functions", "anchor": "Functions Invokable custom logic for LLM applications"}, {"href": "https://www.braintrust.dev/docs/reference/api/Groups", "anchor": "Groups Permission management for collections of users"}, {"href": "https://www.braintrust.dev/docs/reference/api/Logs", "anchor": "Logs Log real-world interactions in your application"}, {"href": "https://www.braintrust.dev/docs/reference/api/Organizations", "anchor": "Organizations The billing unit of Braintrust. Can represent a company or team"}, {"href": "https://www.braintrust.dev/docs/reference/api/Other", "anchor": "Other 'Hello world' endpoint for testing the API"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectScores", "anchor": "ProjectScores Scores scoped to a project to be used in experiment code and human review"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectTags", "anchor": "ProjectTags Tags used to track various kinds of data across your application, and track how they change over time"}, {"href": "https://www.braintrust.dev/docs/reference/api/Projects", "anchor": "Projects A single AI feature. The container for experiments, logs, datasets, prompts, and playgrounds"}, {"href": "https://www.braintrust.dev/docs/reference/api/Prompts", "anchor": "Prompts Versioned prompt that can be referenced in your code"}, {"href": "https://www.braintrust.dev/docs/reference/api/Proxy", "anchor": "Proxy An OpenAI-protocol compatible proxy that supports multiple model formats, caching, and secret management"}, {"href": "https://www.braintrust.dev/docs/reference/api/Roles", "anchor": "Roles Collections of permissions that can be assigned to users or groups"}, {"href": "https://www.braintrust.dev/docs/reference/api/ServiceTokens", "anchor": "ServiceTokens Management of automated Service tokens"}, {"href": "https://www.braintrust.dev/docs/reference/api/SpanIframes", "anchor": "SpanIframes Custom iframe URLs that can be rendered in spans"}, {"href": "https://www.braintrust.dev/docs/reference/api/Users", "anchor": "Users Braintrust users"}, {"href": "https://www.braintrust.dev/docs/reference/api/Views", "anchor": "Views Table configurations that can be saved for quick access"}, {"href": "https://www.braintrust.dev/docs/api/spec", "anchor": "API wrappers"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "Python"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "TypeScript"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}], "depth": 3}, "https://www.braintrust.dev/docs/start/frameworks/openai-agents-sdk": {"url": "https://www.braintrust.dev/docs/start/frameworks/openai-agents-sdk", "title": "OpenAI Agents SDK - Docs - Start - Braintrust", "text": "OpenAI Agents SDK\nThe Braintrust SDK provides trace processors for the OpenAI Agents SDK that send traces and spans to Braintrust for monitoring and evaluation.\nFor TypeScript, install the Braintrust OpenAI Agents integration package:\npnpm add braintrust @braintrust/openai-agents @openai/agents\nThe constructor of the tracing processor can take a braintrust.Span\n, braintrust.Experiment\n, or braintrust.Logger\nthat serves as the root under which all spans will be logged.\nIf not provided, the current span, experiment, or logger\nis selected automatically.\nThe Agents SDK can also be used to implement a task\nin an Eval\n,\nmaking it straightforward to build and evaluate agentic workflows:", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/opentelemetry", "anchor": "OpenTelemetry (OTel)"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/vercel-ai-sdk", "anchor": "Vercel AI SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/openai-agents-sdk", "anchor": "OpenAI Agents SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/instructor", "anchor": "Instructor"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/langchain", "anchor": "LangChain"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/langgraph", "anchor": "LangGraph"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/google", "anchor": "Google ADK"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/openai-agents-sdk", "anchor": "OpenAI Agents SDK"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/openai-agents-sdk", "anchor": "OpenAI Agents SDK"}], "depth": 4}, "https://www.braintrust.dev/docs/start/frameworks/instructor": {"url": "https://www.braintrust.dev/docs/start/frameworks/instructor", "title": "Instructor - Docs - Start - Braintrust", "text": "Instructor\nTo use Instructor to generate structured outputs, you need to wrap the\nOpenAI client with both Instructor and Braintrust. It's important that you call Braintrust's wrap_openai\nfirst,\nbecause it uses low-level usage info and headers returned by the OpenAI call to log metrics to Braintrust.", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/opentelemetry", "anchor": "OpenTelemetry (OTel)"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/vercel-ai-sdk", "anchor": "Vercel AI SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/openai-agents-sdk", "anchor": "OpenAI Agents SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/instructor", "anchor": "Instructor"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/langchain", "anchor": "LangChain"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/langgraph", "anchor": "LangGraph"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/google", "anchor": "Google ADK"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/instructor", "anchor": "Instructor"}, {"href": "https://www.braintrust.dev/docs/providers/openai", "anchor": "wrap_openai"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/instructor", "anchor": "Instructor"}], "depth": 4}, "https://www.braintrust.dev/docs/start/frameworks/langchain": {"url": "https://www.braintrust.dev/docs/start/frameworks/langchain", "title": "LangChain - Docs - Start - Braintrust", "text": "LangChain\nTrace your LangChain applications by configuring a global LangChain callback handler.\nLearn more about LangChain callbacks in their documentation.\nTrace your LangChain applications by configuring a global LangChain callback handler.\nLearn more about LangChain callbacks in their documentation.", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/opentelemetry", "anchor": "OpenTelemetry (OTel)"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/vercel-ai-sdk", "anchor": "Vercel AI SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/openai-agents-sdk", "anchor": "OpenAI Agents SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/instructor", "anchor": "Instructor"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/langchain", "anchor": "LangChain"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/langgraph", "anchor": "LangGraph"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/google", "anchor": "Google ADK"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/langchain", "anchor": "LangChain"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/langchain", "anchor": "LangChain"}], "depth": 4}, "https://www.braintrust.dev/docs/start/frameworks/langgraph": {"url": "https://www.braintrust.dev/docs/start/frameworks/langgraph", "title": "LangGraph - Docs - Start - Braintrust", "text": "Get started/FrameworksCopyLangGraph Trace your LangGraph applications by configuring a global LangChain callback handler. TypeScriptPythontrace-langgraph.tsimport { BraintrustCallbackHandler, setGlobalHandler, } from \"@braintrust/langchain-js\"; import { END, START, StateGraph, StateGraphArgs } from \"@langchain/langgraph\"; import { ChatOpenAI } from \"@langchain/openai\"; import { initLogger } from \"braintrust\"; const logger = initLogger({ projectName: \"My Project\", apiKey: process.env.BRAINTRUST_API_KEY, }); const handler = new BraintrustCallbackHandler({ logger }); setGlobalHandler(handler); // Define the state structure for the graph type HelloWorldGraphState = Record<string, any>; const graphStateChannels: StateGraphArgs<HelloWorldGraphState>[\"channels\"] = {}; const model = new ChatOpenAI({ model: \"gpt-4o-mini\", }); async function sayHello(state: HelloWorldGraphState) { const res = await model.invoke(\"Say hello\"); return { message: res.content }; } function sayBye(state: HelloWorldGraphState) { console.log(`From the 'sayBye' node: Bye world!`); return {}; } async function main() { const graphBuilder = new StateGraph({ channels: graphStateChannels }) .addNode(\"sayHello\", sayHello) .addNode(\"sayBye\", sayBye) .addEdge(START, \"sayHello\") .addEdge(\"sayHello\", \"sayBye\") .addEdge(\"sayBye\", END); const helloWorldGraph = graphBuilder.compile(); // Execute the graph - all operations will be logged to Braintrust await helloWorldGraph.invoke({}); } main();Learn more about LangGraph in their documentation.", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/opentelemetry", "anchor": "OpenTelemetry (OTel)"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/vercel-ai-sdk", "anchor": "Vercel AI SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/openai-agents-sdk", "anchor": "OpenAI Agents SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/instructor", "anchor": "Instructor"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/langchain", "anchor": "LangChain"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/langgraph", "anchor": "LangGraph"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/google", "anchor": "Google ADK"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/langgraph", "anchor": "LangGraph"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/langgraph", "anchor": "LangGraph"}], "depth": 4}, "https://www.braintrust.dev/docs/start/frameworks/google": {"url": "https://www.braintrust.dev/docs/start/frameworks/google", "title": "Google ADK - Docs - Start - Braintrust", "text": "Google ADK (Agent Development Kit)\nThe braintrust-adk\nintegration provides automatic tracing and logging of Google ADK agent executions to Braintrust. It captures agent invocations, tool calls, parallel execution flows, and multi-step reasoning.\nuv add braintrust-adk\nThe integration automatically traces all ADK agent interactions:", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/opentelemetry", "anchor": "OpenTelemetry (OTel)"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/vercel-ai-sdk", "anchor": "Vercel AI SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/openai-agents-sdk", "anchor": "OpenAI Agents SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/instructor", "anchor": "Instructor"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/langchain", "anchor": "LangChain"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/langgraph", "anchor": "LangGraph"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/google", "anchor": "Google ADK"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/google", "anchor": "Google ADK (Agent Development Kit)"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}, {"href": "https://www.braintrust.dev/docs/start/frameworks/google", "anchor": "Google ADK (Agent Development Kit)"}], "depth": 4}, "https://www.braintrust.dev/docs/guides/tracing": {"url": "https://www.braintrust.dev/docs/guides/tracing", "title": "Tracing - Docs - Guides - Braintrust", "text": "Tracing\nTracing is an invaluable tool for exploring the sub-components of your program which produce each top-level input and output. We currently support tracing in logging and evaluations.\nAnatomy of a trace\nA trace represents a single independent request, and is made up of several spans.\nA span represents a unit of work, with a start and end time, and optional fields like input, output, metadata, scores, and metrics (the same fields you can log in an experiment). Each span contains one or more children that are usually run within their parent span, like for example, a nested function call. Common examples of spans include LLM calls, vector searches, the steps of an agent chain, and model evaluations.\nEach trace can be expanded to view all of the spans inside. Well-designed traces make it easy to understand the flow of your application, and to debug issues when they arise. The tracing API works the same way whether you are logging online (production logging) or offline (evaluations).\nWhere to go from here\nLearn more about tracing in Braintrust:", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Customize traces"}, {"href": "https://www.braintrust.dev/docs/guides/traces/view", "anchor": "View traces"}, {"href": "https://www.braintrust.dev/docs/guides/traces/extend", "anchor": "Extend traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/tracing", "anchor": "Tracing"}, {"href": "https://www.braintrust.dev/docs/guides/logging", "anchor": "logging"}, {"href": "https://www.braintrust.dev/docs/guides/evals", "anchor": "evaluations"}, {"href": "https://www.braintrust.dev/docs/guides/tracing", "anchor": "Anatomy of a trace"}, {"href": "https://www.braintrust.dev/docs/guides/evals", "anchor": "experiment"}, {"href": "https://www.braintrust.dev/docs/guides/tracing", "anchor": "Where to go from here"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Wrapping LLM clients (OpenAI and others)"}, {"href": "https://www.braintrust.dev/docs/guides/traces/integrations", "anchor": "OpenTelemetry and other popular library integrations"}, {"href": "https://www.braintrust.dev/docs/guides/traces/customize", "anchor": "Troubleshooting"}, {"href": "https://www.braintrust.dev/docs/guides/traces/view", "anchor": "Viewing traces"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}], "depth": 4}, "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/Evaluator": {"url": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/Evaluator", "title": "Evaluator - Docs - Reference - Braintrust", "text": "Interface: Evaluator<Input, Output, Expected, Metadata, Parameters>\nType parameters\nProperties\nbaseExperimentId\n\u2022 Optional\nbaseExperimentId: string\nAn optional experiment id to use as a base. If specified, the new experiment will be summarized\nand compared to this experiment. This takes precedence over baseExperimentName\nif specified.\nbaseExperimentName\n\u2022 Optional\nbaseExperimentName: string\nAn optional experiment name to use as a base. If specified, the new experiment will be summarized and compared to this experiment.\ndata\n\u2022 data: EvalData\n<Input\n, Expected\n, Metadata\n>\nA function that returns a list of inputs, expected outputs, and metadata.\ndescription\n\u2022 Optional\ndescription: string\nAn optional description for the experiment.\nerrorScoreHandler\n\u2022 Optional\nerrorScoreHandler: ErrorScoreHandler\nOptionally supply a custom function to specifically handle score values when tasks or scoring functions have errored.\nA default implementation is exported as defaultErrorScoreHandler\nwhich will log a 0 score to the root span for any scorer that was not run.\nexperimentName\n\u2022 Optional\nexperimentName: string\nAn optional name for the experiment.\ngitMetadataSettings\n\u2022 Optional\ngitMetadataSettings: Object\nOptional settings for collecting git metadata. By default, will collect all git metadata fields allowed in org-level settings.\nType declaration\nisPublic\n\u2022 Optional\nisPublic: boolean\nWhether the experiment should be public. Defaults to false.\nmaxConcurrency\n\u2022 Optional\nmaxConcurrency: number\nThe maximum number of tasks/scorers that will be run concurrently. Defaults to undefined, in which case there is no max concurrency.\nmetadata\n\u2022 Optional\nmetadata: Record\n<string\n, unknown\n>\nOptional additional metadata for the experiment.\nparameters\n\u2022 Optional\nparameters: Parameters\nA set of parameters that will be passed to the evaluator. Can contain array values that will be converted to single values in the task.\nprojectId\n\u2022 Optional\nprojectId: string\nIf specified, uses the given project ID instead of the evaluator's name to identify the project.\nrepoInfo\n\u2022 Optional\nrepoInfo: Object\nOptionally explicitly specify the git metadata for this experiment. This takes precedence over gitMetadataSettings\nif specified.\nType declaration\nscores\n\u2022 scores: EvalScorer\n<Input\n, Output\n, Expected\n, Metadata\n>[]\nA set of functions that take an input, output, and expected value and return a score.\nsignal\n\u2022 Optional\nsignal: AbortSignal\nAn abort signal that can be used to stop the evaluation.\nstate\n\u2022 Optional\nstate: BraintrustState\nIf specified, uses the logger state to initialize Braintrust objects. If unspecified, falls back to the global state (initialized using your API key).\nsummarizeScores\n\u2022 Optional\nsummarizeScores: boolean\nWhether to summarize the scores of the experiment after it has run. Defaults to true.\ntask\n\u2022 task: EvalTask\n<Input\n, Output\n, Expected\n, Metadata\n, Parameters\n>\nA function that takes an input and returns an output.\ntimeout\n\u2022 Optional\ntimeout: number\nThe duration, in milliseconds, after which to time out the evaluation. Defaults to undefined, in which case there is no timeout.\ntrialCount\n\u2022 Optional\ntrialCount: number\nThe number of times to run the evaluator per input. This is useful for evaluating applications that have non-deterministic behavior and gives you both a stronger aggregate measure and a sense of the variance in the results.\nupdate\n\u2022 Optional\nupdate: boolean\nWhether to update an existing experiment with experiment_name\nif one exists. Defaults to false.", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "TypeScript"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/AttachmentParams", "anchor": "AttachmentParams"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/BackgroundLoggerOpts", "anchor": "BackgroundLoggerOpts"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/DatasetSummary", "anchor": "DatasetSummary"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/DataSummary", "anchor": "DataSummary"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/EvalHooks", "anchor": "EvalHooks"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/Evaluator", "anchor": "Evaluator"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/ExperimentSummary", "anchor": "ExperimentSummary"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/Exportable", "anchor": "Exportable"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/ExternalAttachmentParams", "anchor": "ExternalAttachmentParams"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/InvokeFunctionArgs", "anchor": "InvokeFunctionArgs"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/LoginOptions", "anchor": "LoginOptions"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/LogOptions", "anchor": "LogOptions"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/MetricSummary", "anchor": "MetricSummary"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/ObjectMetadata", "anchor": "ObjectMetadata"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/ParentExperimentIds", "anchor": "ParentExperimentIds"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/ParentProjectLogIds", "anchor": "ParentProjectLogIds"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/ReporterBody", "anchor": "ReporterBody"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/ScoreSummary", "anchor": "ScoreSummary"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/Span", "anchor": "Span"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "Python"}, {"href": "https://www.braintrust.dev/docs/reference/api", "anchor": "API"}, {"href": "https://www.braintrust.dev/docs/reference/autoevals", "anchor": "Autoevals library"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "BTQL query syntax"}, {"href": "https://www.braintrust.dev/docs/reference/streaming", "anchor": "Streaming"}, {"href": "https://www.braintrust.dev/docs/reference/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/reference/reasoning", "anchor": "Reasoning"}, {"href": "https://www.braintrust.dev/docs/reference/organizations", "anchor": "Organizations"}, {"href": "https://www.braintrust.dev/docs/reference/mcp", "anchor": "Model Context Protocol (MCP)"}, {"href": "https://www.braintrust.dev/docs/reference/object-links", "anchor": "Sharing via URL"}, {"href": "https://www.braintrust.dev/docs/reference/glossary", "anchor": "Glossary"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "TypeScript"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/Evaluator", "anchor": "Interface: Evaluator<Input, Output, Expected, Metadata, Parameters>"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/Evaluator", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "BaseMetadata"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "DefaultMetadataType"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "EvalParameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "EvalParameters"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/Evaluator", "anchor": "Properties"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/Evaluator", "anchor": "baseExperimentId"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/Evaluator", "anchor": "baseExperimentName"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/Evaluator", "anchor": "data"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/Evaluator", "anchor": "description"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/Evaluator", "anchor": "errorScoreHandler"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/Evaluator", "anchor": "experimentName"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/Evaluator", "anchor": "gitMetadataSettings"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/Evaluator", "anchor": "Type declaration"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/Evaluator", "anchor": "isPublic"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/Evaluator", "anchor": "maxConcurrency"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/Evaluator", "anchor": "metadata"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/Evaluator", "anchor": "parameters"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/Evaluator", "anchor": "projectId"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/Evaluator", "anchor": "repoInfo"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/Evaluator", "anchor": "Type declaration"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/Evaluator", "anchor": "scores"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "EvalScorer"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/Evaluator", "anchor": "signal"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/Evaluator", "anchor": "state"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/BraintrustState.md", "anchor": "BraintrustState"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/Evaluator", "anchor": "summarizeScores"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/Evaluator", "anchor": "task"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "EvalTask"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/Evaluator", "anchor": "timeout"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/Evaluator", "anchor": "trialCount"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/Evaluator", "anchor": "update"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/Evaluator", "anchor": "Interface: Evaluator<Input, Output, Expected, Metadata, Parameters>"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/Evaluator", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/Evaluator", "anchor": "Properties"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/Evaluator", "anchor": "baseExperimentId"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/Evaluator", "anchor": "baseExperimentName"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/Evaluator", "anchor": "data"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/Evaluator", "anchor": "description"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/Evaluator", "anchor": "errorScoreHandler"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/Evaluator", "anchor": "experimentName"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/Evaluator", "anchor": "gitMetadataSettings"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/Evaluator", "anchor": "Type declaration"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/Evaluator", "anchor": "isPublic"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/Evaluator", "anchor": "maxConcurrency"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/Evaluator", "anchor": "metadata"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/Evaluator", "anchor": "parameters"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/Evaluator", "anchor": "projectId"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/Evaluator", "anchor": "repoInfo"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/Evaluator", "anchor": "Type declaration"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/Evaluator", "anchor": "scores"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/Evaluator", "anchor": "signal"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/Evaluator", "anchor": "state"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/Evaluator", "anchor": "summarizeScores"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/Evaluator", "anchor": "task"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/Evaluator", "anchor": "timeout"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/Evaluator", "anchor": "trialCount"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs/interfaces/Evaluator", "anchor": "update"}], "depth": 4}, "https://www.braintrust.dev/docs/autoevals": {"url": "https://www.braintrust.dev/docs/autoevals", "title": "Autoevals - Docs - Reference - Braintrust", "text": "Autoevals\nAutoevals is a tool to quickly and easily evaluate AI model outputs.\nIt bundles together a variety of automatic evaluation methods including:\n- LLM-as-a-judge\n- Heuristic (e.g. Levenshtein distance)\n- Statistical (e.g. BLEU)\nAutoevals is developed by the team at Braintrust.\nAutoevals uses model-graded evaluation for a variety of subjective tasks including fact checking, safety, and more. Many of these evaluations are adapted from OpenAI's excellent evals project but are implemented so you can flexibly run them on individual examples, tweak the prompts, and debug their outputs.\nYou can also create your own model-graded evaluations with Autoevals. It's easy to add custom prompts, parse outputs, and manage exceptions.\nRequirements\n- Python 3.9 or higher\n- Compatible with both OpenAI Python SDK v0.x and v1.x\nInstallation\npnpm add autoevals\nGetting started\nUse Autoevals to model-grade an example LLM completion using the Factuality prompt.\nBy default, Autoevals uses your OPENAI_API_KEY\nenvironment variable to authenticate with OpenAI's API.\nUsing other AI providers\nWhen you use Autoevals, it will look for an OPENAI_BASE_URL\nenvironment variable to use as the base for requests to an OpenAI compatible API. If OPENAI_BASE_URL\nis not set, it will default to the AI proxy.\nIf you choose to use the proxy, you'll also get:\n- Simplified access to many AI providers\n- Reduced costs with automatic request caching\n- Increased observability when you enable logging to Braintrust\nThe proxy is free to use, even if you don't have a Braintrust account.\nIf you have a Braintrust account, you can optionally set the BRAINTRUST_API_KEY\nenvironment variable instead of OPENAI_API_KEY\nto unlock additional features like logging and monitoring. You can also route requests to supported AI providers and models or custom models you have configured in Braintrust.\nCustom client configuration\nThere are two ways you can configure a custom client when you need to use a different OpenAI compatible API:\n- Global configuration: Initialize a client that will be used by all evaluators\n- Instance configuration: Configure a client for a specific evaluator\nGlobal configuration\nSet up a client that all your evaluators will use:\nInstance configuration\nConfigure a client for a specific evaluator instance:\nUsing Braintrust with Autoevals (optional)\nOnce you grade an output using Autoevals, you can optionally use Braintrust to log and compare your evaluation results. This integration is completely optional and not required for using Autoevals.\nCreate a file named example.eval.js\n(it must take the form *.eval.[ts|tsx|js|jsx]\n):\nThen, run\nSupported evaluation methods\nLLM-as-a-judge evaluations\n- Battle\n- Closed QA\n- Humor\n- Factuality\n- Moderation\n- Security\n- Summarization\n- SQL\n- Translation\n- Fine-tuned binary classifiers\nRAG evaluations\n- Context precision\n- Context relevancy\n- Context recall\n- Context entity recall\n- Faithfulness\n- Answer relevancy\n- Answer similarity\n- Answer correctness\nComposite evaluations\n- Semantic list contains\n- JSON validity\nEmbedding evaluations\n- Embedding similarity\nHeuristic evaluations\n- Levenshtein distance\n- Exact match\n- Numeric difference\n- JSON diff\nCustom evaluation prompts\nAutoevals supports custom evaluation prompts for model-graded evaluation. To use them, simply pass in a prompt and scoring mechanism:\nCreating custom scorers\nYou can also create your own scoring functions that do not use LLMs. For example, to test whether the word 'banana'\nis in the output, you can use the following:\nWhy does this library exist?\nThere is nothing particularly novel about the evaluation methods in this library. They are all well-known and well-documented. However, there are a few things that are particularly difficult when evaluating in practice:\n- Normalizing metrics between 0 and 1 is tough. For example, check out the calculation in number.py to see how it's done for numeric differences.\n- Parsing the outputs on model-graded evaluations is also challenging. There are frameworks that do this, but it's hard to debug one output at a time, propagate errors, and tweak the prompts. Autoevals makes these tasks easy.\n- Collecting metrics behind a uniform interface makes it easy to swap out evaluation methods and compare them. Prior to Autoevals, we couldn't find an open source library where you can simply pass in\ninput\n,output\n, andexpected\nvalues through a bunch of different evaluation methods.\nDocumentation\nThe full docs are available for your reference.\nContributing\nWe welcome contributions!\nTo install the development dependencies, run make develop\n, and run source env.sh\nto activate the environment. Make a .env\nfile from the .env.example\nfile and set the environment variables. Run direnv allow\nto load the environment variables.\nTo run the tests, run pytest\nfrom the root directory.\nSend a PR and we'll review it! We'll take care of versioning and releasing.", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/reference/api", "anchor": "API"}, {"href": "https://www.braintrust.dev/docs/reference/autoevals", "anchor": "Autoevals library"}, {"href": "https://www.braintrust.dev/docs/reference/autoevals/python", "anchor": "Python"}, {"href": "https://www.braintrust.dev/docs/reference/autoevals/nodejs", "anchor": "TypeScript"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "BTQL query syntax"}, {"href": "https://www.braintrust.dev/docs/reference/streaming", "anchor": "Streaming"}, {"href": "https://www.braintrust.dev/docs/reference/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/reference/reasoning", "anchor": "Reasoning"}, {"href": "https://www.braintrust.dev/docs/reference/organizations", "anchor": "Organizations"}, {"href": "https://www.braintrust.dev/docs/reference/mcp", "anchor": "Model Context Protocol (MCP)"}, {"href": "https://www.braintrust.dev/docs/reference/object-links", "anchor": "Sharing via URL"}, {"href": "https://www.braintrust.dev/docs/reference/glossary", "anchor": "Glossary"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/autoevals", "anchor": "Autoevals"}, {"href": "https://braintrust.dev/", "anchor": "Braintrust"}, {"href": "https://www.braintrust.dev/docs/autoevals", "anchor": "Requirements"}, {"href": "https://www.braintrust.dev/docs/autoevals", "anchor": "Installation"}, {"href": "https://www.braintrust.dev/docs/autoevals", "anchor": "Getting started"}, {"href": "https://www.braintrust.dev/docs/templates/factuality.yaml", "anchor": "Factuality prompt"}, {"href": "https://www.braintrust.dev/docs/autoevals", "anchor": "Using other AI providers"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "supported AI providers and models"}, {"href": "https://www.braintrust.dev/docs/autoevals", "anchor": "Custom client configuration"}, {"href": "https://www.braintrust.dev/docs/autoevals", "anchor": "Global configuration"}, {"href": "https://www.braintrust.dev/docs/autoevals", "anchor": "Instance configuration"}, {"href": "https://www.braintrust.dev/docs/autoevals", "anchor": "Using Braintrust with Autoevals (optional)"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "Braintrust"}, {"href": "https://www.braintrust.dev/docs/autoevals", "anchor": "Supported evaluation methods"}, {"href": "https://www.braintrust.dev/docs/autoevals", "anchor": "LLM-as-a-judge evaluations"}, {"href": "https://www.braintrust.dev/docs/autoevals", "anchor": "RAG evaluations"}, {"href": "https://www.braintrust.dev/docs/autoevals", "anchor": "Composite evaluations"}, {"href": "https://www.braintrust.dev/docs/autoevals", "anchor": "Embedding evaluations"}, {"href": "https://www.braintrust.dev/docs/autoevals", "anchor": "Heuristic evaluations"}, {"href": "https://www.braintrust.dev/docs/autoevals", "anchor": "Custom evaluation prompts"}, {"href": "https://www.braintrust.dev/docs/autoevals", "anchor": "Creating custom scorers"}, {"href": "https://www.braintrust.dev/docs/autoevals", "anchor": "Why does this library exist?"}, {"href": "https://www.braintrust.dev/py/autoevals/number.py", "anchor": "number.py"}, {"href": "https://www.braintrust.dev/docs/autoevals", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/reference/autoevals", "anchor": "for your reference"}, {"href": "https://www.braintrust.dev/docs/autoevals", "anchor": "Contributing"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}], "depth": 4}, "https://www.braintrust.dev/docs/libs/python": {"url": "https://www.braintrust.dev/docs/libs/python", "title": "Python - Docs - Reference - Braintrust", "text": "braintrust\nA Python library for interacting with Braintrust. This library contains functionality for running evaluations, logging completions, loading and invoking functions, and more.\nbraintrust\nis distributed as a library on PyPI. It is open source and\navailable on GitHub.\nQuickstart\nInstall the library with pip.\nThen, create a file like eval_hello.py\nwith the following content:\nFinally, run the script with braintrust eval eval_hello.py\n.\nAPI Reference\nbraintrust.logger\nExportable Objects\nexport\nReturn a serialized representation of the object that can be used to start subspans in other places. See Span.start_span\nfor more details.\nSpan Objects\nA Span encapsulates logged data and metrics for a unit of work. This interface is shared by all span implementations.\nWe suggest using one of the various start_span\nmethods, instead of creating Spans directly. See Span.start_span\nfor full details.\nid\nRow ID of the span.\nlog\nIncrementally update the current span with new data. The event will be batched and uploaded behind the scenes.\nArguments:\n**event\n: Data to be logged. SeeExperiment.log\nfor full details.\nlog_feedback\nAdd feedback to the current span. Unlike Experiment.log_feedback\nand Logger.log_feedback\n, this method does not accept an id parameter, because it logs feedback to the current span.\nArguments:\n**event\n: Data to be logged. SeeExperiment.log_feedback\nfor full details.\nstart_span\nCreate a new span. This is useful if you want to log more detailed trace information beyond the scope of a single log event. Data logged over several calls to Span.log\nwill be merged into one logical row.\nWe recommend running spans within context managers (with start_span(...) as span\n) to automatically mark them as current and ensure they are ended. Only spans run within a context manager will be marked current, so they can be accessed using braintrust.current_span()\n. If you wish to start a span outside a context manager, be sure to end it with span.end()\n.\nArguments:\nname\n: Optional name of the span. If not provided, a name will be inferred from the call stack.type\n: Optional type of the span. Use theSpanTypeAttribute\nenum or just provide a string directly. If not provided, the type will be unset.span_attributes\n: Optional additional attributes to attach to the span, such as a type name.start_time\n: Optional start time of the span, as a timestamp in seconds.set_current\n: If true (the default), the span will be marked as the currently-active span for the duration of the context manager.parent\n: Optional parent info string for the span. The string can be generated from[Span,Experiment,Logger].export\n. If not provided, the current span will be used (depending on context). This is useful for adding spans to an existing trace.**event\n: Data to be logged. SeeExperiment.log\nfor full details.\nReturns:\nThe newly-created Span\nexport\nSerialize the identifiers of this span. The return value can be used to identify this span when starting a subspan elsewhere, such as another process or service, without needing to access this Span\nobject. See the parameters of Span.start_span\nfor usage details.\nCallers should treat the return value as opaque. The serialization format may change from time to time. If parsing is needed, use SpanComponentsV3.from_str\n.\nReturns:\nSerialized representation of this span's identifiers.\nlink\nFormat a link to the Braintrust application for viewing this span.\nLinks can be generated at any time, but they will only become viewable after the span and its root have been flushed to the server and ingested.\nThere are some conditions when a Span doesn't have enough information to return a stable link (e.g. during an unresolved experiment). In this case or if there's an error generating link, we'll return a placeholder link.\nReturns:\nA link to the span.\npermalink\nFormat a permalink to the Braintrust application for viewing this span.\nLinks can be generated at any time, but they will only become viewable after the span and its root have been flushed to the server and ingested.\nThis function can block resolving data with the server. For production\napplications it's preferable to call Span.link\ninstead.\nReturns:\nA permalink to the span.\nend\nLog an end time to the span (defaults to the current time). Returns the logged time.\nWill be invoked automatically if the span is bound to a context manager.\nArguments:\nend_time\n: Optional end time of the span, as a timestamp in seconds.\nReturns:\nThe end time logged to the span metrics.\nflush\nFlush any pending rows to the server.\nclose\nAlias for end\n.\nset_attributes\nSet the span's name, type, or other attributes. These attributes will be attached to all log events within the span.\nThe attributes are equivalent to the arguments to start_span.\nArguments:\nname\n: Optional name of the span. If not provided, a name will be inferred from the call stack.type\n: Optional type of the span. Use theSpanTypeAttribute\nenum or just provide a string directly. If not provided, the type will be unset.span_attributes\n: Optional additional attributes to attach to the span, such as a type name.\nset_current\nSet the span as the current span. This is used to mark the span as the active span for the current thread.\nset_http_adapter\nSpecify a custom HTTP adapter to use for all network requests. This is useful for setting custom retry policies, timeouts, etc.\nBraintrust uses the requests\nlibrary, so the adapter should be an instance of requests.adapters.HTTPAdapter\n. Alternatively, consider\nsub-classing our RetryRequestExceptionsAdapter\nto get automatic retries on network-related exceptions.\nArguments:\nadapter\n: The adapter to use.\nRetryRequestExceptionsAdapter Objects\nAn HTTP adapter that automatically retries requests on connection exceptions.\nThis adapter extends requests' HTTPAdapter to add retry logic for common network-related exceptions including connection errors, timeouts, and other HTTP errors. It implements an exponential backoff strategy between retries to avoid overwhelming servers during intermittent connectivity issues.\nAttributes:\nbase_num_retries\n- Maximum number of retries before giving up and re-raising the exception.backoff_factor\n- A multiplier used to determine the time to wait between retries. The actual wait time is calculated as: backoff_factor * (2 ** retry_count).\ninit\nLog in, and then initialize a new experiment in a specified project. If the project does not exist, it will be created.\nArguments:\nproject\n: The name of the project to create the experiment in. Must specify at least one ofproject\norproject_id\n.experiment\n: The name of the experiment to create. If not specified, a name will be generated automatically.description\n: (Optional) An optional description of the experiment.dataset\n: (Optional) A dataset to associate with the experiment. The dataset must be initialized withbraintrust.init_dataset\nbefore passing it into the experiment.update\n: If the experiment already exists, continue logging to it. If it does not exist, creates the experiment with the specified arguments.base_experiment\n: An optional experiment name to use as a base. If specified, the new experiment will be summarized and compared to this experiment. Otherwise, it will pick an experiment by finding the closest ancestor on the default (e.g. main) branch.is_public\n: An optional parameter to control whether the experiment is publicly visible to anybody with the link or privately visible to only members of the organization. Defaults to private.app_url\n: The URL of the Braintrust App. Defaults to https://www.braintrust.dev.api_key\n: The API key to use. If the parameter is not specified, will try to use theBRAINTRUST_API_KEY\nenvironment variable. If no API key is specified, will prompt the user to login.org_name\n: (Optional) The name of a specific organization to connect to. This is useful if you belong to multiple.metadata\n: (Optional) a dictionary with additional data about the test example, model outputs, or just about anything else that's relevant, that you can use to help find and analyze examples later. For example, you could log theprompt\n, example'sid\n, or anything else that would be useful to slice/dice later. The values inmetadata\ncan be any JSON-serializable type, but its keys must be strings.git_metadata_settings\n: (Optional) Settings for collecting git metadata. By default, will collect all git metadata fields allowed in org-level settings.set_current\n: If true (the default), set the global current-experiment to the newly-created one.open\n: If the experiment already exists, open it in read-only mode. Throws an error if the experiment does not already exist.project_id\n: The id of the project to create the experiment in. This takes precedence overproject\nif specified.base_experiment_id\n: An optional experiment id to use as a base. If specified, the new experiment will be summarized and compared to this. This takes precedence overbase_experiment\nif specified.repo_info\n: (Optional) Explicitly specify the git metadata for this experiment. This takes precedence overgit_metadata_settings\nif specified.\nReturns:\nThe experiment object.\ninit_experiment\nAlias for init\ninit_dataset\nCreate a new dataset in a specified project. If the project does not exist, it will be created.\nArguments:\nproject_name\n: The name of the project to create the dataset in. Must specify at least one ofproject_name\norproject_id\n.name\n: The name of the dataset to create. If not specified, a name will be generated automatically.description\n: An optional description of the dataset.version\n: An optional version of the dataset (to read). If not specified, the latest version will be used.app_url\n: The URL of the Braintrust App. Defaults to https://www.braintrust.dev.api_key\n: The API key to use. If the parameter is not specified, will try to use theBRAINTRUST_API_KEY\nenvironment variable. If no API key is specified, will prompt the user to login.org_name\n: (Optional) The name of a specific organization to connect to. This is useful if you belong to multiple.project_id\n: The id of the project to create the dataset in. This takes precedence overproject\nif specified.metadata\n: (Optional) a dictionary with additional data about the dataset. The values inmetadata\ncan be any JSON-serializable type, but its keys must be strings.use_output\n: (Deprecated) If True, records will be fetched from this dataset in the legacy format, with the \"expected\" field renamed to \"output\". This option will be removed in a future version of Braintrust.\nReturns:\nThe dataset object.\ninit_logger\nCreate a new logger in a specified project. If the project does not exist, it will be created.\nArguments:\nproject\n: The name of the project to log into. If unspecified, will default to the Global project.project_id\n: The id of the project to log into. This takes precedence over project if specified.async_flush\n: If true (the default), log events will be batched and sent asynchronously in a background thread. If false, log events will be sent synchronously. Set to false in serverless environments.app_url\n: The URL of the Braintrust API. Defaults to https://www.braintrust.dev.api_key\n: The API key to use. If the parameter is not specified, will try to use theBRAINTRUST_API_KEY\nenvironment variable. If no API key is specified, will prompt the user to login.org_name\n: (Optional) The name of a specific organization to connect to. This is useful if you belong to multiple.force_login\n: Login again, even if you have already logged in (by default, the logger will not login if you are already logged in)set_current\n: If true (the default), set the global current-experiment to the newly-created one.\nReturns:\nThe newly created Logger.\nload_prompt\nLoads a prompt from the specified project.\nArguments:\nproject\n: The name of the project to load the prompt from. Must specify at least one ofproject\norproject_id\n.slug\n: The slug of the prompt to load.version\n: An optional version of the prompt (to read). If not specified, the latest version will be used.project_id\n: The id of the project to load the prompt from. This takes precedence overproject\nif specified.id\n: The id of a specific prompt to load. If specified, this takes precedence over all other parameters (project, slug, version).defaults\n: (Optional) A dictionary of default values to use when rendering the prompt. Prompt values will override these defaults.no_trace\n: If true, do not include logging metadata for this prompt when build() is called.environment\n: The environment to load the prompt from. Cannot be used together with version.app_url\n: The URL of the Braintrust App. Defaults to https://www.braintrust.dev.api_key\n: The API key to use. If the parameter is not specified, will try to use theBRAINTRUST_API_KEY\nenvironment variable. If no API key is specified, will prompt the user to login.org_name\n: (Optional) The name of a specific organization to connect to. This is useful if you belong to multiple.\nReturns:\nThe prompt object.\nlogin\nLog into Braintrust. This will prompt you for your API token, which you can find at\nhttps://www.braintrust.dev/app/token. This method is called automatically by init()\n.\nArguments:\napp_url\n: The URL of the Braintrust App. Defaults to https://www.braintrust.dev.api_key\n: The API key to use. If the parameter is not specified, will try to use theBRAINTRUST_API_KEY\nenvironment variable. If no API key is specified, will prompt the user to login.org_name\n: (Optional) The name of a specific organization to connect to. This is useful if you belong to multiple.force_login\n: Login again, even if you have already logged in (by default, this function will exit quickly if you have already logged in)\nlog\nLog a single event to the current experiment. The event will be batched and uploaded behind the scenes.\nArguments:\n**event\n: Data to be logged. SeeExperiment.log\nfor full details.\nReturns:\nThe id\nof the logged event.\nsummarize\nSummarize the current experiment, including the scores (compared to the closest reference experiment) and metadata.\nArguments:\nsummarize_scores\n: Whether to summarize the scores. If False, only the metadata will be returned.comparison_experiment_id\n: The experiment to compare against. If None, the most recent experiment on the comparison_commit will be used.\nReturns:\nExperimentSummary\ncurrent_experiment\nReturns the currently-active experiment (set by braintrust.init(...)\n). Returns None if no current experiment has been set.\ncurrent_logger\nReturns the currently-active logger (set by braintrust.init_logger(...)\n). Returns None if no current logger has been set.\ncurrent_span\nReturn the currently-active span for logging (set by running a span under a context manager). If there is no active span, returns a no-op span object, which supports the same interface as spans but does no logging.\nSee Span\nfor full details.\nget_span_parent_object\nMainly for internal use. Return the parent object for starting a span in a global context.\ntraced\nDecorator to trace the wrapped function when used without parentheses.\ntraced\nDecorator to trace the wrapped function when used with arguments.\ntraced\nDecorator to trace the wrapped function. Can either be applied bare (@traced\n) or by providing arguments (@traced(*span_args, **span_kwargs)\n), which will be forwarded to the created span. See Span.start_span\nfor full details on the span arguments.\nIt checks the following (in precedence order):\nand creates a span in the first one that is active. If none of these are active, it returns a no-op span object.\nThe decorator will automatically log the input and output of the wrapped function to the corresponding fields of the created span. Pass the kwarg notrace_io=True\nto the decorator to prevent this.\nUnless a name is explicitly provided in span_args\nor span_kwargs\n, the name of the span will be the name of the decorated function.\nstart_span\nLower-level alternative to @traced\nfor starting a span at the toplevel. It creates a span under the first active object (using the same precedence order as @traced\n), or if parent\nis specified, under the specified parent row, or returns a no-op span object.\nWe recommend running spans bound to a context manager (with start_span\n) to automatically mark them as current and ensure they are terminated. If you wish to start a span outside a context manager, be sure to terminate it with span.end()\n.\nSee Span.start_span\nfor full details.\nflush\nFlush any pending rows to the server.\nObjectFetcher Objects\nfetch\nFetch all records.\nReturns:\nAn iterator over the records.\nAttachment Objects\nRepresents an attachment to be uploaded and the associated metadata.\nAttachment\nobjects can be inserted anywhere in an event, allowing you to\nlog arbitrary file data. The SDK will asynchronously upload the file to\nobject storage and replace the Attachment\nobject with an\nAttachmentReference\n.\n__init__\nConstruct an attachment.\nArguments:\ndata\n: A string representing the path of the file on disk, or abytes\n/bytearray\nwith the file's contents. The caller is responsible for ensuring the file on disk or mutablebytearray\nis not modified until upload is complete.filename\n: The desired name of the file in Braintrust after uploading. This parameter is for visualization purposes only and has no effect on attachment storage.content_type\n: The MIME type of the file.\nreference\nThe object that replaces this Attachment\nat upload time.\ndata\nThe attachment contents. This is a lazy value that will read the attachment contents from disk or memory on first access.\nupload\nOn first access, (1) reads the attachment from disk if needed, (2) authenticates with the data plane to request a signed URL, (3) uploads to object store, and (4) updates the attachment.\nReturns:\nThe attachment status.\ndebug_info\nA human-readable description for logging and debugging.\nReturns:\nThe debug object. The return type is not stable and may change in a future release.\nExternalAttachment Objects\nRepresents an attachment that resides in an external object store and the associated metadata.\nExternalAttachment\nobjects can be inserted anywhere in an event, similar to\nAttachment\nobjects, but they reference files that already exist in an external\nobject store rather than requiring upload. The SDK will replace the ExternalAttachment\nobject with an AttachmentReference\nduring logging.\n__init__\nConstruct an external attachment reference.\nArguments:\nurl\n: A fully qualified URL to the object in the external object store.filename\n: The desired name of the file in Braintrust. This parameter is for visualization purposes only and has no effect on attachment storage.content_type\n: The MIME type of the file.\nreference\nThe object that replaces this Attachment\nat upload time.\ndata\nThe attachment contents. This is a lazy value that will read the attachment contents from the external object store on first access.\nupload\nFor ExternalAttachment, this is a no-op since the data already resides\nin the external object store. It marks the attachment as already uploaded.\nReturns:\nThe attachment status, which will always indicate success.\ndebug_info\nA human-readable description for logging and debugging.\nReturns:\nThe debug object. The return type is not stable and may change in a future release.\nReadonlyAttachment Objects\nA readonly alternative to Attachment\n, which can be used for fetching\nalready-uploaded Attachments.\ndata\nThe attachment contents. This is a lazy value that will read the attachment contents from the object store on first access.\nmetadata\nFetch the attachment metadata, which includes a downloadUrl and a status. This will re-fetch the status each time in case it changes over time.\nstatus\nFetch the attachment upload status. This will re-fetch the status each time in case it changes over time.\nupdate_span\nUpdate a span using the output of span.export()\n. It is important that you only resume updating\nto a span once the original span has been fully written and flushed, since otherwise updates to the span may conflict with the original span.\nArguments:\nexported\n: The output ofspan.export()\n.**event\n: Data to update. SeeExperiment.log\nfor a full list of valid fields.\nspan_components_to_object_id\nUtility function to resolve the object ID of a SpanComponentsV3 object. This function may trigger a login to braintrust if the object ID is encoded lazily.\npermalink\nFormat a permalink to the Braintrust application for viewing the span represented by the provided slug\n.\nLinks can be generated at any time, but they will only become viewable after the span and its root have been flushed to the server and ingested.\nIf you have a Span\nobject, use Span.permalink\ninstead.\nArguments:\nslug\n: The identifier generated fromSpan.export\n.org_name\n: The org name to use. If not provided, the org name will be inferred from the global login state.app_url\n: The app URL to use. If not provided, the app URL will be inferred from the global login state.\nReturns:\nA permalink to the exported span.\nExperiment Objects\nAn experiment is a collection of logged events, such as model inputs and outputs, which represent a snapshot of your application at a particular point in time. An experiment is meant to capture more than just the model you use, and includes the data you use to test, pre- and post- processing code, comparison metrics (scores), and any other metadata you want to include.\nExperiments are associated with a project, and two experiments are meant to be easily comparable via\ntheir input\n. You can change the attributes of the experiments in a project (e.g. scoring functions)\nover time, simply by changing what you log.\nYou should not create Experiment\nobjects directly. Instead, use the braintrust.init()\nmethod.\nlog\nLog a single event to the experiment. The event will be batched and uploaded behind the scenes.\nArguments:\ninput\n: The arguments that uniquely define a test case (an arbitrary, JSON serializable object). Later on, Braintrust will use theinput\nto know whether two test cases are the same between experiments, so they should not contain experiment-specific state. A simple rule of thumb is that if you run the same experiment twice, theinput\nshould be identical.output\n: The output of your application, including post-processing (an arbitrary, JSON serializable object), that allows you to determine whether the result is correct or not. For example, in an app that generates SQL queries, theoutput\nshould be the result of the SQL query generated by the model, not the query itself, because there may be multiple valid queries that answer a single question.expected\n: (Optional) the ground truth value (an arbitrary, JSON serializable object) that you'd compare tooutput\nto determine if youroutput\nvalue is correct or not. Braintrust currently does not compareoutput\ntoexpected\nfor you, since there are so many different ways to do that correctly. Instead, these values are just used to help you navigate your experiments while digging into analyses. However, we may later use these values to re-score outputs or fine-tune your models.error\n: (Optional) The error that occurred, if any. If you use tracing to run an experiment, errors are automatically logged when your code throws an exception.scores\n: A dictionary of numeric values (between 0 and 1) to log. The scores should give you a variety of signals that help you determine how accurate the outputs are compared to what you expect and diagnose failures. For example, a summarization app might have one score that tells you how accurate the summary is, and another that measures the word similarity between the generated and grouth truth summary. The word similarity score could help you determine whether the summarization was covering similar concepts or not. You can use these scores to help you sort, filter, and compare experiments.metadata\n: (Optional) a dictionary with additional data about the test example, model outputs, or just about anything else that's relevant, that you can use to help find and analyze examples later. For example, you could log theprompt\n, example'sid\n, or anything else that would be useful to slice/dice later. The values inmetadata\ncan be any JSON-serializable type, but its keys must be strings.tags\n: (Optional) a list of strings that you can use to filter and group records later.metrics\n: (Optional) a dictionary of metrics to log. The following keys are populated automatically: \"start\", \"end\".id\n: (Optional) a unique identifier for the event. If you don't provide one, BrainTrust will generate one for you.allow_concurrent_with_spans\n: (Optional) in rare cases where you need to log at the top level separately from using spans on the experiment elsewhere, set this to True.dataset_record_id\n: (Deprecated) the id of the dataset record that this event is associated with. This field is required if and only if the experiment is associated with a dataset. This field is unused and will be removed in a future version.\nReturns:\nThe id\nof the logged event.\nlog_feedback\nLog feedback to an event in the experiment. Feedback is used to save feedback scores, set an expected value, or add a comment.\nArguments:\nid\n: The id of the event to log feedback for. This is theid\nreturned bylog\nor accessible as theid\nfield of a span.scores\n: (Optional) a dictionary of numeric values (between 0 and 1) to log. These scores will be merged into the existing scores for the event.expected\n: (Optional) the ground truth value (an arbitrary, JSON serializable object) that you'd compare tooutput\nto determine if youroutput\nvalue is correct or not.tags\n: (Optional) a list of strings that you can use to filter and group records later.comment\n: (Optional) an optional comment string to log about the event.metadata\n: (Optional) a dictionary with additional data about the feedback. If you have auser_id\n, you can log it here and access it in the Braintrust UI. Note, this metadata does not correspond to the main event itself, but rather the audit log attached to the event.source\n: (Optional) the source of the feedback. Must be one of \"external\" (default), \"app\", or \"api\".\nstart_span\nCreate a new toplevel span underneath the experiment. The name defaults to \"root\" and the span type to \"eval\".\nSee Span.start_span\nfor full details\nupdate_span\nUpdate a span in the experiment using its id. It is important that you only update a span once the original span has been fully written and flushed,\nsince otherwise updates to the span may conflict with the original span.\nArguments:\nid\n: The id of the span to update.**event\n: Data to update. SeeExperiment.log\nfor a full list of valid fields.\nsummarize\nSummarize the experiment, including the scores (compared to the closest reference experiment) and metadata.\nArguments:\nsummarize_scores\n: Whether to summarize the scores. If False, only the metadata will be returned.comparison_experiment_id\n: The experiment to compare against. If None, the most recent experiment on the origin's main branch will be used.\nReturns:\nExperimentSummary\nclose\nThis function is deprecated. You can simply remove it from your code.\nflush\nFlush any pending rows to the server.\nReadonlyExperiment Objects\nA read-only view of an experiment, initialized by passing open=True\nto init()\n.\nSpanImpl Objects\nPrimary implementation of the Span\ninterface. See the Span\ninterface for full details on each method.\nWe suggest using one of the various start_span\nmethods, instead of creating Spans directly. See Span.start_span\nfor full details.\nflush\nFlush any pending rows to the server.\nDataset Objects\nA dataset is a collection of records, such as model inputs and outputs, which represent data you can use to evaluate and fine-tune models. You can log production data to datasets, curate them with interesting examples, edit/delete records, and run evaluations against them.\nYou should not create Dataset\nobjects directly. Instead, use the braintrust.init_dataset()\nmethod.\ninsert\nInsert a single record to the dataset. The record will be batched and uploaded behind the scenes. If you pass in an id\n,\nand a record with that id\nalready exists, it will be overwritten (upsert).\nArguments:\ninput\n: The argument that uniquely define an input case (an arbitrary, JSON serializable object).expected\n: The output of your application, including post-processing (an arbitrary, JSON serializable object).tags\n: (Optional) a list of strings that you can use to filter and group records later.metadata\n: (Optional) a dictionary with additional data about the test example, model outputs, or just about anything else that's relevant, that you can use to help find and analyze examples later. For example, you could log theprompt\n, example'sid\n, or anything else that would be useful to slice/dice later. The values inmetadata\ncan be any JSON-serializable type, but its keys must be strings.id\n: (Optional) a unique identifier for the event. If you don't provide one, Braintrust will generate one for you.output\n: (Deprecated) The output of your application. Useexpected\ninstead.\nReturns:\nThe id\nof the logged record.\nupdate\nUpdate fields of a single record in the dataset. The updated fields will be batched and uploaded behind the scenes.\nYou must pass in an id\nof the record to update. Only the fields provided will be updated; other fields will remain unchanged.\nArguments:\nid\n: The unique identifier of the record to update.input\n: (Optional) The new input value for the record (an arbitrary, JSON serializable object).expected\n: (Optional) The new expected output value for the record (an arbitrary, JSON serializable object).tags\n: (Optional) A list of strings to update the tags of the record.metadata\n: (Optional) A dictionary to update the metadata of the record. The values inmetadata\ncan be any JSON-serializable type, but its keys must be strings.\nReturns:\nThe id\nof the updated record.\ndelete\nDelete a record from the dataset.\nArguments:\nid\n: Theid\nof the record to delete.\nsummarize\nSummarize the dataset, including high level metrics about its size and other metadata.\nArguments:\nsummarize_data\n: Whether to summarize the data. If False, only the metadata will be returned.\nReturns:\nDatasetSummary\nclose\nThis function is deprecated. You can simply remove it from your code.\nflush\nFlush any pending rows to the server.\nPrompt Objects\nA prompt object consists of prompt text, a model, and model parameters (such as temperature), which\ncan be used to generate completions or chat messages. The prompt object supports calling .build()\nwhich uses mustache templating to build the prompt with the given formatting options and returns a\nplain dictionary that includes the built prompt and arguments. The dictionary can be passed as\nkwargs to the OpenAI client or modified as you see fit.\nYou should not create Prompt\nobjects directly. Instead, use the braintrust.load_prompt()\nmethod.\nbuild\nBuild the prompt with the given formatting options. The args you pass in will\nbe forwarded to the mustache template that defines the prompt and rendered with\nthe chevron\nlibrary.\nReturns:\nA dictionary that includes the rendered prompt and arguments, that can be passed as kwargs to the OpenAI client.\nLogger Objects\nlog\nLog a single event. The event will be batched and uploaded behind the scenes.\nArguments:\ninput\n: (Optional) the arguments that uniquely define a user input (an arbitrary, JSON serializable object).output\n: (Optional) the output of your application, including post-processing (an arbitrary, JSON serializable object), that allows you to determine whether the result is correct or not. For example, in an app that generates SQL queries, theoutput\nshould be the result of the SQL query generated by the model, not the query itself, because there may be multiple valid queries that answer a single question.expected\n: (Optional) the ground truth value (an arbitrary, JSON serializable object) that you'd compare tooutput\nto determine if youroutput\nvalue is correct or not. Braintrust currently does not compareoutput\ntoexpected\nfor you, since there are so many different ways to do that correctly. Instead, these values are just used to help you navigate while digging into analyses. However, we may later use these values to re-score outputs or fine-tune your models.error\n: (Optional) The error that occurred, if any. If you use tracing to run an experiment, errors are automatically logged when your code throws an exception.tags\n: (Optional) a list of strings that you can use to filter and group records later.scores\n: (Optional) a dictionary of numeric values (between 0 and 1) to log. The scores should give you a variety of signals that help you determine how accurate the outputs are compared to what you expect and diagnose failures. For example, a summarization app might have one score that tells you how accurate the summary is, and another that measures the word similarity between the generated and grouth truth summary. The word similarity score could help you determine whether the summarization was covering similar concepts or not. You can use these scores to help you sort, filter, and compare logs.metadata\n: (Optional) a dictionary with additional data about the test example, model outputs, or just about anything else that's relevant, that you can use to help find and analyze examples later. For example, you could log theprompt\n, example'sid\n, or anything else that would be useful to slice/dice later. The values inmetadata\ncan be any JSON-serializable type, but its keys must be strings.metrics\n: (Optional) a dictionary of metrics to log. The following keys are populated automatically: \"start\", \"end\".id\n: (Optional) a unique identifier for the event. If you don't provide one, BrainTrust will generate one for you.allow_concurrent_with_spans\n: (Optional) in rare cases where you need to log at the top level separately from using spans on the logger elsewhere, set this to True.\nlog_feedback\nLog feedback to an event. Feedback is used to save feedback scores, set an expected value, or add a comment.\nArguments:\nid\n: The id of the event to log feedback for. This is theid\nreturned bylog\nor accessible as theid\nfield of a span.scores\n: (Optional) a dictionary of numeric values (between 0 and 1) to log. These scores will be merged into the existing scores for the event.expected\n: (Optional) the ground truth value (an arbitrary, JSON serializable object) that you'd compare tooutput\nto determine if youroutput\nvalue is correct or not.tags\n: (Optional) a list of strings that you can use to filter and group records later.comment\n: (Optional) an optional comment string to log about the event.metadata\n: (Optional) a dictionary with additional data about the feedback. If you have auser_id\n, you can log it here and access it in the Braintrust UI. Note, this metadata does not correspond to the main event itself, but rather the audit log attached to the event.source\n: (Optional) the source of the feedback. Must be one of \"external\" (default), \"app\", or \"api\".\nstart_span\nCreate a new toplevel span underneath the logger. The name defaults to \"root\" and the span type to \"task\".\nSee Span.start_span\nfor full details\nupdate_span\nUpdate a span in the experiment using its id. It is important that you only update a span once the original span\nhas been fully written and flushed, since otherwise updates to the span may conflict with the original span.\nArguments:\nid\n: The id of the span to update.**event\n: Data to update. SeeExperiment.log\nfor a full list of valid fields.\nexport\nReturn a serialized representation of the logger that can be used to start subspans in other places. See Span.start_span\nfor more details.\nflush\nFlush any pending logs to the server.\nScoreSummary Objects\nSummary of a score's performance.\nname\nName of the score.\nscore\nAverage score across all examples.\nimprovements\nNumber of improvements in the score.\nregressions\nNumber of regressions in the score.\ndiff\nDifference in score between the current and reference experiment.\nMetricSummary Objects\nSummary of a metric's performance.\nname\nName of the metric.\nmetric\nAverage metric across all examples.\nunit\nUnit label for the metric.\nimprovements\nNumber of improvements in the metric.\nregressions\nNumber of regressions in the metric.\ndiff\nDifference in metric between the current and reference experiment.\nExperimentSummary Objects\nSummary of an experiment's scores and metadata.\nproject_name\nName of the project that the experiment belongs to.\nproject_id\nID of the project. May be None\nif the eval was run locally.\nexperiment_id\nID of the experiment. May be None\nif the eval was run locally.\nexperiment_name\nName of the experiment.\nproject_url\nURL to the project's page in the Braintrust app.\nexperiment_url\nURL to the experiment's page in the Braintrust app.\ncomparison_experiment_name\nThe experiment scores are baselined against.\nscores\nSummary of the experiment's scores.\nmetrics\nSummary of the experiment's metrics.\nDataSummary Objects\nSummary of a dataset's data.\nnew_records\nNew or updated records added in this session.\ntotal_records\nTotal records in the dataset.\nDatasetSummary Objects\nSummary of a dataset's scores and metadata.\nproject_name\nName of the project that the dataset belongs to.\ndataset_name\nName of the dataset.\nproject_url\nURL to the project's page in the Braintrust app.\ndataset_url\nURL to the experiment's page in the Braintrust app.\ndata_summary\nSummary of the dataset's data.\nget_prompt_versions\nGet the versions for a specific prompt.\nArguments:\nproject_id\n- The ID of the project to queryprompt_id\n- The ID of the prompt to get versions for\nReturns:\nList of transaction IDs (_xact_id) for entries where audit_data.action is \"upsert\"\nbraintrust.framework\nEvalCase Objects\nAn evaluation case. This is a single input to the evaluation task, along with an optional expected output, metadata, and tags.\nEvalResult Objects\nThe result of an evaluation. This includes the input, expected output, actual output, and metadata.\nEvalHooks Objects\nAn object that can be used to add metadata to an evaluation. This is passed to the task\nfunction.\nmetadata\nThe metadata object for the current evaluation. You can mutate this object to add or remove metadata.\nexpected\nThe expected output for the current evaluation.\nspan\nAccess the span under which the task is run. Also accessible via braintrust.current_span()\ntrial_index\nThe index of the current trial (0-based). This is useful when trial_count > 1.\ntags\nThe tags for the current evaluation. You can mutate this object to add or remove tags.\nmeta\nDEPRECATED: Use the metadata field on the hook directly.\nAdds metadata to the evaluation. This metadata will be logged to the Braintrust. You can pass in metadaa\nas keyword arguments, e.g. hooks.meta(foo=\"bar\")\n.\nEvalScorerArgs Objects\nArguments passed to an evaluator scorer. This includes the input, expected output, actual output, and metadata.\nSyncScorerLike Objects\nProtocol for synchronous scorers that implement the callable interface. This is the most common interface and is used when no async version is available.\nAsyncScorerLike Objects\nProtocol for asynchronous scorers that implement the eval_async interface. The framework will prefer this interface if available.\nBaseExperiment Objects\nUse this to specify that the dataset should actually be the data from a previous (base) experiment. If you do not specify a name, Braintrust will automatically figure out the best base experiment to use based on your git history (or fall back to timestamps).\nname\nThe name of the base experiment to use. If unspecified, Braintrust will automatically figure out the best base using your git history (or fall back to timestamps).\nEvaluator Objects\nAn evaluator is an abstraction that defines an evaluation dataset, a task to run on the dataset, and a set of scorers to evaluate the results of the task. Each method attribute can be synchronous or asynchronous (for optimal performance, it is recommended to provide asynchronous implementations).\nYou should not create Evaluators directly if you plan to use the Braintrust eval framework. Instead, you should\ncreate them using the Eval()\nmethod, which will register them so that braintrust eval ...\ncan find them.\nproject_name\nThe name of the project the eval falls under.\neval_name\nA name that describes the experiment. You do not need to change it each time the experiment runs.\ndata\nReturns an iterator over the evaluation dataset. Each element of the iterator should be an EvalCase\nor a dict\nwith the same fields as an EvalCase\n(input\n, expected\n, metadata\n).\ntask\nRuns the evaluation task on a single input. The hooks\nobject can be used to add metadata to the evaluation.\nscores\nA list of scorers to evaluate the results of the task. Each scorer can be a Scorer object or a function\nthat takes input\n, output\n, and expected\narguments and returns a Score\nobject. The function can be async.\nexperiment_name\nOptional experiment name. If not specified, a name will be generated automatically.\nmetadata\nA dictionary with additional data about the test example, model outputs, or just about anything else that's\nrelevant, that you can use to help find and analyze examples later. For example, you could log the prompt\n,\nexample's id\n, or anything else that would be useful to slice/dice later. The values in metadata\ncan be any\nJSON-serializable type, but its keys must be strings.\ntrial_count\nThe number of times to run the evaluator per input. This is useful for evaluating applications that have non-deterministic behavior and gives you both a stronger aggregate measure and a sense of the variance in the results.\nis_public\nWhether the experiment should be public. Defaults to false.\nupdate\nWhether to update an existing experiment with experiment_name\nif one exists. Defaults to false.\ntimeout\nThe duration, in seconds, after which to time out the evaluation. Defaults to None, in which case there is no timeout.\nmax_concurrency\nThe maximum number of tasks/scorers that will be run concurrently. Defaults to None, in which case there is no max concurrency.\nproject_id\nIf specified, uses the given project ID instead of the evaluator's name to identify the project.\nbase_experiment_name\nAn optional experiment name to use as a base. If specified, the new experiment will be summarized and compared to this experiment.\nbase_experiment_id\nAn optional experiment id to use as a base. If specified, the new experiment will be summarized and\ncompared to this experiment. This takes precedence over base_experiment_name\nif specified.\ngit_metadata_settings\nOptional settings for collecting git metadata. By default, will collect all git metadata fields allowed in org-level settings.\nrepo_info\nOptionally explicitly specify the git metadata for this experiment. This\ntakes precedence over git_metadata_settings\nif specified.\nerror_score_handler\nOptionally supply a custom function to specifically handle score values when tasks or scoring functions have errored.\nA default implementation is exported as default_error_score_handler\nwhich will log a 0 score to the root span for any scorer that was not run.\ndescription\nAn optional description for the experiment.\nsummarize_scores\nWhether to summarize the scores of the experiment after it has run.\nReporterDef Objects\nA reporter takes an evaluator and its result and returns a report.\nname\nThe name of the reporter.\nreport_eval\nA function that takes an evaluator and its result and returns a report.\nreport_run\nA function that takes all evaluator results and returns a boolean indicating whether the run was successful.\nIf you return false, the braintrust eval\ncommand will exit with a non-zero status code.\nEvalAsync\nA function you can use to define an evaluator. This is a convenience wrapper around the Evaluator\nclass.\nUse this function over Eval()\nwhen you are running in an async context, including in a Jupyter notebook.\nExample:\nArguments:\nname\n: The name of the evaluator. This corresponds to a project name in Braintrust.data\n: Returns an iterator over the evaluation dataset. Each element of the iterator should be aEvalCase\n.task\n: Runs the evaluation task on a single input. Thehooks\nobject can be used to add metadata to the evaluation.scores\n: A list of scorers to evaluate the results of the task. Each scorer can be a Scorer object or a function that takes anEvalScorerArgs\nobject and returns aScore\nobject.experiment_name\n: (Optional) Experiment name. If not specified, a name will be generated automatically.trial_count\n: The number of times to run the evaluator per input. This is useful for evaluating applications that have non-deterministic behavior and gives you both a stronger aggregate measure and a sense of the variance in the results.metadata\n: (Optional) A dictionary with additional data about the test example, model outputs, or just about anything else that's relevant, that you can use to help find and analyze examples later. For example, you could log theprompt\n, example'sid\n, or anything else that would be useful to slice/dice later. The values inmetadata\ncan be any JSON-serializable type, but its keys must be strings.is_public\n: (Optional) Whether the experiment should be public. Defaults to false.reporter\n: (Optional) A reporter that takes an evaluator and its result and returns a report.timeout\n: (Optional) The duration, in seconds, after which to time out the evaluation. Defaults to None, in which case there is no timeout.project_id\n: (Optional) If specified, uses the given project ID instead of the evaluator's name to identify the project.base_experiment_name\n: An optional experiment name to use as a base. If specified, the new experiment will be summarized and compared to this experiment.base_experiment_id\n: An optional experiment id to use as a base. If specified, the new experiment will be summarized and compared to this experiment. This takes precedence overbase_experiment_name\nif specified.git_metadata_settings\n: Optional settings for collecting git metadata. By default, will collect all git metadata fields allowed in org-level settings.repo_info\n: Optionally explicitly specify the git metadata for this experiment. This takes precedence overgit_metadata_settings\nif specified.error_score_handler\n: Optionally supply a custom function to specifically handle score values when tasks or scoring functions have errored.description\n: An optional description for the experiment.summarize_scores\n: Whether to summarize the scores of the experiment after it has run.no_send_logs\n: Do not send logs to Braintrust. When True, the evaluation runs locally and builds a local summary instead of creating an experiment. Defaults to False.\nReturns:\nAn EvalResultWithSummary\nobject, which contains all results and a summary.\nEval\nA function you can use to define an evaluator. This is a convenience wrapper around the Evaluator\nclass.\nFor callers running in an async context, use EvalAsync()\ninstead.\nExample:\nArguments:\nname\n: The name of the evaluator. This corresponds to a project name in Braintrust.data\n: Returns an iterator over the evaluation dataset. Each element of the iterator should be aEvalCase\n.task\n: Runs the evaluation task on a single input. Thehooks\nobject can be used to add metadata to the evaluation.scores\n: A list of scorers to evaluate the results of the task. Each scorer can be a Scorer object or a function that takes anEvalScorerArgs\nobject and returns aScore\nobject.experiment_name\n: (Optional) Experiment name. If not specified, a name will be generated automatically.trial_count\n: The number of times to run the evaluator per input. This is useful for evaluating applications that have non-deterministic behavior and gives you both a stronger aggregate measure and a sense of the variance in the results.metadata\n: (Optional) A dictionary with additional data about the test example, model outputs, or just about anything else that's relevant, that you can use to help find and analyze examples later. For example, you could log theprompt\n, example'sid\n, or anything else that would be useful to slice/dice later. The values inmetadata\ncan be any JSON-serializable type, but its keys must be strings.is_public\n: (Optional) Whether the experiment should be public. Defaults to false.reporter\n: (Optional) A reporter that takes an evaluator and its result and returns a report.timeout\n: (Optional) The duration, in seconds, after which to time out the evaluation. Defaults to None, in which case there is no timeout.project_id\n: (Optional) If specified, uses the given project ID instead of the evaluator's name to identify the project.base_experiment_name\n: An optional experiment name to use as a base. If specified, the new experiment will be summarized and compared to this experiment.base_experiment_id\n: An optional experiment id to use as a base. If specified, the new experiment will be summarized and compared to this experiment. This takes precedence overbase_experiment_name\nif specified.git_metadata_settings\n: Optional settings for collecting git metadata. By default, will collect all git metadata fields allowed in org-level settings.repo_info\n: Optionally explicitly specify the git metadata for this experiment. This takes precedence overgit_metadata_settings\nif specified.error_score_handler\n: Optionally supply a custom function to specifically handle score values when tasks or scoring functions have errored.description\n: An optional description for the experiment.summarize_scores\n: Whether to summarize the scores of the experiment after it has run.no_send_logs\n: Do not send logs to Braintrust. When True, the evaluation runs locally and builds a local summary instead of creating an experiment. Defaults to False.\nReturns:\nAn EvalResultWithSummary\nobject, which contains all results and a summary.\nReporter\nA function you can use to define a reporter. This is a convenience wrapper around the ReporterDef\nclass.\nExample:\nArguments:\nname\n: The name of the reporter.report_eval\n: A function that takes an evaluator and its result and returns a report.report_run\n: A function that takes all evaluator results and returns a boolean indicating whether the run was successful.\nset_thread_pool_max_workers\nSet the maximum number of threads to use for running evaluators. By default, this is the number of CPUs on the machine.\nrun_evaluator\nWrapper on _run_evaluator_internal that times out execution after evaluator.timeout.\nbraintrust.functions.stream\nThis module provides classes and functions for handling Braintrust streams.\nA Braintrust stream is a wrapper around a generator of BraintrustStreamChunk\n,\nwith utility methods to make them easy to log and convert into various formats.\nBraintrustTextChunk Objects\nA chunk of text data from a Braintrust stream.\nBraintrustJsonChunk Objects\nA chunk of JSON data from a Braintrust stream.\nBraintrustErrorChunk Objects\nAn error chunk from a Braintrust stream.\nBraintrustConsoleChunk Objects\nA console chunk from a Braintrust stream.\nBraintrustProgressChunk Objects\nA progress chunk from a Braintrust stream.\nBraintrustInvokeError Objects\nAn error that occurs during a Braintrust stream.\nBraintrustStream Objects\nA Braintrust stream. This is a wrapper around a generator of BraintrustStreamChunk\n,\nwith utility methods to make them easy to log and convert into various formats.\n__init__\nInitialize a BraintrustStream.\nArguments:\nbase_stream\n- Either an SSEClient or a list of BraintrustStreamChunks.\ncopy\nCopy the stream. This returns a new stream that shares the same underlying\ngenerator (via tee\n). Since generators are consumed in Python, use copy()\nif you\nneed to use the stream multiple times.\nReturns:\nBraintrustStream\n- A new stream that you can independently consume.\nfinal_value\nGet the final value of the stream. The final value is the concatenation of all the chunks in the stream, deserialized into a string or object, depending on the value's type.\nThis function consumes the stream, so if you need to use the stream multiple\ntimes, you should call copy()\nfirst.\nReturns:\nThe final value of the stream.\n__iter__\nIterate over the stream chunks.\nYields:\nBraintrustStreamChunk\n- The next chunk in the stream.\nparse_stream\nParse a BraintrustStream into its final value.\nArguments:\nstream\n- The BraintrustStream to parse.\nReturns:\nThe final value of the stream.\nbraintrust.functions.invoke\ninvoke\nInvoke a Braintrust function, returning a BraintrustStream\nor the value as a plain\nPython object.\nArguments:\ninput\n- The input to the function. This will be logged as theinput\nfield in the span.messages\n- Additional OpenAI-style messages to add to the prompt (only works for llm functions).metadata\n- Additional metadata to add to the span. This will be logged as themetadata\nfield in the span. It will also be available as the {{metadata}} field in the prompt and as themetadata\nargument to the function.tags\n- Tags to add to the span. This will be logged as thetags\nfield in the span.parent\n- The parent of the function. This can be an existing span, logger, or experiment, or the output of.export()\nif you are distributed tracing. If unspecified, will use the same semantics astraced()\nto determine the parent and no-op if not in a tracing context.stream\n- Whether to stream the function's output. If True, the function will return aBraintrustStream\n, otherwise it will return the output of the function as a JSON object.mode\n- The response shape of the function if returning tool calls. If \"auto\", will return a string if the function returns a string, and a JSON object otherwise. If \"parallel\", will return an array of JSON objects with one object per tool call.strict\n- Whether to use strict mode for the function. If true, the function will throw an error if the variable names in the prompt do not match the input keys.org_name\n- The name of the Braintrust organization to use.api_key\n- The API key to use for authentication.app_url\n- The URL of the Braintrust application.force_login\n- Whether to force a new login even if already logged in.function_id\n- The ID of the function to invoke.version\n- The version of the function to invoke.prompt_session_id\n- The ID of the prompt session to invoke the function from.prompt_session_function_id\n- The ID of the function in the prompt session to invoke.project_name\n- The name of the project containing the function to invoke.slug\n- The slug of the function to invoke.global_function\n- The name of the global function to invoke.\nReturns:\nThe output of the function. If stream\nis True, returns a BraintrustStream\n,\notherwise returns the output as a Python object.\ninit_function\nCreates a function that can be used as either a task or scorer in the Eval framework.\nWhen used as a task, it will invoke the specified Braintrust function with the input. When used as a scorer, it will invoke the function with the scorer arguments.\nExample:\nArguments:\nproject_name\n: The name of the project containing the function.slug\n: The slug of the function to invoke.version\n: Optional version of the function to use. Defaults to latest.\nReturns:\nA function that can be used as a task or scorer.", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "TypeScript"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "Python"}, {"href": "https://www.braintrust.dev/docs/reference/api", "anchor": "API"}, {"href": "https://www.braintrust.dev/docs/reference/autoevals", "anchor": "Autoevals library"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "BTQL query syntax"}, {"href": "https://www.braintrust.dev/docs/reference/streaming", "anchor": "Streaming"}, {"href": "https://www.braintrust.dev/docs/reference/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/reference/reasoning", "anchor": "Reasoning"}, {"href": "https://www.braintrust.dev/docs/reference/organizations", "anchor": "Organizations"}, {"href": "https://www.braintrust.dev/docs/reference/mcp", "anchor": "Model Context Protocol (MCP)"}, {"href": "https://www.braintrust.dev/docs/reference/object-links", "anchor": "Sharing via URL"}, {"href": "https://www.braintrust.dev/docs/reference/glossary", "anchor": "Glossary"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "braintrust"}, {"href": "https://braintrust.dev/", "anchor": "Braintrust"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "Quickstart"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "API Reference"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "braintrust.logger"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "Exportable Objects"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "export"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "Span Objects"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "id"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "log"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "log_feedback"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "start_span"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "export"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "link"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "permalink"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "end"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "flush"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "close"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "set_attributes"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "set_current"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "set_http_adapter"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "RetryRequestExceptionsAdapter Objects"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "init"}, {"href": "https://www.braintrust.dev", "anchor": "https://www.braintrust.dev"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "init_experiment"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "init_dataset"}, {"href": "https://www.braintrust.dev", "anchor": "https://www.braintrust.dev"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "init_logger"}, {"href": "https://www.braintrust.dev", "anchor": "https://www.braintrust.dev"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "load_prompt"}, {"href": "https://www.braintrust.dev", "anchor": "https://www.braintrust.dev"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "login"}, {"href": "https://www.braintrust.dev/app/token", "anchor": "https://www.braintrust.dev/app/token"}, {"href": "https://www.braintrust.dev", "anchor": "https://www.braintrust.dev"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "log"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "summarize"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "current_experiment"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "current_logger"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "current_span"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "get_span_parent_object"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "traced"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "traced"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "traced"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "start_span"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "flush"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "ObjectFetcher Objects"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "fetch"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "Attachment Objects"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "__init__"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "reference"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "data"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "upload"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "debug_info"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "ExternalAttachment Objects"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "__init__"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "reference"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "data"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "upload"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "debug_info"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "ReadonlyAttachment Objects"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "data"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "metadata"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "status"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "update_span"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "span_components_to_object_id"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "permalink"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "Experiment Objects"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "log"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "log_feedback"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "start_span"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "update_span"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "summarize"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "close"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "flush"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "ReadonlyExperiment Objects"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "SpanImpl Objects"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "flush"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "Dataset Objects"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "insert"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "update"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "delete"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "summarize"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "close"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "flush"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "Prompt Objects"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "build"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "Logger Objects"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "log"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "log_feedback"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "start_span"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "update_span"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "export"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "flush"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "ScoreSummary Objects"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "name"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "score"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "improvements"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "regressions"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "diff"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "MetricSummary Objects"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "name"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "metric"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "unit"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "improvements"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "regressions"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "diff"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "ExperimentSummary Objects"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "project_name"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "project_id"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "experiment_id"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "experiment_name"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "project_url"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "experiment_url"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "comparison_experiment_name"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "scores"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "metrics"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "DataSummary Objects"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "new_records"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "total_records"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "DatasetSummary Objects"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "project_name"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "dataset_name"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "project_url"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "dataset_url"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "data_summary"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "get_prompt_versions"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "braintrust.framework"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "EvalCase Objects"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "EvalResult Objects"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "EvalHooks Objects"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "metadata"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "expected"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "span"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "trial_index"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "tags"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "meta"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "EvalScorerArgs Objects"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "SyncScorerLike Objects"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "AsyncScorerLike Objects"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "BaseExperiment Objects"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "name"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "Evaluator Objects"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "project_name"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "eval_name"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "data"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "task"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "scores"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "experiment_name"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "metadata"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "trial_count"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "is_public"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "update"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "timeout"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "max_concurrency"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "project_id"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "base_experiment_name"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "base_experiment_id"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "git_metadata_settings"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "repo_info"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "error_score_handler"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "description"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "summarize_scores"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "ReporterDef Objects"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "name"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "report_eval"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "report_run"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "EvalAsync"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "Eval"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "Reporter"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "set_thread_pool_max_workers"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "run_evaluator"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "braintrust.functions.stream"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "BraintrustTextChunk Objects"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "BraintrustJsonChunk Objects"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "BraintrustErrorChunk Objects"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "BraintrustConsoleChunk Objects"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "BraintrustProgressChunk Objects"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "BraintrustInvokeError Objects"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "BraintrustStream Objects"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "__init__"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "copy"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "final_value"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "__iter__"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "parse_stream"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "braintrust.functions.invoke"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "invoke"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "init_function"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "braintrust"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "Quickstart"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "API Reference"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "braintrust.logger"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "Exportable Objects"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "export"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "Span Objects"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "id"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "log"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "log_feedback"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "start_span"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "export"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "link"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "permalink"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "end"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "flush"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "close"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "set_attributes"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "set_current"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "set_http_adapter"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "RetryRequestExceptionsAdapter Objects"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "init"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "init_experiment"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "init_dataset"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "init_logger"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "load_prompt"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "login"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "log"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "summarize"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "current_experiment"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "current_logger"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "current_span"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "get_span_parent_object"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "traced"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "traced"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "traced"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "start_span"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "flush"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "ObjectFetcher Objects"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "fetch"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "Attachment Objects"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "__init__"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "reference"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "data"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "upload"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "debug_info"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "ExternalAttachment Objects"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "__init__"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "reference"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "data"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "upload"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "debug_info"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "ReadonlyAttachment Objects"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "data"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "metadata"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "status"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "update_span"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "span_components_to_object_id"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "permalink"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "Experiment Objects"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "log"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "log_feedback"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "start_span"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "update_span"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "summarize"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "close"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "flush"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "ReadonlyExperiment Objects"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "SpanImpl Objects"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "flush"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "Dataset Objects"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "insert"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "update"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "delete"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "summarize"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "close"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "flush"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "Prompt Objects"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "build"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "Logger Objects"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "log"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "log_feedback"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "start_span"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "update_span"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "export"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "flush"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "ScoreSummary Objects"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "name"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "score"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "improvements"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "regressions"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "diff"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "MetricSummary Objects"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "name"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "metric"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "unit"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "improvements"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "regressions"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "diff"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "ExperimentSummary Objects"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "project_name"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "project_id"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "experiment_id"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "experiment_name"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "project_url"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "experiment_url"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "comparison_experiment_name"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "scores"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "metrics"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "DataSummary Objects"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "new_records"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "total_records"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "DatasetSummary Objects"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "project_name"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "dataset_name"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "project_url"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "dataset_url"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "data_summary"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "get_prompt_versions"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "braintrust.framework"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "EvalCase Objects"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "EvalResult Objects"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "EvalHooks Objects"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "metadata"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "expected"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "span"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "trial_index"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "tags"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "meta"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "EvalScorerArgs Objects"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "SyncScorerLike Objects"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "AsyncScorerLike Objects"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "BaseExperiment Objects"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "name"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "Evaluator Objects"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "project_name"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "eval_name"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "data"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "task"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "scores"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "experiment_name"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "metadata"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "trial_count"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "is_public"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "update"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "timeout"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "max_concurrency"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "project_id"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "base_experiment_name"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "base_experiment_id"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "git_metadata_settings"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "repo_info"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "error_score_handler"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "description"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "summarize_scores"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "ReporterDef Objects"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "name"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "report_eval"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "report_run"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "EvalAsync"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "Eval"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "Reporter"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "set_thread_pool_max_workers"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "run_evaluator"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "braintrust.functions.stream"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "BraintrustTextChunk Objects"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "BraintrustJsonChunk Objects"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "BraintrustErrorChunk Objects"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "BraintrustConsoleChunk Objects"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "BraintrustProgressChunk Objects"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "BraintrustInvokeError Objects"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "BraintrustStream Objects"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "__init__"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "copy"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "final_value"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "__iter__"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "parse_stream"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "braintrust.functions.invoke"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "invoke"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "init_function"}], "depth": 4}, "https://www.braintrust.dev/docs/libs/nodejs": {"url": "https://www.braintrust.dev/docs/libs/nodejs", "title": "TypeScript - Docs - Reference - Braintrust", "text": "braintrust\nAn isomorphic JS library for working with Braintrust. This library contains functionality for running evaluations, logging completions, loading and invoking functions, and more.\nbraintrust\nis distributed as a library on NPM.\nIt is also open source and available on GitHub.\nQuickstart\nInstall the library with npm (or yarn).\nThen, create a file like hello.eval.ts\nwith the following content:\nFinally, run the script with npx braintrust eval hello.eval.ts\n.\nClasses\n- Attachment\n- BaseAttachment\n- BraintrustState\n- BraintrustStream\n- CodeFunction\n- CodePrompt\n- Dataset\n- EvalResultWithSummary\n- Experiment\n- ExternalAttachment\n- FailedHTTPResponse\n- LazyValue\n- Logger\n- NoopSpan\n- Project\n- Prompt\n- PromptBuilder\n- ReadonlyAttachment\n- ReadonlyExperiment\n- ScorerBuilder\n- SpanImpl\n- TestBackgroundLogger\n- ToolBuilder\nInterfaces\n- AttachmentParams\n- BackgroundLoggerOpts\n- DataSummary\n- DatasetSummary\n- EvalHooks\n- Evaluator\n- ExperimentSummary\n- Exportable\n- ExternalAttachmentParams\n- InvokeFunctionArgs\n- LogOptions\n- LoginOptions\n- MetricSummary\n- ObjectMetadata\n- ParentExperimentIds\n- ParentProjectLogIds\n- ReporterBody\n- ScoreSummary\n- Span\nNamespaces\nFunctions\nBaseExperiment\n\u25b8 BaseExperiment<Input\n, Expected\n, Metadata\n>(options?\n): BaseExperiment\n<Input\n, Expected\n, Metadata\n>\nUse this to specify that the dataset should actually be the data from a previous (base) experiment. If you do not specify a name, Braintrust will automatically figure out the best base experiment to use based on your git history (or fall back to timestamps).\nType parameters\nParameters\nReturns\nBaseExperiment\n<Input\n, Expected\n, Metadata\n>\nEval\n\u25b8 Eval<Input\n, Output\n, Expected\n, Metadata\n, EvalReport\n, Parameters\n>(name\n, evaluator\n, reporterOrOpts?\n): Promise\n<EvalResultWithSummary\n<Input\n, Output\n, Expected\n, Metadata\n>>\nType parameters\nParameters\nReturns\nPromise\n<EvalResultWithSummary\n<Input\n, Output\n, Expected\n, Metadata\n>>\nReporter\n\u25b8 Reporter<EvalReport\n>(name\n, reporter\n): ReporterDef\n<EvalReport\n>\nType parameters\nParameters\nReturns\nReporterDef\n<EvalReport\n>\nbuildLocalSummary\n\u25b8 buildLocalSummary(evaluator\n, results\n): ExperimentSummary\nParameters\nReturns\ncreateFinalValuePassThroughStream\n\u25b8 createFinalValuePassThroughStream<T\n>(onFinal\n, onError\n): TransformStream\n<T\n, BraintrustStreamChunk\n>\nCreate a stream that passes through the final value of the stream. This is\nused to implement BraintrustStream.finalValue()\n.\nType parameters\nParameters\nReturns\nTransformStream\n<T\n, BraintrustStreamChunk\n>\nA new stream that passes through the final value of the stream.\ncurrentExperiment\n\u25b8 currentExperiment(options?\n): Experiment\n| undefined\nReturns the currently-active experiment (set by init). Returns undefined if no current experiment has been set.\nParameters\nReturns\nExperiment\n| undefined\ncurrentLogger\n\u25b8 currentLogger<IsAsyncFlush\n>(options?\n): Logger\n<IsAsyncFlush\n> | undefined\nReturns the currently-active logger (set by initLogger). Returns undefined if no current logger has been set.\nType parameters\nParameters\nReturns\nLogger\n<IsAsyncFlush\n> | undefined\ncurrentSpan\n\u25b8 currentSpan(options?\n): Span\nReturn the currently-active span for logging (set by one of the traced\nmethods). If there is no active span, returns a no-op span object, which supports the same interface as spans but does no logging.\nSee Span for full details.\nParameters\nReturns\ndefaultErrorScoreHandler\n\u25b8 defaultErrorScoreHandler(args\n): undefined\n| void\n| Record\n<string\n, number\n>\nParameters\nReturns\nundefined\n| void\n| Record\n<string\n, number\n>\ndeserializePlainStringAsJSON\n\u25b8 deserializePlainStringAsJSON(s\n): { error\n: undefined\n= undefined; value\n: any\n} | { error\n: unknown\n= e; value\n: string\n= s }\nParameters\nReturns\n{ error\n: undefined\n= undefined; value\n: any\n} | { error\n: unknown\n= e; value\n: string\n= s }\ndevNullWritableStream\n\u25b8 devNullWritableStream(): WritableStream\nReturns\nWritableStream\nflush\n\u25b8 flush(options?\n): Promise\n<void\n>\nFlush any pending rows to the server.\nParameters\nReturns\nPromise\n<void\n>\ngetSpanParentObject\n\u25b8 getSpanParentObject<IsAsyncFlush\n>(options?\n): Span\n| Experiment\n| Logger\n<IsAsyncFlush\n>\nMainly for internal use. Return the parent object for starting a span in a global context.\nType parameters\nParameters\nReturns\nSpan\n| Experiment\n| Logger\n<IsAsyncFlush\n>\ninit\n\u25b8 init<IsOpen\n>(options\n): InitializedExperiment\n<IsOpen\n>\nLog in, and then initialize a new experiment in a specified project. If the project does not exist, it will be created.\nType parameters\nParameters\nReturns\nInitializedExperiment\n<IsOpen\n>\nThe newly created Experiment.\n\u25b8 init<IsOpen\n>(project\n, options?\n): InitializedExperiment\n<IsOpen\n>\nLegacy form of init\nwhich accepts the project name as the first parameter,\nseparately from the remaining options. See init(options)\nfor full details.\nType parameters\nParameters\nReturns\nInitializedExperiment\n<IsOpen\n>\ninitDataset\n\u25b8 initDataset<IsLegacyDataset\n>(options\n): Dataset\n<IsLegacyDataset\n>\nCreate a new dataset in a specified project. If the project does not exist, it will be created.\nType parameters\nParameters\nReturns\nDataset\n<IsLegacyDataset\n>\nThe newly created Dataset.\n\u25b8 initDataset<IsLegacyDataset\n>(project\n, options?\n): Dataset\n<IsLegacyDataset\n>\nLegacy form of initDataset\nwhich accepts the project name as the first\nparameter, separately from the remaining options.\nSee initDataset(options)\nfor full details.\nType parameters\nParameters\nReturns\nDataset\n<IsLegacyDataset\n>\ninitExperiment\n\u25b8 initExperiment<IsOpen\n>(options\n): InitializedExperiment\n<IsOpen\n>\nAlias for init(options).\nType parameters\nParameters\nReturns\nInitializedExperiment\n<IsOpen\n>\n\u25b8 initExperiment<IsOpen\n>(project\n, options?\n): InitializedExperiment\n<IsOpen\n>\nAlias for init(project, options).\nType parameters\nParameters\nReturns\nInitializedExperiment\n<IsOpen\n>\ninitFunction\n\u25b8 initFunction(options\n): (input\n: any\n) => Promise\n<any\n>\nCreates a function that can be used as a task or scorer in the Braintrust evaluation framework. The returned function wraps a Braintrust function and can be passed directly to Eval().\nWhen used as a task:\nWhen used as a scorer:\nParameters\nReturns\nfn\nA function that can be used as a task or scorer in Eval().\n\u25b8 (input\n): Promise\n<any\n>\nParameters\nReturns\nPromise\n<any\n>\ninitLogger\n\u25b8 initLogger<IsAsyncFlush\n>(options?\n): Logger\n<IsAsyncFlush\n>\nCreate a new logger in a specified project. If the project does not exist, it will be created.\nType parameters\nParameters\nReturns\nLogger\n<IsAsyncFlush\n>\nThe newly created Logger.\ninvoke\n\u25b8 invoke<Input\n, Output\n, Stream\n>(args\n): Promise\n<InvokeReturn\n<Stream\n, Output\n>>\nInvoke a Braintrust function, returning a BraintrustStream\nor the value as a plain\nJavascript object.\nType parameters\nParameters\nReturns\nPromise\n<InvokeReturn\n<Stream\n, Output\n>>\nThe output of the function.\nloadPrompt\n\u25b8 loadPrompt(options\n): Promise\n<Prompt\n<true\n, true\n>>\nLoad a prompt from the specified project.\nParameters\nReturns\nPromise\n<Prompt\n<true\n, true\n>>\nThe prompt object.\nThrows\nIf the prompt is not found.\nThrows\nIf multiple prompts are found with the same slug in the same project (this should never happen).\nExample\nlog\n\u25b8 log(event\n): string\nLog a single event to the current experiment. The event will be batched and uploaded behind the scenes.\nParameters\nReturns\nstring\nThe id\nof the logged event.\nlogError\n\u25b8 logError(span\n, error\n): void\nParameters\nReturns\nvoid\nlogin\n\u25b8 login(options?\n): Promise\n<BraintrustState\n>\nLog into Braintrust. This will prompt you for your API token, which you can find at\nhttps://www.braintrust.dev/app/token. This method is called automatically by init()\n.\nParameters\nReturns\nPromise\n<BraintrustState\n>\nloginToState\n\u25b8 loginToState(options?\n): Promise\n<BraintrustState\n>\nParameters\nReturns\nPromise\n<BraintrustState\n>\nnewId\n\u25b8 newId(): string\nReturns\nstring\nparseCachedHeader\n\u25b8 parseCachedHeader(value\n): number\n| undefined\nParameters\nReturns\nnumber\n| undefined\npermalink\n\u25b8 permalink(slug\n, opts?\n): Promise\n<string\n>\nFormat a permalink to the Braintrust application for viewing the span\nrepresented by the provided slug\n.\nLinks can be generated at any time, but they will only become viewable after the span and its root have been flushed to the server and ingested.\nIf you have a Span\nobject, use Span.link instead.\nParameters\nReturns\nPromise\n<string\n>\nA permalink to the exported span.\npromptDefinitionToPromptData\n\u25b8 promptDefinitionToPromptData(promptDefinition\n, rawTools?\n): PromptData\nParameters\nReturns\nPromptData\nrenderMessage\n\u25b8 renderMessage<T\n>(render\n, message\n): T\nType parameters\nParameters\nReturns\nT\nrenderPromptParams\n\u25b8 renderPromptParams(params\n, args\n, options\n): ModelParams\n| undefined\nParameters\nReturns\nModelParams\n| undefined\nreportFailures\n\u25b8 reportFailures<Input\n, Output\n, Expected\n, Metadata\n>(evaluator\n, failingResults\n, \u00abdestructured\u00bb\n): void\nType parameters\nParameters\nReturns\nvoid\nrunEvaluator\n\u25b8 runEvaluator(experiment\n, evaluator\n, progressReporter\n, filters\n, stream\n, parameters?\n): Promise\n<EvalResultWithSummary\n<any\n, any\n, any\n, any\n>>\nParameters\nReturns\nPromise\n<EvalResultWithSummary\n<any\n, any\n, any\n, any\n>>\nsetFetch\n\u25b8 setFetch(fetch\n): void\nSet the fetch implementation to use for requests. You can specify it here,\nor when you call login\n.\nParameters\nReturns\nvoid\nspanComponentsToObjectId\n\u25b8 spanComponentsToObjectId(\u00abdestructured\u00bb\n): Promise\n<string\n>\nParameters\nReturns\nPromise\n<string\n>\nstartSpan\n\u25b8 startSpan<IsAsyncFlush\n>(args?\n): Span\nLower-level alternative to traced\n. This allows you to start a span yourself, and can be useful in situations\nwhere you cannot use callbacks. However, spans started with startSpan\nwill not be marked as the \"current span\",\nso currentSpan()\nand traced()\nwill be no-ops. If you want to mark a span as current, use traced\ninstead.\nSee traced for full details.\nType parameters\nParameters\nReturns\nsummarize\n\u25b8 summarize(options?\n): Promise\n<ExperimentSummary\n>\nSummarize the current experiment, including the scores (compared to the closest reference experiment) and metadata.\nParameters\nReturns\nPromise\n<ExperimentSummary\n>\nA summary of the experiment, including the scores (compared to the closest reference experiment) and metadata.\ntraceable\n\u25b8 traceable<F\n, IsAsyncFlush\n>(fn\n, args?\n): IsAsyncFlush\nextends false\n? (...args\n: Parameters\n<F\n>) => Promise\n<Awaited\n<ReturnType\n<F\n>>> : F\nA synonym for wrapTraced\n. If you're porting from systems that use traceable\n, you can use this to\nmake your codebase more consistent.\nType parameters\nParameters\nReturns\nIsAsyncFlush\nextends false\n? (...args\n: Parameters\n<F\n>) => Promise\n<Awaited\n<ReturnType\n<F\n>>> : F\ntraced\n\u25b8 traced<IsAsyncFlush\n, R\n>(callback\n, args?\n): PromiseUnless\n<IsAsyncFlush\n, R\n>\nToplevel function for starting a span. It checks the following (in precedence order):\n- Currently-active span\n- Currently-active experiment\n- Currently-active logger\nand creates a span under the first one that is active. Alternatively, if parent\nis specified, it creates a span under the specified parent row. If none of these are active, it returns a no-op span object.\nSee Span.traced for full details.\nType parameters\nParameters\nReturns\nPromiseUnless\n<IsAsyncFlush\n, R\n>\nupdateSpan\n\u25b8 updateSpan(\u00abdestructured\u00bb\n): void\nUpdate a span using the output of span.export()\n. It is important that you only resume updating\nto a span once the original span has been fully written and flushed, since otherwise updates to\nthe span may conflict with the original span.\nParameters\nReturns\nvoid\nwithCurrent\n\u25b8 withCurrent<R\n>(span\n, callback\n, state?\n): R\nRuns the provided callback with the span as the current span.\nType parameters\nParameters\nReturns\nR\nwithDataset\n\u25b8 withDataset<R\n, IsLegacyDataset\n>(project\n, callback\n, options?\n): R\nType parameters\nParameters\nReturns\nR\nDeprecated\nUse initDataset instead.\nwithExperiment\n\u25b8 withExperiment<R\n>(project\n, callback\n, options?\n): R\nType parameters\nParameters\nReturns\nR\nDeprecated\nUse init instead.\nwithLogger\n\u25b8 withLogger<IsAsyncFlush\n, R\n>(callback\n, options?\n): R\nType parameters\nParameters\nReturns\nR\nDeprecated\nUse initLogger instead.\nwithParent\n\u25b8 withParent<R\n>(parent\n, callback\n, state?\n): R\nType parameters\nParameters\nReturns\nR\nwrapAISDKModel\n\u25b8 wrapAISDKModel<T\n>(model\n): T\nWrap an ai-sdk model (created with .chat()\n, .completion()\n, etc.) to add tracing. If Braintrust is\nnot configured, this is a no-op\nType parameters\nParameters\nReturns\nT\nThe wrapped object.\nwrapAnthropic\n\u25b8 wrapAnthropic<T\n>(anthropic\n): T\nWrap an Anthropic\nobject (created with new Anthropic(...)\n) to add tracing. If Braintrust is\nnot configured, nothing will be traced. If this is not an Anthropic\nobject, this function is\na no-op.\nCurrently, this only supports the v4\nAPI.\nType parameters\nParameters\nReturns\nT\nThe wrapped Anthropic\nobject.\nwrapOpenAI\n\u25b8 wrapOpenAI<T\n>(openai\n): T\nWrap an OpenAI\nobject (created with new OpenAI(...)\n) to add tracing. If Braintrust is\nnot configured, nothing will be traced. If this is not an OpenAI\nobject, this function is\na no-op.\nCurrently, this only supports the v4\nAPI.\nType parameters\nParameters\nReturns\nT\nThe wrapped OpenAI\nobject.\nwrapOpenAIv4\n\u25b8 wrapOpenAIv4<T\n>(openai\n): T\nType parameters\nParameters\nReturns\nT\nwrapTraced\n\u25b8 wrapTraced<F\n, IsAsyncFlush\n>(fn\n, args?\n): IsAsyncFlush\nextends false\n? (...args\n: Parameters\n<F\n>) => Promise\n<Awaited\n<ReturnType\n<F\n>>> : F\nWrap a function with traced\n, using the arguments as input\nand return value as output\n.\nAny functions wrapped this way will automatically be traced, similar to the @traced\ndecorator\nin Python. If you want to correctly propagate the function's name and define it in one go, then\nyou can do so like this:\nNow, any calls to myFunc\nwill be traced, and the input and output will be logged automatically.\nIf tracing is inactive, i.e. there is no active logger or experiment, it's just a no-op.\nIf you're using NextJS or another framework that minifies your code before deployment, the function name will be obfuscated when deployed. To trace the name properly, you can specify it in the span-level arguments for wrapTraced\nlike so.\nType parameters\nParameters\nReturns\nIsAsyncFlush\nextends false\n? (...args\n: Parameters\n<F\n>) => Promise\n<Awaited\n<ReturnType\n<F\n>>> : F\nThe wrapped function.\nType Aliases\nAnyDataset\n\u01ac AnyDataset: Dataset\n<boolean\n>\nBaseExperiment\n\u01ac BaseExperiment<Input\n, Expected\n, Metadata\n>: Object\nType parameters\nType declaration\nBaseMetadata\n\u01ac BaseMetadata: Record\n<string\n, unknown\n> | void\nBraintrustStreamChunk\n\u01ac BraintrustStreamChunk: z.infer\n<typeof braintrustStreamChunkSchema\n>\nA chunk of data from a Braintrust stream. Each chunk type matches an SSE event type.\nChatPrompt\n\u01ac ChatPrompt: Object\nType declaration\nCodeOpts\n\u01ac CodeOpts<Params\n, Returns\n, Fn\n>: Partial\n<BaseFnOpts\n> & { handler\n: Fn\n} & Schema\n<Params\n, Returns\n>\nType parameters\nCommentEvent\n\u01ac CommentEvent: IdField\n& { _audit_metadata?\n: Record\n<string\n, unknown\n> ; _audit_source\n: Source\n; comment\n: { text\n: string\n} ; created\n: string\n; origin\n: { id\n: string\n} } & ParentExperimentIds\n| ParentProjectLogIds\n| ParentPlaygroundLogIds\nCompiledPrompt\n\u01ac CompiledPrompt<Flavor\n>: CompiledPromptParams\n& { span_info?\n: { metadata\n: { prompt\n: { id\n: string\n; project_id\n: string\n; variables\n: Record\n<string\n, unknown\n> ; version\n: string\n} } ; name?\n: string\n; spanAttributes?\n: Record\n<any\n, any\n> } } & Flavor\nextends \"chat\"\n? ChatPrompt\n: Flavor\nextends \"completion\"\n? CompletionPrompt\n: {}\nType parameters\nCompiledPromptParams\n\u01ac CompiledPromptParams: Omit\n<NonNullable\n<PromptData\n[\"options\"\n]>[\"params\"\n], \"use_cache\"\n> & { model\n: NonNullable\n<NonNullable\n<PromptData\n[\"options\"\n]>[\"model\"\n]> }\nCompletionPrompt\n\u01ac CompletionPrompt: Object\nType declaration\nCreateProjectOpts\n\u01ac CreateProjectOpts: NameOrId\nDatasetRecord\n\u01ac DatasetRecord<IsLegacyDataset\n>: IsLegacyDataset\nextends true\n? LegacyDatasetRecord\n: NewDatasetRecord\nType parameters\nDefaultMetadataType\n\u01ac DefaultMetadataType: void\nDefaultPromptArgs\n\u01ac DefaultPromptArgs: Partial\n<CompiledPromptParams\n& AnyModelParam\n& ChatPrompt\n& CompletionPrompt\n>\nEndSpanArgs\n\u01ac EndSpanArgs: Object\nType declaration\nEvalCase\n\u01ac EvalCase<Input\n, Expected\n, Metadata\n>: { _xact_id?\n: TransactionId\n; created?\n: string\n| null\n; id?\n: string\n; input\n: Input\n; tags?\n: string\n[] ; upsert_id?\n: string\n} & Expected\nextends void\n? object\n: { expected\n: Expected\n} & Metadata\nextends void\n? object\n: { metadata\n: Metadata\n}\nType parameters\nEvalParameterSerializedSchema\n\u01ac EvalParameterSerializedSchema: z.infer\n<typeof evalParametersSerializedSchema\n>\nEvalParameters\n\u01ac EvalParameters: z.infer\n<typeof evalParametersSchema\n>\nEvalResult\n\u01ac EvalResult<Input\n, Output\n, Expected\n, Metadata\n>: EvalCase\n<Input\n, Expected\n, Metadata\n> & { error\n: unknown\n; origin?\n: ObjectReference\n; output\n: Output\n; scores\n: Record\n<string\n, number\n| null\n> }\nType parameters\nEvalScorer\n\u01ac EvalScorer<Input\n, Output\n, Expected\n, Metadata\n>: (args\n: EvalScorerArgs\n<Input\n, Output\n, Expected\n, Metadata\n>) => OneOrMoreScores\n| Promise\n<OneOrMoreScores\n>\nType parameters\nType declaration\n\u25b8 (args\n): OneOrMoreScores\n| Promise\n<OneOrMoreScores\n>\nParameters\nReturns\nOneOrMoreScores\n| Promise\n<OneOrMoreScores\n>\nEvalScorerArgs\n\u01ac EvalScorerArgs<Input\n, Output\n, Expected\n, Metadata\n>: EvalCase\n<Input\n, Expected\n, Metadata\n> & { output\n: Output\n}\nType parameters\nEvalTask\n\u01ac EvalTask<Input\n, Output\n, Expected\n, Metadata\n, Parameters\n>: (input\n: Input\n, hooks\n: EvalHooks\n<Expected\n, Metadata\n, Parameters\n>) => Promise\n<Output\n> | (input\n: Input\n, hooks\n: EvalHooks\n<Expected\n, Metadata\n, Parameters\n>) => Output\nType parameters\nEvaluatorDef\n\u01ac EvaluatorDef<Input\n, Output\n, Expected\n, Metadata\n, Parameters\n>: { evalName\n: string\n; projectName\n: string\n} & Evaluator\n<Input\n, Output\n, Expected\n, Metadata\n, Parameters\n>\nType parameters\nEvaluatorDefinition\n\u01ac EvaluatorDefinition: z.infer\n<typeof evaluatorDefinitionSchema\n>\nEvaluatorDefinitions\n\u01ac EvaluatorDefinitions: z.infer\n<typeof evaluatorDefinitionsSchema\n>\nEvaluatorFile\n\u01ac EvaluatorFile: Object\nType declaration\nEvaluatorManifest\n\u01ac EvaluatorManifest: Record\n<string\n, EvaluatorDef\n<unknown\n, unknown\n, unknown\n, BaseMetadata\n>>\nExperimentLogFullArgs\n\u01ac ExperimentLogFullArgs: Partial\n<Omit\n<OtherExperimentLogFields\n, \"output\"\n| \"scores\"\n>> & Required\n<Pick\n<OtherExperimentLogFields\n, \"output\"\n| \"scores\"\n>> & Partial\n<InputField\n> & Partial\n<IdField\n>\nExperimentLogPartialArgs\n\u01ac ExperimentLogPartialArgs: Partial\n<OtherExperimentLogFields\n> & Partial\n<InputField\n>\nFullInitOptions\n\u01ac FullInitOptions<IsOpen\n>: { project?\n: string\n} & InitOptions\n<IsOpen\n>\nType parameters\nFullLoginOptions\n\u01ac FullLoginOptions: LoginOptions\n& { forceLogin?\n: boolean\n}\nIdField\n\u01ac IdField: Object\nType declaration\nInitOptions\n\u01ac InitOptions<IsOpen\n>: FullLoginOptions\n& { baseExperiment?\n: string\n; baseExperimentId?\n: string\n; dataset?\n: AnyDataset\n; description?\n: string\n; experiment?\n: string\n; gitMetadataSettings?\n: GitMetadataSettings\n; isPublic?\n: boolean\n; metadata?\n: Record\n<string\n, unknown\n> ; projectId?\n: string\n; repoInfo?\n: RepoInfo\n; setCurrent?\n: boolean\n; state?\n: BraintrustState\n; update?\n: boolean\n} & InitOpenOption\n<IsOpen\n>\nType parameters\nInputField\n\u01ac InputField: Object\nType declaration\nInvokeReturn\n\u01ac InvokeReturn<Stream\n, Output\n>: Stream\nextends true\n? BraintrustStream\n: Output\nThe return type of the invoke\nfunction. Conditionally returns a BraintrustStream\nif stream\nis true, otherwise returns the output of the function using the Zod schema's\ntype if present.\nType parameters\nLogCommentFullArgs\n\u01ac LogCommentFullArgs: IdField\n& { _audit_metadata?\n: Record\n<string\n, unknown\n> ; _audit_source\n: Source\n; comment\n: { text\n: string\n} ; created\n: string\n; origin\n: { id\n: string\n} } & ParentExperimentIds\n| ParentProjectLogIds\nLogFeedbackFullArgs\n\u01ac LogFeedbackFullArgs: IdField\n& Partial\n<Omit\n<OtherExperimentLogFields\n, \"output\"\n| \"metrics\"\n| \"datasetRecordId\"\n> & { comment\n: string\n; source\n: Source\n}>\nOtherExperimentLogFields\n\u01ac OtherExperimentLogFields: Object\nType declaration\nPromiseUnless\n\u01ac PromiseUnless<B\n, R\n>: B\nextends true\n? R\n: Promise\n<Awaited\n<R\n>>\nType parameters\nPromptContents\n\u01ac PromptContents: z.infer\n<typeof promptContentsSchema\n>\nPromptDefinition\n\u01ac PromptDefinition: z.infer\n<typeof promptDefinitionSchema\n>\nPromptDefinitionWithTools\n\u01ac PromptDefinitionWithTools: z.infer\n<typeof promptDefinitionWithToolsSchema\n>\nPromptOpts\n\u01ac PromptOpts<HasId\n, HasVersion\n, HasTools\n, HasNoTrace\n>: Partial\n<Omit\n<BaseFnOpts\n, \"name\"\n>> & { name\n: string\n} & HasId\nextends true\n? PromptId\n: Partial\n<PromptId\n> & HasVersion\nextends true\n? PromptVersion\n: Partial\n<PromptVersion\n> & HasTools\nextends true\n? Partial\n<PromptTools\n> : {} & HasNoTrace\nextends true\n? Partial\n<PromptNoTrace\n> : {} & PromptDefinition\nType parameters\nPromptRowWithId\n\u01ac PromptRowWithId<HasId\n, HasVersion\n>: Omit\n<PromptRow\n, \"log_id\"\n| \"org_id\"\n| \"project_id\"\n| \"id\"\n| \"_xact_id\"\n> & Partial\n<Pick\n<PromptRow\n, \"project_id\"\n>> & HasId\nextends true\n? Pick\n<PromptRow\n, \"id\"\n> : Partial\n<Pick\n<PromptRow\n, \"id\"\n>> & HasVersion\nextends true\n? Pick\n<PromptRow\n, \"_xact_id\"\n> : Partial\n<Pick\n<PromptRow\n, \"_xact_id\"\n>>\nType parameters\nScorerOpts\n\u01ac ScorerOpts<Output\n, Input\n, Params\n, Returns\n, Fn\n>: CodeOpts\n<Exact\n<Params\n, ScorerArgs\n<Output\n, Input\n>>, Returns\n, Fn\n> | ScorerPromptOpts\nType parameters\nSerializedBraintrustState\n\u01ac SerializedBraintrustState: z.infer\n<typeof loginSchema\n>\nSetCurrentArg\n\u01ac SetCurrentArg: Object\nType declaration\nSpanContext\n\u01ac SpanContext: Object\nType declaration\nStartSpanArgs\n\u01ac StartSpanArgs: Object\nType declaration\nToolFunctionDefinition\n\u01ac ToolFunctionDefinition: z.infer\n<typeof toolFunctionDefinitionSchema\n>\nWithTransactionId\n\u01ac WithTransactionId<R\n>: R\n& { _xact_id\n: TransactionId\n}\nType parameters\nVariables\nERR_PERMALINK\n\u2022 Const\nERR_PERMALINK: \"https://braintrust.dev/error-generating-link\"\nINTERNAL_BTQL_LIMIT\n\u2022 Const\nINTERNAL_BTQL_LIMIT: 1000\nLEGACY_CACHED_HEADER\n\u2022 Const\nLEGACY_CACHED_HEADER: \"x-cached\"\nNOOP_SPAN\n\u2022 Const\nNOOP_SPAN: NoopSpan\nNOOP_SPAN_PERMALINK\n\u2022 Const\nNOOP_SPAN_PERMALINK: \"https://braintrust.dev/noop-span\"\nX_CACHED_HEADER\n\u2022 Const\nX_CACHED_HEADER: \"x-bt-cached\"\n_exportsForTestingOnly\n\u2022 Const\n_exportsForTestingOnly: Object\nType declaration\nbraintrustStreamChunkSchema\n\u2022 Const\nbraintrustStreamChunkSchema: ZodUnion\n<[ZodObject\n<{ data\n: ZodString\n; type\n: ZodLiteral\n<\"text_delta\"\n> }, \"strip\"\n, ZodTypeAny\n, { data\n: string\n; type\n: \"text_delta\"\n}, { data\n: string\n; type\n: \"text_delta\"\n}>, ZodObject\n<{ data\n: ZodString\n; type\n: ZodLiteral\n<\"reasoning_delta\"\n> }, \"strip\"\n, ZodTypeAny\n, { data\n: string\n; type\n: \"reasoning_delta\"\n}, { data\n: string\n; type\n: \"reasoning_delta\"\n}>, ZodObject\n<{ data\n: ZodString\n; type\n: ZodLiteral\n<\"json_delta\"\n> }, \"strip\"\n, ZodTypeAny\n, { data\n: string\n; type\n: \"json_delta\"\n}, { data\n: string\n; type\n: \"json_delta\"\n}>]>\nevaluatorDefinitionSchema\n\u2022 Const\nevaluatorDefinitionSchema: ZodObject\n<{ parameters\n: ZodOptional\n<ZodRecord\n<ZodString\n, ZodUnion\n<[ZodObject\n<{ default\n: ZodOptional\n<ZodObject\n<{ options\n: ZodOptional\n<ZodNullable\n<ZodObject\n<{ model\n: ZodOptional\n<ZodString\n> ; params\n: ZodOptional\n<ZodUnion\n<[ZodObject\n<{ frequency_penalty\n: ZodOptional\n<ZodNumber\n> ; function_call\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}>]>> ; max_completion_tokens\n: ZodOptional\n<ZodNumber\n> ; max_tokens\n: ZodOptional\n<ZodNumber\n> ; n\n: ZodOptional\n<ZodNumber\n> ; presence_penalty\n: ZodOptional\n<ZodNumber\n> ; reasoning_effort\n: ZodOptional\n<ZodEnum\n<[\"low\"\n, \"medium\"\n, \"high\"\n]>> ; response_format\n: ZodOptional\n<ZodNullable\n<ZodUnion\n<[ZodObject\n<{ type\n: ZodLiteral\n<\"json_object\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"json_object\"\n}, { type\n: \"json_object\"\n}>, ZodObject\n<{ json_schema\n: ZodObject\n<{ description\n: ZodOptional\n<ZodString\n> ; name\n: ZodString\n; schema\n: ZodOptional\n<ZodUnion\n<[ZodRecord\n<ZodString\n, ZodUnknown\n>, ZodString\n]>> ; strict\n: ZodOptional\n<ZodNullable\n<ZodBoolean\n>> }, \"strip\"\n, ZodTypeAny\n, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}> ; type\n: ZodLiteral\n<\"json_schema\"\n> }, \"strip\"\n, ZodTypeAny\n, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}>, ZodObject\n<{ type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"text\"\n}, { type\n: \"text\"\n}>]>>> ; stop\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodOptional\n<ZodNumber\n> ; tool_choice\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodLiteral\n<\"required\"\n>, ZodObject\n<{ function\n: ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}> ; type\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { function\n: { name\n: string\n} ; type\n: \"function\"\n}, { function\n: { name\n: string\n} ; type\n: \"function\"\n}>]>> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, \"passthrough\"\n, ZodTypeAny\n, objectOutputType\n<{ frequency_penalty\n: ZodOptional\n<ZodNumber\n> ; function_call\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}>]>> ; max_completion_tokens\n: ZodOptional\n<ZodNumber\n> ; max_tokens\n: ZodOptional\n<ZodNumber\n> ; n\n: ZodOptional\n<ZodNumber\n> ; presence_penalty\n: ZodOptional\n<ZodNumber\n> ; reasoning_effort\n: ZodOptional\n<ZodEnum\n<[\"low\"\n, \"medium\"\n, \"high\"\n]>> ; response_format\n: ZodOptional\n<ZodNullable\n<ZodUnion\n<[ZodObject\n<{ type\n: ZodLiteral\n<\"json_object\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"json_object\"\n}, { type\n: \"json_object\"\n}>, ZodObject\n<{ json_schema\n: ZodObject\n<{ description\n: ZodOptional\n<ZodString\n> ; name\n: ZodString\n; schema\n: ZodOptional\n<ZodUnion\n<[ZodRecord\n<ZodString\n, ZodUnknown\n>, ZodString\n]>> ; strict\n: ZodOptional\n<ZodNullable\n<ZodBoolean\n>> }, \"strip\"\n, ZodTypeAny\n, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}> ; type\n: ZodLiteral\n<\"json_schema\"\n> }, \"strip\"\n, ZodTypeAny\n, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}>, ZodObject\n<{ type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"text\"\n}, { type\n: \"text\"\n}>]>>> ; stop\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodOptional\n<ZodNumber\n> ; tool_choice\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodLiteral\n<\"required\"\n>, ZodObject\n<{ function\n: ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}> ; type\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { function\n: { name\n: string\n} ; type\n: \"function\"\n}, { function\n: { name\n: string\n} ; type\n: \"function\"\n}>]>> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>, objectInputType\n<{ frequency_penalty\n: ZodOptional\n<ZodNumber\n> ; function_call\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}>]>> ; max_completion_tokens\n: ZodOptional\n<ZodNumber\n> ; max_tokens\n: ZodOptional\n<ZodNumber\n> ; n\n: ZodOptional\n<ZodNumber\n> ; presence_penalty\n: ZodOptional\n<ZodNumber\n> ; reasoning_effort\n: ZodOptional\n<ZodEnum\n<[\"low\"\n, \"medium\"\n, \"high\"\n]>> ; response_format\n: ZodOptional\n<ZodNullable\n<ZodUnion\n<[ZodObject\n<{ type\n: ZodLiteral\n<\"json_object\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"json_object\"\n}, { type\n: \"json_object\"\n}>, ZodObject\n<{ json_schema\n: ZodObject\n<{ description\n: ZodOptional\n<ZodString\n> ; name\n: ZodString\n; schema\n: ZodOptional\n<ZodUnion\n<[ZodRecord\n<ZodString\n, ZodUnknown\n>, ZodString\n]>> ; strict\n: ZodOptional\n<ZodNullable\n<ZodBoolean\n>> }, \"strip\"\n, ZodTypeAny\n, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}> ; type\n: ZodLiteral\n<\"json_schema\"\n> }, \"strip\"\n, ZodTypeAny\n, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}>, ZodObject\n<{ type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"text\"\n}, { type\n: \"text\"\n}>]>>> ; stop\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodOptional\n<ZodNumber\n> ; tool_choice\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodLiteral\n<\"required\"\n>, ZodObject\n<{ function\n: ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}> ; type\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { function\n: { name\n: string\n} ; type\n: \"function\"\n}, { function\n: { name\n: string\n} ; type\n: \"function\"\n}>]>> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>>, ZodObject\n<{ max_tokens\n: ZodNumber\n; max_tokens_to_sample\n: ZodOptional\n<ZodNumber\n> ; stop_sequences\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodNumber\n; top_k\n: ZodOptional\n<ZodNumber\n> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, \"passthrough\"\n, ZodTypeAny\n, objectOutputType\n<{ max_tokens\n: ZodNumber\n; max_tokens_to_sample\n: ZodOptional\n<ZodNumber\n> ; stop_sequences\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodNumber\n; top_k\n: ZodOptional\n<ZodNumber\n> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>, objectInputType\n<{ max_tokens\n: ZodNumber\n; max_tokens_to_sample\n: ZodOptional\n<ZodNumber\n> ; stop_sequences\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodNumber\n; top_k\n: ZodOptional\n<ZodNumber\n> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>>, ZodObject\n<{ maxOutputTokens\n: ZodOptional\n<ZodNumber\n> ; temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; topP\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, \"passthrough\"\n, ZodTypeAny\n, objectOutputType\n<{ maxOutputTokens\n: ZodOptional\n<ZodNumber\n> ; temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; topP\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>, objectInputType\n<{ maxOutputTokens\n: ZodOptional\n<ZodNumber\n> ; temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; topP\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>>, ZodObject\n<{ temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, \"passthrough\"\n, ZodTypeAny\n, objectOutputType\n<{ temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>, objectInputType\n<{ temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>>, ZodObject\n<{ use_cache\n: ZodOptional\n<ZodBoolean\n> }, \"passthrough\"\n, ZodTypeAny\n, objectOutputType\n<{ use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>, objectInputType\n<{ use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>>]>> ; position\n: ZodOptional\n<ZodString\n> }, \"strip\"\n, ZodTypeAny\n, { model?\n: string\n; params?\n: objectOutputType\n<{ frequency_penalty\n: ZodOptional\n<ZodNumber\n> ; function_call\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}>]>> ; max_completion_tokens\n: ZodOptional\n<ZodNumber\n> ; max_tokens\n: ZodOptional\n<ZodNumber\n> ; n\n: ZodOptional\n<ZodNumber\n> ; presence_penalty\n: ZodOptional\n<ZodNumber\n> ; reasoning_effort\n: ZodOptional\n<ZodEnum\n<[\"low\"\n, \"medium\"\n, \"high\"\n]>> ; response_format\n: ZodOptional\n<ZodNullable\n<ZodUnion\n<[ZodObject\n<{ type\n: ZodLiteral\n<\"json_object\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"json_object\"\n}, { type\n: \"json_object\"\n}>, ZodObject\n<{ json_schema\n: ZodObject\n<{ description\n: ZodOptional\n<ZodString\n> ; name\n: ZodString\n; schema\n: ZodOptional\n<ZodUnion\n<[ZodRecord\n<ZodString\n, ZodUnknown\n>, ZodString\n]>> ; strict\n: ZodOptional\n<ZodNullable\n<ZodBoolean\n>> }, \"strip\"\n, ZodTypeAny\n, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}> ; type\n: ZodLiteral\n<\"json_schema\"\n> }, \"strip\"\n, ZodTypeAny\n, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}>, ZodObject\n<{ type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"text\"\n}, { type\n: \"text\"\n}>]>>> ; stop\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodOptional\n<ZodNumber\n> ; tool_choice\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodLiteral\n<\"required\"\n>, ZodObject\n<{ function\n: ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}> ; type\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { function\n: { name\n: string\n} ; type\n: \"function\"\n}, { function\n: { name\n: string\n} ; type\n: \"function\"\n}>]>> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectOutputType\n<{ max_tokens\n: ZodNumber\n; max_tokens_to_sample\n: ZodOptional\n<ZodNumber\n> ; stop_sequences\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodNumber\n; top_k\n: ZodOptional\n<ZodNumber\n> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectOutputType\n<{ maxOutputTokens\n: ZodOptional\n<ZodNumber\n> ; temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; topP\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectOutputType\n<{ temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectOutputType\n<{ use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> ; position?\n: string\n}, { model?\n: string\n; params?\n: objectInputType\n<{ frequency_penalty\n: ZodOptional\n<ZodNumber\n> ; function_call\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}>]>> ; max_completion_tokens\n: ZodOptional\n<ZodNumber\n> ; max_tokens\n: ZodOptional\n<ZodNumber\n> ; n\n: ZodOptional\n<ZodNumber\n> ; presence_penalty\n: ZodOptional\n<ZodNumber\n> ; reasoning_effort\n: ZodOptional\n<ZodEnum\n<[\"low\"\n, \"medium\"\n, \"high\"\n]>> ; response_format\n: ZodOptional\n<ZodNullable\n<ZodUnion\n<[ZodObject\n<{ type\n: ZodLiteral\n<\"json_object\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"json_object\"\n}, { type\n: \"json_object\"\n}>, ZodObject\n<{ json_schema\n: ZodObject\n<{ description\n: ZodOptional\n<ZodString\n> ; name\n: ZodString\n; schema\n: ZodOptional\n<ZodUnion\n<[ZodRecord\n<ZodString\n, ZodUnknown\n>, ZodString\n]>> ; strict\n: ZodOptional\n<ZodNullable\n<ZodBoolean\n>> }, \"strip\"\n, ZodTypeAny\n, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}> ; type\n: ZodLiteral\n<\"json_schema\"\n> }, \"strip\"\n, ZodTypeAny\n, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}>, ZodObject\n<{ type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"text\"\n}, { type\n: \"text\"\n}>]>>> ; stop\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodOptional\n<ZodNumber\n> ; tool_choice\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodLiteral\n<\"required\"\n>, ZodObject\n<{ function\n: ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}> ; type\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { function\n: { name\n: string\n} ; type\n: \"function\"\n}, { function\n: { name\n: string\n} ; type\n: \"function\"\n}>]>> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectInputType\n<{ max_tokens\n: ZodNumber\n; max_tokens_to_sample\n: ZodOptional\n<ZodNumber\n> ; stop_sequences\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodNumber\n; top_k\n: ZodOptional\n<ZodNumber\n> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectInputType\n<{ maxOutputTokens\n: ZodOptional\n<ZodNumber\n> ; temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; topP\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectInputType\n<{ temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectInputType\n<{ use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> ; position?\n: string\n}>>> ; origin\n: ZodOptional\n<ZodNullable\n<ZodObject\n<{ project_id\n: ZodOptional\n<ZodString\n> ; prompt_id\n: ZodOptional\n<ZodString\n> ; prompt_version\n: ZodOptional\n<ZodString\n> }, \"strip\"\n, ZodTypeAny\n, { project_id?\n: string\n; prompt_id?\n: string\n; prompt_version?\n: string\n}, { project_id?\n: string\n; prompt_id?\n: string\n; prompt_version?\n: string\n}>>> ; parser\n: ZodOptional\n<ZodNullable\n<ZodObject\n<{ choice_scores\n: ZodRecord\n<ZodString\n, ZodNumber\n> ; type\n: ZodLiteral\n<\"llm_classifier\"\n> ; use_cot\n: ZodBoolean\n}, \"strip\"\n, ZodTypeAny\n, { choice_scores\n: Record\n<string\n, number\n> ; type\n: \"llm_classifier\"\n; use_cot\n: boolean\n}, { choice_scores\n: Record\n<string\n, number\n> ; type\n: \"llm_classifier\"\n; use_cot\n: boolean\n}>>> ; prompt\n: ZodOptional\n<ZodNullable\n<ZodUnion\n<[ZodObject\n<{ content\n: ZodString\n; type\n: ZodLiteral\n<\"completion\"\n> }, \"strip\"\n, ZodTypeAny\n, { content\n: string\n; type\n: \"completion\"\n}, { content\n: string\n; type\n: \"completion\"\n}>, ZodObject\n<{ messages\n: ZodArray\n<ZodUnion\n<[ZodUnion\n<[ZodObject\n<{ content\n: ZodUnion\n<[ZodDefault\n<ZodString\n>, ZodArray\n<ZodObject\n<{ cache_control\n: ZodOptional\n<ZodObject\n<{ type\n: ZodEnum\n<[\"ephemeral\"\n]> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"ephemeral\"\n}, { type\n: \"ephemeral\"\n}>> ; text\n: ZodDefault\n<ZodString\n> ; type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text\n: string\n; type\n: \"text\"\n}, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}>, \"many\"\n>]> ; name\n: ZodOptional\n<ZodString\n> ; role\n: ZodLiteral\n<\"system\"\n> }, \"strip\"\n, ZodTypeAny\n, { content\n: {} ; name?\n: string\n; role\n: \"system\"\n}, { content?\n: string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; name?\n: string\n; role\n: \"system\"\n}>, ZodObject\n<{ content\n: ZodUnion\n<[ZodDefault\n<ZodString\n>, ZodArray\n<ZodUnion\n<[ZodObject\n<{ cache_control\n: ZodOptional\n<ZodObject\n<{ type\n: ZodEnum\n<[\"ephemeral\"\n]> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"ephemeral\"\n}, { type\n: \"ephemeral\"\n}>> ; text\n: ZodDefault\n<ZodString\n> ; type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text\n: string\n; type\n: \"text\"\n}, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}>, ZodObject\n<{ image_url\n: ZodObject\n<{ detail\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"low\"\n>, ZodLiteral\n<\"high\"\n>]>> ; url\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { detail?\n: \"auto\"\n| \"high\"\n| \"low\"\n; url\n: string\n}, { detail?\n: \"auto\"\n| \"high\"\n| \"low\"\n; url\n: string\n}> ; type\n: ZodLiteral\n<\"image_url\"\n> }, \"strip\"\n, ZodTypeAny\n, { image_url\n: { detail?\n: \"auto\"\n| \"high\"\n| \"low\"\n; url\n: string\n} ; type\n: \"image_url\"\n}, { image_url\n: { detail?\n: \"auto\"\n| \"high\"\n| \"low\"\n; url\n: string\n} ; type\n: \"image_url\"\n}>]>, \"many\"\n>]> ; name\n: ZodOptional\n<ZodString\n> ; role\n: ZodLiteral\n<\"user\"\n> }, \"strip\"\n, ZodTypeAny\n, { content\n: {} ; name?\n: string\n; role\n: \"user\"\n}, { content?\n: string\n| ({ cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n} | { image_url\n: { detail?\n: \"auto\"\n| \"high\"\n| \"low\"\n; url\n: string\n} ; type\n: \"image_url\"\n})[] ; name?\n: string\n; role\n: \"user\"\n}>, ZodObject\n<{ content\n: ZodOptional\n<ZodNullable\n<ZodUnion\n<[ZodString\n, ZodArray\n<ZodObject\n<{ cache_control\n: ZodOptional\n<ZodObject\n<{ type\n: ZodEnum\n<[\"ephemeral\"\n]> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"ephemeral\"\n}, { type\n: \"ephemeral\"\n}>> ; text\n: ZodDefault\n<ZodString\n> ; type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text\n: string\n; type\n: \"text\"\n}, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}>, \"many\"\n>]>>> ; function_call\n: ZodEffects\n<ZodOptional\n<ZodNullable\n<ZodObject\n<{ arguments\n: ZodString\n; name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { arguments\n: string\n; name\n: string\n}, { arguments\n: string\n; name\n: string\n}>>>, undefined\n| { arguments\n: string\n; name\n: string\n}, undefined\n| null\n| { arguments\n: string\n; name\n: string\n}> ; name\n: ZodEffects\n<ZodOptional\n<ZodNullable\n<ZodString\n>>, undefined\n| string\n, undefined\n| null\n| string\n> ; reasoning\n: ZodEffects\n<ZodOptional\n<ZodNullable\n<ZodArray\n<ZodObject\n<{ content\n: ZodEffects\n<ZodOptional\n<ZodNullable\n<ZodString\n>>, undefined\n| string\n, undefined\n| null\n| string\n> ; id\n: ZodEffects\n<ZodOptional\n<ZodNullable\n<ZodString\n>>, undefined\n| string\n, undefined\n| null\n| string\n> }, \"strip\"\n, ZodTypeAny\n, { content?\n: string\n; id?\n: string\n}, { content?\n: null\n| string\n; id?\n: null\n| string\n}>, \"many\"\n>>>, undefined\n| { content?\n: string\n; id?\n: string\n}[], undefined\n| null\n| { content?\n: null\n| string\n; id?\n: null\n| string\n}[]> ; role\n: ZodLiteral\n<\"assistant\"\n> ; tool_calls\n: ZodEffects\n<ZodOptional\n<ZodNullable\n<ZodArray\n<ZodObject\n<{ function\n: ZodObject\n<{ arguments\n: ZodString\n; name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { arguments\n: string\n; name\n: string\n}, { arguments\n: string\n; name\n: string\n}> ; id\n: ZodString\n; type\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}, { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}>, \"many\"\n>>>, undefined\n| { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[], undefined\n| null\n| { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[]> }, \"strip\"\n, ZodTypeAny\n, { content?\n: null\n| string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text\n: string\n; type\n: \"text\"\n}[] ; function_call?\n: { arguments\n: string\n; name\n: string\n} ; name?\n: string\n; reasoning?\n: { content?\n: string\n; id?\n: string\n}[] ; role\n: \"assistant\"\n; tool_calls?\n: { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[] }, { content?\n: null\n| string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; function_call?\n: null\n| { arguments\n: string\n; name\n: string\n} ; name?\n: null\n| string\n; reasoning?\n: null\n| { content?\n: null\n| string\n; id?\n: null\n| string\n}[] ; role\n: \"assistant\"\n; tool_calls?\n: null\n| { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[] }>, ZodObject\n<{ content\n: ZodUnion\n<[ZodDefault\n<ZodString\n>, ZodArray\n<ZodObject\n<{ cache_control\n: ZodOptional\n<ZodObject\n<{ type\n: ZodEnum\n<[\"ephemeral\"\n]> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"ephemeral\"\n}, { type\n: \"ephemeral\"\n}>> ; text\n: ZodDefault\n<ZodString\n> ; type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text\n: string\n; type\n: \"text\"\n}, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}>, \"many\"\n>]> ; role\n: ZodLiteral\n<\"tool\"\n> ; tool_call_id\n: ZodDefault\n<ZodString\n> }, \"strip\"\n, ZodTypeAny\n, { content\n: {} ; role\n: \"tool\"\n; tool_call_id\n: string\n}, { content?\n: string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; role\n: \"tool\"\n; tool_call_id?\n: string\n}>, ZodObject\n<{ content\n: ZodNullable\n<ZodString\n> ; name\n: ZodString\n; role\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { content\n: null\n| string\n; name\n: string\n; role\n: \"function\"\n}, { content\n: null\n| string\n; name\n: string\n; role\n: \"function\"\n}>]>, ZodObject\n<{ content\n: ZodOptional\n<ZodNullable\n<ZodString\n>> ; role\n: ZodEnum\n<[\"model\"\n]> }, \"strip\"\n, ZodTypeAny\n, { content?\n: null\n| string\n; role\n: \"model\"\n}, { content?\n: null\n| string\n; role\n: \"model\"\n}>]>, \"many\"\n> ; tools\n: ZodOptional\n<ZodString\n> ; type\n: ZodLiteral\n<\"chat\"\n> }, \"strip\"\n, ZodTypeAny\n, { messages\n: ({ content\n: {} ; name?\n: string\n; role\n: \"system\"\n} | { content\n: {} ; name?\n: string\n; role\n: \"user\"\n} | { content\n: {} ; role\n: \"tool\"\n; tool_call_id\n: string\n} | { content\n: null\n| string\n; name\n: string\n; role\n: \"function\"\n} | { content?\n: null\n| string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text\n: string\n; type\n: \"text\"\n}[] ; function_call?\n: { arguments\n: string\n; name\n: string\n} ; name?\n: string\n; reasoning?\n: { content?\n: string\n; id?\n: string\n}[] ; role\n: \"assistant\"\n; tool_calls?\n: { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[] } | { content?\n: null\n| string\n; role\n: \"model\"\n})[] ; tools?\n: string\n; type\n: \"chat\"\n}, { messages\n: ({ content?\n: string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; name?\n: string\n; role\n: \"system\"\n} | { content?\n: string\n| ({ cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n} | { image_url\n: { detail?\n: \"auto\"\n| \"high\"\n| \"low\"\n; url\n: string\n} ; type\n: \"image_url\"\n})[] ; name?\n: string\n; role\n: \"user\"\n} | { content?\n: string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; role\n: \"tool\"\n; tool_call_id?\n: string\n} | { content\n: null\n| string\n; name\n: string\n; role\n: \"function\"\n} | { content?\n: null\n| string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; function_call?\n: null\n| { arguments\n: string\n; name\n: string\n} ; name?\n: null\n| string\n; reasoning?\n: null\n| { content?\n: null\n| string\n; id?\n: null\n| string\n}[] ; role\n: \"assistant\"\n; tool_calls?\n: null\n| { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[] } | { content?\n: null\n| string\n; role\n: \"model\"\n})[] ; tools?\n: string\n; type\n: \"chat\"\n}>]>>> ; tool_functions\n: ZodOptional\n<ZodNullable\n<ZodArray\n<ZodUnion\n<[ZodObject\n<{ id\n: ZodString\n; type\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { id\n: string\n; type\n: \"function\"\n}, { id\n: string\n; type\n: \"function\"\n}>, ZodObject\n<{ name\n: ZodString\n; type\n: ZodLiteral\n<\"global\"\n> }, \"strip\"\n, ZodTypeAny\n, { name\n: string\n; type\n: \"global\"\n}, { name\n: string\n; type\n: \"global\"\n}>]>, \"many\"\n>>> }, \"strip\"\n, ZodTypeAny\n, { options?\n: null\n| { model?\n: string\n; params?\n: objectOutputType\n<{ frequency_penalty\n: ZodOptional\n<ZodNumber\n> ; function_call\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}>]>> ; max_completion_tokens\n: ZodOptional\n<ZodNumber\n> ; max_tokens\n: ZodOptional\n<ZodNumber\n> ; n\n: ZodOptional\n<ZodNumber\n> ; presence_penalty\n: ZodOptional\n<ZodNumber\n> ; reasoning_effort\n: ZodOptional\n<ZodEnum\n<[\"low\"\n, \"medium\"\n, \"high\"\n]>> ; response_format\n: ZodOptional\n<ZodNullable\n<ZodUnion\n<[ZodObject\n<{ type\n: ZodLiteral\n<\"json_object\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"json_object\"\n}, { type\n: \"json_object\"\n}>, ZodObject\n<{ json_schema\n: ZodObject\n<{ description\n: ZodOptional\n<ZodString\n> ; name\n: ZodString\n; schema\n: ZodOptional\n<ZodUnion\n<[ZodRecord\n<ZodString\n, ZodUnknown\n>, ZodString\n]>> ; strict\n: ZodOptional\n<ZodNullable\n<ZodBoolean\n>> }, \"strip\"\n, ZodTypeAny\n, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}> ; type\n: ZodLiteral\n<\"json_schema\"\n> }, \"strip\"\n, ZodTypeAny\n, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}>, ZodObject\n<{ type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"text\"\n}, { type\n: \"text\"\n}>]>>> ; stop\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodOptional\n<ZodNumber\n> ; tool_choice\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodLiteral\n<\"required\"\n>, ZodObject\n<{ function\n: ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}> ; type\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { function\n: { name\n: string\n} ; type\n: \"function\"\n}, { function\n: { name\n: string\n} ; type\n: \"function\"\n}>]>> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectOutputType\n<{ max_tokens\n: ZodNumber\n; max_tokens_to_sample\n: ZodOptional\n<ZodNumber\n> ; stop_sequences\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodNumber\n; top_k\n: ZodOptional\n<ZodNumber\n> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectOutputType\n<{ maxOutputTokens\n: ZodOptional\n<ZodNumber\n> ; temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; topP\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectOutputType\n<{ temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectOutputType\n<{ use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> ; position?\n: string\n} ; origin?\n: null\n| { project_id?\n: string\n; prompt_id?\n: string\n; prompt_version?\n: string\n} ; parser?\n: null\n| { choice_scores\n: Record\n<string\n, number\n> ; type\n: \"llm_classifier\"\n; use_cot\n: boolean\n} ; prompt?\n: null\n| { content\n: string\n; type\n: \"completion\"\n} | { messages\n: ({ content\n: {} ; name?\n: string\n; role\n: \"system\"\n} | { content\n: {} ; name?\n: string\n; role\n: \"user\"\n} | { content\n: {} ; role\n: \"tool\"\n; tool_call_id\n: string\n} | { content\n: null\n| string\n; name\n: string\n; role\n: \"function\"\n} | { content?\n: null\n| string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text\n: string\n; type\n: \"text\"\n}[] ; function_call?\n: { arguments\n: string\n; name\n: string\n} ; name?\n: string\n; reasoning?\n: { content?\n: string\n; id?\n: string\n}[] ; role\n: \"assistant\"\n; tool_calls?\n: { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[] } | { content?\n: null\n| string\n; role\n: \"model\"\n})[] ; tools?\n: string\n; type\n: \"chat\"\n} ; tool_functions?\n: null\n| ({ id\n: string\n; type\n: \"function\"\n} | { name\n: string\n; type\n: \"global\"\n})[] }, { options?\n: null\n| { model?\n: string\n; params?\n: objectInputType\n<{ frequency_penalty\n: ZodOptional\n<ZodNumber\n> ; function_call\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}>]>> ; max_completion_tokens\n: ZodOptional\n<ZodNumber\n> ; max_tokens\n: ZodOptional\n<ZodNumber\n> ; n\n: ZodOptional\n<ZodNumber\n> ; presence_penalty\n: ZodOptional\n<ZodNumber\n> ; reasoning_effort\n: ZodOptional\n<ZodEnum\n<[\"low\"\n, \"medium\"\n, \"high\"\n]>> ; response_format\n: ZodOptional\n<ZodNullable\n<ZodUnion\n<[ZodObject\n<{ type\n: ZodLiteral\n<\"json_object\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"json_object\"\n}, { type\n: \"json_object\"\n}>, ZodObject\n<{ json_schema\n: ZodObject\n<{ description\n: ZodOptional\n<ZodString\n> ; name\n: ZodString\n; schema\n: ZodOptional\n<ZodUnion\n<[ZodRecord\n<ZodString\n, ZodUnknown\n>, ZodString\n]>> ; strict\n: ZodOptional\n<ZodNullable\n<ZodBoolean\n>> }, \"strip\"\n, ZodTypeAny\n, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}> ; type\n: ZodLiteral\n<\"json_schema\"\n> }, \"strip\"\n, ZodTypeAny\n, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}>, ZodObject\n<{ type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"text\"\n}, { type\n: \"text\"\n}>]>>> ; stop\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodOptional\n<ZodNumber\n> ; tool_choice\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodLiteral\n<\"required\"\n>, ZodObject\n<{ function\n: ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}> ; type\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { function\n: { name\n: string\n} ; type\n: \"function\"\n}, { function\n: { name\n: string\n} ; type\n: \"function\"\n}>]>> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectInputType\n<{ max_tokens\n: ZodNumber\n; max_tokens_to_sample\n: ZodOptional\n<ZodNumber\n> ; stop_sequences\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodNumber\n; top_k\n: ZodOptional\n<ZodNumber\n> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectInputType\n<{ maxOutputTokens\n: ZodOptional\n<ZodNumber\n> ; temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; topP\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectInputType\n<{ temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectInputType\n<{ use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> ; position?\n: string\n} ; origin?\n: null\n| { project_id?\n: string\n; prompt_id?\n: string\n; prompt_version?\n: string\n} ; parser?\n: null\n| { choice_scores\n: Record\n<string\n, number\n> ; type\n: \"llm_classifier\"\n; use_cot\n: boolean\n} ; prompt?\n: null\n| { content\n: string\n; type\n: \"completion\"\n} | { messages\n: ({ content?\n: string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; name?\n: string\n; role\n: \"system\"\n} | { content?\n: string\n| ({ cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n} | { image_url\n: { detail?\n: \"auto\"\n| \"high\"\n| \"low\"\n; url\n: string\n} ; type\n: \"image_url\"\n})[] ; name?\n: string\n; role\n: \"user\"\n} | { content?\n: string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; role\n: \"tool\"\n; tool_call_id?\n: string\n} | { content\n: null\n| string\n; name\n: string\n; role\n: \"function\"\n} | { content?\n: null\n| string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; function_call?\n: null\n| { arguments\n: string\n; name\n: string\n} ; name?\n: null\n| string\n; reasoning?\n: null\n| { content?\n: null\n| string\n; id?\n: null\n| string\n}[] ; role\n: \"assistant\"\n; tool_calls?\n: null\n| { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[] } | { content?\n: null\n| string\n; role\n: \"model\"\n})[] ; tools?\n: string\n; type\n: \"chat\"\n} ; tool_functions?\n: null\n| ({ id\n: string\n; type\n: \"function\"\n} | { name\n: string\n; type\n: \"global\"\n})[] }>> ; description\n: ZodOptional\n<ZodString\n> ; type\n: ZodLiteral\n<\"prompt\"\n> }, \"strip\"\n, ZodTypeAny\n, { default?\n: { options?\n: null\n| { model?\n: string\n; params?\n: objectOutputType\n<{ frequency_penalty\n: ZodOptional\n<ZodNumber\n> ; function_call\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}>]>> ; max_completion_tokens\n: ZodOptional\n<ZodNumber\n> ; max_tokens\n: ZodOptional\n<ZodNumber\n> ; n\n: ZodOptional\n<ZodNumber\n> ; presence_penalty\n: ZodOptional\n<ZodNumber\n> ; reasoning_effort\n: ZodOptional\n<ZodEnum\n<[\"low\"\n, \"medium\"\n, \"high\"\n]>> ; response_format\n: ZodOptional\n<ZodNullable\n<ZodUnion\n<[ZodObject\n<{ type\n: ZodLiteral\n<\"json_object\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"json_object\"\n}, { type\n: \"json_object\"\n}>, ZodObject\n<{ json_schema\n: ZodObject\n<{ description\n: ZodOptional\n<ZodString\n> ; name\n: ZodString\n; schema\n: ZodOptional\n<ZodUnion\n<[ZodRecord\n<ZodString\n, ZodUnknown\n>, ZodString\n]>> ; strict\n: ZodOptional\n<ZodNullable\n<ZodBoolean\n>> }, \"strip\"\n, ZodTypeAny\n, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}> ; type\n: ZodLiteral\n<\"json_schema\"\n> }, \"strip\"\n, ZodTypeAny\n, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}>, ZodObject\n<{ type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"text\"\n}, { type\n: \"text\"\n}>]>>> ; stop\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodOptional\n<ZodNumber\n> ; tool_choice\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodLiteral\n<\"required\"\n>, ZodObject\n<{ function\n: ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}> ; type\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { function\n: { name\n: string\n} ; type\n: \"function\"\n}, { function\n: { name\n: string\n} ; type\n: \"function\"\n}>]>> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectOutputType\n<{ max_tokens\n: ZodNumber\n; max_tokens_to_sample\n: ZodOptional\n<ZodNumber\n> ; stop_sequences\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodNumber\n; top_k\n: ZodOptional\n<ZodNumber\n> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectOutputType\n<{ maxOutputTokens\n: ZodOptional\n<ZodNumber\n> ; temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; topP\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectOutputType\n<{ temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectOutputType\n<{ use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> ; position?\n: string\n} ; origin?\n: null\n| { project_id?\n: string\n; prompt_id?\n: string\n; prompt_version?\n: string\n} ; parser?\n: null\n| { choice_scores\n: Record\n<string\n, number\n> ; type\n: \"llm_classifier\"\n; use_cot\n: boolean\n} ; prompt?\n: null\n| { content\n: string\n; type\n: \"completion\"\n} | { messages\n: ({ content\n: {} ; name?\n: string\n; role\n: \"system\"\n} | { content\n: {} ; name?\n: string\n; role\n: \"user\"\n} | { content\n: {} ; role\n: \"tool\"\n; tool_call_id\n: string\n} | { content\n: null\n| string\n; name\n: string\n; role\n: \"function\"\n} | { content?\n: null\n| string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text\n: string\n; type\n: \"text\"\n}[] ; function_call?\n: { arguments\n: string\n; name\n: string\n} ; name?\n: string\n; reasoning?\n: { content?\n: string\n; id?\n: string\n}[] ; role\n: \"assistant\"\n; tool_calls?\n: { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[] } | { content?\n: null\n| string\n; role\n: \"model\"\n})[] ; tools?\n: string\n; type\n: \"chat\"\n} ; tool_functions?\n: null\n| ({ id\n: string\n; type\n: \"function\"\n} | { name\n: string\n; type\n: \"global\"\n})[] } ; description?\n: string\n; type\n: \"prompt\"\n}, { default?\n: { options?\n: null\n| { model?\n: string\n; params?\n: objectInputType\n<{ frequency_penalty\n: ZodOptional\n<ZodNumber\n> ; function_call\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}>]>> ; max_completion_tokens\n: ZodOptional\n<ZodNumber\n> ; max_tokens\n: ZodOptional\n<ZodNumber\n> ; n\n: ZodOptional\n<ZodNumber\n> ; presence_penalty\n: ZodOptional\n<ZodNumber\n> ; reasoning_effort\n: ZodOptional\n<ZodEnum\n<[\"low\"\n, \"medium\"\n, \"high\"\n]>> ; response_format\n: ZodOptional\n<ZodNullable\n<ZodUnion\n<[ZodObject\n<{ type\n: ZodLiteral\n<\"json_object\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"json_object\"\n}, { type\n: \"json_object\"\n}>, ZodObject\n<{ json_schema\n: ZodObject\n<{ description\n: ZodOptional\n<ZodString\n> ; name\n: ZodString\n; schema\n: ZodOptional\n<ZodUnion\n<[ZodRecord\n<ZodString\n, ZodUnknown\n>, ZodString\n]>> ; strict\n: ZodOptional\n<ZodNullable\n<ZodBoolean\n>> }, \"strip\"\n, ZodTypeAny\n, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}> ; type\n: ZodLiteral\n<\"json_schema\"\n> }, \"strip\"\n, ZodTypeAny\n, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}>, ZodObject\n<{ type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"text\"\n}, { type\n: \"text\"\n}>]>>> ; stop\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodOptional\n<ZodNumber\n> ; tool_choice\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodLiteral\n<\"required\"\n>, ZodObject\n<{ function\n: ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}> ; type\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { function\n: { name\n: string\n} ; type\n: \"function\"\n}, { function\n: { name\n: string\n} ; type\n: \"function\"\n}>]>> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectInputType\n<{ max_tokens\n: ZodNumber\n; max_tokens_to_sample\n: ZodOptional\n<ZodNumber\n> ; stop_sequences\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodNumber\n; top_k\n: ZodOptional\n<ZodNumber\n> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectInputType\n<{ maxOutputTokens\n: ZodOptional\n<ZodNumber\n> ; temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; topP\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectInputType\n<{ temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectInputType\n<{ use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> ; position?\n: string\n} ; origin?\n: null\n| { project_id?\n: string\n; prompt_id?\n: string\n; prompt_version?\n: string\n} ; parser?\n: null\n| { choice_scores\n: Record\n<string\n, number\n> ; type\n: \"llm_classifier\"\n; use_cot\n: boolean\n} ; prompt?\n: null\n| { content\n: string\n; type\n: \"completion\"\n} | { messages\n: ({ content?\n: string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; name?\n: string\n; role\n: \"system\"\n} | { content?\n: string\n| ({ cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n} | { image_url\n: { detail?\n: \"auto\"\n| \"high\"\n| \"low\"\n; url\n: string\n} ; type\n: \"image_url\"\n})[] ; name?\n: string\n; role\n: \"user\"\n} | { content?\n: string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; role\n: \"tool\"\n; tool_call_id?\n: string\n} | { content\n: null\n| string\n; name\n: string\n; role\n: \"function\"\n} | { content?\n: null\n| string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; function_call?\n: null\n| { arguments\n: string\n; name\n: string\n} ; name?\n: null\n| string\n; reasoning?\n: null\n| { content?\n: null\n| string\n; id?\n: null\n| string\n}[] ; role\n: \"assistant\"\n; tool_calls?\n: null\n| { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[] } | { content?\n: null\n| string\n; role\n: \"model\"\n})[] ; tools?\n: string\n; type\n: \"chat\"\n} ; tool_functions?\n: null\n| ({ id\n: string\n; type\n: \"function\"\n} | { name\n: string\n; type\n: \"global\"\n})[] } ; description?\n: string\n; type\n: \"prompt\"\n}>, ZodObject\n<{ default\n: ZodOptional\n<ZodUnknown\n> ; description\n: ZodOptional\n<ZodString\n> ; schema\n: ZodRecord\n<ZodString\n, ZodUnknown\n> ; type\n: ZodLiteral\n<\"data\"\n> }, \"strip\"\n, ZodTypeAny\n, { default?\n: unknown\n; description?\n: string\n; schema\n: Record\n<string\n, unknown\n> ; type\n: \"data\"\n}, { default?\n: unknown\n; description?\n: string\n; schema\n: Record\n<string\n, unknown\n> ; type\n: \"data\"\n}>]>>> }, \"strip\"\n, ZodTypeAny\n, { parameters?\n: Record\n<string\n, { type: \"prompt\"; default?: { prompt?: { type: \"completion\"; content: string; } | { type: \"chat\"; messages: ({ content: (string | { type: \"text\"; text: string; cache_control?: { type: \"ephemeral\"; } | undefined; }[]) & (string | ... 1 more ... | undefined); role: \"system\"; name?: string | undefined; } | ... 4 more .... | { type: \"data\"; schema: Record<string, unknown>; default?: unknown; description?: string | undefined; }> }, { parameters?\n: Record\n<string\n, { type: \"prompt\"; default?: { prompt?: { type: \"completion\"; content: string; } | { type: \"chat\"; messages: ({ role: \"system\"; content?: string | { type: \"text\"; text?: string | undefined; cache_control?: { ...; } | undefined; }[] | undefined; name?: string | undefined; } | ... 4 more ... | { ...; })[]; tools?: stri... | { type: \"data\"; schema: Record<string, unknown>; default?: unknown; description?: string | undefined; }> }>\nevaluatorDefinitionsSchema\n\u2022 Const\nevaluatorDefinitionsSchema: ZodRecord\n<ZodString\n, ZodObject\n<{ parameters\n: ZodOptional\n<ZodRecord\n<ZodString\n, ZodUnion\n<[ZodObject\n<{ default\n: ZodOptional\n<ZodObject\n<{ options\n: ZodOptional\n<ZodNullable\n<ZodObject\n<{ model\n: ZodOptional\n<ZodString\n> ; params\n: ZodOptional\n<ZodUnion\n<[ZodObject\n<{ frequency_penalty\n: ZodOptional\n<ZodNumber\n> ; function_call\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}>]>> ; max_completion_tokens\n: ZodOptional\n<ZodNumber\n> ; max_tokens\n: ZodOptional\n<ZodNumber\n> ; n\n: ZodOptional\n<ZodNumber\n> ; presence_penalty\n: ZodOptional\n<ZodNumber\n> ; reasoning_effort\n: ZodOptional\n<ZodEnum\n<[\"low\"\n, \"medium\"\n, \"high\"\n]>> ; response_format\n: ZodOptional\n<ZodNullable\n<ZodUnion\n<[ZodObject\n<{ type\n: ZodLiteral\n<\"json_object\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"json_object\"\n}, { type\n: \"json_object\"\n}>, ZodObject\n<{ json_schema\n: ZodObject\n<{ description\n: ZodOptional\n<ZodString\n> ; name\n: ZodString\n; schema\n: ZodOptional\n<ZodUnion\n<[ZodRecord\n<ZodString\n, ZodUnknown\n>, ZodString\n]>> ; strict\n: ZodOptional\n<ZodNullable\n<ZodBoolean\n>> }, \"strip\"\n, ZodTypeAny\n, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}> ; type\n: ZodLiteral\n<\"json_schema\"\n> }, \"strip\"\n, ZodTypeAny\n, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}>, ZodObject\n<{ type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"text\"\n}, { type\n: \"text\"\n}>]>>> ; stop\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodOptional\n<ZodNumber\n> ; tool_choice\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodLiteral\n<\"required\"\n>, ZodObject\n<{ function\n: ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}> ; type\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { function\n: { name\n: string\n} ; type\n: \"function\"\n}, { function\n: { name\n: string\n} ; type\n: \"function\"\n}>]>> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, \"passthrough\"\n, ZodTypeAny\n, objectOutputType\n<{ frequency_penalty\n: ZodOptional\n<ZodNumber\n> ; function_call\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}>]>> ; max_completion_tokens\n: ZodOptional\n<ZodNumber\n> ; max_tokens\n: ZodOptional\n<ZodNumber\n> ; n\n: ZodOptional\n<ZodNumber\n> ; presence_penalty\n: ZodOptional\n<ZodNumber\n> ; reasoning_effort\n: ZodOptional\n<ZodEnum\n<[\"low\"\n, \"medium\"\n, \"high\"\n]>> ; response_format\n: ZodOptional\n<ZodNullable\n<ZodUnion\n<[ZodObject\n<{ type\n: ZodLiteral\n<\"json_object\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"json_object\"\n}, { type\n: \"json_object\"\n}>, ZodObject\n<{ json_schema\n: ZodObject\n<{ description\n: ZodOptional\n<ZodString\n> ; name\n: ZodString\n; schema\n: ZodOptional\n<ZodUnion\n<[ZodRecord\n<ZodString\n, ZodUnknown\n>, ZodString\n]>> ; strict\n: ZodOptional\n<ZodNullable\n<ZodBoolean\n>> }, \"strip\"\n, ZodTypeAny\n, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}> ; type\n: ZodLiteral\n<\"json_schema\"\n> }, \"strip\"\n, ZodTypeAny\n, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}>, ZodObject\n<{ type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"text\"\n}, { type\n: \"text\"\n}>]>>> ; stop\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodOptional\n<ZodNumber\n> ; tool_choice\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodLiteral\n<\"required\"\n>, ZodObject\n<{ function\n: ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}> ; type\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { function\n: { name\n: string\n} ; type\n: \"function\"\n}, { function\n: { name\n: string\n} ; type\n: \"function\"\n}>]>> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>, objectInputType\n<{ frequency_penalty\n: ZodOptional\n<ZodNumber\n> ; function_call\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}>]>> ; max_completion_tokens\n: ZodOptional\n<ZodNumber\n> ; max_tokens\n: ZodOptional\n<ZodNumber\n> ; n\n: ZodOptional\n<ZodNumber\n> ; presence_penalty\n: ZodOptional\n<ZodNumber\n> ; reasoning_effort\n: ZodOptional\n<ZodEnum\n<[\"low\"\n, \"medium\"\n, \"high\"\n]>> ; response_format\n: ZodOptional\n<ZodNullable\n<ZodUnion\n<[ZodObject\n<{ type\n: ZodLiteral\n<\"json_object\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"json_object\"\n}, { type\n: \"json_object\"\n}>, ZodObject\n<{ json_schema\n: ZodObject\n<{ description\n: ZodOptional\n<ZodString\n> ; name\n: ZodString\n; schema\n: ZodOptional\n<ZodUnion\n<[ZodRecord\n<ZodString\n, ZodUnknown\n>, ZodString\n]>> ; strict\n: ZodOptional\n<ZodNullable\n<ZodBoolean\n>> }, \"strip\"\n, ZodTypeAny\n, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}> ; type\n: ZodLiteral\n<\"json_schema\"\n> }, \"strip\"\n, ZodTypeAny\n, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}>, ZodObject\n<{ type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"text\"\n}, { type\n: \"text\"\n}>]>>> ; stop\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodOptional\n<ZodNumber\n> ; tool_choice\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodLiteral\n<\"required\"\n>, ZodObject\n<{ function\n: ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}> ; type\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { function\n: { name\n: string\n} ; type\n: \"function\"\n}, { function\n: { name\n: string\n} ; type\n: \"function\"\n}>]>> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>>, ZodObject\n<{ max_tokens\n: ZodNumber\n; max_tokens_to_sample\n: ZodOptional\n<ZodNumber\n> ; stop_sequences\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodNumber\n; top_k\n: ZodOptional\n<ZodNumber\n> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, \"passthrough\"\n, ZodTypeAny\n, objectOutputType\n<{ max_tokens\n: ZodNumber\n; max_tokens_to_sample\n: ZodOptional\n<ZodNumber\n> ; stop_sequences\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodNumber\n; top_k\n: ZodOptional\n<ZodNumber\n> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>, objectInputType\n<{ max_tokens\n: ZodNumber\n; max_tokens_to_sample\n: ZodOptional\n<ZodNumber\n> ; stop_sequences\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodNumber\n; top_k\n: ZodOptional\n<ZodNumber\n> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>>, ZodObject\n<{ maxOutputTokens\n: ZodOptional\n<ZodNumber\n> ; temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; topP\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, \"passthrough\"\n, ZodTypeAny\n, objectOutputType\n<{ maxOutputTokens\n: ZodOptional\n<ZodNumber\n> ; temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; topP\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>, objectInputType\n<{ maxOutputTokens\n: ZodOptional\n<ZodNumber\n> ; temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; topP\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>>, ZodObject\n<{ temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, \"passthrough\"\n, ZodTypeAny\n, objectOutputType\n<{ temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>, objectInputType\n<{ temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>>, ZodObject\n<{ use_cache\n: ZodOptional\n<ZodBoolean\n> }, \"passthrough\"\n, ZodTypeAny\n, objectOutputType\n<{ use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>, objectInputType\n<{ use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>>]>> ; position\n: ZodOptional\n<ZodString\n> }, \"strip\"\n, ZodTypeAny\n, { model?\n: string\n; params?\n: objectOutputType\n<{ frequency_penalty\n: ZodOptional\n<ZodNumber\n> ; function_call\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}>]>> ; max_completion_tokens\n: ZodOptional\n<ZodNumber\n> ; max_tokens\n: ZodOptional\n<ZodNumber\n> ; n\n: ZodOptional\n<ZodNumber\n> ; presence_penalty\n: ZodOptional\n<ZodNumber\n> ; reasoning_effort\n: ZodOptional\n<ZodEnum\n<[\"low\"\n, \"medium\"\n, \"high\"\n]>> ; response_format\n: ZodOptional\n<ZodNullable\n<ZodUnion\n<[ZodObject\n<{ type\n: ZodLiteral\n<\"json_object\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"json_object\"\n}, { type\n: \"json_object\"\n}>, ZodObject\n<{ json_schema\n: ZodObject\n<{ description\n: ZodOptional\n<ZodString\n> ; name\n: ZodString\n; schema\n: ZodOptional\n<ZodUnion\n<[ZodRecord\n<ZodString\n, ZodUnknown\n>, ZodString\n]>> ; strict\n: ZodOptional\n<ZodNullable\n<ZodBoolean\n>> }, \"strip\"\n, ZodTypeAny\n, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}> ; type\n: ZodLiteral\n<\"json_schema\"\n> }, \"strip\"\n, ZodTypeAny\n, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}>, ZodObject\n<{ type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"text\"\n}, { type\n: \"text\"\n}>]>>> ; stop\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodOptional\n<ZodNumber\n> ; tool_choice\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodLiteral\n<\"required\"\n>, ZodObject\n<{ function\n: ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}> ; type\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { function\n: { name\n: string\n} ; type\n: \"function\"\n}, { function\n: { name\n: string\n} ; type\n: \"function\"\n}>]>> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectOutputType\n<{ max_tokens\n: ZodNumber\n; max_tokens_to_sample\n: ZodOptional\n<ZodNumber\n> ; stop_sequences\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodNumber\n; top_k\n: ZodOptional\n<ZodNumber\n> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectOutputType\n<{ maxOutputTokens\n: ZodOptional\n<ZodNumber\n> ; temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; topP\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectOutputType\n<{ temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectOutputType\n<{ use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> ; position?\n: string\n}, { model?\n: string\n; params?\n: objectInputType\n<{ frequency_penalty\n: ZodOptional\n<ZodNumber\n> ; function_call\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}>]>> ; max_completion_tokens\n: ZodOptional\n<ZodNumber\n> ; max_tokens\n: ZodOptional\n<ZodNumber\n> ; n\n: ZodOptional\n<ZodNumber\n> ; presence_penalty\n: ZodOptional\n<ZodNumber\n> ; reasoning_effort\n: ZodOptional\n<ZodEnum\n<[\"low\"\n, \"medium\"\n, \"high\"\n]>> ; response_format\n: ZodOptional\n<ZodNullable\n<ZodUnion\n<[ZodObject\n<{ type\n: ZodLiteral\n<\"json_object\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"json_object\"\n}, { type\n: \"json_object\"\n}>, ZodObject\n<{ json_schema\n: ZodObject\n<{ description\n: ZodOptional\n<ZodString\n> ; name\n: ZodString\n; schema\n: ZodOptional\n<ZodUnion\n<[ZodRecord\n<ZodString\n, ZodUnknown\n>, ZodString\n]>> ; strict\n: ZodOptional\n<ZodNullable\n<ZodBoolean\n>> }, \"strip\"\n, ZodTypeAny\n, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}> ; type\n: ZodLiteral\n<\"json_schema\"\n> }, \"strip\"\n, ZodTypeAny\n, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}>, ZodObject\n<{ type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"text\"\n}, { type\n: \"text\"\n}>]>>> ; stop\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodOptional\n<ZodNumber\n> ; tool_choice\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodLiteral\n<\"required\"\n>, ZodObject\n<{ function\n: ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}> ; type\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { function\n: { name\n: string\n} ; type\n: \"function\"\n}, { function\n: { name\n: string\n} ; type\n: \"function\"\n}>]>> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectInputType\n<{ max_tokens\n: ZodNumber\n; max_tokens_to_sample\n: ZodOptional\n<ZodNumber\n> ; stop_sequences\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodNumber\n; top_k\n: ZodOptional\n<ZodNumber\n> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectInputType\n<{ maxOutputTokens\n: ZodOptional\n<ZodNumber\n> ; temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; topP\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectInputType\n<{ temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectInputType\n<{ use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> ; position?\n: string\n}>>> ; origin\n: ZodOptional\n<ZodNullable\n<ZodObject\n<{ project_id\n: ZodOptional\n<ZodString\n> ; prompt_id\n: ZodOptional\n<ZodString\n> ; prompt_version\n: ZodOptional\n<ZodString\n> }, \"strip\"\n, ZodTypeAny\n, { project_id?\n: string\n; prompt_id?\n: string\n; prompt_version?\n: string\n}, { project_id?\n: string\n; prompt_id?\n: string\n; prompt_version?\n: string\n}>>> ; parser\n: ZodOptional\n<ZodNullable\n<ZodObject\n<{ choice_scores\n: ZodRecord\n<ZodString\n, ZodNumber\n> ; type\n: ZodLiteral\n<\"llm_classifier\"\n> ; use_cot\n: ZodBoolean\n}, \"strip\"\n, ZodTypeAny\n, { choice_scores\n: Record\n<string\n, number\n> ; type\n: \"llm_classifier\"\n; use_cot\n: boolean\n}, { choice_scores\n: Record\n<string\n, number\n> ; type\n: \"llm_classifier\"\n; use_cot\n: boolean\n}>>> ; prompt\n: ZodOptional\n<ZodNullable\n<ZodUnion\n<[ZodObject\n<{ content\n: ZodString\n; type\n: ZodLiteral\n<\"completion\"\n> }, \"strip\"\n, ZodTypeAny\n, { content\n: string\n; type\n: \"completion\"\n}, { content\n: string\n; type\n: \"completion\"\n}>, ZodObject\n<{ messages\n: ZodArray\n<ZodUnion\n<[ZodUnion\n<[ZodObject\n<{ content\n: ZodUnion\n<[ZodDefault\n<ZodString\n>, ZodArray\n<ZodObject\n<{ cache_control\n: ZodOptional\n<ZodObject\n<{ type\n: ZodEnum\n<[\"ephemeral\"\n]> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"ephemeral\"\n}, { type\n: \"ephemeral\"\n}>> ; text\n: ZodDefault\n<ZodString\n> ; type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text\n: string\n; type\n: \"text\"\n}, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}>, \"many\"\n>]> ; name\n: ZodOptional\n<ZodString\n> ; role\n: ZodLiteral\n<\"system\"\n> }, \"strip\"\n, ZodTypeAny\n, { content\n: {} ; name?\n: string\n; role\n: \"system\"\n}, { content?\n: string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; name?\n: string\n; role\n: \"system\"\n}>, ZodObject\n<{ content\n: ZodUnion\n<[ZodDefault\n<ZodString\n>, ZodArray\n<ZodUnion\n<[ZodObject\n<{ cache_control\n: ZodOptional\n<ZodObject\n<{ type\n: ZodEnum\n<[\"ephemeral\"\n]> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"ephemeral\"\n}, { type\n: \"ephemeral\"\n}>> ; text\n: ZodDefault\n<ZodString\n> ; type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text\n: string\n; type\n: \"text\"\n}, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}>, ZodObject\n<{ image_url\n: ZodObject\n<{ detail\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"low\"\n>, ZodLiteral\n<\"high\"\n>]>> ; url\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { detail?\n: \"auto\"\n| \"high\"\n| \"low\"\n; url\n: string\n}, { detail?\n: \"auto\"\n| \"high\"\n| \"low\"\n; url\n: string\n}> ; type\n: ZodLiteral\n<\"image_url\"\n> }, \"strip\"\n, ZodTypeAny\n, { image_url\n: { detail?\n: \"auto\"\n| \"high\"\n| \"low\"\n; url\n: string\n} ; type\n: \"image_url\"\n}, { image_url\n: { detail?\n: \"auto\"\n| \"high\"\n| \"low\"\n; url\n: string\n} ; type\n: \"image_url\"\n}>]>, \"many\"\n>]> ; name\n: ZodOptional\n<ZodString\n> ; role\n: ZodLiteral\n<\"user\"\n> }, \"strip\"\n, ZodTypeAny\n, { content\n: {} ; name?\n: string\n; role\n: \"user\"\n}, { content?\n: string\n| ({ cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n} | { image_url\n: { detail?\n: \"auto\"\n| \"high\"\n| \"low\"\n; url\n: string\n} ; type\n: \"image_url\"\n})[] ; name?\n: string\n; role\n: \"user\"\n}>, ZodObject\n<{ content\n: ZodOptional\n<ZodNullable\n<ZodUnion\n<[ZodString\n, ZodArray\n<ZodObject\n<{ cache_control\n: ZodOptional\n<ZodObject\n<{ type\n: ZodEnum\n<[\"ephemeral\"\n]> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"ephemeral\"\n}, { type\n: \"ephemeral\"\n}>> ; text\n: ZodDefault\n<ZodString\n> ; type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text\n: string\n; type\n: \"text\"\n}, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}>, \"many\"\n>]>>> ; function_call\n: ZodEffects\n<ZodOptional\n<ZodNullable\n<ZodObject\n<{ arguments\n: ZodString\n; name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { arguments\n: string\n; name\n: string\n}, { arguments\n: string\n; name\n: string\n}>>>, undefined\n| { arguments\n: string\n; name\n: string\n}, undefined\n| null\n| { arguments\n: string\n; name\n: string\n}> ; name\n: ZodEffects\n<ZodOptional\n<ZodNullable\n<ZodString\n>>, undefined\n| string\n, undefined\n| null\n| string\n> ; reasoning\n: ZodEffects\n<ZodOptional\n<ZodNullable\n<ZodArray\n<ZodObject\n<{ content\n: ZodEffects\n<ZodOptional\n<ZodNullable\n<ZodString\n>>, undefined\n| string\n, undefined\n| null\n| string\n> ; id\n: ZodEffects\n<ZodOptional\n<ZodNullable\n<ZodString\n>>, undefined\n| string\n, undefined\n| null\n| string\n> }, \"strip\"\n, ZodTypeAny\n, { content?\n: string\n; id?\n: string\n}, { content?\n: null\n| string\n; id?\n: null\n| string\n}>, \"many\"\n>>>, undefined\n| { content?\n: string\n; id?\n: string\n}[], undefined\n| null\n| { content?\n: null\n| string\n; id?\n: null\n| string\n}[]> ; role\n: ZodLiteral\n<\"assistant\"\n> ; tool_calls\n: ZodEffects\n<ZodOptional\n<ZodNullable\n<ZodArray\n<ZodObject\n<{ function\n: ZodObject\n<{ arguments\n: ZodString\n; name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { arguments\n: string\n; name\n: string\n}, { arguments\n: string\n; name\n: string\n}> ; id\n: ZodString\n; type\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}, { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}>, \"many\"\n>>>, undefined\n| { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[], undefined\n| null\n| { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[]> }, \"strip\"\n, ZodTypeAny\n, { content?\n: null\n| string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text\n: string\n; type\n: \"text\"\n}[] ; function_call?\n: { arguments\n: string\n; name\n: string\n} ; name?\n: string\n; reasoning?\n: { content?\n: string\n; id?\n: string\n}[] ; role\n: \"assistant\"\n; tool_calls?\n: { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[] }, { content?\n: null\n| string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; function_call?\n: null\n| { arguments\n: string\n; name\n: string\n} ; name?\n: null\n| string\n; reasoning?\n: null\n| { content?\n: null\n| string\n; id?\n: null\n| string\n}[] ; role\n: \"assistant\"\n; tool_calls?\n: null\n| { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[] }>, ZodObject\n<{ content\n: ZodUnion\n<[ZodDefault\n<ZodString\n>, ZodArray\n<ZodObject\n<{ cache_control\n: ZodOptional\n<ZodObject\n<{ type\n: ZodEnum\n<[\"ephemeral\"\n]> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"ephemeral\"\n}, { type\n: \"ephemeral\"\n}>> ; text\n: ZodDefault\n<ZodString\n> ; type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text\n: string\n; type\n: \"text\"\n}, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}>, \"many\"\n>]> ; role\n: ZodLiteral\n<\"tool\"\n> ; tool_call_id\n: ZodDefault\n<ZodString\n> }, \"strip\"\n, ZodTypeAny\n, { content\n: {} ; role\n: \"tool\"\n; tool_call_id\n: string\n}, { content?\n: string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; role\n: \"tool\"\n; tool_call_id?\n: string\n}>, ZodObject\n<{ content\n: ZodNullable\n<ZodString\n> ; name\n: ZodString\n; role\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { content\n: null\n| string\n; name\n: string\n; role\n: \"function\"\n}, { content\n: null\n| string\n; name\n: string\n; role\n: \"function\"\n}>]>, ZodObject\n<{ content\n: ZodOptional\n<ZodNullable\n<ZodString\n>> ; role\n: ZodEnum\n<[\"model\"\n]> }, \"strip\"\n, ZodTypeAny\n, { content?\n: null\n| string\n; role\n: \"model\"\n}, { content?\n: null\n| string\n; role\n: \"model\"\n}>]>, \"many\"\n> ; tools\n: ZodOptional\n<ZodString\n> ; type\n: ZodLiteral\n<\"chat\"\n> }, \"strip\"\n, ZodTypeAny\n, { messages\n: ({ content\n: {} ; name?\n: string\n; role\n: \"system\"\n} | { content\n: {} ; name?\n: string\n; role\n: \"user\"\n} | { content\n: {} ; role\n: \"tool\"\n; tool_call_id\n: string\n} | { content\n: null\n| string\n; name\n: string\n; role\n: \"function\"\n} | { content?\n: null\n| string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text\n: string\n; type\n: \"text\"\n}[] ; function_call?\n: { arguments\n: string\n; name\n: string\n} ; name?\n: string\n; reasoning?\n: { content?\n: string\n; id?\n: string\n}[] ; role\n: \"assistant\"\n; tool_calls?\n: { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[] } | { content?\n: null\n| string\n; role\n: \"model\"\n})[] ; tools?\n: string\n; type\n: \"chat\"\n}, { messages\n: ({ content?\n: string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; name?\n: string\n; role\n: \"system\"\n} | { content?\n: string\n| ({ cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n} | { image_url\n: { detail?\n: \"auto\"\n| \"high\"\n| \"low\"\n; url\n: string\n} ; type\n: \"image_url\"\n})[] ; name?\n: string\n; role\n: \"user\"\n} | { content?\n: string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; role\n: \"tool\"\n; tool_call_id?\n: string\n} | { content\n: null\n| string\n; name\n: string\n; role\n: \"function\"\n} | { content?\n: null\n| string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; function_call?\n: null\n| { arguments\n: string\n; name\n: string\n} ; name?\n: null\n| string\n; reasoning?\n: null\n| { content?\n: null\n| string\n; id?\n: null\n| string\n}[] ; role\n: \"assistant\"\n; tool_calls?\n: null\n| { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[] } | { content?\n: null\n| string\n; role\n: \"model\"\n})[] ; tools?\n: string\n; type\n: \"chat\"\n}>]>>> ; tool_functions\n: ZodOptional\n<ZodNullable\n<ZodArray\n<ZodUnion\n<[ZodObject\n<{ id\n: ZodString\n; type\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { id\n: string\n; type\n: \"function\"\n}, { id\n: string\n; type\n: \"function\"\n}>, ZodObject\n<{ name\n: ZodString\n; type\n: ZodLiteral\n<\"global\"\n> }, \"strip\"\n, ZodTypeAny\n, { name\n: string\n; type\n: \"global\"\n}, { name\n: string\n; type\n: \"global\"\n}>]>, \"many\"\n>>> }, \"strip\"\n, ZodTypeAny\n, { options?\n: null\n| { model?\n: string\n; params?\n: objectOutputType\n<{ frequency_penalty\n: ZodOptional\n<ZodNumber\n> ; function_call\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}>]>> ; max_completion_tokens\n: ZodOptional\n<ZodNumber\n> ; max_tokens\n: ZodOptional\n<ZodNumber\n> ; n\n: ZodOptional\n<ZodNumber\n> ; presence_penalty\n: ZodOptional\n<ZodNumber\n> ; reasoning_effort\n: ZodOptional\n<ZodEnum\n<[\"low\"\n, \"medium\"\n, \"high\"\n]>> ; response_format\n: ZodOptional\n<ZodNullable\n<ZodUnion\n<[ZodObject\n<{ type\n: ZodLiteral\n<\"json_object\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"json_object\"\n}, { type\n: \"json_object\"\n}>, ZodObject\n<{ json_schema\n: ZodObject\n<{ description\n: ZodOptional\n<ZodString\n> ; name\n: ZodString\n; schema\n: ZodOptional\n<ZodUnion\n<[ZodRecord\n<ZodString\n, ZodUnknown\n>, ZodString\n]>> ; strict\n: ZodOptional\n<ZodNullable\n<ZodBoolean\n>> }, \"strip\"\n, ZodTypeAny\n, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}> ; type\n: ZodLiteral\n<\"json_schema\"\n> }, \"strip\"\n, ZodTypeAny\n, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}>, ZodObject\n<{ type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"text\"\n}, { type\n: \"text\"\n}>]>>> ; stop\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodOptional\n<ZodNumber\n> ; tool_choice\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodLiteral\n<\"required\"\n>, ZodObject\n<{ function\n: ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}> ; type\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { function\n: { name\n: string\n} ; type\n: \"function\"\n}, { function\n: { name\n: string\n} ; type\n: \"function\"\n}>]>> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectOutputType\n<{ max_tokens\n: ZodNumber\n; max_tokens_to_sample\n: ZodOptional\n<ZodNumber\n> ; stop_sequences\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodNumber\n; top_k\n: ZodOptional\n<ZodNumber\n> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectOutputType\n<{ maxOutputTokens\n: ZodOptional\n<ZodNumber\n> ; temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; topP\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectOutputType\n<{ temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectOutputType\n<{ use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> ; position?\n: string\n} ; origin?\n: null\n| { project_id?\n: string\n; prompt_id?\n: string\n; prompt_version?\n: string\n} ; parser?\n: null\n| { choice_scores\n: Record\n<string\n, number\n> ; type\n: \"llm_classifier\"\n; use_cot\n: boolean\n} ; prompt?\n: null\n| { content\n: string\n; type\n: \"completion\"\n} | { messages\n: ({ content\n: {} ; name?\n: string\n; role\n: \"system\"\n} | { content\n: {} ; name?\n: string\n; role\n: \"user\"\n} | { content\n: {} ; role\n: \"tool\"\n; tool_call_id\n: string\n} | { content\n: null\n| string\n; name\n: string\n; role\n: \"function\"\n} | { content?\n: null\n| string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text\n: string\n; type\n: \"text\"\n}[] ; function_call?\n: { arguments\n: string\n; name\n: string\n} ; name?\n: string\n; reasoning?\n: { content?\n: string\n; id?\n: string\n}[] ; role\n: \"assistant\"\n; tool_calls?\n: { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[] } | { content?\n: null\n| string\n; role\n: \"model\"\n})[] ; tools?\n: string\n; type\n: \"chat\"\n} ; tool_functions?\n: null\n| ({ id\n: string\n; type\n: \"function\"\n} | { name\n: string\n; type\n: \"global\"\n})[] }, { options?\n: null\n| { model?\n: string\n; params?\n: objectInputType\n<{ frequency_penalty\n: ZodOptional\n<ZodNumber\n> ; function_call\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}>]>> ; max_completion_tokens\n: ZodOptional\n<ZodNumber\n> ; max_tokens\n: ZodOptional\n<ZodNumber\n> ; n\n: ZodOptional\n<ZodNumber\n> ; presence_penalty\n: ZodOptional\n<ZodNumber\n> ; reasoning_effort\n: ZodOptional\n<ZodEnum\n<[\"low\"\n, \"medium\"\n, \"high\"\n]>> ; response_format\n: ZodOptional\n<ZodNullable\n<ZodUnion\n<[ZodObject\n<{ type\n: ZodLiteral\n<\"json_object\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"json_object\"\n}, { type\n: \"json_object\"\n}>, ZodObject\n<{ json_schema\n: ZodObject\n<{ description\n: ZodOptional\n<ZodString\n> ; name\n: ZodString\n; schema\n: ZodOptional\n<ZodUnion\n<[ZodRecord\n<ZodString\n, ZodUnknown\n>, ZodString\n]>> ; strict\n: ZodOptional\n<ZodNullable\n<ZodBoolean\n>> }, \"strip\"\n, ZodTypeAny\n, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}> ; type\n: ZodLiteral\n<\"json_schema\"\n> }, \"strip\"\n, ZodTypeAny\n, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}>, ZodObject\n<{ type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"text\"\n}, { type\n: \"text\"\n}>]>>> ; stop\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodOptional\n<ZodNumber\n> ; tool_choice\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodLiteral\n<\"required\"\n>, ZodObject\n<{ function\n: ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}> ; type\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { function\n: { name\n: string\n} ; type\n: \"function\"\n}, { function\n: { name\n: string\n} ; type\n: \"function\"\n}>]>> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectInputType\n<{ max_tokens\n: ZodNumber\n; max_tokens_to_sample\n: ZodOptional\n<ZodNumber\n> ; stop_sequences\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodNumber\n; top_k\n: ZodOptional\n<ZodNumber\n> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectInputType\n<{ maxOutputTokens\n: ZodOptional\n<ZodNumber\n> ; temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; topP\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectInputType\n<{ temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectInputType\n<{ use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> ; position?\n: string\n} ; origin?\n: null\n| { project_id?\n: string\n; prompt_id?\n: string\n; prompt_version?\n: string\n} ; parser?\n: null\n| { choice_scores\n: Record\n<string\n, number\n> ; type\n: \"llm_classifier\"\n; use_cot\n: boolean\n} ; prompt?\n: null\n| { content\n: string\n; type\n: \"completion\"\n} | { messages\n: ({ content?\n: string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; name?\n: string\n; role\n: \"system\"\n} | { content?\n: string\n| ({ cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n} | { image_url\n: { detail?\n: \"auto\"\n| \"high\"\n| \"low\"\n; url\n: string\n} ; type\n: \"image_url\"\n})[] ; name?\n: string\n; role\n: \"user\"\n} | { content?\n: string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; role\n: \"tool\"\n; tool_call_id?\n: string\n} | { content\n: null\n| string\n; name\n: string\n; role\n: \"function\"\n} | { content?\n: null\n| string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; function_call?\n: null\n| { arguments\n: string\n; name\n: string\n} ; name?\n: null\n| string\n; reasoning?\n: null\n| { content?\n: null\n| string\n; id?\n: null\n| string\n}[] ; role\n: \"assistant\"\n; tool_calls?\n: null\n| { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[] } | { content?\n: null\n| string\n; role\n: \"model\"\n})[] ; tools?\n: string\n; type\n: \"chat\"\n} ; tool_functions?\n: null\n| ({ id\n: string\n; type\n: \"function\"\n} | { name\n: string\n; type\n: \"global\"\n})[] }>> ; description\n: ZodOptional\n<ZodString\n> ; type\n: ZodLiteral\n<\"prompt\"\n> }, \"strip\"\n, ZodTypeAny\n, { default?\n: { options?\n: null\n| { model?\n: string\n; params?\n: objectOutputType\n<{ frequency_penalty\n: ZodOptional\n<ZodNumber\n> ; function_call\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}>]>> ; max_completion_tokens\n: ZodOptional\n<ZodNumber\n> ; max_tokens\n: ZodOptional\n<ZodNumber\n> ; n\n: ZodOptional\n<ZodNumber\n> ; presence_penalty\n: ZodOptional\n<ZodNumber\n> ; reasoning_effort\n: ZodOptional\n<ZodEnum\n<[\"low\"\n, \"medium\"\n, \"high\"\n]>> ; response_format\n: ZodOptional\n<ZodNullable\n<ZodUnion\n<[ZodObject\n<{ type\n: ZodLiteral\n<\"json_object\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"json_object\"\n}, { type\n: \"json_object\"\n}>, ZodObject\n<{ json_schema\n: ZodObject\n<{ description\n: ZodOptional\n<ZodString\n> ; name\n: ZodString\n; schema\n: ZodOptional\n<ZodUnion\n<[ZodRecord\n<ZodString\n, ZodUnknown\n>, ZodString\n]>> ; strict\n: ZodOptional\n<ZodNullable\n<ZodBoolean\n>> }, \"strip\"\n, ZodTypeAny\n, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}> ; type\n: ZodLiteral\n<\"json_schema\"\n> }, \"strip\"\n, ZodTypeAny\n, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}>, ZodObject\n<{ type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"text\"\n}, { type\n: \"text\"\n}>]>>> ; stop\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodOptional\n<ZodNumber\n> ; tool_choice\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodLiteral\n<\"required\"\n>, ZodObject\n<{ function\n: ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}> ; type\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { function\n: { name\n: string\n} ; type\n: \"function\"\n}, { function\n: { name\n: string\n} ; type\n: \"function\"\n}>]>> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectOutputType\n<{ max_tokens\n: ZodNumber\n; max_tokens_to_sample\n: ZodOptional\n<ZodNumber\n> ; stop_sequences\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodNumber\n; top_k\n: ZodOptional\n<ZodNumber\n> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectOutputType\n<{ maxOutputTokens\n: ZodOptional\n<ZodNumber\n> ; temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; topP\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectOutputType\n<{ temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectOutputType\n<{ use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> ; position?\n: string\n} ; origin?\n: null\n| { project_id?\n: string\n; prompt_id?\n: string\n; prompt_version?\n: string\n} ; parser?\n: null\n| { choice_scores\n: Record\n<string\n, number\n> ; type\n: \"llm_classifier\"\n; use_cot\n: boolean\n} ; prompt?\n: null\n| { content\n: string\n; type\n: \"completion\"\n} | { messages\n: ({ content\n: {} ; name?\n: string\n; role\n: \"system\"\n} | { content\n: {} ; name?\n: string\n; role\n: \"user\"\n} | { content\n: {} ; role\n: \"tool\"\n; tool_call_id\n: string\n} | { content\n: null\n| string\n; name\n: string\n; role\n: \"function\"\n} | { content?\n: null\n| string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text\n: string\n; type\n: \"text\"\n}[] ; function_call?\n: { arguments\n: string\n; name\n: string\n} ; name?\n: string\n; reasoning?\n: { content?\n: string\n; id?\n: string\n}[] ; role\n: \"assistant\"\n; tool_calls?\n: { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[] } | { content?\n: null\n| string\n; role\n: \"model\"\n})[] ; tools?\n: string\n; type\n: \"chat\"\n} ; tool_functions?\n: null\n| ({ id\n: string\n; type\n: \"function\"\n} | { name\n: string\n; type\n: \"global\"\n})[] } ; description?\n: string\n; type\n: \"prompt\"\n}, { default?\n: { options?\n: null\n| { model?\n: string\n; params?\n: objectInputType\n<{ frequency_penalty\n: ZodOptional\n<ZodNumber\n> ; function_call\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}>]>> ; max_completion_tokens\n: ZodOptional\n<ZodNumber\n> ; max_tokens\n: ZodOptional\n<ZodNumber\n> ; n\n: ZodOptional\n<ZodNumber\n> ; presence_penalty\n: ZodOptional\n<ZodNumber\n> ; reasoning_effort\n: ZodOptional\n<ZodEnum\n<[\"low\"\n, \"medium\"\n, \"high\"\n]>> ; response_format\n: ZodOptional\n<ZodNullable\n<ZodUnion\n<[ZodObject\n<{ type\n: ZodLiteral\n<\"json_object\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"json_object\"\n}, { type\n: \"json_object\"\n}>, ZodObject\n<{ json_schema\n: ZodObject\n<{ description\n: ZodOptional\n<ZodString\n> ; name\n: ZodString\n; schema\n: ZodOptional\n<ZodUnion\n<[ZodRecord\n<ZodString\n, ZodUnknown\n>, ZodString\n]>> ; strict\n: ZodOptional\n<ZodNullable\n<ZodBoolean\n>> }, \"strip\"\n, ZodTypeAny\n, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}> ; type\n: ZodLiteral\n<\"json_schema\"\n> }, \"strip\"\n, ZodTypeAny\n, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}>, ZodObject\n<{ type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"text\"\n}, { type\n: \"text\"\n}>]>>> ; stop\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodOptional\n<ZodNumber\n> ; tool_choice\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodLiteral\n<\"required\"\n>, ZodObject\n<{ function\n: ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}> ; type\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { function\n: { name\n: string\n} ; type\n: \"function\"\n}, { function\n: { name\n: string\n} ; type\n: \"function\"\n}>]>> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectInputType\n<{ max_tokens\n: ZodNumber\n; max_tokens_to_sample\n: ZodOptional\n<ZodNumber\n> ; stop_sequences\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodNumber\n; top_k\n: ZodOptional\n<ZodNumber\n> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectInputType\n<{ maxOutputTokens\n: ZodOptional\n<ZodNumber\n> ; temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; topP\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectInputType\n<{ temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectInputType\n<{ use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> ; position?\n: string\n} ; origin?\n: null\n| { project_id?\n: string\n; prompt_id?\n: string\n; prompt_version?\n: string\n} ; parser?\n: null\n| { choice_scores\n: Record\n<string\n, number\n> ; type\n: \"llm_classifier\"\n; use_cot\n: boolean\n} ; prompt?\n: null\n| { content\n: string\n; type\n: \"completion\"\n} | { messages\n: ({ content?\n: string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; name?\n: string\n; role\n: \"system\"\n} | { content?\n: string\n| ({ cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n} | { image_url\n: { detail?\n: \"auto\"\n| \"high\"\n| \"low\"\n; url\n: string\n} ; type\n: \"image_url\"\n})[] ; name?\n: string\n; role\n: \"user\"\n} | { content?\n: string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; role\n: \"tool\"\n; tool_call_id?\n: string\n} | { content\n: null\n| string\n; name\n: string\n; role\n: \"function\"\n} | { content?\n: null\n| string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; function_call?\n: null\n| { arguments\n: string\n; name\n: string\n} ; name?\n: null\n| string\n; reasoning?\n: null\n| { content?\n: null\n| string\n; id?\n: null\n| string\n}[] ; role\n: \"assistant\"\n; tool_calls?\n: null\n| { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[] } | { content?\n: null\n| string\n; role\n: \"model\"\n})[] ; tools?\n: string\n; type\n: \"chat\"\n} ; tool_functions?\n: null\n| ({ id\n: string\n; type\n: \"function\"\n} | { name\n: string\n; type\n: \"global\"\n})[] } ; description?\n: string\n; type\n: \"prompt\"\n}>, ZodObject\n<{ default\n: ZodOptional\n<ZodUnknown\n> ; description\n: ZodOptional\n<ZodString\n> ; schema\n: ZodRecord\n<ZodString\n, ZodUnknown\n> ; type\n: ZodLiteral\n<\"data\"\n> }, \"strip\"\n, ZodTypeAny\n, { default?\n: unknown\n; description?\n: string\n; schema\n: Record\n<string\n, unknown\n> ; type\n: \"data\"\n}, { default?\n: unknown\n; description?\n: string\n; schema\n: Record\n<string\n, unknown\n> ; type\n: \"data\"\n}>]>>> }, \"strip\"\n, ZodTypeAny\n, { parameters?\n: Record\n<string\n, { type: \"prompt\"; default?: { prompt?: { type: \"completion\"; content: string; } | { type: \"chat\"; messages: ({ content: (string | { type: \"text\"; text: string; cache_control?: { type: \"ephemeral\"; } | undefined; }[]) & (string | ... 1 more ... | undefined); role: \"system\"; name?: string | undefined; } | ... 4 more .... | { type: \"data\"; schema: Record<string, unknown>; default?: unknown; description?: string | undefined; }> }, { parameters?\n: Record\n<string\n, { type: \"prompt\"; default?: { prompt?: { type: \"completion\"; content: string; } | { type: \"chat\"; messages: ({ role: \"system\"; content?: string | { type: \"text\"; text?: string | undefined; cache_control?: { ...; } | undefined; }[] | undefined; name?: string | undefined; } | ... 4 more ... | { ...; })[]; tools?: stri... | { type: \"data\"; schema: Record<string, unknown>; default?: unknown; description?: string | undefined; }> }>>\nprojects\n\u2022 Const\nprojects: ProjectBuilder\npromptContentsSchema\n\u2022 Const\npromptContentsSchema: ZodUnion\n<[ZodObject\n<{ prompt\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { prompt\n: string\n}, { prompt\n: string\n}>, ZodObject\n<{ messages\n: ZodArray\n<ZodUnion\n<[ZodUnion\n<[ZodObject\n<{ content\n: ZodUnion\n<[ZodDefault\n<ZodString\n>, ZodArray\n<ZodObject\n<{ cache_control\n: ZodOptional\n<ZodObject\n<{ type\n: ZodEnum\n<[\"ephemeral\"\n]> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"ephemeral\"\n}, { type\n: \"ephemeral\"\n}>> ; text\n: ZodDefault\n<ZodString\n> ; type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text\n: string\n; type\n: \"text\"\n}, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}>, \"many\"\n>]> ; name\n: ZodOptional\n<ZodString\n> ; role\n: ZodLiteral\n<\"system\"\n> }, \"strip\"\n, ZodTypeAny\n, { content\n: {} ; name?\n: string\n; role\n: \"system\"\n}, { content?\n: string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; name?\n: string\n; role\n: \"system\"\n}>, ZodObject\n<{ content\n: ZodUnion\n<[ZodDefault\n<ZodString\n>, ZodArray\n<ZodUnion\n<[ZodObject\n<{ cache_control\n: ZodOptional\n<ZodObject\n<{ type\n: ZodEnum\n<[\"ephemeral\"\n]> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"ephemeral\"\n}, { type\n: \"ephemeral\"\n}>> ; text\n: ZodDefault\n<ZodString\n> ; type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text\n: string\n; type\n: \"text\"\n}, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}>, ZodObject\n<{ image_url\n: ZodObject\n<{ detail\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"low\"\n>, ZodLiteral\n<\"high\"\n>]>> ; url\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { detail?\n: \"auto\"\n| \"high\"\n| \"low\"\n; url\n: string\n}, { detail?\n: \"auto\"\n| \"high\"\n| \"low\"\n; url\n: string\n}> ; type\n: ZodLiteral\n<\"image_url\"\n> }, \"strip\"\n, ZodTypeAny\n, { image_url\n: { detail?\n: \"auto\"\n| \"high\"\n| \"low\"\n; url\n: string\n} ; type\n: \"image_url\"\n}, { image_url\n: { detail?\n: \"auto\"\n| \"high\"\n| \"low\"\n; url\n: string\n} ; type\n: \"image_url\"\n}>]>, \"many\"\n>]> ; name\n: ZodOptional\n<ZodString\n> ; role\n: ZodLiteral\n<\"user\"\n> }, \"strip\"\n, ZodTypeAny\n, { content\n: {} ; name?\n: string\n; role\n: \"user\"\n}, { content?\n: string\n| ({ cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n} | { image_url\n: { detail?\n: \"auto\"\n| \"high\"\n| \"low\"\n; url\n: string\n} ; type\n: \"image_url\"\n})[] ; name?\n: string\n; role\n: \"user\"\n}>, ZodObject\n<{ content\n: ZodOptional\n<ZodNullable\n<ZodUnion\n<[ZodString\n, ZodArray\n<ZodObject\n<{ cache_control\n: ZodOptional\n<ZodObject\n<{ type\n: ZodEnum\n<[\"ephemeral\"\n]> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"ephemeral\"\n}, { type\n: \"ephemeral\"\n}>> ; text\n: ZodDefault\n<ZodString\n> ; type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text\n: string\n; type\n: \"text\"\n}, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}>, \"many\"\n>]>>> ; function_call\n: ZodEffects\n<ZodOptional\n<ZodNullable\n<ZodObject\n<{ arguments\n: ZodString\n; name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { arguments\n: string\n; name\n: string\n}, { arguments\n: string\n; name\n: string\n}>>>, undefined\n| { arguments\n: string\n; name\n: string\n}, undefined\n| null\n| { arguments\n: string\n; name\n: string\n}> ; name\n: ZodEffects\n<ZodOptional\n<ZodNullable\n<ZodString\n>>, undefined\n| string\n, undefined\n| null\n| string\n> ; reasoning\n: ZodEffects\n<ZodOptional\n<ZodNullable\n<ZodArray\n<ZodObject\n<{ content\n: ZodEffects\n<ZodOptional\n<ZodNullable\n<ZodString\n>>, undefined\n| string\n, undefined\n| null\n| string\n> ; id\n: ZodEffects\n<ZodOptional\n<ZodNullable\n<ZodString\n>>, undefined\n| string\n, undefined\n| null\n| string\n> }, \"strip\"\n, ZodTypeAny\n, { content?\n: string\n; id?\n: string\n}, { content?\n: null\n| string\n; id?\n: null\n| string\n}>, \"many\"\n>>>, undefined\n| { content?\n: string\n; id?\n: string\n}[], undefined\n| null\n| { content?\n: null\n| string\n; id?\n: null\n| string\n}[]> ; role\n: ZodLiteral\n<\"assistant\"\n> ; tool_calls\n: ZodEffects\n<ZodOptional\n<ZodNullable\n<ZodArray\n<ZodObject\n<{ function\n: ZodObject\n<{ arguments\n: ZodString\n; name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { arguments\n: string\n; name\n: string\n}, { arguments\n: string\n; name\n: string\n}> ; id\n: ZodString\n; type\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}, { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}>, \"many\"\n>>>, undefined\n| { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[], undefined\n| null\n| { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[]> }, \"strip\"\n, ZodTypeAny\n, { content?\n: null\n| string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text\n: string\n; type\n: \"text\"\n}[] ; function_call?\n: { arguments\n: string\n; name\n: string\n} ; name?\n: string\n; reasoning?\n: { content?\n: string\n; id?\n: string\n}[] ; role\n: \"assistant\"\n; tool_calls?\n: { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[] }, { content?\n: null\n| string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; function_call?\n: null\n| { arguments\n: string\n; name\n: string\n} ; name?\n: null\n| string\n; reasoning?\n: null\n| { content?\n: null\n| string\n; id?\n: null\n| string\n}[] ; role\n: \"assistant\"\n; tool_calls?\n: null\n| { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[] }>, ZodObject\n<{ content\n: ZodUnion\n<[ZodDefault\n<ZodString\n>, ZodArray\n<ZodObject\n<{ cache_control\n: ZodOptional\n<ZodObject\n<{ type\n: ZodEnum\n<[\"ephemeral\"\n]> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"ephemeral\"\n}, { type\n: \"ephemeral\"\n}>> ; text\n: ZodDefault\n<ZodString\n> ; type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text\n: string\n; type\n: \"text\"\n}, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}>, \"many\"\n>]> ; role\n: ZodLiteral\n<\"tool\"\n> ; tool_call_id\n: ZodDefault\n<ZodString\n> }, \"strip\"\n, ZodTypeAny\n, { content\n: {} ; role\n: \"tool\"\n; tool_call_id\n: string\n}, { content?\n: string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; role\n: \"tool\"\n; tool_call_id?\n: string\n}>, ZodObject\n<{ content\n: ZodNullable\n<ZodString\n> ; name\n: ZodString\n; role\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { content\n: null\n| string\n; name\n: string\n; role\n: \"function\"\n}, { content\n: null\n| string\n; name\n: string\n; role\n: \"function\"\n}>]>, ZodObject\n<{ content\n: ZodOptional\n<ZodNullable\n<ZodString\n>> ; role\n: ZodEnum\n<[\"model\"\n]> }, \"strip\"\n, ZodTypeAny\n, { content?\n: null\n| string\n; role\n: \"model\"\n}, { content?\n: null\n| string\n; role\n: \"model\"\n}>]>, \"many\"\n> }, \"strip\"\n, ZodTypeAny\n, { messages\n: ({ content\n: {} ; name?\n: string\n; role\n: \"system\"\n} | { content\n: {} ; name?\n: string\n; role\n: \"user\"\n} | { content?\n: null\n| string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text\n: string\n; type\n: \"text\"\n}[] ; function_call?\n: { arguments\n: string\n; name\n: string\n} ; name?\n: string\n; reasoning?\n: { content?\n: string\n; id?\n: string\n}[] ; role\n: \"assistant\"\n; tool_calls?\n: { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[] } | { content\n: {} ; role\n: \"tool\"\n; tool_call_id\n: string\n} | { content\n: null\n| string\n; name\n: string\n; role\n: \"function\"\n} | { content?\n: null\n| string\n; role\n: \"model\"\n})[] }, { messages\n: ({ content?\n: string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; name?\n: string\n; role\n: \"system\"\n} | { content?\n: string\n| ({ cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n} | { image_url\n: { detail?\n: \"auto\"\n| \"high\"\n| \"low\"\n; url\n: string\n} ; type\n: \"image_url\"\n})[] ; name?\n: string\n; role\n: \"user\"\n} | { content?\n: null\n| string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; function_call?\n: null\n| { arguments\n: string\n; name\n: string\n} ; name?\n: null\n| string\n; reasoning?\n: null\n| { content?\n: null\n| string\n; id?\n: null\n| string\n}[] ; role\n: \"assistant\"\n; tool_calls?\n: null\n| { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[] } | { content?\n: string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; role\n: \"tool\"\n; tool_call_id?\n: string\n} | { content\n: null\n| string\n; name\n: string\n; role\n: \"function\"\n} | { content?\n: null\n| string\n; role\n: \"model\"\n})[] }>]>\npromptDefinitionSchema\n\u2022 Const\npromptDefinitionSchema: ZodIntersection\n<ZodUnion\n<[ZodObject\n<{ prompt\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { prompt\n: string\n}, { prompt\n: string\n}>, ZodObject\n<{ messages\n: ZodArray\n<ZodUnion\n<[ZodUnion\n<[ZodObject\n<{ content\n: ZodUnion\n<[ZodDefault\n<ZodString\n>, ZodArray\n<ZodObject\n<{ cache_control\n: ZodOptional\n<ZodObject\n<{ type\n: ZodEnum\n<[\"ephemeral\"\n]> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"ephemeral\"\n}, { type\n: \"ephemeral\"\n}>> ; text\n: ZodDefault\n<ZodString\n> ; type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text\n: string\n; type\n: \"text\"\n}, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}>, \"many\"\n>]> ; name\n: ZodOptional\n<ZodString\n> ; role\n: ZodLiteral\n<\"system\"\n> }, \"strip\"\n, ZodTypeAny\n, { content\n: {} ; name?\n: string\n; role\n: \"system\"\n}, { content?\n: string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; name?\n: string\n; role\n: \"system\"\n}>, ZodObject\n<{ content\n: ZodUnion\n<[ZodDefault\n<ZodString\n>, ZodArray\n<ZodUnion\n<[ZodObject\n<{ cache_control\n: ZodOptional\n<ZodObject\n<{ type\n: ZodEnum\n<[\"ephemeral\"\n]> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"ephemeral\"\n}, { type\n: \"ephemeral\"\n}>> ; text\n: ZodDefault\n<ZodString\n> ; type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text\n: string\n; type\n: \"text\"\n}, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}>, ZodObject\n<{ image_url\n: ZodObject\n<{ detail\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"low\"\n>, ZodLiteral\n<\"high\"\n>]>> ; url\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { detail?\n: \"auto\"\n| \"high\"\n| \"low\"\n; url\n: string\n}, { detail?\n: \"auto\"\n| \"high\"\n| \"low\"\n; url\n: string\n}> ; type\n: ZodLiteral\n<\"image_url\"\n> }, \"strip\"\n, ZodTypeAny\n, { image_url\n: { detail?\n: \"auto\"\n| \"high\"\n| \"low\"\n; url\n: string\n} ; type\n: \"image_url\"\n}, { image_url\n: { detail?\n: \"auto\"\n| \"high\"\n| \"low\"\n; url\n: string\n} ; type\n: \"image_url\"\n}>]>, \"many\"\n>]> ; name\n: ZodOptional\n<ZodString\n> ; role\n: ZodLiteral\n<\"user\"\n> }, \"strip\"\n, ZodTypeAny\n, { content\n: {} ; name?\n: string\n; role\n: \"user\"\n}, { content?\n: string\n| ({ cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n} | { image_url\n: { detail?\n: \"auto\"\n| \"high\"\n| \"low\"\n; url\n: string\n} ; type\n: \"image_url\"\n})[] ; name?\n: string\n; role\n: \"user\"\n}>, ZodObject\n<{ content\n: ZodOptional\n<ZodNullable\n<ZodUnion\n<[ZodString\n, ZodArray\n<ZodObject\n<{ cache_control\n: ZodOptional\n<ZodObject\n<{ type\n: ZodEnum\n<[\"ephemeral\"\n]> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"ephemeral\"\n}, { type\n: \"ephemeral\"\n}>> ; text\n: ZodDefault\n<ZodString\n> ; type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text\n: string\n; type\n: \"text\"\n}, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}>, \"many\"\n>]>>> ; function_call\n: ZodEffects\n<ZodOptional\n<ZodNullable\n<ZodObject\n<{ arguments\n: ZodString\n; name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { arguments\n: string\n; name\n: string\n}, { arguments\n: string\n; name\n: string\n}>>>, undefined\n| { arguments\n: string\n; name\n: string\n}, undefined\n| null\n| { arguments\n: string\n; name\n: string\n}> ; name\n: ZodEffects\n<ZodOptional\n<ZodNullable\n<ZodString\n>>, undefined\n| string\n, undefined\n| null\n| string\n> ; reasoning\n: ZodEffects\n<ZodOptional\n<ZodNullable\n<ZodArray\n<ZodObject\n<{ content\n: ZodEffects\n<ZodOptional\n<ZodNullable\n<ZodString\n>>, undefined\n| string\n, undefined\n| null\n| string\n> ; id\n: ZodEffects\n<ZodOptional\n<ZodNullable\n<ZodString\n>>, undefined\n| string\n, undefined\n| null\n| string\n> }, \"strip\"\n, ZodTypeAny\n, { content?\n: string\n; id?\n: string\n}, { content?\n: null\n| string\n; id?\n: null\n| string\n}>, \"many\"\n>>>, undefined\n| { content?\n: string\n; id?\n: string\n}[], undefined\n| null\n| { content?\n: null\n| string\n; id?\n: null\n| string\n}[]> ; role\n: ZodLiteral\n<\"assistant\"\n> ; tool_calls\n: ZodEffects\n<ZodOptional\n<ZodNullable\n<ZodArray\n<ZodObject\n<{ function\n: ZodObject\n<{ arguments\n: ZodString\n; name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { arguments\n: string\n; name\n: string\n}, { arguments\n: string\n; name\n: string\n}> ; id\n: ZodString\n; type\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}, { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}>, \"many\"\n>>>, undefined\n| { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[], undefined\n| null\n| { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[]> }, \"strip\"\n, ZodTypeAny\n, { content?\n: null\n| string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text\n: string\n; type\n: \"text\"\n}[] ; function_call?\n: { arguments\n: string\n; name\n: string\n} ; name?\n: string\n; reasoning?\n: { content?\n: string\n; id?\n: string\n}[] ; role\n: \"assistant\"\n; tool_calls?\n: { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[] }, { content?\n: null\n| string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; function_call?\n: null\n| { arguments\n: string\n; name\n: string\n} ; name?\n: null\n| string\n; reasoning?\n: null\n| { content?\n: null\n| string\n; id?\n: null\n| string\n}[] ; role\n: \"assistant\"\n; tool_calls?\n: null\n| { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[] }>, ZodObject\n<{ content\n: ZodUnion\n<[ZodDefault\n<ZodString\n>, ZodArray\n<ZodObject\n<{ cache_control\n: ZodOptional\n<ZodObject\n<{ type\n: ZodEnum\n<[\"ephemeral\"\n]> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"ephemeral\"\n}, { type\n: \"ephemeral\"\n}>> ; text\n: ZodDefault\n<ZodString\n> ; type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text\n: string\n; type\n: \"text\"\n}, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}>, \"many\"\n>]> ; role\n: ZodLiteral\n<\"tool\"\n> ; tool_call_id\n: ZodDefault\n<ZodString\n> }, \"strip\"\n, ZodTypeAny\n, { content\n: {} ; role\n: \"tool\"\n; tool_call_id\n: string\n}, { content?\n: string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; role\n: \"tool\"\n; tool_call_id?\n: string\n}>, ZodObject\n<{ content\n: ZodNullable\n<ZodString\n> ; name\n: ZodString\n; role\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { content\n: null\n| string\n; name\n: string\n; role\n: \"function\"\n}, { content\n: null\n| string\n; name\n: string\n; role\n: \"function\"\n}>]>, ZodObject\n<{ content\n: ZodOptional\n<ZodNullable\n<ZodString\n>> ; role\n: ZodEnum\n<[\"model\"\n]> }, \"strip\"\n, ZodTypeAny\n, { content?\n: null\n| string\n; role\n: \"model\"\n}, { content?\n: null\n| string\n; role\n: \"model\"\n}>]>, \"many\"\n> }, \"strip\"\n, ZodTypeAny\n, { messages\n: ({ content\n: {} ; name?\n: string\n; role\n: \"system\"\n} | { content\n: {} ; name?\n: string\n; role\n: \"user\"\n} | { content?\n: null\n| string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text\n: string\n; type\n: \"text\"\n}[] ; function_call?\n: { arguments\n: string\n; name\n: string\n} ; name?\n: string\n; reasoning?\n: { content?\n: string\n; id?\n: string\n}[] ; role\n: \"assistant\"\n; tool_calls?\n: { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[] } | { content\n: {} ; role\n: \"tool\"\n; tool_call_id\n: string\n} | { content\n: null\n| string\n; name\n: string\n; role\n: \"function\"\n} | { content?\n: null\n| string\n; role\n: \"model\"\n})[] }, { messages\n: ({ content?\n: string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; name?\n: string\n; role\n: \"system\"\n} | { content?\n: string\n| ({ cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n} | { image_url\n: { detail?\n: \"auto\"\n| \"high\"\n| \"low\"\n; url\n: string\n} ; type\n: \"image_url\"\n})[] ; name?\n: string\n; role\n: \"user\"\n} | { content?\n: null\n| string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; function_call?\n: null\n| { arguments\n: string\n; name\n: string\n} ; name?\n: null\n| string\n; reasoning?\n: null\n| { content?\n: null\n| string\n; id?\n: null\n| string\n}[] ; role\n: \"assistant\"\n; tool_calls?\n: null\n| { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[] } | { content?\n: string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; role\n: \"tool\"\n; tool_call_id?\n: string\n} | { content\n: null\n| string\n; name\n: string\n; role\n: \"function\"\n} | { content?\n: null\n| string\n; role\n: \"model\"\n})[] }>]>, ZodObject\n<{ model\n: ZodString\n; params\n: ZodOptional\n<ZodUnion\n<[ZodObject\n<{ frequency_penalty\n: ZodOptional\n<ZodNumber\n> ; function_call\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}>]>> ; max_completion_tokens\n: ZodOptional\n<ZodNumber\n> ; max_tokens\n: ZodOptional\n<ZodNumber\n> ; n\n: ZodOptional\n<ZodNumber\n> ; presence_penalty\n: ZodOptional\n<ZodNumber\n> ; reasoning_effort\n: ZodOptional\n<ZodEnum\n<[\"low\"\n, \"medium\"\n, \"high\"\n]>> ; response_format\n: ZodOptional\n<ZodNullable\n<ZodUnion\n<[ZodObject\n<{ type\n: ZodLiteral\n<\"json_object\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"json_object\"\n}, { type\n: \"json_object\"\n}>, ZodObject\n<{ json_schema\n: ZodObject\n<{ description\n: ZodOptional\n<ZodString\n> ; name\n: ZodString\n; schema\n: ZodOptional\n<ZodUnion\n<[ZodRecord\n<ZodString\n, ZodUnknown\n>, ZodString\n]>> ; strict\n: ZodOptional\n<ZodNullable\n<ZodBoolean\n>> }, \"strip\"\n, ZodTypeAny\n, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}> ; type\n: ZodLiteral\n<\"json_schema\"\n> }, \"strip\"\n, ZodTypeAny\n, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}>, ZodObject\n<{ type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"text\"\n}, { type\n: \"text\"\n}>]>>> ; stop\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodOptional\n<ZodNumber\n> ; tool_choice\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodLiteral\n<\"required\"\n>, ZodObject\n<{ function\n: ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}> ; type\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { function\n: { name\n: string\n} ; type\n: \"function\"\n}, { function\n: { name\n: string\n} ; type\n: \"function\"\n}>]>> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, \"passthrough\"\n, ZodTypeAny\n, objectOutputType\n<{ frequency_penalty\n: ZodOptional\n<ZodNumber\n> ; function_call\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}>]>> ; max_completion_tokens\n: ZodOptional\n<ZodNumber\n> ; max_tokens\n: ZodOptional\n<ZodNumber\n> ; n\n: ZodOptional\n<ZodNumber\n> ; presence_penalty\n: ZodOptional\n<ZodNumber\n> ; reasoning_effort\n: ZodOptional\n<ZodEnum\n<[\"low\"\n, \"medium\"\n, \"high\"\n]>> ; response_format\n: ZodOptional\n<ZodNullable\n<ZodUnion\n<[ZodObject\n<{ type\n: ZodLiteral\n<\"json_object\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"json_object\"\n}, { type\n: \"json_object\"\n}>, ZodObject\n<{ json_schema\n: ZodObject\n<{ description\n: ZodOptional\n<ZodString\n> ; name\n: ZodString\n; schema\n: ZodOptional\n<ZodUnion\n<[ZodRecord\n<ZodString\n, ZodUnknown\n>, ZodString\n]>> ; strict\n: ZodOptional\n<ZodNullable\n<ZodBoolean\n>> }, \"strip\"\n, ZodTypeAny\n, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}> ; type\n: ZodLiteral\n<\"json_schema\"\n> }, \"strip\"\n, ZodTypeAny\n, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}>, ZodObject\n<{ type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"text\"\n}, { type\n: \"text\"\n}>]>>> ; stop\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodOptional\n<ZodNumber\n> ; tool_choice\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodLiteral\n<\"required\"\n>, ZodObject\n<{ function\n: ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}> ; type\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { function\n: { name\n: string\n} ; type\n: \"function\"\n}, { function\n: { name\n: string\n} ; type\n: \"function\"\n}>]>> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>, objectInputType\n<{ frequency_penalty\n: ZodOptional\n<ZodNumber\n> ; function_call\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}>]>> ; max_completion_tokens\n: ZodOptional\n<ZodNumber\n> ; max_tokens\n: ZodOptional\n<ZodNumber\n> ; n\n: ZodOptional\n<ZodNumber\n> ; presence_penalty\n: ZodOptional\n<ZodNumber\n> ; reasoning_effort\n: ZodOptional\n<ZodEnum\n<[\"low\"\n, \"medium\"\n, \"high\"\n]>> ; response_format\n: ZodOptional\n<ZodNullable\n<ZodUnion\n<[ZodObject\n<{ type\n: ZodLiteral\n<\"json_object\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"json_object\"\n}, { type\n: \"json_object\"\n}>, ZodObject\n<{ json_schema\n: ZodObject\n<{ description\n: ZodOptional\n<ZodString\n> ; name\n: ZodString\n; schema\n: ZodOptional\n<ZodUnion\n<[ZodRecord\n<ZodString\n, ZodUnknown\n>, ZodString\n]>> ; strict\n: ZodOptional\n<ZodNullable\n<ZodBoolean\n>> }, \"strip\"\n, ZodTypeAny\n, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}> ; type\n: ZodLiteral\n<\"json_schema\"\n> }, \"strip\"\n, ZodTypeAny\n, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}>, ZodObject\n<{ type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"text\"\n}, { type\n: \"text\"\n}>]>>> ; stop\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodOptional\n<ZodNumber\n> ; tool_choice\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodLiteral\n<\"required\"\n>, ZodObject\n<{ function\n: ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}> ; type\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { function\n: { name\n: string\n} ; type\n: \"function\"\n}, { function\n: { name\n: string\n} ; type\n: \"function\"\n}>]>> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>>, ZodObject\n<{ max_tokens\n: ZodNumber\n; max_tokens_to_sample\n: ZodOptional\n<ZodNumber\n> ; stop_sequences\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodNumber\n; top_k\n: ZodOptional\n<ZodNumber\n> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, \"passthrough\"\n, ZodTypeAny\n, objectOutputType\n<{ max_tokens\n: ZodNumber\n; max_tokens_to_sample\n: ZodOptional\n<ZodNumber\n> ; stop_sequences\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodNumber\n; top_k\n: ZodOptional\n<ZodNumber\n> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>, objectInputType\n<{ max_tokens\n: ZodNumber\n; max_tokens_to_sample\n: ZodOptional\n<ZodNumber\n> ; stop_sequences\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodNumber\n; top_k\n: ZodOptional\n<ZodNumber\n> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>>, ZodObject\n<{ maxOutputTokens\n: ZodOptional\n<ZodNumber\n> ; temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; topP\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, \"passthrough\"\n, ZodTypeAny\n, objectOutputType\n<{ maxOutputTokens\n: ZodOptional\n<ZodNumber\n> ; temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; topP\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>, objectInputType\n<{ maxOutputTokens\n: ZodOptional\n<ZodNumber\n> ; temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; topP\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>>, ZodObject\n<{ temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, \"passthrough\"\n, ZodTypeAny\n, objectOutputType\n<{ temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>, objectInputType\n<{ temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>>, ZodObject\n<{ use_cache\n: ZodOptional\n<ZodBoolean\n> }, \"passthrough\"\n, ZodTypeAny\n, objectOutputType\n<{ use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>, objectInputType\n<{ use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>>]>> }, \"strip\"\n, ZodTypeAny\n, { model\n: string\n; params?\n: objectOutputType\n<{ frequency_penalty\n: ZodOptional\n<ZodNumber\n> ; function_call\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}>]>> ; max_completion_tokens\n: ZodOptional\n<ZodNumber\n> ; max_tokens\n: ZodOptional\n<ZodNumber\n> ; n\n: ZodOptional\n<ZodNumber\n> ; presence_penalty\n: ZodOptional\n<ZodNumber\n> ; reasoning_effort\n: ZodOptional\n<ZodEnum\n<[\"low\"\n, \"medium\"\n, \"high\"\n]>> ; response_format\n: ZodOptional\n<ZodNullable\n<ZodUnion\n<[ZodObject\n<{ type\n: ZodLiteral\n<\"json_object\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"json_object\"\n}, { type\n: \"json_object\"\n}>, ZodObject\n<{ json_schema\n: ZodObject\n<{ description\n: ZodOptional\n<ZodString\n> ; name\n: ZodString\n; schema\n: ZodOptional\n<ZodUnion\n<[ZodRecord\n<ZodString\n, ZodUnknown\n>, ZodString\n]>> ; strict\n: ZodOptional\n<ZodNullable\n<ZodBoolean\n>> }, \"strip\"\n, ZodTypeAny\n, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}> ; type\n: ZodLiteral\n<\"json_schema\"\n> }, \"strip\"\n, ZodTypeAny\n, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}>, ZodObject\n<{ type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"text\"\n}, { type\n: \"text\"\n}>]>>> ; stop\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodOptional\n<ZodNumber\n> ; tool_choice\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodLiteral\n<\"required\"\n>, ZodObject\n<{ function\n: ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}> ; type\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { function\n: { name\n: string\n} ; type\n: \"function\"\n}, { function\n: { name\n: string\n} ; type\n: \"function\"\n}>]>> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectOutputType\n<{ max_tokens\n: ZodNumber\n; max_tokens_to_sample\n: ZodOptional\n<ZodNumber\n> ; stop_sequences\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodNumber\n; top_k\n: ZodOptional\n<ZodNumber\n> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectOutputType\n<{ maxOutputTokens\n: ZodOptional\n<ZodNumber\n> ; temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; topP\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectOutputType\n<{ temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectOutputType\n<{ use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> }, { model\n: string\n; params?\n: objectInputType\n<{ frequency_penalty\n: ZodOptional\n<ZodNumber\n> ; function_call\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}>]>> ; max_completion_tokens\n: ZodOptional\n<ZodNumber\n> ; max_tokens\n: ZodOptional\n<ZodNumber\n> ; n\n: ZodOptional\n<ZodNumber\n> ; presence_penalty\n: ZodOptional\n<ZodNumber\n> ; reasoning_effort\n: ZodOptional\n<ZodEnum\n<[\"low\"\n, \"medium\"\n, \"high\"\n]>> ; response_format\n: ZodOptional\n<ZodNullable\n<ZodUnion\n<[ZodObject\n<{ type\n: ZodLiteral\n<\"json_object\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"json_object\"\n}, { type\n: \"json_object\"\n}>, ZodObject\n<{ json_schema\n: ZodObject\n<{ description\n: ZodOptional\n<ZodString\n> ; name\n: ZodString\n; schema\n: ZodOptional\n<ZodUnion\n<[ZodRecord\n<ZodString\n, ZodUnknown\n>, ZodString\n]>> ; strict\n: ZodOptional\n<ZodNullable\n<ZodBoolean\n>> }, \"strip\"\n, ZodTypeAny\n, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}> ; type\n: ZodLiteral\n<\"json_schema\"\n> }, \"strip\"\n, ZodTypeAny\n, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}>, ZodObject\n<{ type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"text\"\n}, { type\n: \"text\"\n}>]>>> ; stop\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodOptional\n<ZodNumber\n> ; tool_choice\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodLiteral\n<\"required\"\n>, ZodObject\n<{ function\n: ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}> ; type\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { function\n: { name\n: string\n} ; type\n: \"function\"\n}, { function\n: { name\n: string\n} ; type\n: \"function\"\n}>]>> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectInputType\n<{ max_tokens\n: ZodNumber\n; max_tokens_to_sample\n: ZodOptional\n<ZodNumber\n> ; stop_sequences\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodNumber\n; top_k\n: ZodOptional\n<ZodNumber\n> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectInputType\n<{ maxOutputTokens\n: ZodOptional\n<ZodNumber\n> ; temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; topP\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectInputType\n<{ temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectInputType\n<{ use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> }>>\npromptDefinitionWithToolsSchema\n\u2022 Const\npromptDefinitionWithToolsSchema: ZodIntersection\n<ZodIntersection\n<ZodUnion\n<[ZodObject\n<{ prompt\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { prompt\n: string\n}, { prompt\n: string\n}>, ZodObject\n<{ messages\n: ZodArray\n<ZodUnion\n<[ZodUnion\n<[ZodObject\n<{ content\n: ZodUnion\n<[ZodDefault\n<ZodString\n>, ZodArray\n<ZodObject\n<{ cache_control\n: ZodOptional\n<ZodObject\n<{ type\n: ZodEnum\n<[\"ephemeral\"\n]> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"ephemeral\"\n}, { type\n: \"ephemeral\"\n}>> ; text\n: ZodDefault\n<ZodString\n> ; type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text\n: string\n; type\n: \"text\"\n}, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}>, \"many\"\n>]> ; name\n: ZodOptional\n<ZodString\n> ; role\n: ZodLiteral\n<\"system\"\n> }, \"strip\"\n, ZodTypeAny\n, { content\n: {} ; name?\n: string\n; role\n: \"system\"\n}, { content?\n: string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; name?\n: string\n; role\n: \"system\"\n}>, ZodObject\n<{ content\n: ZodUnion\n<[ZodDefault\n<ZodString\n>, ZodArray\n<ZodUnion\n<[ZodObject\n<{ cache_control\n: ZodOptional\n<ZodObject\n<{ type\n: ZodEnum\n<[\"ephemeral\"\n]> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"ephemeral\"\n}, { type\n: \"ephemeral\"\n}>> ; text\n: ZodDefault\n<ZodString\n> ; type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text\n: string\n; type\n: \"text\"\n}, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}>, ZodObject\n<{ image_url\n: ZodObject\n<{ detail\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"low\"\n>, ZodLiteral\n<\"high\"\n>]>> ; url\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { detail?\n: \"auto\"\n| \"high\"\n| \"low\"\n; url\n: string\n}, { detail?\n: \"auto\"\n| \"high\"\n| \"low\"\n; url\n: string\n}> ; type\n: ZodLiteral\n<\"image_url\"\n> }, \"strip\"\n, ZodTypeAny\n, { image_url\n: { detail?\n: \"auto\"\n| \"high\"\n| \"low\"\n; url\n: string\n} ; type\n: \"image_url\"\n}, { image_url\n: { detail?\n: \"auto\"\n| \"high\"\n| \"low\"\n; url\n: string\n} ; type\n: \"image_url\"\n}>]>, \"many\"\n>]> ; name\n: ZodOptional\n<ZodString\n> ; role\n: ZodLiteral\n<\"user\"\n> }, \"strip\"\n, ZodTypeAny\n, { content\n: {} ; name?\n: string\n; role\n: \"user\"\n}, { content?\n: string\n| ({ cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n} | { image_url\n: { detail?\n: \"auto\"\n| \"high\"\n| \"low\"\n; url\n: string\n} ; type\n: \"image_url\"\n})[] ; name?\n: string\n; role\n: \"user\"\n}>, ZodObject\n<{ content\n: ZodOptional\n<ZodNullable\n<ZodUnion\n<[ZodString\n, ZodArray\n<ZodObject\n<{ cache_control\n: ZodOptional\n<ZodObject\n<{ type\n: ZodEnum\n<[\"ephemeral\"\n]> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"ephemeral\"\n}, { type\n: \"ephemeral\"\n}>> ; text\n: ZodDefault\n<ZodString\n> ; type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text\n: string\n; type\n: \"text\"\n}, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}>, \"many\"\n>]>>> ; function_call\n: ZodEffects\n<ZodOptional\n<ZodNullable\n<ZodObject\n<{ arguments\n: ZodString\n; name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { arguments\n: string\n; name\n: string\n}, { arguments\n: string\n; name\n: string\n}>>>, undefined\n| { arguments\n: string\n; name\n: string\n}, undefined\n| null\n| { arguments\n: string\n; name\n: string\n}> ; name\n: ZodEffects\n<ZodOptional\n<ZodNullable\n<ZodString\n>>, undefined\n| string\n, undefined\n| null\n| string\n> ; reasoning\n: ZodEffects\n<ZodOptional\n<ZodNullable\n<ZodArray\n<ZodObject\n<{ content\n: ZodEffects\n<ZodOptional\n<ZodNullable\n<ZodString\n>>, undefined\n| string\n, undefined\n| null\n| string\n> ; id\n: ZodEffects\n<ZodOptional\n<ZodNullable\n<ZodString\n>>, undefined\n| string\n, undefined\n| null\n| string\n> }, \"strip\"\n, ZodTypeAny\n, { content?\n: string\n; id?\n: string\n}, { content?\n: null\n| string\n; id?\n: null\n| string\n}>, \"many\"\n>>>, undefined\n| { content?\n: string\n; id?\n: string\n}[], undefined\n| null\n| { content?\n: null\n| string\n; id?\n: null\n| string\n}[]> ; role\n: ZodLiteral\n<\"assistant\"\n> ; tool_calls\n: ZodEffects\n<ZodOptional\n<ZodNullable\n<ZodArray\n<ZodObject\n<{ function\n: ZodObject\n<{ arguments\n: ZodString\n; name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { arguments\n: string\n; name\n: string\n}, { arguments\n: string\n; name\n: string\n}> ; id\n: ZodString\n; type\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}, { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}>, \"many\"\n>>>, undefined\n| { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[], undefined\n| null\n| { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[]> }, \"strip\"\n, ZodTypeAny\n, { content?\n: null\n| string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text\n: string\n; type\n: \"text\"\n}[] ; function_call?\n: { arguments\n: string\n; name\n: string\n} ; name?\n: string\n; reasoning?\n: { content?\n: string\n; id?\n: string\n}[] ; role\n: \"assistant\"\n; tool_calls?\n: { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[] }, { content?\n: null\n| string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; function_call?\n: null\n| { arguments\n: string\n; name\n: string\n} ; name?\n: null\n| string\n; reasoning?\n: null\n| { content?\n: null\n| string\n; id?\n: null\n| string\n}[] ; role\n: \"assistant\"\n; tool_calls?\n: null\n| { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[] }>, ZodObject\n<{ content\n: ZodUnion\n<[ZodDefault\n<ZodString\n>, ZodArray\n<ZodObject\n<{ cache_control\n: ZodOptional\n<ZodObject\n<{ type\n: ZodEnum\n<[\"ephemeral\"\n]> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"ephemeral\"\n}, { type\n: \"ephemeral\"\n}>> ; text\n: ZodDefault\n<ZodString\n> ; type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text\n: string\n; type\n: \"text\"\n}, { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}>, \"many\"\n>]> ; role\n: ZodLiteral\n<\"tool\"\n> ; tool_call_id\n: ZodDefault\n<ZodString\n> }, \"strip\"\n, ZodTypeAny\n, { content\n: {} ; role\n: \"tool\"\n; tool_call_id\n: string\n}, { content?\n: string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; role\n: \"tool\"\n; tool_call_id?\n: string\n}>, ZodObject\n<{ content\n: ZodNullable\n<ZodString\n> ; name\n: ZodString\n; role\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { content\n: null\n| string\n; name\n: string\n; role\n: \"function\"\n}, { content\n: null\n| string\n; name\n: string\n; role\n: \"function\"\n}>]>, ZodObject\n<{ content\n: ZodOptional\n<ZodNullable\n<ZodString\n>> ; role\n: ZodEnum\n<[\"model\"\n]> }, \"strip\"\n, ZodTypeAny\n, { content?\n: null\n| string\n; role\n: \"model\"\n}, { content?\n: null\n| string\n; role\n: \"model\"\n}>]>, \"many\"\n> }, \"strip\"\n, ZodTypeAny\n, { messages\n: ({ content\n: {} ; name?\n: string\n; role\n: \"system\"\n} | { content\n: {} ; name?\n: string\n; role\n: \"user\"\n} | { content?\n: null\n| string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text\n: string\n; type\n: \"text\"\n}[] ; function_call?\n: { arguments\n: string\n; name\n: string\n} ; name?\n: string\n; reasoning?\n: { content?\n: string\n; id?\n: string\n}[] ; role\n: \"assistant\"\n; tool_calls?\n: { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[] } | { content\n: {} ; role\n: \"tool\"\n; tool_call_id\n: string\n} | { content\n: null\n| string\n; name\n: string\n; role\n: \"function\"\n} | { content?\n: null\n| string\n; role\n: \"model\"\n})[] }, { messages\n: ({ content?\n: string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; name?\n: string\n; role\n: \"system\"\n} | { content?\n: string\n| ({ cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n} | { image_url\n: { detail?\n: \"auto\"\n| \"high\"\n| \"low\"\n; url\n: string\n} ; type\n: \"image_url\"\n})[] ; name?\n: string\n; role\n: \"user\"\n} | { content?\n: null\n| string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; function_call?\n: null\n| { arguments\n: string\n; name\n: string\n} ; name?\n: null\n| string\n; reasoning?\n: null\n| { content?\n: null\n| string\n; id?\n: null\n| string\n}[] ; role\n: \"assistant\"\n; tool_calls?\n: null\n| { function\n: { arguments\n: string\n; name\n: string\n} ; id\n: string\n; type\n: \"function\"\n}[] } | { content?\n: string\n| { cache_control?\n: { type\n: \"ephemeral\"\n} ; text?\n: string\n; type\n: \"text\"\n}[] ; role\n: \"tool\"\n; tool_call_id?\n: string\n} | { content\n: null\n| string\n; name\n: string\n; role\n: \"function\"\n} | { content?\n: null\n| string\n; role\n: \"model\"\n})[] }>]>, ZodObject\n<{ model\n: ZodString\n; params\n: ZodOptional\n<ZodUnion\n<[ZodObject\n<{ frequency_penalty\n: ZodOptional\n<ZodNumber\n> ; function_call\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}>]>> ; max_completion_tokens\n: ZodOptional\n<ZodNumber\n> ; max_tokens\n: ZodOptional\n<ZodNumber\n> ; n\n: ZodOptional\n<ZodNumber\n> ; presence_penalty\n: ZodOptional\n<ZodNumber\n> ; reasoning_effort\n: ZodOptional\n<ZodEnum\n<[\"low\"\n, \"medium\"\n, \"high\"\n]>> ; response_format\n: ZodOptional\n<ZodNullable\n<ZodUnion\n<[ZodObject\n<{ type\n: ZodLiteral\n<\"json_object\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"json_object\"\n}, { type\n: \"json_object\"\n}>, ZodObject\n<{ json_schema\n: ZodObject\n<{ description\n: ZodOptional\n<ZodString\n> ; name\n: ZodString\n; schema\n: ZodOptional\n<ZodUnion\n<[ZodRecord\n<ZodString\n, ZodUnknown\n>, ZodString\n]>> ; strict\n: ZodOptional\n<ZodNullable\n<ZodBoolean\n>> }, \"strip\"\n, ZodTypeAny\n, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}> ; type\n: ZodLiteral\n<\"json_schema\"\n> }, \"strip\"\n, ZodTypeAny\n, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}>, ZodObject\n<{ type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"text\"\n}, { type\n: \"text\"\n}>]>>> ; stop\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodOptional\n<ZodNumber\n> ; tool_choice\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodLiteral\n<\"required\"\n>, ZodObject\n<{ function\n: ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}> ; type\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { function\n: { name\n: string\n} ; type\n: \"function\"\n}, { function\n: { name\n: string\n} ; type\n: \"function\"\n}>]>> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, \"passthrough\"\n, ZodTypeAny\n, objectOutputType\n<{ frequency_penalty\n: ZodOptional\n<ZodNumber\n> ; function_call\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}>]>> ; max_completion_tokens\n: ZodOptional\n<ZodNumber\n> ; max_tokens\n: ZodOptional\n<ZodNumber\n> ; n\n: ZodOptional\n<ZodNumber\n> ; presence_penalty\n: ZodOptional\n<ZodNumber\n> ; reasoning_effort\n: ZodOptional\n<ZodEnum\n<[\"low\"\n, \"medium\"\n, \"high\"\n]>> ; response_format\n: ZodOptional\n<ZodNullable\n<ZodUnion\n<[ZodObject\n<{ type\n: ZodLiteral\n<\"json_object\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"json_object\"\n}, { type\n: \"json_object\"\n}>, ZodObject\n<{ json_schema\n: ZodObject\n<{ description\n: ZodOptional\n<ZodString\n> ; name\n: ZodString\n; schema\n: ZodOptional\n<ZodUnion\n<[ZodRecord\n<ZodString\n, ZodUnknown\n>, ZodString\n]>> ; strict\n: ZodOptional\n<ZodNullable\n<ZodBoolean\n>> }, \"strip\"\n, ZodTypeAny\n, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}> ; type\n: ZodLiteral\n<\"json_schema\"\n> }, \"strip\"\n, ZodTypeAny\n, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}>, ZodObject\n<{ type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"text\"\n}, { type\n: \"text\"\n}>]>>> ; stop\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodOptional\n<ZodNumber\n> ; tool_choice\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodLiteral\n<\"required\"\n>, ZodObject\n<{ function\n: ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}> ; type\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { function\n: { name\n: string\n} ; type\n: \"function\"\n}, { function\n: { name\n: string\n} ; type\n: \"function\"\n}>]>> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>, objectInputType\n<{ frequency_penalty\n: ZodOptional\n<ZodNumber\n> ; function_call\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}>]>> ; max_completion_tokens\n: ZodOptional\n<ZodNumber\n> ; max_tokens\n: ZodOptional\n<ZodNumber\n> ; n\n: ZodOptional\n<ZodNumber\n> ; presence_penalty\n: ZodOptional\n<ZodNumber\n> ; reasoning_effort\n: ZodOptional\n<ZodEnum\n<[\"low\"\n, \"medium\"\n, \"high\"\n]>> ; response_format\n: ZodOptional\n<ZodNullable\n<ZodUnion\n<[ZodObject\n<{ type\n: ZodLiteral\n<\"json_object\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"json_object\"\n}, { type\n: \"json_object\"\n}>, ZodObject\n<{ json_schema\n: ZodObject\n<{ description\n: ZodOptional\n<ZodString\n> ; name\n: ZodString\n; schema\n: ZodOptional\n<ZodUnion\n<[ZodRecord\n<ZodString\n, ZodUnknown\n>, ZodString\n]>> ; strict\n: ZodOptional\n<ZodNullable\n<ZodBoolean\n>> }, \"strip\"\n, ZodTypeAny\n, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}> ; type\n: ZodLiteral\n<\"json_schema\"\n> }, \"strip\"\n, ZodTypeAny\n, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}>, ZodObject\n<{ type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"text\"\n}, { type\n: \"text\"\n}>]>>> ; stop\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodOptional\n<ZodNumber\n> ; tool_choice\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodLiteral\n<\"required\"\n>, ZodObject\n<{ function\n: ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}> ; type\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { function\n: { name\n: string\n} ; type\n: \"function\"\n}, { function\n: { name\n: string\n} ; type\n: \"function\"\n}>]>> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>>, ZodObject\n<{ max_tokens\n: ZodNumber\n; max_tokens_to_sample\n: ZodOptional\n<ZodNumber\n> ; stop_sequences\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodNumber\n; top_k\n: ZodOptional\n<ZodNumber\n> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, \"passthrough\"\n, ZodTypeAny\n, objectOutputType\n<{ max_tokens\n: ZodNumber\n; max_tokens_to_sample\n: ZodOptional\n<ZodNumber\n> ; stop_sequences\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodNumber\n; top_k\n: ZodOptional\n<ZodNumber\n> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>, objectInputType\n<{ max_tokens\n: ZodNumber\n; max_tokens_to_sample\n: ZodOptional\n<ZodNumber\n> ; stop_sequences\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodNumber\n; top_k\n: ZodOptional\n<ZodNumber\n> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>>, ZodObject\n<{ maxOutputTokens\n: ZodOptional\n<ZodNumber\n> ; temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; topP\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, \"passthrough\"\n, ZodTypeAny\n, objectOutputType\n<{ maxOutputTokens\n: ZodOptional\n<ZodNumber\n> ; temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; topP\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>, objectInputType\n<{ maxOutputTokens\n: ZodOptional\n<ZodNumber\n> ; temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; topP\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>>, ZodObject\n<{ temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, \"passthrough\"\n, ZodTypeAny\n, objectOutputType\n<{ temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>, objectInputType\n<{ temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>>, ZodObject\n<{ use_cache\n: ZodOptional\n<ZodBoolean\n> }, \"passthrough\"\n, ZodTypeAny\n, objectOutputType\n<{ use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>, objectInputType\n<{ use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n>>]>> }, \"strip\"\n, ZodTypeAny\n, { model\n: string\n; params?\n: objectOutputType\n<{ frequency_penalty\n: ZodOptional\n<ZodNumber\n> ; function_call\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}>]>> ; max_completion_tokens\n: ZodOptional\n<ZodNumber\n> ; max_tokens\n: ZodOptional\n<ZodNumber\n> ; n\n: ZodOptional\n<ZodNumber\n> ; presence_penalty\n: ZodOptional\n<ZodNumber\n> ; reasoning_effort\n: ZodOptional\n<ZodEnum\n<[\"low\"\n, \"medium\"\n, \"high\"\n]>> ; response_format\n: ZodOptional\n<ZodNullable\n<ZodUnion\n<[ZodObject\n<{ type\n: ZodLiteral\n<\"json_object\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"json_object\"\n}, { type\n: \"json_object\"\n}>, ZodObject\n<{ json_schema\n: ZodObject\n<{ description\n: ZodOptional\n<ZodString\n> ; name\n: ZodString\n; schema\n: ZodOptional\n<ZodUnion\n<[ZodRecord\n<ZodString\n, ZodUnknown\n>, ZodString\n]>> ; strict\n: ZodOptional\n<ZodNullable\n<ZodBoolean\n>> }, \"strip\"\n, ZodTypeAny\n, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}> ; type\n: ZodLiteral\n<\"json_schema\"\n> }, \"strip\"\n, ZodTypeAny\n, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}>, ZodObject\n<{ type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"text\"\n}, { type\n: \"text\"\n}>]>>> ; stop\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodOptional\n<ZodNumber\n> ; tool_choice\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodLiteral\n<\"required\"\n>, ZodObject\n<{ function\n: ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}> ; type\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { function\n: { name\n: string\n} ; type\n: \"function\"\n}, { function\n: { name\n: string\n} ; type\n: \"function\"\n}>]>> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectOutputType\n<{ max_tokens\n: ZodNumber\n; max_tokens_to_sample\n: ZodOptional\n<ZodNumber\n> ; stop_sequences\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodNumber\n; top_k\n: ZodOptional\n<ZodNumber\n> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectOutputType\n<{ maxOutputTokens\n: ZodOptional\n<ZodNumber\n> ; temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; topP\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectOutputType\n<{ temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectOutputType\n<{ use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> }, { model\n: string\n; params?\n: objectInputType\n<{ frequency_penalty\n: ZodOptional\n<ZodNumber\n> ; function_call\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}>]>> ; max_completion_tokens\n: ZodOptional\n<ZodNumber\n> ; max_tokens\n: ZodOptional\n<ZodNumber\n> ; n\n: ZodOptional\n<ZodNumber\n> ; presence_penalty\n: ZodOptional\n<ZodNumber\n> ; reasoning_effort\n: ZodOptional\n<ZodEnum\n<[\"low\"\n, \"medium\"\n, \"high\"\n]>> ; response_format\n: ZodOptional\n<ZodNullable\n<ZodUnion\n<[ZodObject\n<{ type\n: ZodLiteral\n<\"json_object\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"json_object\"\n}, { type\n: \"json_object\"\n}>, ZodObject\n<{ json_schema\n: ZodObject\n<{ description\n: ZodOptional\n<ZodString\n> ; name\n: ZodString\n; schema\n: ZodOptional\n<ZodUnion\n<[ZodRecord\n<ZodString\n, ZodUnknown\n>, ZodString\n]>> ; strict\n: ZodOptional\n<ZodNullable\n<ZodBoolean\n>> }, \"strip\"\n, ZodTypeAny\n, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}, { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}> ; type\n: ZodLiteral\n<\"json_schema\"\n> }, \"strip\"\n, ZodTypeAny\n, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}, { json_schema\n: { description?\n: string\n; name\n: string\n; schema?\n: string\n| Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"json_schema\"\n}>, ZodObject\n<{ type\n: ZodLiteral\n<\"text\"\n> }, \"strip\"\n, ZodTypeAny\n, { type\n: \"text\"\n}, { type\n: \"text\"\n}>]>>> ; stop\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodOptional\n<ZodNumber\n> ; tool_choice\n: ZodOptional\n<ZodUnion\n<[ZodLiteral\n<\"auto\"\n>, ZodLiteral\n<\"none\"\n>, ZodLiteral\n<\"required\"\n>, ZodObject\n<{ function\n: ZodObject\n<{ name\n: ZodString\n}, \"strip\"\n, ZodTypeAny\n, { name\n: string\n}, { name\n: string\n}> ; type\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { function\n: { name\n: string\n} ; type\n: \"function\"\n}, { function\n: { name\n: string\n} ; type\n: \"function\"\n}>]>> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectInputType\n<{ max_tokens\n: ZodNumber\n; max_tokens_to_sample\n: ZodOptional\n<ZodNumber\n> ; stop_sequences\n: ZodOptional\n<ZodArray\n<ZodString\n, \"many\"\n>> ; temperature\n: ZodNumber\n; top_k\n: ZodOptional\n<ZodNumber\n> ; top_p\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectInputType\n<{ maxOutputTokens\n: ZodOptional\n<ZodNumber\n> ; temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; topP\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectInputType\n<{ temperature\n: ZodOptional\n<ZodNumber\n> ; topK\n: ZodOptional\n<ZodNumber\n> ; use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> | objectInputType\n<{ use_cache\n: ZodOptional\n<ZodBoolean\n> }, ZodTypeAny\n, \"passthrough\"\n> }>>, ZodObject\n<{ tools\n: ZodOptional\n<ZodArray\n<ZodObject\n<{ function\n: ZodObject\n<{ description\n: ZodOptional\n<ZodString\n> ; name\n: ZodString\n; parameters\n: ZodOptional\n<ZodRecord\n<ZodString\n, ZodUnknown\n>> ; strict\n: ZodOptional\n<ZodNullable\n<ZodBoolean\n>> }, \"strip\"\n, ZodTypeAny\n, { description?\n: string\n; name\n: string\n; parameters?\n: Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}, { description?\n: string\n; name\n: string\n; parameters?\n: Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n}> ; type\n: ZodLiteral\n<\"function\"\n> }, \"strip\"\n, ZodTypeAny\n, { function\n: { description?\n: string\n; name\n: string\n; parameters?\n: Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"function\"\n}, { function\n: { description?\n: string\n; name\n: string\n; parameters?\n: Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"function\"\n}>, \"many\"\n>> }, \"strip\"\n, ZodTypeAny\n, { tools?\n: { function\n: { description?\n: string\n; name\n: string\n; parameters?\n: Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"function\"\n}[] }, { tools?\n: { function\n: { description?\n: string\n; name\n: string\n; parameters?\n: Record\n<string\n, unknown\n> ; strict?\n: null\n| boolean\n} ; type\n: \"function\"\n}[] }>>\ntoolFunctionDefinitionSchema\n\u2022 Const\ntoolFunctionDefinitionSchema: z.ZodObject\n<{ function\n: z.ZodObject\n<{ description\n: z.ZodOptional\n<z.ZodString\n> ; name\n: z.ZodString\n; parameters\n: z.ZodOptional\n<z.ZodRecord\n<z.ZodString\n, z.ZodUnknown\n>> ; strict\n: z.ZodOptional\n<z.ZodNullable\n<z.ZodBoolean\n>> }, \"strip\"\n, z.ZodTypeAny\n, { description?\n: string\n; name\n: string\n; parameters?\n: Record\n<string\n, unknown\n> ; strict?\n: boolean\n| null\n}, { description?\n: string\n; name\n: string\n; parameters?\n: Record\n<string\n, unknown\n> ; strict?\n: boolean\n| null\n}> ; type\n: z.ZodLiteral\n<\"function\"\n> }, \"strip\"\n, z.ZodTypeAny\n, { function\n: { description?\n: string\n; name\n: string\n; parameters?\n: Record\n<string\n, unknown\n> ; strict?\n: boolean\n| null\n} ; type\n: \"function\"\n}, { function\n: { description?\n: string\n; name\n: string\n; parameters?\n: Record\n<string\n, unknown\n> ; strict?\n: boolean\n| null\n} ; type\n: \"function\"\n}>", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "TypeScript"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "Python"}, {"href": "https://www.braintrust.dev/docs/reference/api", "anchor": "API"}, {"href": "https://www.braintrust.dev/docs/reference/autoevals", "anchor": "Autoevals library"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "BTQL query syntax"}, {"href": "https://www.braintrust.dev/docs/reference/streaming", "anchor": "Streaming"}, {"href": "https://www.braintrust.dev/docs/reference/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/reference/reasoning", "anchor": "Reasoning"}, {"href": "https://www.braintrust.dev/docs/reference/organizations", "anchor": "Organizations"}, {"href": "https://www.braintrust.dev/docs/reference/mcp", "anchor": "Model Context Protocol (MCP)"}, {"href": "https://www.braintrust.dev/docs/reference/object-links", "anchor": "Sharing via URL"}, {"href": "https://www.braintrust.dev/docs/reference/glossary", "anchor": "Glossary"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "braintrust"}, {"href": "https://braintrust.dev/", "anchor": "Braintrust"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Quickstart"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Classes"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/Attachment.md", "anchor": "Attachment"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/BaseAttachment.md", "anchor": "BaseAttachment"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/BraintrustState.md", "anchor": "BraintrustState"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/BraintrustStream.md", "anchor": "BraintrustStream"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/CodeFunction.md", "anchor": "CodeFunction"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/CodePrompt.md", "anchor": "CodePrompt"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/Dataset.md", "anchor": "Dataset"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/EvalResultWithSummary.md", "anchor": "EvalResultWithSummary"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/Experiment.md", "anchor": "Experiment"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/ExternalAttachment.md", "anchor": "ExternalAttachment"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/FailedHTTPResponse.md", "anchor": "FailedHTTPResponse"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/LazyValue.md", "anchor": "LazyValue"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/Logger.md", "anchor": "Logger"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/NoopSpan.md", "anchor": "NoopSpan"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/Project.md", "anchor": "Project"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/Prompt.md", "anchor": "Prompt"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/PromptBuilder.md", "anchor": "PromptBuilder"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/ReadonlyAttachment.md", "anchor": "ReadonlyAttachment"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/ReadonlyExperiment.md", "anchor": "ReadonlyExperiment"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/ScorerBuilder.md", "anchor": "ScorerBuilder"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/SpanImpl.md", "anchor": "SpanImpl"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/TestBackgroundLogger.md", "anchor": "TestBackgroundLogger"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/ToolBuilder.md", "anchor": "ToolBuilder"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Interfaces"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/AttachmentParams.md", "anchor": "AttachmentParams"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/BackgroundLoggerOpts.md", "anchor": "BackgroundLoggerOpts"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/DataSummary.md", "anchor": "DataSummary"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/DatasetSummary.md", "anchor": "DatasetSummary"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/EvalHooks.md", "anchor": "EvalHooks"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/Evaluator.md", "anchor": "Evaluator"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/ExperimentSummary.md", "anchor": "ExperimentSummary"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/Exportable.md", "anchor": "Exportable"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/ExternalAttachmentParams.md", "anchor": "ExternalAttachmentParams"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/InvokeFunctionArgs.md", "anchor": "InvokeFunctionArgs"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/LogOptions.md", "anchor": "LogOptions"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/LoginOptions.md", "anchor": "LoginOptions"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/MetricSummary.md", "anchor": "MetricSummary"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/ObjectMetadata.md", "anchor": "ObjectMetadata"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/ParentExperimentIds.md", "anchor": "ParentExperimentIds"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/ParentProjectLogIds.md", "anchor": "ParentProjectLogIds"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/ReporterBody.md", "anchor": "ReporterBody"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/ScoreSummary.md", "anchor": "ScoreSummary"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/Span.md", "anchor": "Span"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Namespaces"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/modules/default.md", "anchor": "default"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/modules/graph.md", "anchor": "graph"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "BaseExperiment"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "BaseExperiment"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "BaseMetadata"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "BaseExperiment"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Eval"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/EvalResultWithSummary.md", "anchor": "EvalResultWithSummary"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "BaseMetadata"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/Evaluator.md", "anchor": "Evaluator"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/EvalResultWithSummary.md", "anchor": "EvalResultWithSummary"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Reporter"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/ReporterBody.md", "anchor": "ReporterBody"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "buildLocalSummary"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/ExperimentSummary.md", "anchor": "ExperimentSummary"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "EvaluatorDef"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "EvalResult"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/ExperimentSummary.md", "anchor": "ExperimentSummary"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "createFinalValuePassThroughStream"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "BraintrustStreamChunk"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "BraintrustStreamChunk"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "currentExperiment"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/Experiment.md", "anchor": "Experiment"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "init"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/Experiment.md", "anchor": "Experiment"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "currentLogger"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/Logger.md", "anchor": "Logger"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "initLogger"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/Logger.md", "anchor": "Logger"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "currentSpan"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/Span.md", "anchor": "Span"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/Span.md", "anchor": "Span"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/Span.md", "anchor": "Span"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "defaultErrorScoreHandler"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "EvalCase"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/Span.md", "anchor": "Span"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "deserializePlainStringAsJSON"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "devNullWritableStream"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "flush"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "getSpanParentObject"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/Span.md", "anchor": "Span"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/Experiment.md", "anchor": "Experiment"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/Logger.md", "anchor": "Logger"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/Span.md", "anchor": "Span"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/Experiment.md", "anchor": "Experiment"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/Logger.md", "anchor": "Logger"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "init"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "FullInitOptions"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "InitOptions"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "initDataset"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/Dataset.md", "anchor": "Dataset"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/Dataset.md", "anchor": "Dataset"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/Dataset.md", "anchor": "Dataset"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/Dataset.md", "anchor": "Dataset"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "initExperiment"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "InitOptions"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "InitOptions"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "initFunction"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "initLogger"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/Logger.md", "anchor": "Logger"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/Logger.md", "anchor": "Logger"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "invoke"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "InvokeReturn"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/InvokeFunctionArgs.md", "anchor": "InvokeFunctionArgs"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/LoginOptions.md", "anchor": "LoginOptions"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/InvokeFunctionArgs.md", "anchor": "InvokeFunctionArgs"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "InvokeReturn"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "loadPrompt"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/Prompt.md", "anchor": "Prompt"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/Prompt.md", "anchor": "Prompt"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "log"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "ExperimentLogFullArgs"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/Experiment.md", "anchor": "Experiment.log"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "logError"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/Span.md", "anchor": "Span"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "login"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/BraintrustState.md", "anchor": "BraintrustState"}, {"href": "https://www.braintrust.dev/app/token", "anchor": "https://www.braintrust.dev/app/token"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/LoginOptions.md", "anchor": "LoginOptions"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/BraintrustState.md", "anchor": "BraintrustState"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "loginToState"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/BraintrustState.md", "anchor": "BraintrustState"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/LoginOptions.md", "anchor": "LoginOptions"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/BraintrustState.md", "anchor": "BraintrustState"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "newId"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "parseCachedHeader"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "permalink"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/Span.md", "anchor": "Span.link"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/Span.md", "anchor": "Span.export"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/BraintrustState.md", "anchor": "BraintrustState"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "promptDefinitionToPromptData"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "renderMessage"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "renderPromptParams"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "reportFailures"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "BaseMetadata"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "EvaluatorDef"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "EvalResult"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "runEvaluator"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/EvalResultWithSummary.md", "anchor": "EvalResultWithSummary"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/Experiment.md", "anchor": "Experiment"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "EvaluatorDef"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/EvalResultWithSummary.md", "anchor": "EvalResultWithSummary"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "setFetch"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "spanComponentsToObjectId"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/BraintrustState.md", "anchor": "BraintrustState"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "startSpan"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/Span.md", "anchor": "Span"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "traced"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "StartSpanArgs"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/Span.md", "anchor": "Span"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "summarize"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/ExperimentSummary.md", "anchor": "ExperimentSummary"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/ExperimentSummary.md", "anchor": "ExperimentSummary"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "traceable"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "StartSpanArgs"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "SetCurrentArg"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "traced"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "PromiseUnless"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/Span.md", "anchor": "Span.traced"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/Span.md", "anchor": "Span"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "StartSpanArgs"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "SetCurrentArg"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "PromiseUnless"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "updateSpan"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "withCurrent"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/Span.md", "anchor": "Span"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/Span.md", "anchor": "Span"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/BraintrustState.md", "anchor": "BraintrustState"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "withDataset"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/Dataset.md", "anchor": "Dataset"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "initDataset"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "withExperiment"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/Experiment.md", "anchor": "Experiment"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/LoginOptions.md", "anchor": "LoginOptions"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "AnyDataset"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/BraintrustState.md", "anchor": "BraintrustState"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "SetCurrentArg"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "init"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "withLogger"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/Logger.md", "anchor": "Logger"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/LoginOptions.md", "anchor": "LoginOptions"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/BraintrustState.md", "anchor": "BraintrustState"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "SetCurrentArg"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "initLogger"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "withParent"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/BraintrustState.md", "anchor": "BraintrustState"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "wrapAISDKModel"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "wrapAnthropic"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "wrapOpenAI"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "wrapOpenAIv4"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "wrapTraced"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "StartSpanArgs"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "SetCurrentArg"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Type Aliases"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "AnyDataset"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/Dataset.md", "anchor": "Dataset"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "BaseExperiment"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "BaseMetadata"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "DefaultMetadataType"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Type declaration"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "BaseMetadata"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "BraintrustStreamChunk"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "braintrustStreamChunkSchema"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "ChatPrompt"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Type declaration"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "CodeOpts"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "CommentEvent"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "IdField"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/ParentExperimentIds.md", "anchor": "ParentExperimentIds"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/ParentProjectLogIds.md", "anchor": "ParentProjectLogIds"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "CompiledPrompt"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "CompiledPromptParams"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "ChatPrompt"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "CompletionPrompt"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "CompiledPromptParams"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "CompletionPrompt"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Type declaration"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "CreateProjectOpts"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "DatasetRecord"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "DefaultMetadataType"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "DefaultPromptArgs"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "CompiledPromptParams"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "ChatPrompt"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "CompletionPrompt"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "EndSpanArgs"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Type declaration"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "EvalCase"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "EvalParameterSerializedSchema"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "EvalParameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "EvalResult"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "EvalCase"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "BaseMetadata"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "DefaultMetadataType"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "EvalScorer"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "EvalScorerArgs"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "BaseMetadata"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "DefaultMetadataType"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Type declaration"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "EvalScorerArgs"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Returns"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "EvalScorerArgs"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "EvalCase"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "BaseMetadata"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "DefaultMetadataType"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "EvalTask"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/EvalHooks.md", "anchor": "EvalHooks"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/EvalHooks.md", "anchor": "EvalHooks"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "BaseMetadata"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "EvalParameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "EvaluatorDef"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/Evaluator.md", "anchor": "Evaluator"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "BaseMetadata"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "DefaultMetadataType"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "EvalParameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "EvalParameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "EvaluatorDefinition"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "evaluatorDefinitionSchema"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "EvaluatorDefinitions"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "evaluatorDefinitionsSchema"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "EvaluatorFile"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Type declaration"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "EvaluatorDef"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "BaseMetadata"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "EvalParameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/CodeFunction.md", "anchor": "CodeFunction"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/CodePrompt.md", "anchor": "CodePrompt"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "EvaluatorManifest"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "EvaluatorDef"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "BaseMetadata"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "ExperimentLogFullArgs"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "OtherExperimentLogFields"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "OtherExperimentLogFields"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "InputField"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "IdField"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "ExperimentLogPartialArgs"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "OtherExperimentLogFields"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "InputField"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "FullInitOptions"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "InitOptions"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "FullLoginOptions"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/LoginOptions.md", "anchor": "LoginOptions"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "IdField"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Type declaration"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "InitOptions"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "FullLoginOptions"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "AnyDataset"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/BraintrustState.md", "anchor": "BraintrustState"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "InputField"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Type declaration"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "InvokeReturn"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/BraintrustStream.md", "anchor": "BraintrustStream"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "LogCommentFullArgs"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "IdField"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/ParentExperimentIds.md", "anchor": "ParentExperimentIds"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/interfaces/ParentProjectLogIds.md", "anchor": "ParentProjectLogIds"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "LogFeedbackFullArgs"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "IdField"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "OtherExperimentLogFields"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "OtherExperimentLogFields"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Type declaration"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "PromiseUnless"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "PromptContents"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "promptContentsSchema"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "PromptDefinition"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "promptDefinitionSchema"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "PromptDefinitionWithTools"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "promptDefinitionWithToolsSchema"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "PromptOpts"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "PromptDefinition"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "PromptRowWithId"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "ScorerOpts"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "CodeOpts"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "SerializedBraintrustState"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "SetCurrentArg"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Type declaration"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "SpanContext"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Type declaration"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "NOOP_SPAN"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "currentSpan"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "startSpan"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "withCurrent"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "StartSpanArgs"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Type declaration"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "ToolFunctionDefinition"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "toolFunctionDefinitionSchema"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "WithTransactionId"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Type parameters"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Variables"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "ERR_PERMALINK"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "INTERNAL_BTQL_LIMIT"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "LEGACY_CACHED_HEADER"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "NOOP_SPAN"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/NoopSpan.md", "anchor": "NoopSpan"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "NOOP_SPAN_PERMALINK"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "X_CACHED_HEADER"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "_exportsForTestingOnly"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "Type declaration"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/BaseAttachment.md", "anchor": "BaseAttachment"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/BraintrustState.md", "anchor": "BraintrustState"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/BraintrustState.md", "anchor": "BraintrustState"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs/classes/TestBackgroundLogger.md", "anchor": "TestBackgroundLogger"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "braintrustStreamChunkSchema"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "evaluatorDefinitionSchema"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "evaluatorDefinitionsSchema"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "projects"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "promptContentsSchema"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "promptDefinitionSchema"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "promptDefinitionWithToolsSchema"}, {"href": "https://www.braintrust.dev/docs/libs/nodejs", "anchor": "toolFunctionDefinitionSchema"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}], "depth": 4}, "https://www.braintrust.dev/docs/reference/autoevals": {"url": "https://www.braintrust.dev/docs/reference/autoevals", "title": "Autoevals - Docs - Reference - Braintrust", "text": "Autoevals\nAutoevals is a tool to quickly and easily evaluate AI model outputs.\nIt bundles together a variety of automatic evaluation methods including:\n- LLM-as-a-judge\n- Heuristic (e.g. Levenshtein distance)\n- Statistical (e.g. BLEU)\nAutoevals is developed by the team at Braintrust.\nAutoevals uses model-graded evaluation for a variety of subjective tasks including fact checking, safety, and more. Many of these evaluations are adapted from OpenAI's excellent evals project but are implemented so you can flexibly run them on individual examples, tweak the prompts, and debug their outputs.\nYou can also create your own model-graded evaluations with Autoevals. It's easy to add custom prompts, parse outputs, and manage exceptions.\nRequirements\n- Python 3.9 or higher\n- Compatible with both OpenAI Python SDK v0.x and v1.x\nInstallation\npnpm add autoevals\nGetting started\nUse Autoevals to model-grade an example LLM completion using the Factuality prompt.\nBy default, Autoevals uses your OPENAI_API_KEY\nenvironment variable to authenticate with OpenAI's API.\nUsing other AI providers\nWhen you use Autoevals, it will look for an OPENAI_BASE_URL\nenvironment variable to use as the base for requests to an OpenAI compatible API. If OPENAI_BASE_URL\nis not set, it will default to the AI proxy.\nIf you choose to use the proxy, you'll also get:\n- Simplified access to many AI providers\n- Reduced costs with automatic request caching\n- Increased observability when you enable logging to Braintrust\nThe proxy is free to use, even if you don't have a Braintrust account.\nIf you have a Braintrust account, you can optionally set the BRAINTRUST_API_KEY\nenvironment variable instead of OPENAI_API_KEY\nto unlock additional features like logging and monitoring. You can also route requests to supported AI providers and models or custom models you have configured in Braintrust.\nCustom client configuration\nThere are two ways you can configure a custom client when you need to use a different OpenAI compatible API:\n- Global configuration: Initialize a client that will be used by all evaluators\n- Instance configuration: Configure a client for a specific evaluator\nGlobal configuration\nSet up a client that all your evaluators will use:\nInstance configuration\nConfigure a client for a specific evaluator instance:\nUsing Braintrust with Autoevals (optional)\nOnce you grade an output using Autoevals, you can optionally use Braintrust to log and compare your evaluation results. This integration is completely optional and not required for using Autoevals.\nCreate a file named example.eval.js\n(it must take the form *.eval.[ts|tsx|js|jsx]\n):\nThen, run\nSupported evaluation methods\nLLM-as-a-judge evaluations\n- Battle\n- Closed QA\n- Humor\n- Factuality\n- Moderation\n- Security\n- Summarization\n- SQL\n- Translation\n- Fine-tuned binary classifiers\nRAG evaluations\n- Context precision\n- Context relevancy\n- Context recall\n- Context entity recall\n- Faithfulness\n- Answer relevancy\n- Answer similarity\n- Answer correctness\nComposite evaluations\n- Semantic list contains\n- JSON validity\nEmbedding evaluations\n- Embedding similarity\nHeuristic evaluations\n- Levenshtein distance\n- Exact match\n- Numeric difference\n- JSON diff\nCustom evaluation prompts\nAutoevals supports custom evaluation prompts for model-graded evaluation. To use them, simply pass in a prompt and scoring mechanism:\nCreating custom scorers\nYou can also create your own scoring functions that do not use LLMs. For example, to test whether the word 'banana'\nis in the output, you can use the following:\nWhy does this library exist?\nThere is nothing particularly novel about the evaluation methods in this library. They are all well-known and well-documented. However, there are a few things that are particularly difficult when evaluating in practice:\n- Normalizing metrics between 0 and 1 is tough. For example, check out the calculation in number.py to see how it's done for numeric differences.\n- Parsing the outputs on model-graded evaluations is also challenging. There are frameworks that do this, but it's hard to debug one output at a time, propagate errors, and tweak the prompts. Autoevals makes these tasks easy.\n- Collecting metrics behind a uniform interface makes it easy to swap out evaluation methods and compare them. Prior to Autoevals, we couldn't find an open source library where you can simply pass in\ninput\n,output\n, andexpected\nvalues through a bunch of different evaluation methods.\nDocumentation\nThe full docs are available for your reference.\nContributing\nWe welcome contributions!\nTo install the development dependencies, run make develop\n, and run source env.sh\nto activate the environment. Make a .env\nfile from the .env.example\nfile and set the environment variables. Run direnv allow\nto load the environment variables.\nTo run the tests, run pytest\nfrom the root directory.\nSend a PR and we'll review it! We'll take care of versioning and releasing.", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/reference/api", "anchor": "API"}, {"href": "https://www.braintrust.dev/docs/reference/autoevals", "anchor": "Autoevals library"}, {"href": "https://www.braintrust.dev/docs/reference/autoevals/python", "anchor": "Python"}, {"href": "https://www.braintrust.dev/docs/reference/autoevals/nodejs", "anchor": "TypeScript"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "BTQL query syntax"}, {"href": "https://www.braintrust.dev/docs/reference/streaming", "anchor": "Streaming"}, {"href": "https://www.braintrust.dev/docs/reference/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/reference/reasoning", "anchor": "Reasoning"}, {"href": "https://www.braintrust.dev/docs/reference/organizations", "anchor": "Organizations"}, {"href": "https://www.braintrust.dev/docs/reference/mcp", "anchor": "Model Context Protocol (MCP)"}, {"href": "https://www.braintrust.dev/docs/reference/object-links", "anchor": "Sharing via URL"}, {"href": "https://www.braintrust.dev/docs/reference/glossary", "anchor": "Glossary"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/reference/autoevals", "anchor": "Autoevals"}, {"href": "https://braintrust.dev/", "anchor": "Braintrust"}, {"href": "https://www.braintrust.dev/docs/reference/autoevals", "anchor": "Requirements"}, {"href": "https://www.braintrust.dev/docs/reference/autoevals", "anchor": "Installation"}, {"href": "https://www.braintrust.dev/docs/reference/autoevals", "anchor": "Getting started"}, {"href": "https://www.braintrust.dev/docs/reference/templates/factuality.yaml", "anchor": "Factuality prompt"}, {"href": "https://www.braintrust.dev/docs/reference/autoevals", "anchor": "Using other AI providers"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "supported AI providers and models"}, {"href": "https://www.braintrust.dev/docs/reference/autoevals", "anchor": "Custom client configuration"}, {"href": "https://www.braintrust.dev/docs/reference/autoevals", "anchor": "Global configuration"}, {"href": "https://www.braintrust.dev/docs/reference/autoevals", "anchor": "Instance configuration"}, {"href": "https://www.braintrust.dev/docs/reference/autoevals", "anchor": "Using Braintrust with Autoevals (optional)"}, {"href": "https://www.braintrust.dev/docs/libs/python", "anchor": "Braintrust"}, {"href": "https://www.braintrust.dev/docs/reference/autoevals", "anchor": "Supported evaluation methods"}, {"href": "https://www.braintrust.dev/docs/reference/autoevals", "anchor": "LLM-as-a-judge evaluations"}, {"href": "https://www.braintrust.dev/docs/reference/autoevals", "anchor": "RAG evaluations"}, {"href": "https://www.braintrust.dev/docs/reference/autoevals", "anchor": "Composite evaluations"}, {"href": "https://www.braintrust.dev/docs/reference/autoevals", "anchor": "Embedding evaluations"}, {"href": "https://www.braintrust.dev/docs/reference/autoevals", "anchor": "Heuristic evaluations"}, {"href": "https://www.braintrust.dev/docs/reference/autoevals", "anchor": "Custom evaluation prompts"}, {"href": "https://www.braintrust.dev/docs/reference/autoevals", "anchor": "Creating custom scorers"}, {"href": "https://www.braintrust.dev/docs/reference/autoevals", "anchor": "Why does this library exist?"}, {"href": "https://www.braintrust.dev/py/autoevals/number.py", "anchor": "number.py"}, {"href": "https://www.braintrust.dev/docs/reference/autoevals", "anchor": "Documentation"}, {"href": "https://www.braintrust.dev/docs/reference/autoevals", "anchor": "for your reference"}, {"href": "https://www.braintrust.dev/docs/reference/autoevals", "anchor": "Contributing"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}], "depth": 4}, "https://www.braintrust.dev/docs/reference/api": {"url": "https://www.braintrust.dev/docs/reference/api", "title": "API - Docs - Reference - Braintrust", "text": "API reference\nThis section contains the full API specification for Braintrust's data plane. The API is hosted globally at https://api.braintrust.dev or in your own environment. The API allows you to access all of the core objects in Braintrust, including experiments, datasets, prompts, users, groups, roles, and more. It also enables you to access Braintrust from languages other than TypeScript and Python.\nYou can access the full OpenAPI spec for this API at https://github.com/braintrustdata/braintrust-openapi.\nAcls\nManage role-based access controls for the organization\nAiSecrets\nManage external AI provider secrets scoped to the organization\nApiKeys\nManagement of API keys for Braintrust users\nCrossObject\nEvents and feedback across object types\nDatasets\nCollections of data used to run evaluations and track improvements over time\nEnvVars\nEnvironment variables that your scoring functions and tools can access\nEvals\nRun evaluations through the API\nExperiments\nA set of traces run to test the behavior of code\nFunctions\nInvokable custom logic for LLM applications\nGroups\nPermission management for collections of users\nLogs\nLog real-world interactions in your application\nOrganizations\nThe billing unit of Braintrust. Can represent a company or team\nOther\n'Hello world' endpoint for testing the API\nProjectScores\nScores scoped to a project to be used in experiment code and human review\nProjectTags\nTags used to track various kinds of data across your application, and track how they change over time\nProjects\nA single AI feature. The container for experiments, logs, datasets, prompts, and playgrounds\nPrompts\nVersioned prompt that can be referenced in your code\nProxy\nAn OpenAI-protocol compatible proxy that supports multiple model formats, caching, and secret management\nRoles\nCollections of permissions that can be assigned to users or groups\nServiceTokens\nManagement of automated Service tokens\nSpanIframes\nCustom iframe URLs that can be rendered in spans\nUsers\nBraintrust users\nViews\nTable configurations that can be saved for quick access\nAPI wrappers\nThrough Stainless, we have language-specific wrappers over the bare REST API for a variety of languages. Note that unlike our custom-built Python or TypeScript SDKs, these libraries map essentially 1:1 with the REST API:", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/reference/api", "anchor": "API"}, {"href": "https://www.braintrust.dev/docs/reference/api/Acls", "anchor": "Acls"}, {"href": "https://www.braintrust.dev/docs/reference/api/AiSecrets", "anchor": "Ai Secrets"}, {"href": "https://www.braintrust.dev/docs/reference/api/ApiKeys", "anchor": "Api Keys"}, {"href": "https://www.braintrust.dev/docs/reference/api/CrossObject", "anchor": "Cross Object"}, {"href": "https://www.braintrust.dev/docs/reference/api/Datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/reference/api/EnvVars", "anchor": "Env Vars"}, {"href": "https://www.braintrust.dev/docs/reference/api/Evals", "anchor": "Evals"}, {"href": "https://www.braintrust.dev/docs/reference/api/Experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/reference/api/Functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/reference/api/Groups", "anchor": "Groups"}, {"href": "https://www.braintrust.dev/docs/reference/api/Logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/reference/api/Organizations", "anchor": "Organizations"}, {"href": "https://www.braintrust.dev/docs/reference/api/Other", "anchor": "Other"}, {"href": "https://www.braintrust.dev/docs/reference/api/Projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectScores", "anchor": "Project Scores"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectTags", "anchor": "Project Tags"}, {"href": "https://www.braintrust.dev/docs/reference/api/Prompts", "anchor": "Prompts"}, {"href": "https://www.braintrust.dev/docs/reference/api/Proxy", "anchor": "Proxy"}, {"href": "https://www.braintrust.dev/docs/reference/api/Roles", "anchor": "Roles"}, {"href": "https://www.braintrust.dev/docs/reference/api/ServiceTokens", "anchor": "Service tokens"}, {"href": "https://www.braintrust.dev/docs/reference/api/SpanIframes", "anchor": "Span Iframes"}, {"href": "https://www.braintrust.dev/docs/reference/api/Users", "anchor": "Users"}, {"href": "https://www.braintrust.dev/docs/reference/api/Views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/reference/autoevals", "anchor": "Autoevals library"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "BTQL query syntax"}, {"href": "https://www.braintrust.dev/docs/reference/streaming", "anchor": "Streaming"}, {"href": "https://www.braintrust.dev/docs/reference/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/reference/reasoning", "anchor": "Reasoning"}, {"href": "https://www.braintrust.dev/docs/reference/organizations", "anchor": "Organizations"}, {"href": "https://www.braintrust.dev/docs/reference/mcp", "anchor": "Model Context Protocol (MCP)"}, {"href": "https://www.braintrust.dev/docs/reference/object-links", "anchor": "Sharing via URL"}, {"href": "https://www.braintrust.dev/docs/reference/glossary", "anchor": "Glossary"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/reference/api", "anchor": "API reference"}, {"href": "https://api.braintrust.dev", "anchor": "https://api.braintrust.dev"}, {"href": "https://www.braintrust.dev/docs/reference/api/Acls", "anchor": "Acls Manage role-based access controls for the organization"}, {"href": "https://www.braintrust.dev/docs/reference/api/AiSecrets", "anchor": "AiSecrets Manage external AI provider secrets scoped to the organization"}, {"href": "https://www.braintrust.dev/docs/reference/api/ApiKeys", "anchor": "ApiKeys Management of API keys for Braintrust users"}, {"href": "https://www.braintrust.dev/docs/reference/api/CrossObject", "anchor": "CrossObject Events and feedback across object types"}, {"href": "https://www.braintrust.dev/docs/reference/api/Datasets", "anchor": "Datasets Collections of data used to run evaluations and track improvements over time"}, {"href": "https://www.braintrust.dev/docs/reference/api/EnvVars", "anchor": "EnvVars Environment variables that your scoring functions and tools can access"}, {"href": "https://www.braintrust.dev/docs/reference/api/Evals", "anchor": "Evals Run evaluations through the API"}, {"href": "https://www.braintrust.dev/docs/reference/api/Experiments", "anchor": "Experiments A set of traces run to test the behavior of code"}, {"href": "https://www.braintrust.dev/docs/reference/api/Functions", "anchor": "Functions Invokable custom logic for LLM applications"}, {"href": "https://www.braintrust.dev/docs/reference/api/Groups", "anchor": "Groups Permission management for collections of users"}, {"href": "https://www.braintrust.dev/docs/reference/api/Logs", "anchor": "Logs Log real-world interactions in your application"}, {"href": "https://www.braintrust.dev/docs/reference/api/Organizations", "anchor": "Organizations The billing unit of Braintrust. Can represent a company or team"}, {"href": "https://www.braintrust.dev/docs/reference/api/Other", "anchor": "Other 'Hello world' endpoint for testing the API"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectScores", "anchor": "ProjectScores Scores scoped to a project to be used in experiment code and human review"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectTags", "anchor": "ProjectTags Tags used to track various kinds of data across your application, and track how they change over time"}, {"href": "https://www.braintrust.dev/docs/reference/api/Projects", "anchor": "Projects A single AI feature. The container for experiments, logs, datasets, prompts, and playgrounds"}, {"href": "https://www.braintrust.dev/docs/reference/api/Prompts", "anchor": "Prompts Versioned prompt that can be referenced in your code"}, {"href": "https://www.braintrust.dev/docs/reference/api/Proxy", "anchor": "Proxy An OpenAI-protocol compatible proxy that supports multiple model formats, caching, and secret management"}, {"href": "https://www.braintrust.dev/docs/reference/api/Roles", "anchor": "Roles Collections of permissions that can be assigned to users or groups"}, {"href": "https://www.braintrust.dev/docs/reference/api/ServiceTokens", "anchor": "ServiceTokens Management of automated Service tokens"}, {"href": "https://www.braintrust.dev/docs/reference/api/SpanIframes", "anchor": "SpanIframes Custom iframe URLs that can be rendered in spans"}, {"href": "https://www.braintrust.dev/docs/reference/api/Users", "anchor": "Users Braintrust users"}, {"href": "https://www.braintrust.dev/docs/reference/api/Views", "anchor": "Views Table configurations that can be saved for quick access"}, {"href": "https://www.braintrust.dev/docs/reference/api", "anchor": "API wrappers"}, {"href": "https://www.braintrust.dev/docs/reference/libs/python", "anchor": "Python"}, {"href": "https://www.braintrust.dev/docs/reference/libs/nodejs", "anchor": "TypeScript"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}], "depth": 4}, "https://www.braintrust.dev/docs/reference/api/Acls": {"url": "https://www.braintrust.dev/docs/reference/api/Acls", "title": "Acls - Docs - Reference - Braintrust", "text": "List acls\nList out all acls. The acls are sorted by creation date, with the most recently-created acls coming first\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nQuery Parameters\nlimit\nintegerLimit the number of objects to return\n0\nstarting_after\nstringPagination cursor id.\nFor example, if the final item in the last page you fetched had an id of foo\n, pass starting_after=foo\nto fetch the next page. Note: you may only pass one of starting_after\nand ending_before\n\"uuid\"\nending_before\nstringPagination cursor id.\nFor example, if the initial item in the last page you fetched had an id of foo\n, pass ending_before=foo\nto fetch the previous page. Note: you may only pass one of starting_after\nand ending_before\n\"uuid\"\nids\nAny properties in string, array<string>Filter search results to a particular set of object IDs. To specify a list of IDs, include the query param multiple times\nobject_type\nThe object type that the ACL applies to\n\"organization\" | \"project\" | \"experiment\" | \"dataset\" | \"prompt\" | \"prompt_session\" | \"group\" | \"role\" | \"org_member\" | \"project_log\" | \"org_project\"\nobject_id\nThe id of the object the ACL applies to\n\"uuid\"\nuser_id\nstringId of the user the ACL applies to. Exactly one of user_id\nand group_id\nwill be provided\n\"uuid\"\ngroup_id\nstringId of the group the ACL applies to. Exactly one of user_id\nand group_id\nwill be provided\n\"uuid\"\npermission\nstringEach permission permits a certain type of operation on an object in the system\nPermissions can be assigned to to objects on an individual basis, or grouped into roles\n\"create\" | \"read\" | \"update\" | \"delete\" | \"create_acls\" | \"read_acls\" | \"update_acls\" | \"delete_acls\"\nrestrict_object_type\nstringThe object type that the ACL applies to\n\"organization\" | \"project\" | \"experiment\" | \"dataset\" | \"prompt\" | \"prompt_session\" | \"group\" | \"role\" | \"org_member\" | \"project_log\" | \"org_project\"\nrole_id\nstringId of the role the ACL grants. Exactly one of permission\nand role_id\nwill be provided\n\"uuid\"\nReturns a list of acl objects\nCreate acl\nCreate a new acl. If there is an existing acl with the same contents as the one specified in the request, will return the existing acl unmodified\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nRequest Body\napplication/json\nOptionalAny desired information about the new acl object\nobject_type\nThe object type that the ACL applies to\n\"organization\" | \"project\" | \"experiment\" | \"dataset\" | \"prompt\" | \"prompt_session\" | \"group\" | \"role\" | \"org_member\" | \"project_log\" | \"org_project\"\nobject_id\nThe id of the object the ACL applies to\n\"uuid\"\nuser_id\nstringId of the user the ACL applies to. Exactly one of user_id\nand group_id\nwill be provided\n\"uuid\"\ngroup_id\nstringId of the group the ACL applies to. Exactly one of user_id\nand group_id\nwill be provided\n\"uuid\"\npermission\nstringPermission the ACL grants. Exactly one of permission\nand role_id\nwill be provided\n\"create\" | \"read\" | \"update\" | \"delete\" | \"create_acls\" | \"read_acls\" | \"update_acls\" | \"delete_acls\"\nrestrict_object_type\nstringWhen setting a permission directly, optionally restricts the permission grant to just the specified object type. Cannot be set alongside a role_id\n.\n\"organization\" | \"project\" | \"experiment\" | \"dataset\" | \"prompt\" | \"prompt_session\" | \"group\" | \"role\" | \"org_member\" | \"project_log\" | \"org_project\"\nrole_id\nstringId of the role the ACL grants. Exactly one of permission\nand role_id\nwill be provided\n\"uuid\"\nReturns the new acl object\nDelete single acl\nDelete a single acl\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nRequest Body\napplication/json\nOptionalParameters which uniquely specify the acl to delete\nobject_type\nThe object type that the ACL applies to\n\"organization\" | \"project\" | \"experiment\" | \"dataset\" | \"prompt\" | \"prompt_session\" | \"group\" | \"role\" | \"org_member\" | \"project_log\" | \"org_project\"\nobject_id\nThe id of the object the ACL applies to\n\"uuid\"\nuser_id\nstringId of the user the ACL applies to. Exactly one of user_id\nand group_id\nwill be provided\n\"uuid\"\ngroup_id\nstringId of the group the ACL applies to. Exactly one of user_id\nand group_id\nwill be provided\n\"uuid\"\npermission\nstringPermission the ACL grants. Exactly one of permission\nand role_id\nwill be provided\n\"create\" | \"read\" | \"update\" | \"delete\" | \"create_acls\" | \"read_acls\" | \"update_acls\" | \"delete_acls\"\nrestrict_object_type\nstringWhen setting a permission directly, optionally restricts the permission grant to just the specified object type. Cannot be set alongside a role_id\n.\n\"organization\" | \"project\" | \"experiment\" | \"dataset\" | \"prompt\" | \"prompt_session\" | \"group\" | \"role\" | \"org_member\" | \"project_log\" | \"org_project\"\nrole_id\nstringId of the role the ACL grants. Exactly one of permission\nand role_id\nwill be provided\n\"uuid\"\nReturns the deleted acl object\nGet acl\nGet an acl object by its id\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nPath Parameters\nacl_id\nAcl id\n\"uuid\"\nReturns the acl object\nDelete acl\nDelete an acl object by its id\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nPath Parameters\nacl_id\nAcl id\n\"uuid\"\nReturns the deleted acl object\nBatch update acls\nBatch update acls. This operation is idempotent, so adding acls which already exist will have no effect, and removing acls which do not exist will have no effect.\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nRequest Body\napplication/json\nOptionalAcls to add/remove.\nadd_acls\narray<object>remove_acls\narray<object>A success status\nList org acls\nList all acls in the org. This query requires the caller to have read_acls\npermission at the organization level\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nQuery Parameters\nlimit\nintegerLimit the number of objects to return\n0\nids\nAny properties in string, array<string>Filter search results to a particular set of object IDs. To specify a list of IDs, include the query param multiple times\nstarting_after\nstringPagination cursor id.\nFor example, if the final item in the last page you fetched had an id of foo\n, pass starting_after=foo\nto fetch the next page. Note: you may only pass one of starting_after\nand ending_before\n\"uuid\"\nending_before\nstringPagination cursor id.\nFor example, if the initial item in the last page you fetched had an id of foo\n, pass ending_before=foo\nto fetch the previous page. Note: you may only pass one of starting_after\nand ending_before\n\"uuid\"\nobject_type\nstringThe object type that the ACL applies to\n\"organization\" | \"project\" | \"experiment\" | \"dataset\" | \"prompt\" | \"prompt_session\" | \"group\" | \"role\" | \"org_member\" | \"project_log\" | \"org_project\"\nobject_id\nstringThe id of the object the ACL applies to\n\"uuid\"\nuser_id\nstringId of the user the ACL applies to. Exactly one of user_id\nand group_id\nwill be provided\n\"uuid\"\ngroup_id\nstringId of the group the ACL applies to. Exactly one of user_id\nand group_id\nwill be provided\n\"uuid\"\npermission\nstringEach permission permits a certain type of operation on an object in the system\nPermissions can be assigned to to objects on an individual basis, or grouped into roles\n\"create\" | \"read\" | \"update\" | \"delete\" | \"create_acls\" | \"read_acls\" | \"update_acls\" | \"delete_acls\"\nrestrict_object_type\nstringThe object type that the ACL applies to\n\"organization\" | \"project\" | \"experiment\" | \"dataset\" | \"prompt\" | \"prompt_session\" | \"group\" | \"role\" | \"org_member\" | \"project_log\" | \"org_project\"\nrole_id\nstringId of the role the ACL grants. Exactly one of permission\nand role_id\nwill be provided\n\"uuid\"\norg_name\nstringFilter search results to within a particular organization\nA list of acls", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/reference/api", "anchor": "API"}, {"href": "https://www.braintrust.dev/docs/reference/api/Acls", "anchor": "Acls"}, {"href": "https://www.braintrust.dev/docs/reference/api/AiSecrets", "anchor": "Ai Secrets"}, {"href": "https://www.braintrust.dev/docs/reference/api/ApiKeys", "anchor": "Api Keys"}, {"href": "https://www.braintrust.dev/docs/reference/api/CrossObject", "anchor": "Cross Object"}, {"href": "https://www.braintrust.dev/docs/reference/api/Datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/reference/api/EnvVars", "anchor": "Env Vars"}, {"href": "https://www.braintrust.dev/docs/reference/api/Evals", "anchor": "Evals"}, {"href": "https://www.braintrust.dev/docs/reference/api/Experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/reference/api/Functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/reference/api/Groups", "anchor": "Groups"}, {"href": "https://www.braintrust.dev/docs/reference/api/Logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/reference/api/Organizations", "anchor": "Organizations"}, {"href": "https://www.braintrust.dev/docs/reference/api/Other", "anchor": "Other"}, {"href": "https://www.braintrust.dev/docs/reference/api/Projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectScores", "anchor": "Project Scores"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectTags", "anchor": "Project Tags"}, {"href": "https://www.braintrust.dev/docs/reference/api/Prompts", "anchor": "Prompts"}, {"href": "https://www.braintrust.dev/docs/reference/api/Proxy", "anchor": "Proxy"}, {"href": "https://www.braintrust.dev/docs/reference/api/Roles", "anchor": "Roles"}, {"href": "https://www.braintrust.dev/docs/reference/api/ServiceTokens", "anchor": "Service tokens"}, {"href": "https://www.braintrust.dev/docs/reference/api/SpanIframes", "anchor": "Span Iframes"}, {"href": "https://www.braintrust.dev/docs/reference/api/Users", "anchor": "Users"}, {"href": "https://www.braintrust.dev/docs/reference/api/Views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/reference/autoevals", "anchor": "Autoevals library"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "BTQL query syntax"}, {"href": "https://www.braintrust.dev/docs/reference/streaming", "anchor": "Streaming"}, {"href": "https://www.braintrust.dev/docs/reference/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/reference/reasoning", "anchor": "Reasoning"}, {"href": "https://www.braintrust.dev/docs/reference/organizations", "anchor": "Organizations"}, {"href": "https://www.braintrust.dev/docs/reference/mcp", "anchor": "Model Context Protocol (MCP)"}, {"href": "https://www.braintrust.dev/docs/reference/object-links", "anchor": "Sharing via URL"}, {"href": "https://www.braintrust.dev/docs/reference/glossary", "anchor": "Glossary"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/reference/api", "anchor": "API"}, {"href": "https://www.braintrust.dev/docs/reference/api/Acls", "anchor": "Acls"}, {"href": "https://www.braintrust.dev/docs/reference/api/Acls", "anchor": "List acls"}, {"href": "https://www.braintrust.dev/docs/reference/api/Acls", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/Acls", "anchor": "Query Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/api/Acls", "anchor": "Create acl"}, {"href": "https://www.braintrust.dev/docs/reference/api/Acls", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/Acls", "anchor": "Request Body"}, {"href": "https://www.braintrust.dev/docs/reference/api/Acls", "anchor": "Delete single acl"}, {"href": "https://www.braintrust.dev/docs/reference/api/Acls", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/Acls", "anchor": "Request Body"}, {"href": "https://www.braintrust.dev/docs/reference/api/Acls", "anchor": "Get acl"}, {"href": "https://www.braintrust.dev/docs/reference/api/Acls", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/Acls", "anchor": "Path Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/api/Acls", "anchor": "Delete acl"}, {"href": "https://www.braintrust.dev/docs/reference/api/Acls", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/Acls", "anchor": "Path Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/api/Acls", "anchor": "Batch update acls"}, {"href": "https://www.braintrust.dev/docs/reference/api/Acls", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/Acls", "anchor": "Request Body"}, {"href": "https://www.braintrust.dev/docs/reference/api/Acls", "anchor": "List org acls"}, {"href": "https://www.braintrust.dev/docs/reference/api/Acls", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/Acls", "anchor": "Query Parameters"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}, {"href": "https://www.braintrust.dev/docs/reference/api/Acls", "anchor": "List acls"}, {"href": "https://www.braintrust.dev/docs/reference/api/Acls", "anchor": "Create acl"}, {"href": "https://www.braintrust.dev/docs/reference/api/Acls", "anchor": "Delete single acl"}, {"href": "https://www.braintrust.dev/docs/reference/api/Acls", "anchor": "Get acl"}, {"href": "https://www.braintrust.dev/docs/reference/api/Acls", "anchor": "Delete acl"}, {"href": "https://www.braintrust.dev/docs/reference/api/Acls", "anchor": "Batch update acls"}, {"href": "https://www.braintrust.dev/docs/reference/api/Acls", "anchor": "List org acls"}], "depth": 4}, "https://www.braintrust.dev/docs/reference/api/AiSecrets": {"url": "https://www.braintrust.dev/docs/reference/api/AiSecrets", "title": "Ai Secrets - Docs - Reference - Braintrust", "text": "List ai_secrets\nList out all ai_secrets. The ai_secrets are sorted by creation date, with the most recently-created ai_secrets coming first\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nQuery Parameters\nlimit\nintegerLimit the number of objects to return\n0\nstarting_after\nstringPagination cursor id.\nFor example, if the final item in the last page you fetched had an id of foo\n, pass starting_after=foo\nto fetch the next page. Note: you may only pass one of starting_after\nand ending_before\n\"uuid\"\nending_before\nstringPagination cursor id.\nFor example, if the initial item in the last page you fetched had an id of foo\n, pass ending_before=foo\nto fetch the previous page. Note: you may only pass one of starting_after\nand ending_before\n\"uuid\"\nids\nAny properties in string, array<string>Filter search results to a particular set of object IDs. To specify a list of IDs, include the query param multiple times\nai_secret_name\nstringName of the ai_secret to search for\norg_name\nstringFilter search results to within a particular organization\nai_secret_type\nAny properties in string, array<string>Returns a list of ai_secret objects\nCreate ai_secret\nCreate a new ai_secret. If there is an existing ai_secret with the same name as the one specified in the request, will return the existing ai_secret unmodified\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nRequest Body\napplication/json\nOptionalAny desired information about the new ai_secret object\nname\nName of the AI secret\ntype\nstringmetadata\nobjectsecret\nstringSecret value. If omitted in a PUT request, the existing secret value will be left intact, not replaced with null.\norg_name\nstringFor nearly all users, this parameter should be unnecessary. But in the rare case that your API key belongs to multiple organizations, you may specify the name of the organization the AI Secret belongs in.\nReturns the new ai_secret object\nDelete single ai_secret\nDelete a single ai_secret\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nRequest Body\napplication/json\nOptionalParameters which uniquely specify the ai_secret to delete\nname\nName of the AI secret\norg_name\nstringFor nearly all users, this parameter should be unnecessary. But in the rare case that your API key belongs to multiple organizations, you may specify the name of the organization the AI Secret belongs in.\nReturns the deleted ai_secret object\nCreate or replace ai_secret\nCreate or replace ai_secret. If there is an existing ai_secret with the same name as the one specified in the request, will replace the existing ai_secret with the provided fields\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nRequest Body\napplication/json\nOptionalAny desired information about the new ai_secret object\nname\nName of the AI secret\ntype\nstringmetadata\nobjectsecret\nstringSecret value. If omitted in a PUT request, the existing secret value will be left intact, not replaced with null.\norg_name\nstringFor nearly all users, this parameter should be unnecessary. But in the rare case that your API key belongs to multiple organizations, you may specify the name of the organization the AI Secret belongs in.\nReturns the new ai_secret object\nGet ai_secret\nGet an ai_secret object by its id\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nPath Parameters\nai_secret_id\nAiSecret id\n\"uuid\"\nReturns the ai_secret object\nPartially update ai_secret\nPartially update an ai_secret object. Specify the fields to update in the payload. Any object-type fields will be deep-merged with existing content. Currently we do not support removing fields or setting them to null.\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nRequest Body\napplication/json\nOptionalFields to update\nname\nstringName of the AI secret\ntype\nstringmetadata\nobjectsecret\nstringPath Parameters\nai_secret_id\nAiSecret id\n\"uuid\"\nReturns the ai_secret object\nDelete ai_secret\nDelete an ai_secret object by its id\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nPath Parameters\nai_secret_id\nAiSecret id\n\"uuid\"\nReturns the deleted ai_secret object", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/reference/api", "anchor": "API"}, {"href": "https://www.braintrust.dev/docs/reference/api/Acls", "anchor": "Acls"}, {"href": "https://www.braintrust.dev/docs/reference/api/AiSecrets", "anchor": "Ai Secrets"}, {"href": "https://www.braintrust.dev/docs/reference/api/ApiKeys", "anchor": "Api Keys"}, {"href": "https://www.braintrust.dev/docs/reference/api/CrossObject", "anchor": "Cross Object"}, {"href": "https://www.braintrust.dev/docs/reference/api/Datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/reference/api/EnvVars", "anchor": "Env Vars"}, {"href": "https://www.braintrust.dev/docs/reference/api/Evals", "anchor": "Evals"}, {"href": "https://www.braintrust.dev/docs/reference/api/Experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/reference/api/Functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/reference/api/Groups", "anchor": "Groups"}, {"href": "https://www.braintrust.dev/docs/reference/api/Logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/reference/api/Organizations", "anchor": "Organizations"}, {"href": "https://www.braintrust.dev/docs/reference/api/Other", "anchor": "Other"}, {"href": "https://www.braintrust.dev/docs/reference/api/Projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectScores", "anchor": "Project Scores"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectTags", "anchor": "Project Tags"}, {"href": "https://www.braintrust.dev/docs/reference/api/Prompts", "anchor": "Prompts"}, {"href": "https://www.braintrust.dev/docs/reference/api/Proxy", "anchor": "Proxy"}, {"href": "https://www.braintrust.dev/docs/reference/api/Roles", "anchor": "Roles"}, {"href": "https://www.braintrust.dev/docs/reference/api/ServiceTokens", "anchor": "Service tokens"}, {"href": "https://www.braintrust.dev/docs/reference/api/SpanIframes", "anchor": "Span Iframes"}, {"href": "https://www.braintrust.dev/docs/reference/api/Users", "anchor": "Users"}, {"href": "https://www.braintrust.dev/docs/reference/api/Views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/reference/autoevals", "anchor": "Autoevals library"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "BTQL query syntax"}, {"href": "https://www.braintrust.dev/docs/reference/streaming", "anchor": "Streaming"}, {"href": "https://www.braintrust.dev/docs/reference/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/reference/reasoning", "anchor": "Reasoning"}, {"href": "https://www.braintrust.dev/docs/reference/organizations", "anchor": "Organizations"}, {"href": "https://www.braintrust.dev/docs/reference/mcp", "anchor": "Model Context Protocol (MCP)"}, {"href": "https://www.braintrust.dev/docs/reference/object-links", "anchor": "Sharing via URL"}, {"href": "https://www.braintrust.dev/docs/reference/glossary", "anchor": "Glossary"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/reference/api", "anchor": "API"}, {"href": "https://www.braintrust.dev/docs/reference/api/AiSecrets", "anchor": "Ai Secrets"}, {"href": "https://www.braintrust.dev/docs/reference/api/AiSecrets", "anchor": "List ai_secrets"}, {"href": "https://www.braintrust.dev/docs/reference/api/AiSecrets", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/AiSecrets", "anchor": "Query Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/api/AiSecrets", "anchor": "Create ai_secret"}, {"href": "https://www.braintrust.dev/docs/reference/api/AiSecrets", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/AiSecrets", "anchor": "Request Body"}, {"href": "https://www.braintrust.dev/docs/reference/api/AiSecrets", "anchor": "Delete single ai_secret"}, {"href": "https://www.braintrust.dev/docs/reference/api/AiSecrets", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/AiSecrets", "anchor": "Request Body"}, {"href": "https://www.braintrust.dev/docs/reference/api/AiSecrets", "anchor": "Create or replace ai_secret"}, {"href": "https://www.braintrust.dev/docs/reference/api/AiSecrets", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/AiSecrets", "anchor": "Request Body"}, {"href": "https://www.braintrust.dev/docs/reference/api/AiSecrets", "anchor": "Get ai_secret"}, {"href": "https://www.braintrust.dev/docs/reference/api/AiSecrets", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/AiSecrets", "anchor": "Path Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/api/AiSecrets", "anchor": "Partially update ai_secret"}, {"href": "https://www.braintrust.dev/docs/reference/api/AiSecrets", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/AiSecrets", "anchor": "Request Body"}, {"href": "https://www.braintrust.dev/docs/reference/api/AiSecrets", "anchor": "Path Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/api/AiSecrets", "anchor": "Delete ai_secret"}, {"href": "https://www.braintrust.dev/docs/reference/api/AiSecrets", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/AiSecrets", "anchor": "Path Parameters"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}, {"href": "https://www.braintrust.dev/docs/reference/api/AiSecrets", "anchor": "List ai_secrets"}, {"href": "https://www.braintrust.dev/docs/reference/api/AiSecrets", "anchor": "Create ai_secret"}, {"href": "https://www.braintrust.dev/docs/reference/api/AiSecrets", "anchor": "Delete single ai_secret"}, {"href": "https://www.braintrust.dev/docs/reference/api/AiSecrets", "anchor": "Create or replace ai_secret"}, {"href": "https://www.braintrust.dev/docs/reference/api/AiSecrets", "anchor": "Get ai_secret"}, {"href": "https://www.braintrust.dev/docs/reference/api/AiSecrets", "anchor": "Partially update ai_secret"}, {"href": "https://www.braintrust.dev/docs/reference/api/AiSecrets", "anchor": "Delete ai_secret"}], "depth": 4}, "https://www.braintrust.dev/docs/reference/api/ApiKeys": {"url": "https://www.braintrust.dev/docs/reference/api/ApiKeys", "title": "Api Keys - Docs - Reference - Braintrust", "text": "List api_keys\nList out all api_keys. The api_keys are sorted by creation date, with the most recently-created api_keys coming first\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nQuery Parameters\nlimit\nintegerLimit the number of objects to return\n0\nstarting_after\nstringPagination cursor id.\nFor example, if the final item in the last page you fetched had an id of foo\n, pass starting_after=foo\nto fetch the next page. Note: you may only pass one of starting_after\nand ending_before\n\"uuid\"\nending_before\nstringPagination cursor id.\nFor example, if the initial item in the last page you fetched had an id of foo\n, pass ending_before=foo\nto fetch the previous page. Note: you may only pass one of starting_after\nand ending_before\n\"uuid\"\nids\nAny properties in string, array<string>Filter search results to a particular set of object IDs. To specify a list of IDs, include the query param multiple times\napi_key_name\nstringName of the api_key to search for\norg_name\nstringFilter search results to within a particular organization\nReturns a list of api_key objects\nCreate api_key\nCreate a new api_key. It is possible to have multiple API keys with the same name. There is no de-duplication\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nRequest Body\napplication/json\nOptionalAny desired information about the new api_key object\nname\nName of the api key. Does not have to be unique\norg_name\nstringFor nearly all users, this parameter should be unnecessary. But in the rare case that your API key belongs to multiple organizations, you may specify the name of the organization the API key belongs in.\nReturns an object containing the raw API key. This is the only time the raw API key will be exposed\nGet api_key\nGet an api_key object by its id\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nPath Parameters\napi_key_id\nApiKey id\n\"uuid\"\nReturns the api_key object\nDelete api_key\nDelete an api_key object by its id\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nPath Parameters\napi_key_id\nApiKey id\n\"uuid\"\nReturns the deleted api_key object", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/reference/api", "anchor": "API"}, {"href": "https://www.braintrust.dev/docs/reference/api/Acls", "anchor": "Acls"}, {"href": "https://www.braintrust.dev/docs/reference/api/AiSecrets", "anchor": "Ai Secrets"}, {"href": "https://www.braintrust.dev/docs/reference/api/ApiKeys", "anchor": "Api Keys"}, {"href": "https://www.braintrust.dev/docs/reference/api/CrossObject", "anchor": "Cross Object"}, {"href": "https://www.braintrust.dev/docs/reference/api/Datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/reference/api/EnvVars", "anchor": "Env Vars"}, {"href": "https://www.braintrust.dev/docs/reference/api/Evals", "anchor": "Evals"}, {"href": "https://www.braintrust.dev/docs/reference/api/Experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/reference/api/Functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/reference/api/Groups", "anchor": "Groups"}, {"href": "https://www.braintrust.dev/docs/reference/api/Logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/reference/api/Organizations", "anchor": "Organizations"}, {"href": "https://www.braintrust.dev/docs/reference/api/Other", "anchor": "Other"}, {"href": "https://www.braintrust.dev/docs/reference/api/Projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectScores", "anchor": "Project Scores"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectTags", "anchor": "Project Tags"}, {"href": "https://www.braintrust.dev/docs/reference/api/Prompts", "anchor": "Prompts"}, {"href": "https://www.braintrust.dev/docs/reference/api/Proxy", "anchor": "Proxy"}, {"href": "https://www.braintrust.dev/docs/reference/api/Roles", "anchor": "Roles"}, {"href": "https://www.braintrust.dev/docs/reference/api/ServiceTokens", "anchor": "Service tokens"}, {"href": "https://www.braintrust.dev/docs/reference/api/SpanIframes", "anchor": "Span Iframes"}, {"href": "https://www.braintrust.dev/docs/reference/api/Users", "anchor": "Users"}, {"href": "https://www.braintrust.dev/docs/reference/api/Views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/reference/autoevals", "anchor": "Autoevals library"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "BTQL query syntax"}, {"href": "https://www.braintrust.dev/docs/reference/streaming", "anchor": "Streaming"}, {"href": "https://www.braintrust.dev/docs/reference/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/reference/reasoning", "anchor": "Reasoning"}, {"href": "https://www.braintrust.dev/docs/reference/organizations", "anchor": "Organizations"}, {"href": "https://www.braintrust.dev/docs/reference/mcp", "anchor": "Model Context Protocol (MCP)"}, {"href": "https://www.braintrust.dev/docs/reference/object-links", "anchor": "Sharing via URL"}, {"href": "https://www.braintrust.dev/docs/reference/glossary", "anchor": "Glossary"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/reference/api", "anchor": "API"}, {"href": "https://www.braintrust.dev/docs/reference/api/ApiKeys", "anchor": "Api Keys"}, {"href": "https://www.braintrust.dev/docs/reference/api/ApiKeys", "anchor": "List api_keys"}, {"href": "https://www.braintrust.dev/docs/reference/api/ApiKeys", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/ApiKeys", "anchor": "Query Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/api/ApiKeys", "anchor": "Create api_key"}, {"href": "https://www.braintrust.dev/docs/reference/api/ApiKeys", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/ApiKeys", "anchor": "Request Body"}, {"href": "https://www.braintrust.dev/docs/reference/api/ApiKeys", "anchor": "Get api_key"}, {"href": "https://www.braintrust.dev/docs/reference/api/ApiKeys", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/ApiKeys", "anchor": "Path Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/api/ApiKeys", "anchor": "Delete api_key"}, {"href": "https://www.braintrust.dev/docs/reference/api/ApiKeys", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/ApiKeys", "anchor": "Path Parameters"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}, {"href": "https://www.braintrust.dev/docs/reference/api/ApiKeys", "anchor": "List api_keys"}, {"href": "https://www.braintrust.dev/docs/reference/api/ApiKeys", "anchor": "Create api_key"}, {"href": "https://www.braintrust.dev/docs/reference/api/ApiKeys", "anchor": "Get api_key"}, {"href": "https://www.braintrust.dev/docs/reference/api/ApiKeys", "anchor": "Delete api_key"}], "depth": 4}, "https://www.braintrust.dev/docs/reference/api/CrossObject": {"url": "https://www.braintrust.dev/docs/reference/api/CrossObject", "title": "Cross Object - Docs - Reference - Braintrust", "text": "Cross-object insert\nInsert events and feedback across object types\nPOST /v1/insert\nAuthorization\nAuthorization\nRequired\nBearer <token>Most Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nRequest Body\napplication/json\nOptionalA mapping from event object type -> object id -> events to insert\nexperiment\nobjectA mapping from experiment id to a set of log events and feedback items to insert\ndataset\nobjectA mapping from dataset id to a set of log events and feedback items to insert\nproject_logs\nobjectA mapping from project id to a set of log events and feedback items to insert\nReturns the inserted row ids for the events on each individual object", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/reference/api", "anchor": "API"}, {"href": "https://www.braintrust.dev/docs/reference/api/Acls", "anchor": "Acls"}, {"href": "https://www.braintrust.dev/docs/reference/api/AiSecrets", "anchor": "Ai Secrets"}, {"href": "https://www.braintrust.dev/docs/reference/api/ApiKeys", "anchor": "Api Keys"}, {"href": "https://www.braintrust.dev/docs/reference/api/CrossObject", "anchor": "Cross Object"}, {"href": "https://www.braintrust.dev/docs/reference/api/Datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/reference/api/EnvVars", "anchor": "Env Vars"}, {"href": "https://www.braintrust.dev/docs/reference/api/Evals", "anchor": "Evals"}, {"href": "https://www.braintrust.dev/docs/reference/api/Experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/reference/api/Functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/reference/api/Groups", "anchor": "Groups"}, {"href": "https://www.braintrust.dev/docs/reference/api/Logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/reference/api/Organizations", "anchor": "Organizations"}, {"href": "https://www.braintrust.dev/docs/reference/api/Other", "anchor": "Other"}, {"href": "https://www.braintrust.dev/docs/reference/api/Projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectScores", "anchor": "Project Scores"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectTags", "anchor": "Project Tags"}, {"href": "https://www.braintrust.dev/docs/reference/api/Prompts", "anchor": "Prompts"}, {"href": "https://www.braintrust.dev/docs/reference/api/Proxy", "anchor": "Proxy"}, {"href": "https://www.braintrust.dev/docs/reference/api/Roles", "anchor": "Roles"}, {"href": "https://www.braintrust.dev/docs/reference/api/ServiceTokens", "anchor": "Service tokens"}, {"href": "https://www.braintrust.dev/docs/reference/api/SpanIframes", "anchor": "Span Iframes"}, {"href": "https://www.braintrust.dev/docs/reference/api/Users", "anchor": "Users"}, {"href": "https://www.braintrust.dev/docs/reference/api/Views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/reference/autoevals", "anchor": "Autoevals library"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "BTQL query syntax"}, {"href": "https://www.braintrust.dev/docs/reference/streaming", "anchor": "Streaming"}, {"href": "https://www.braintrust.dev/docs/reference/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/reference/reasoning", "anchor": "Reasoning"}, {"href": "https://www.braintrust.dev/docs/reference/organizations", "anchor": "Organizations"}, {"href": "https://www.braintrust.dev/docs/reference/mcp", "anchor": "Model Context Protocol (MCP)"}, {"href": "https://www.braintrust.dev/docs/reference/object-links", "anchor": "Sharing via URL"}, {"href": "https://www.braintrust.dev/docs/reference/glossary", "anchor": "Glossary"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/reference/api", "anchor": "API"}, {"href": "https://www.braintrust.dev/docs/reference/api/CrossObject", "anchor": "Cross Object"}, {"href": "https://www.braintrust.dev/docs/reference/api/CrossObject", "anchor": "Cross-object insert"}, {"href": "https://www.braintrust.dev/docs/reference/api/CrossObject", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/CrossObject", "anchor": "Request Body"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}, {"href": "https://www.braintrust.dev/docs/reference/api/CrossObject", "anchor": "Cross-object insert"}], "depth": 4}, "https://www.braintrust.dev/docs/reference/api/Datasets": {"url": "https://www.braintrust.dev/docs/reference/api/Datasets", "title": "Datasets - Docs - Reference - Braintrust", "text": "List datasets\nList out all datasets. The datasets are sorted by creation date, with the most recently-created datasets coming first\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nQuery Parameters\nlimit\nintegerLimit the number of objects to return\n0\nstarting_after\nstringPagination cursor id.\nFor example, if the final item in the last page you fetched had an id of foo\n, pass starting_after=foo\nto fetch the next page. Note: you may only pass one of starting_after\nand ending_before\n\"uuid\"\nending_before\nstringPagination cursor id.\nFor example, if the initial item in the last page you fetched had an id of foo\n, pass ending_before=foo\nto fetch the previous page. Note: you may only pass one of starting_after\nand ending_before\n\"uuid\"\nids\nAny properties in string, array<string>Filter search results to a particular set of object IDs. To specify a list of IDs, include the query param multiple times\ndataset_name\nstringName of the dataset to search for\nproject_name\nstringName of the project to search for\nproject_id\nstringProject id\n\"uuid\"\norg_name\nstringFilter search results to within a particular organization\nReturns a list of dataset objects\nCreate dataset\nCreate a new dataset. If there is an existing dataset in the project with the same name as the one specified in the request, will return the existing dataset unmodified\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nRequest Body\napplication/json\nOptionalAny desired information about the new dataset object\nproject_id\nUnique identifier for the project that the dataset belongs under\n\"uuid\"\nname\nName of the dataset. Within a project, dataset names are unique\n1\ndescription\nstringTextual description of the dataset\nmetadata\nobjectUser-controlled metadata about the dataset\nReturns the new dataset object\nGet dataset\nGet a dataset object by its id\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nPath Parameters\ndataset_id\nDataset id\n\"uuid\"\nReturns the dataset object\nPartially update dataset\nPartially update a dataset object. Specify the fields to update in the payload. Any object-type fields will be deep-merged with existing content. Currently we do not support removing fields or setting them to null.\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nRequest Body\napplication/json\nOptionalFields to update\nname\nstringName of the dataset. Within a project, dataset names are unique\ndescription\nstringTextual description of the dataset\nmetadata\nobjectUser-controlled metadata about the dataset\nPath Parameters\ndataset_id\nDataset id\n\"uuid\"\nReturns the dataset object\nDelete dataset\nDelete a dataset object by its id\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nPath Parameters\ndataset_id\nDataset id\n\"uuid\"\nReturns the deleted dataset object\nInsert dataset events\nInsert a set of events into the dataset\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nRequest Body\napplication/json\nOptionalAn array of dataset events to insert\nevents\nA list of dataset events to insert\nPath Parameters\ndataset_id\nDataset id\n\"uuid\"\nReturns the inserted row ids\nFetch dataset (GET form)\nFetch the events in a dataset. Equivalent to the POST form of the same path, but with the parameters in the URL query rather than in the request body. For more complex queries, use the POST /btql\nendpoint.\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nPath Parameters\ndataset_id\nDataset id\n\"uuid\"\nQuery Parameters\nlimit\nintegerlimit the number of traces fetched\nFetch queries may be paginated if the total result size is expected to be large (e.g. project_logs which accumulate over a long time). Note that fetch queries only support pagination in descending time order (from latest to earliest _xact_id\n. Furthermore, later pages may return rows which showed up in earlier pages, except with an earlier _xact_id\n. This happens because pagination occurs over the whole version history of the event log. You will most likely want to exclude any such duplicate, outdated rows (by id\n) from your combined result set.\nThe limit\nparameter controls the number of full traces to return. So you may end up with more individual rows than the specified limit if you are fetching events containing traces.\n0\nmax_xact_id\nstringDEPRECATION NOTICE: The manually-constructed pagination cursor is deprecated in favor of the explicit 'cursor' returned by object fetch requests. Please prefer the 'cursor' argument going forwards.\nTogether, max_xact_id\nand max_root_span_id\nform a pagination cursor\nSince a paginated fetch query returns results in order from latest to earliest, the cursor for the next page can be found as the row with the minimum (earliest) value of the tuple (_xact_id, root_span_id)\n. See the documentation of limit\nfor an overview of paginating fetch queries.\nmax_root_span_id\nstringDEPRECATION NOTICE: The manually-constructed pagination cursor is deprecated in favor of the explicit 'cursor' returned by object fetch requests. Please prefer the 'cursor' argument going forwards.\nTogether, max_xact_id\nand max_root_span_id\nform a pagination cursor\nSince a paginated fetch query returns results in order from latest to earliest, the cursor for the next page can be found as the row with the minimum (earliest) value of the tuple (_xact_id, root_span_id)\n. See the documentation of limit\nfor an overview of paginating fetch queries.\nversion\nstringRetrieve a snapshot of events from a past time\nThe version id is essentially a filter on the latest event transaction id. You can use the max_xact_id\nreturned by a past fetch as the version to reproduce that exact fetch.\nReturns the fetched rows\nFetch dataset (POST form)\nFetch the events in a dataset. Equivalent to the GET form of the same path, but with the parameters in the request body rather than in the URL query. For more complex queries, use the POST /btql\nendpoint.\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nRequest Body\napplication/json\nOptionalFilters for the fetch query\nlimit\nintegerlimit the number of traces fetched\nFetch queries may be paginated if the total result size is expected to be large (e.g. project_logs which accumulate over a long time). Note that fetch queries only support pagination in descending time order (from latest to earliest _xact_id\n. Furthermore, later pages may return rows which showed up in earlier pages, except with an earlier _xact_id\n. This happens because pagination occurs over the whole version history of the event log. You will most likely want to exclude any such duplicate, outdated rows (by id\n) from your combined result set.\nThe limit\nparameter controls the number of full traces to return. So you may end up with more individual rows than the specified limit if you are fetching events containing traces.\n0\ncursor\nstringAn opaque string to be used as a cursor for the next page of results, in order from latest to earliest.\nThe string can be obtained directly from the cursor\nproperty of the previous fetch query\nmax_xact_id\nstringDEPRECATION NOTICE: The manually-constructed pagination cursor is deprecated in favor of the explicit 'cursor' returned by object fetch requests. Please prefer the 'cursor' argument going forwards.\nTogether, max_xact_id\nand max_root_span_id\nform a pagination cursor\nSince a paginated fetch query returns results in order from latest to earliest, the cursor for the next page can be found as the row with the minimum (earliest) value of the tuple (_xact_id, root_span_id)\n. See the documentation of limit\nfor an overview of paginating fetch queries.\nmax_root_span_id\nstringDEPRECATION NOTICE: The manually-constructed pagination cursor is deprecated in favor of the explicit 'cursor' returned by object fetch requests. Please prefer the 'cursor' argument going forwards.\nTogether, max_xact_id\nand max_root_span_id\nform a pagination cursor\nSince a paginated fetch query returns results in order from latest to earliest, the cursor for the next page can be found as the row with the minimum (earliest) value of the tuple (_xact_id, root_span_id)\n. See the documentation of limit\nfor an overview of paginating fetch queries.\nversion\nstringRetrieve a snapshot of events from a past time\nThe version id is essentially a filter on the latest event transaction id. You can use the max_xact_id\nreturned by a past fetch as the version to reproduce that exact fetch.\nPath Parameters\ndataset_id\nDataset id\n\"uuid\"\nReturns the fetched rows\nFeedback for dataset events\nLog feedback for a set of dataset events\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nRequest Body\napplication/json\nOptionalAn array of feedback objects\nfeedback\nA list of dataset feedback items\nPath Parameters\ndataset_id\nDataset id\n\"uuid\"\nReturns a success status\nSummarize dataset\nSummarize dataset\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nPath Parameters\ndataset_id\nDataset id\n\"uuid\"\nQuery Parameters\nsummarize_data\nbooleanWhether to summarize the data. If false (or omitted), only the metadata will be returned.\nDataset summary", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/reference/api", "anchor": "API"}, {"href": "https://www.braintrust.dev/docs/reference/api/Acls", "anchor": "Acls"}, {"href": "https://www.braintrust.dev/docs/reference/api/AiSecrets", "anchor": "Ai Secrets"}, {"href": "https://www.braintrust.dev/docs/reference/api/ApiKeys", "anchor": "Api Keys"}, {"href": "https://www.braintrust.dev/docs/reference/api/CrossObject", "anchor": "Cross Object"}, {"href": "https://www.braintrust.dev/docs/reference/api/Datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/reference/api/EnvVars", "anchor": "Env Vars"}, {"href": "https://www.braintrust.dev/docs/reference/api/Evals", "anchor": "Evals"}, {"href": "https://www.braintrust.dev/docs/reference/api/Experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/reference/api/Functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/reference/api/Groups", "anchor": "Groups"}, {"href": "https://www.braintrust.dev/docs/reference/api/Logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/reference/api/Organizations", "anchor": "Organizations"}, {"href": "https://www.braintrust.dev/docs/reference/api/Other", "anchor": "Other"}, {"href": "https://www.braintrust.dev/docs/reference/api/Projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectScores", "anchor": "Project Scores"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectTags", "anchor": "Project Tags"}, {"href": "https://www.braintrust.dev/docs/reference/api/Prompts", "anchor": "Prompts"}, {"href": "https://www.braintrust.dev/docs/reference/api/Proxy", "anchor": "Proxy"}, {"href": "https://www.braintrust.dev/docs/reference/api/Roles", "anchor": "Roles"}, {"href": "https://www.braintrust.dev/docs/reference/api/ServiceTokens", "anchor": "Service tokens"}, {"href": "https://www.braintrust.dev/docs/reference/api/SpanIframes", "anchor": "Span Iframes"}, {"href": "https://www.braintrust.dev/docs/reference/api/Users", "anchor": "Users"}, {"href": "https://www.braintrust.dev/docs/reference/api/Views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/reference/autoevals", "anchor": "Autoevals library"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "BTQL query syntax"}, {"href": "https://www.braintrust.dev/docs/reference/streaming", "anchor": "Streaming"}, {"href": "https://www.braintrust.dev/docs/reference/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/reference/reasoning", "anchor": "Reasoning"}, {"href": "https://www.braintrust.dev/docs/reference/organizations", "anchor": "Organizations"}, {"href": "https://www.braintrust.dev/docs/reference/mcp", "anchor": "Model Context Protocol (MCP)"}, {"href": "https://www.braintrust.dev/docs/reference/object-links", "anchor": "Sharing via URL"}, {"href": "https://www.braintrust.dev/docs/reference/glossary", "anchor": "Glossary"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/reference/api", "anchor": "API"}, {"href": "https://www.braintrust.dev/docs/reference/api/Datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/reference/api/Datasets", "anchor": "List datasets"}, {"href": "https://www.braintrust.dev/docs/reference/api/Datasets", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/Datasets", "anchor": "Query Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/api/Datasets", "anchor": "Create dataset"}, {"href": "https://www.braintrust.dev/docs/reference/api/Datasets", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/Datasets", "anchor": "Request Body"}, {"href": "https://www.braintrust.dev/docs/reference/api/Datasets", "anchor": "Get dataset"}, {"href": "https://www.braintrust.dev/docs/reference/api/Datasets", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/Datasets", "anchor": "Path Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/api/Datasets", "anchor": "Partially update dataset"}, {"href": "https://www.braintrust.dev/docs/reference/api/Datasets", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/Datasets", "anchor": "Request Body"}, {"href": "https://www.braintrust.dev/docs/reference/api/Datasets", "anchor": "Path Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/api/Datasets", "anchor": "Delete dataset"}, {"href": "https://www.braintrust.dev/docs/reference/api/Datasets", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/Datasets", "anchor": "Path Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/api/Datasets", "anchor": "Insert dataset events"}, {"href": "https://www.braintrust.dev/docs/reference/api/Datasets", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/Datasets", "anchor": "Request Body"}, {"href": "https://www.braintrust.dev/docs/reference/api/Datasets", "anchor": "Path Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/api/Datasets", "anchor": "Fetch dataset (GET form)"}, {"href": "https://www.braintrust.dev/docs/reference/api/Datasets", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/Datasets", "anchor": "Path Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/api/Datasets", "anchor": "Query Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/api/Datasets", "anchor": "Fetch dataset (POST form)"}, {"href": "https://www.braintrust.dev/docs/reference/api/Datasets", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/Datasets", "anchor": "Request Body"}, {"href": "https://www.braintrust.dev/docs/reference/api/Datasets", "anchor": "Path Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/api/Datasets", "anchor": "Feedback for dataset events"}, {"href": "https://www.braintrust.dev/docs/reference/api/Datasets", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/Datasets", "anchor": "Request Body"}, {"href": "https://www.braintrust.dev/docs/reference/api/Datasets", "anchor": "Path Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/api/Datasets", "anchor": "Summarize dataset"}, {"href": "https://www.braintrust.dev/docs/reference/api/Datasets", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/Datasets", "anchor": "Path Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/api/Datasets", "anchor": "Query Parameters"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}, {"href": "https://www.braintrust.dev/docs/reference/api/Datasets", "anchor": "List datasets"}, {"href": "https://www.braintrust.dev/docs/reference/api/Datasets", "anchor": "Create dataset"}, {"href": "https://www.braintrust.dev/docs/reference/api/Datasets", "anchor": "Get dataset"}, {"href": "https://www.braintrust.dev/docs/reference/api/Datasets", "anchor": "Partially update dataset"}, {"href": "https://www.braintrust.dev/docs/reference/api/Datasets", "anchor": "Delete dataset"}, {"href": "https://www.braintrust.dev/docs/reference/api/Datasets", "anchor": "Insert dataset events"}, {"href": "https://www.braintrust.dev/docs/reference/api/Datasets", "anchor": "Fetch dataset (GET form)"}, {"href": "https://www.braintrust.dev/docs/reference/api/Datasets", "anchor": "Fetch dataset (POST form)"}, {"href": "https://www.braintrust.dev/docs/reference/api/Datasets", "anchor": "Feedback for dataset events"}, {"href": "https://www.braintrust.dev/docs/reference/api/Datasets", "anchor": "Summarize dataset"}], "depth": 4}, "https://www.braintrust.dev/docs/reference/api/EnvVars": {"url": "https://www.braintrust.dev/docs/reference/api/EnvVars", "title": "Env Vars - Docs - Reference - Braintrust", "text": "List env_vars\nList out all env_vars. The env_vars are sorted by creation date, with the most recently-created env_vars coming first\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nQuery Parameters\nlimit\nintegerLimit the number of objects to return\n0\nids\nAny properties in string, array<string>Filter search results to a particular set of object IDs. To specify a list of IDs, include the query param multiple times\nenv_var_name\nstringName of the env_var to search for\nobject_type\nstringThe type of the object the environment variable is scoped for\n\"organization\" | \"project\" | \"function\"\nobject_id\nstringThe id of the object the environment variable is scoped for\n\"uuid\"\nReturns a list of env_var objects\nCreate env_var\nCreate a new env_var. If there is an existing env_var with the same name as the one specified in the request, will return the existing env_var unmodified\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nRequest Body\napplication/json\nOptionalAny desired information about the new env_var object\nobject_type\nThe type of the object the environment variable is scoped for\n\"organization\" | \"project\" | \"function\"\nobject_id\nThe id of the object the environment variable is scoped for\n\"uuid\"\nname\nThe name of the environment variable\nvalue\nstringThe value of the environment variable. Will be encrypted at rest.\nReturns the new env_var object\nCreate or replace env_var\nCreate or replace env_var. If there is an existing env_var with the same name as the one specified in the request, will replace the existing env_var with the provided fields\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nRequest Body\napplication/json\nOptionalAny desired information about the new env_var object\nobject_type\nThe type of the object the environment variable is scoped for\n\"organization\" | \"project\" | \"function\"\nobject_id\nThe id of the object the environment variable is scoped for\n\"uuid\"\nname\nThe name of the environment variable\nvalue\nstringThe value of the environment variable. Will be encrypted at rest.\nReturns the new env_var object\nGet env_var\nGet an env_var object by its id\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nPath Parameters\nenv_var_id\nEnvVar id\n\"uuid\"\nReturns the env_var object\nPartially update env_var\nPartially update an env_var object. Specify the fields to update in the payload. Any object-type fields will be deep-merged with existing content. Currently we do not support removing fields or setting them to null.\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nRequest Body\napplication/json\nOptionalFields to update\nname\nThe name of the environment variable\nvalue\nstringThe value of the environment variable. Will be encrypted at rest.\nPath Parameters\nenv_var_id\nEnvVar id\n\"uuid\"\nReturns the env_var object\nDelete env_var\nDelete an env_var object by its id\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nPath Parameters\nenv_var_id\nEnvVar id\n\"uuid\"\nReturns the deleted env_var object", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/reference/api", "anchor": "API"}, {"href": "https://www.braintrust.dev/docs/reference/api/Acls", "anchor": "Acls"}, {"href": "https://www.braintrust.dev/docs/reference/api/AiSecrets", "anchor": "Ai Secrets"}, {"href": "https://www.braintrust.dev/docs/reference/api/ApiKeys", "anchor": "Api Keys"}, {"href": "https://www.braintrust.dev/docs/reference/api/CrossObject", "anchor": "Cross Object"}, {"href": "https://www.braintrust.dev/docs/reference/api/Datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/reference/api/EnvVars", "anchor": "Env Vars"}, {"href": "https://www.braintrust.dev/docs/reference/api/Evals", "anchor": "Evals"}, {"href": "https://www.braintrust.dev/docs/reference/api/Experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/reference/api/Functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/reference/api/Groups", "anchor": "Groups"}, {"href": "https://www.braintrust.dev/docs/reference/api/Logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/reference/api/Organizations", "anchor": "Organizations"}, {"href": "https://www.braintrust.dev/docs/reference/api/Other", "anchor": "Other"}, {"href": "https://www.braintrust.dev/docs/reference/api/Projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectScores", "anchor": "Project Scores"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectTags", "anchor": "Project Tags"}, {"href": "https://www.braintrust.dev/docs/reference/api/Prompts", "anchor": "Prompts"}, {"href": "https://www.braintrust.dev/docs/reference/api/Proxy", "anchor": "Proxy"}, {"href": "https://www.braintrust.dev/docs/reference/api/Roles", "anchor": "Roles"}, {"href": "https://www.braintrust.dev/docs/reference/api/ServiceTokens", "anchor": "Service tokens"}, {"href": "https://www.braintrust.dev/docs/reference/api/SpanIframes", "anchor": "Span Iframes"}, {"href": "https://www.braintrust.dev/docs/reference/api/Users", "anchor": "Users"}, {"href": "https://www.braintrust.dev/docs/reference/api/Views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/reference/autoevals", "anchor": "Autoevals library"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "BTQL query syntax"}, {"href": "https://www.braintrust.dev/docs/reference/streaming", "anchor": "Streaming"}, {"href": "https://www.braintrust.dev/docs/reference/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/reference/reasoning", "anchor": "Reasoning"}, {"href": "https://www.braintrust.dev/docs/reference/organizations", "anchor": "Organizations"}, {"href": "https://www.braintrust.dev/docs/reference/mcp", "anchor": "Model Context Protocol (MCP)"}, {"href": "https://www.braintrust.dev/docs/reference/object-links", "anchor": "Sharing via URL"}, {"href": "https://www.braintrust.dev/docs/reference/glossary", "anchor": "Glossary"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/reference/api", "anchor": "API"}, {"href": "https://www.braintrust.dev/docs/reference/api/EnvVars", "anchor": "Env Vars"}, {"href": "https://www.braintrust.dev/docs/reference/api/EnvVars", "anchor": "List env_vars"}, {"href": "https://www.braintrust.dev/docs/reference/api/EnvVars", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/EnvVars", "anchor": "Query Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/api/EnvVars", "anchor": "Create env_var"}, {"href": "https://www.braintrust.dev/docs/reference/api/EnvVars", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/EnvVars", "anchor": "Request Body"}, {"href": "https://www.braintrust.dev/docs/reference/api/EnvVars", "anchor": "Create or replace env_var"}, {"href": "https://www.braintrust.dev/docs/reference/api/EnvVars", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/EnvVars", "anchor": "Request Body"}, {"href": "https://www.braintrust.dev/docs/reference/api/EnvVars", "anchor": "Get env_var"}, {"href": "https://www.braintrust.dev/docs/reference/api/EnvVars", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/EnvVars", "anchor": "Path Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/api/EnvVars", "anchor": "Partially update env_var"}, {"href": "https://www.braintrust.dev/docs/reference/api/EnvVars", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/EnvVars", "anchor": "Request Body"}, {"href": "https://www.braintrust.dev/docs/reference/api/EnvVars", "anchor": "Path Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/api/EnvVars", "anchor": "Delete env_var"}, {"href": "https://www.braintrust.dev/docs/reference/api/EnvVars", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/EnvVars", "anchor": "Path Parameters"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}, {"href": "https://www.braintrust.dev/docs/reference/api/EnvVars", "anchor": "List env_vars"}, {"href": "https://www.braintrust.dev/docs/reference/api/EnvVars", "anchor": "Create env_var"}, {"href": "https://www.braintrust.dev/docs/reference/api/EnvVars", "anchor": "Create or replace env_var"}, {"href": "https://www.braintrust.dev/docs/reference/api/EnvVars", "anchor": "Get env_var"}, {"href": "https://www.braintrust.dev/docs/reference/api/EnvVars", "anchor": "Partially update env_var"}, {"href": "https://www.braintrust.dev/docs/reference/api/EnvVars", "anchor": "Delete env_var"}], "depth": 4}, "https://www.braintrust.dev/docs/reference/api/Evals": {"url": "https://www.braintrust.dev/docs/reference/api/Evals", "title": "Evals - Docs - Reference - Braintrust", "text": "Launch an eval\nLaunch an evaluation. This is the API-equivalent of the Eval\nfunction that is built into the Braintrust SDK. In the Eval API, you provide pointers to a dataset, task function, and scoring functions. The API will then run the evaluation, create an experiment, and return the results along with a link to the experiment. To learn more about evals, see the Evals guide.\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nRequest Body\napplication/json\nRequiredEval launch parameters\nproject_id\nUnique identifier for the project to run the eval in\ndata\nThe dataset to use\ntask\nThe function to evaluate\nscores\nThe functions to score the eval on\nexperiment_name\nstringAn optional name for the experiment created by this eval. If it conflicts with an existing experiment, it will be suffixed with a unique identifier.\nmetadata\nobjectOptional experiment-level metadata to store about the evaluation. You can later use this to slice & dice across experiments.\nparent\nAny properties in span_parent_struct, stringstream\nbooleanWhether to stream the results of the eval. If true, the request will return two events: one to indicate the experiment has started, and another upon completion. If false, the request will return the evaluation's summary upon completion.\ntrial_count\nnumberThe number of times to run the evaluator per input. This is useful for evaluating applications that have non-deterministic behavior and gives you both a stronger aggregate measure and a sense of the variance in the results.\nis_public\nbooleanWhether the experiment should be public. Defaults to false.\ntimeout\nnumberThe maximum duration, in milliseconds, to run the evaluation. Defaults to undefined, in which case there is no timeout.\nmax_concurrency\nnumberThe maximum number of tasks/scorers that will be run concurrently. Defaults to 10. If null is provided, no max concurrency will be used.\n10\nbase_experiment_name\nstringAn optional experiment name to use as a base. If specified, the new experiment will be summarized and compared to this experiment.\nbase_experiment_id\nstringAn optional experiment id to use as a base. If specified, the new experiment will be summarized and compared to this experiment.\ngit_metadata_settings\nobjectOptional settings for collecting git metadata. By default, will collect all git metadata fields allowed in org-level settings.\nrepo_info\nobjectstrict\nbooleanIf true, throw an error if one of the variables in the prompt is not present in the input\nstop_token\nstringThe token to stop the run\nextra_messages\nstringA template path of extra messages to append to the conversion. These messages will be appended to the end of the conversation, after the last message.\ntags\narray<string>Optional tags that will be added to the experiment.\nEval launch response", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/reference/api", "anchor": "API"}, {"href": "https://www.braintrust.dev/docs/reference/api/Acls", "anchor": "Acls"}, {"href": "https://www.braintrust.dev/docs/reference/api/AiSecrets", "anchor": "Ai Secrets"}, {"href": "https://www.braintrust.dev/docs/reference/api/ApiKeys", "anchor": "Api Keys"}, {"href": "https://www.braintrust.dev/docs/reference/api/CrossObject", "anchor": "Cross Object"}, {"href": "https://www.braintrust.dev/docs/reference/api/Datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/reference/api/EnvVars", "anchor": "Env Vars"}, {"href": "https://www.braintrust.dev/docs/reference/api/Evals", "anchor": "Evals"}, {"href": "https://www.braintrust.dev/docs/reference/api/Experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/reference/api/Functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/reference/api/Groups", "anchor": "Groups"}, {"href": "https://www.braintrust.dev/docs/reference/api/Logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/reference/api/Organizations", "anchor": "Organizations"}, {"href": "https://www.braintrust.dev/docs/reference/api/Other", "anchor": "Other"}, {"href": "https://www.braintrust.dev/docs/reference/api/Projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectScores", "anchor": "Project Scores"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectTags", "anchor": "Project Tags"}, {"href": "https://www.braintrust.dev/docs/reference/api/Prompts", "anchor": "Prompts"}, {"href": "https://www.braintrust.dev/docs/reference/api/Proxy", "anchor": "Proxy"}, {"href": "https://www.braintrust.dev/docs/reference/api/Roles", "anchor": "Roles"}, {"href": "https://www.braintrust.dev/docs/reference/api/ServiceTokens", "anchor": "Service tokens"}, {"href": "https://www.braintrust.dev/docs/reference/api/SpanIframes", "anchor": "Span Iframes"}, {"href": "https://www.braintrust.dev/docs/reference/api/Users", "anchor": "Users"}, {"href": "https://www.braintrust.dev/docs/reference/api/Views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/reference/autoevals", "anchor": "Autoevals library"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "BTQL query syntax"}, {"href": "https://www.braintrust.dev/docs/reference/streaming", "anchor": "Streaming"}, {"href": "https://www.braintrust.dev/docs/reference/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/reference/reasoning", "anchor": "Reasoning"}, {"href": "https://www.braintrust.dev/docs/reference/organizations", "anchor": "Organizations"}, {"href": "https://www.braintrust.dev/docs/reference/mcp", "anchor": "Model Context Protocol (MCP)"}, {"href": "https://www.braintrust.dev/docs/reference/object-links", "anchor": "Sharing via URL"}, {"href": "https://www.braintrust.dev/docs/reference/glossary", "anchor": "Glossary"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/reference/api", "anchor": "API"}, {"href": "https://www.braintrust.dev/docs/reference/api/Evals", "anchor": "Evals"}, {"href": "https://www.braintrust.dev/docs/reference/api/Evals", "anchor": "Launch an eval"}, {"href": "https://www.braintrust.dev/docs/guides/evals", "anchor": "Evals guide"}, {"href": "https://www.braintrust.dev/docs/reference/api/Evals", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/Evals", "anchor": "Request Body"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}, {"href": "https://www.braintrust.dev/docs/reference/api/Evals", "anchor": "Launch an eval"}], "depth": 4}, "https://www.braintrust.dev/docs/reference/api/Experiments": {"url": "https://www.braintrust.dev/docs/reference/api/Experiments", "title": "Experiments - Docs - Reference - Braintrust", "text": "List experiments\nList out all experiments. The experiments are sorted by creation date, with the most recently-created experiments coming first\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nQuery Parameters\nlimit\nintegerLimit the number of objects to return\n0\nstarting_after\nstringPagination cursor id.\nFor example, if the final item in the last page you fetched had an id of foo\n, pass starting_after=foo\nto fetch the next page. Note: you may only pass one of starting_after\nand ending_before\n\"uuid\"\nending_before\nstringPagination cursor id.\nFor example, if the initial item in the last page you fetched had an id of foo\n, pass ending_before=foo\nto fetch the previous page. Note: you may only pass one of starting_after\nand ending_before\n\"uuid\"\nids\nAny properties in string, array<string>Filter search results to a particular set of object IDs. To specify a list of IDs, include the query param multiple times\nexperiment_name\nstringName of the experiment to search for\nproject_name\nstringName of the project to search for\nproject_id\nstringProject id\n\"uuid\"\norg_name\nstringFilter search results to within a particular organization\nReturns a list of experiment objects\nCreate experiment\nCreate a new experiment. If there is an existing experiment in the project with the same name as the one specified in the request, will return the existing experiment unmodified\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nRequest Body\napplication/json\nOptionalAny desired information about the new experiment object\nproject_id\nUnique identifier for the project that the experiment belongs under\n\"uuid\"\nname\nstringName of the experiment. Within a project, experiment names are unique\n1\ndescription\nstringTextual description of the experiment\nrepo_info\nobjectMetadata about the state of the repo when the experiment was created\nbase_exp_id\nstringId of default base experiment to compare against when viewing this experiment\n\"uuid\"\ndataset_id\nstringIdentifier of the linked dataset, or null if the experiment is not linked to a dataset\n\"uuid\"\ndataset_version\nstringVersion number of the linked dataset the experiment was run against. This can be used to reproduce the experiment after the dataset has been modified.\npublic\nbooleanWhether or not the experiment is public. Public experiments can be viewed by anybody inside or outside the organization\nmetadata\nobjectUser-controlled metadata about the experiment\ntags\narray<string>A list of tags for the experiment\nensure_new\nbooleanNormally, creating an experiment with the same name as an existing experiment will return the existing one un-modified. But if ensure_new\nis true, registration will generate a new experiment with a unique name in case of a conflict.\nReturns the new experiment object\nGet experiment\nGet an experiment object by its id\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nPath Parameters\nexperiment_id\nExperiment id\n\"uuid\"\nReturns the experiment object\nPartially update experiment\nPartially update an experiment object. Specify the fields to update in the payload. Any object-type fields will be deep-merged with existing content. Currently we do not support removing fields or setting them to null.\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nRequest Body\napplication/json\nOptionalFields to update\nname\nstringName of the experiment. Within a project, experiment names are unique\ndescription\nstringTextual description of the experiment\nrepo_info\nobjectMetadata about the state of the repo when the experiment was created\nbase_exp_id\nstringId of default base experiment to compare against when viewing this experiment\n\"uuid\"\ndataset_id\nstringIdentifier of the linked dataset, or null if the experiment is not linked to a dataset\n\"uuid\"\ndataset_version\nstringVersion number of the linked dataset the experiment was run against. This can be used to reproduce the experiment after the dataset has been modified.\npublic\nbooleanWhether or not the experiment is public. Public experiments can be viewed by anybody inside or outside the organization\nmetadata\nobjectUser-controlled metadata about the experiment\ntags\narray<string>A list of tags for the experiment\nPath Parameters\nexperiment_id\nExperiment id\n\"uuid\"\nReturns the experiment object\nDelete experiment\nDelete an experiment object by its id\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nPath Parameters\nexperiment_id\nExperiment id\n\"uuid\"\nReturns the deleted experiment object\nInsert experiment events\nInsert a set of events into the experiment\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nRequest Body\napplication/json\nOptionalAn array of experiment events to insert\nevents\nA list of experiment events to insert\nPath Parameters\nexperiment_id\nExperiment id\n\"uuid\"\nReturns the inserted row ids\nFetch experiment (GET form)\nFetch the events in an experiment. Equivalent to the POST form of the same path, but with the parameters in the URL query rather than in the request body. For more complex queries, use the POST /btql\nendpoint.\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nPath Parameters\nexperiment_id\nExperiment id\n\"uuid\"\nQuery Parameters\nlimit\nintegerlimit the number of traces fetched\nFetch queries may be paginated if the total result size is expected to be large (e.g. project_logs which accumulate over a long time). Note that fetch queries only support pagination in descending time order (from latest to earliest _xact_id\n. Furthermore, later pages may return rows which showed up in earlier pages, except with an earlier _xact_id\n. This happens because pagination occurs over the whole version history of the event log. You will most likely want to exclude any such duplicate, outdated rows (by id\n) from your combined result set.\nThe limit\nparameter controls the number of full traces to return. So you may end up with more individual rows than the specified limit if you are fetching events containing traces.\n0\nmax_xact_id\nstringDEPRECATION NOTICE: The manually-constructed pagination cursor is deprecated in favor of the explicit 'cursor' returned by object fetch requests. Please prefer the 'cursor' argument going forwards.\nTogether, max_xact_id\nand max_root_span_id\nform a pagination cursor\nSince a paginated fetch query returns results in order from latest to earliest, the cursor for the next page can be found as the row with the minimum (earliest) value of the tuple (_xact_id, root_span_id)\n. See the documentation of limit\nfor an overview of paginating fetch queries.\nmax_root_span_id\nstringDEPRECATION NOTICE: The manually-constructed pagination cursor is deprecated in favor of the explicit 'cursor' returned by object fetch requests. Please prefer the 'cursor' argument going forwards.\nTogether, max_xact_id\nand max_root_span_id\nform a pagination cursor\nSince a paginated fetch query returns results in order from latest to earliest, the cursor for the next page can be found as the row with the minimum (earliest) value of the tuple (_xact_id, root_span_id)\n. See the documentation of limit\nfor an overview of paginating fetch queries.\nversion\nstringRetrieve a snapshot of events from a past time\nThe version id is essentially a filter on the latest event transaction id. You can use the max_xact_id\nreturned by a past fetch as the version to reproduce that exact fetch.\nReturns the fetched rows\nFetch experiment (POST form)\nFetch the events in an experiment. Equivalent to the GET form of the same path, but with the parameters in the request body rather than in the URL query. For more complex queries, use the POST /btql\nendpoint.\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nRequest Body\napplication/json\nOptionalFilters for the fetch query\nlimit\nintegerlimit the number of traces fetched\nFetch queries may be paginated if the total result size is expected to be large (e.g. project_logs which accumulate over a long time). Note that fetch queries only support pagination in descending time order (from latest to earliest _xact_id\n. Furthermore, later pages may return rows which showed up in earlier pages, except with an earlier _xact_id\n. This happens because pagination occurs over the whole version history of the event log. You will most likely want to exclude any such duplicate, outdated rows (by id\n) from your combined result set.\nThe limit\nparameter controls the number of full traces to return. So you may end up with more individual rows than the specified limit if you are fetching events containing traces.\n0\ncursor\nstringAn opaque string to be used as a cursor for the next page of results, in order from latest to earliest.\nThe string can be obtained directly from the cursor\nproperty of the previous fetch query\nmax_xact_id\nstringDEPRECATION NOTICE: The manually-constructed pagination cursor is deprecated in favor of the explicit 'cursor' returned by object fetch requests. Please prefer the 'cursor' argument going forwards.\nTogether, max_xact_id\nand max_root_span_id\nform a pagination cursor\nSince a paginated fetch query returns results in order from latest to earliest, the cursor for the next page can be found as the row with the minimum (earliest) value of the tuple (_xact_id, root_span_id)\n. See the documentation of limit\nfor an overview of paginating fetch queries.\nmax_root_span_id\nstringDEPRECATION NOTICE: The manually-constructed pagination cursor is deprecated in favor of the explicit 'cursor' returned by object fetch requests. Please prefer the 'cursor' argument going forwards.\nTogether, max_xact_id\nand max_root_span_id\nform a pagination cursor\nSince a paginated fetch query returns results in order from latest to earliest, the cursor for the next page can be found as the row with the minimum (earliest) value of the tuple (_xact_id, root_span_id)\n. See the documentation of limit\nfor an overview of paginating fetch queries.\nversion\nstringRetrieve a snapshot of events from a past time\nThe version id is essentially a filter on the latest event transaction id. You can use the max_xact_id\nreturned by a past fetch as the version to reproduce that exact fetch.\nPath Parameters\nexperiment_id\nExperiment id\n\"uuid\"\nReturns the fetched rows\nFeedback for experiment events\nLog feedback for a set of experiment events\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nRequest Body\napplication/json\nOptionalAn array of feedback objects\nfeedback\nA list of experiment feedback items\nPath Parameters\nexperiment_id\nExperiment id\n\"uuid\"\nReturns a success status\nSummarize experiment\nSummarize experiment\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nPath Parameters\nexperiment_id\nExperiment id\n\"uuid\"\nQuery Parameters\nsummarize_scores\nbooleanWhether to summarize the scores and metrics. If false (or omitted), only the metadata will be returned.\ncomparison_experiment_id\nstringThe experiment to compare against, if summarizing scores and metrics. If omitted, will fall back to the base_exp_id\nstored in the experiment metadata, and then to the most recent experiment run in the same project. Must pass summarize_scores=true\nfor this id to be used\n\"uuid\"\nExperiment summary", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/reference/api", "anchor": "API"}, {"href": "https://www.braintrust.dev/docs/reference/api/Acls", "anchor": "Acls"}, {"href": "https://www.braintrust.dev/docs/reference/api/AiSecrets", "anchor": "Ai Secrets"}, {"href": "https://www.braintrust.dev/docs/reference/api/ApiKeys", "anchor": "Api Keys"}, {"href": "https://www.braintrust.dev/docs/reference/api/CrossObject", "anchor": "Cross Object"}, {"href": "https://www.braintrust.dev/docs/reference/api/Datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/reference/api/EnvVars", "anchor": "Env Vars"}, {"href": "https://www.braintrust.dev/docs/reference/api/Evals", "anchor": "Evals"}, {"href": "https://www.braintrust.dev/docs/reference/api/Experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/reference/api/Functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/reference/api/Groups", "anchor": "Groups"}, {"href": "https://www.braintrust.dev/docs/reference/api/Logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/reference/api/Organizations", "anchor": "Organizations"}, {"href": "https://www.braintrust.dev/docs/reference/api/Other", "anchor": "Other"}, {"href": "https://www.braintrust.dev/docs/reference/api/Projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectScores", "anchor": "Project Scores"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectTags", "anchor": "Project Tags"}, {"href": "https://www.braintrust.dev/docs/reference/api/Prompts", "anchor": "Prompts"}, {"href": "https://www.braintrust.dev/docs/reference/api/Proxy", "anchor": "Proxy"}, {"href": "https://www.braintrust.dev/docs/reference/api/Roles", "anchor": "Roles"}, {"href": "https://www.braintrust.dev/docs/reference/api/ServiceTokens", "anchor": "Service tokens"}, {"href": "https://www.braintrust.dev/docs/reference/api/SpanIframes", "anchor": "Span Iframes"}, {"href": "https://www.braintrust.dev/docs/reference/api/Users", "anchor": "Users"}, {"href": "https://www.braintrust.dev/docs/reference/api/Views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/reference/autoevals", "anchor": "Autoevals library"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "BTQL query syntax"}, {"href": "https://www.braintrust.dev/docs/reference/streaming", "anchor": "Streaming"}, {"href": "https://www.braintrust.dev/docs/reference/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/reference/reasoning", "anchor": "Reasoning"}, {"href": "https://www.braintrust.dev/docs/reference/organizations", "anchor": "Organizations"}, {"href": "https://www.braintrust.dev/docs/reference/mcp", "anchor": "Model Context Protocol (MCP)"}, {"href": "https://www.braintrust.dev/docs/reference/object-links", "anchor": "Sharing via URL"}, {"href": "https://www.braintrust.dev/docs/reference/glossary", "anchor": "Glossary"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/reference/api", "anchor": "API"}, {"href": "https://www.braintrust.dev/docs/reference/api/Experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/reference/api/Experiments", "anchor": "List experiments"}, {"href": "https://www.braintrust.dev/docs/reference/api/Experiments", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/Experiments", "anchor": "Query Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/api/Experiments", "anchor": "Create experiment"}, {"href": "https://www.braintrust.dev/docs/reference/api/Experiments", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/Experiments", "anchor": "Request Body"}, {"href": "https://www.braintrust.dev/docs/reference/api/Experiments", "anchor": "Get experiment"}, {"href": "https://www.braintrust.dev/docs/reference/api/Experiments", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/Experiments", "anchor": "Path Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/api/Experiments", "anchor": "Partially update experiment"}, {"href": "https://www.braintrust.dev/docs/reference/api/Experiments", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/Experiments", "anchor": "Request Body"}, {"href": "https://www.braintrust.dev/docs/reference/api/Experiments", "anchor": "Path Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/api/Experiments", "anchor": "Delete experiment"}, {"href": "https://www.braintrust.dev/docs/reference/api/Experiments", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/Experiments", "anchor": "Path Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/api/Experiments", "anchor": "Insert experiment events"}, {"href": "https://www.braintrust.dev/docs/reference/api/Experiments", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/Experiments", "anchor": "Request Body"}, {"href": "https://www.braintrust.dev/docs/reference/api/Experiments", "anchor": "Path Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/api/Experiments", "anchor": "Fetch experiment (GET form)"}, {"href": "https://www.braintrust.dev/docs/reference/api/Experiments", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/Experiments", "anchor": "Path Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/api/Experiments", "anchor": "Query Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/api/Experiments", "anchor": "Fetch experiment (POST form)"}, {"href": "https://www.braintrust.dev/docs/reference/api/Experiments", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/Experiments", "anchor": "Request Body"}, {"href": "https://www.braintrust.dev/docs/reference/api/Experiments", "anchor": "Path Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/api/Experiments", "anchor": "Feedback for experiment events"}, {"href": "https://www.braintrust.dev/docs/reference/api/Experiments", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/Experiments", "anchor": "Request Body"}, {"href": "https://www.braintrust.dev/docs/reference/api/Experiments", "anchor": "Path Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/api/Experiments", "anchor": "Summarize experiment"}, {"href": "https://www.braintrust.dev/docs/reference/api/Experiments", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/Experiments", "anchor": "Path Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/api/Experiments", "anchor": "Query Parameters"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}, {"href": "https://www.braintrust.dev/docs/reference/api/Experiments", "anchor": "List experiments"}, {"href": "https://www.braintrust.dev/docs/reference/api/Experiments", "anchor": "Create experiment"}, {"href": "https://www.braintrust.dev/docs/reference/api/Experiments", "anchor": "Get experiment"}, {"href": "https://www.braintrust.dev/docs/reference/api/Experiments", "anchor": "Partially update experiment"}, {"href": "https://www.braintrust.dev/docs/reference/api/Experiments", "anchor": "Delete experiment"}, {"href": "https://www.braintrust.dev/docs/reference/api/Experiments", "anchor": "Insert experiment events"}, {"href": "https://www.braintrust.dev/docs/reference/api/Experiments", "anchor": "Fetch experiment (GET form)"}, {"href": "https://www.braintrust.dev/docs/reference/api/Experiments", "anchor": "Fetch experiment (POST form)"}, {"href": "https://www.braintrust.dev/docs/reference/api/Experiments", "anchor": "Feedback for experiment events"}, {"href": "https://www.braintrust.dev/docs/reference/api/Experiments", "anchor": "Summarize experiment"}], "depth": 4}, "https://www.braintrust.dev/docs/reference/api/Groups": {"url": "https://www.braintrust.dev/docs/reference/api/Groups", "title": "Groups - Docs - Reference - Braintrust", "text": "List groups\nList out all groups. The groups are sorted by creation date, with the most recently-created groups coming first\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nQuery Parameters\nlimit\nintegerLimit the number of objects to return\n0\nstarting_after\nstringPagination cursor id.\nFor example, if the final item in the last page you fetched had an id of foo\n, pass starting_after=foo\nto fetch the next page. Note: you may only pass one of starting_after\nand ending_before\n\"uuid\"\nending_before\nstringPagination cursor id.\nFor example, if the initial item in the last page you fetched had an id of foo\n, pass ending_before=foo\nto fetch the previous page. Note: you may only pass one of starting_after\nand ending_before\n\"uuid\"\nids\nAny properties in string, array<string>Filter search results to a particular set of object IDs. To specify a list of IDs, include the query param multiple times\ngroup_name\nstringName of the group to search for\norg_name\nstringFilter search results to within a particular organization\nReturns a list of group objects\nCreate group\nCreate a new group. If there is an existing group with the same name as the one specified in the request, will return the existing group unmodified\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nRequest Body\napplication/json\nOptionalAny desired information about the new group object\nname\nName of the group\n1\ndescription\nstringTextual description of the group\nmember_users\narray<string>Ids of users which belong to this group\nmember_groups\narray<string>Ids of the groups this group inherits from\nAn inheriting group has all the users contained in its member groups, as well as all of their inherited users\norg_name\nstringFor nearly all users, this parameter should be unnecessary. But in the rare case that your API key belongs to multiple organizations, you may specify the name of the organization the group belongs in.\nReturns the new group object\nCreate or replace group\nCreate or replace group. If there is an existing group with the same name as the one specified in the request, will replace the existing group with the provided fields\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nRequest Body\napplication/json\nOptionalAny desired information about the new group object\nname\nName of the group\n1\ndescription\nstringTextual description of the group\nmember_users\narray<string>Ids of users which belong to this group\nmember_groups\narray<string>Ids of the groups this group inherits from\nAn inheriting group has all the users contained in its member groups, as well as all of their inherited users\norg_name\nstringFor nearly all users, this parameter should be unnecessary. But in the rare case that your API key belongs to multiple organizations, you may specify the name of the organization the group belongs in.\nReturns the new group object\nGet group\nGet a group object by its id\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nPath Parameters\ngroup_id\nGroup id\n\"uuid\"\nReturns the group object\nPartially update group\nPartially update a group object. Specify the fields to update in the payload. Any object-type fields will be deep-merged with existing content. Currently we do not support removing fields or setting them to null.\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nRequest Body\napplication/json\nOptionalFields to update\ndescription\nstringTextual description of the group\nname\nstringName of the group\n1\nadd_member_users\narray<string>A list of user IDs to add to the group\nremove_member_users\narray<string>A list of user IDs to remove from the group\nadd_member_groups\narray<string>A list of group IDs to add to the group's inheriting-from set\nremove_member_groups\narray<string>A list of group IDs to remove from the group's inheriting-from set\nPath Parameters\ngroup_id\nGroup id\n\"uuid\"\nReturns the group object\nDelete group\nDelete a group object by its id\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nPath Parameters\ngroup_id\nGroup id\n\"uuid\"\nReturns the deleted group object", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/reference/api", "anchor": "API"}, {"href": "https://www.braintrust.dev/docs/reference/api/Acls", "anchor": "Acls"}, {"href": "https://www.braintrust.dev/docs/reference/api/AiSecrets", "anchor": "Ai Secrets"}, {"href": "https://www.braintrust.dev/docs/reference/api/ApiKeys", "anchor": "Api Keys"}, {"href": "https://www.braintrust.dev/docs/reference/api/CrossObject", "anchor": "Cross Object"}, {"href": "https://www.braintrust.dev/docs/reference/api/Datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/reference/api/EnvVars", "anchor": "Env Vars"}, {"href": "https://www.braintrust.dev/docs/reference/api/Evals", "anchor": "Evals"}, {"href": "https://www.braintrust.dev/docs/reference/api/Experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/reference/api/Functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/reference/api/Groups", "anchor": "Groups"}, {"href": "https://www.braintrust.dev/docs/reference/api/Logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/reference/api/Organizations", "anchor": "Organizations"}, {"href": "https://www.braintrust.dev/docs/reference/api/Other", "anchor": "Other"}, {"href": "https://www.braintrust.dev/docs/reference/api/Projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectScores", "anchor": "Project Scores"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectTags", "anchor": "Project Tags"}, {"href": "https://www.braintrust.dev/docs/reference/api/Prompts", "anchor": "Prompts"}, {"href": "https://www.braintrust.dev/docs/reference/api/Proxy", "anchor": "Proxy"}, {"href": "https://www.braintrust.dev/docs/reference/api/Roles", "anchor": "Roles"}, {"href": "https://www.braintrust.dev/docs/reference/api/ServiceTokens", "anchor": "Service tokens"}, {"href": "https://www.braintrust.dev/docs/reference/api/SpanIframes", "anchor": "Span Iframes"}, {"href": "https://www.braintrust.dev/docs/reference/api/Users", "anchor": "Users"}, {"href": "https://www.braintrust.dev/docs/reference/api/Views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/reference/autoevals", "anchor": "Autoevals library"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "BTQL query syntax"}, {"href": "https://www.braintrust.dev/docs/reference/streaming", "anchor": "Streaming"}, {"href": "https://www.braintrust.dev/docs/reference/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/reference/reasoning", "anchor": "Reasoning"}, {"href": "https://www.braintrust.dev/docs/reference/organizations", "anchor": "Organizations"}, {"href": "https://www.braintrust.dev/docs/reference/mcp", "anchor": "Model Context Protocol (MCP)"}, {"href": "https://www.braintrust.dev/docs/reference/object-links", "anchor": "Sharing via URL"}, {"href": "https://www.braintrust.dev/docs/reference/glossary", "anchor": "Glossary"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/reference/api", "anchor": "API"}, {"href": "https://www.braintrust.dev/docs/reference/api/Groups", "anchor": "Groups"}, {"href": "https://www.braintrust.dev/docs/reference/api/Groups", "anchor": "List groups"}, {"href": "https://www.braintrust.dev/docs/reference/api/Groups", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/Groups", "anchor": "Query Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/api/Groups", "anchor": "Create group"}, {"href": "https://www.braintrust.dev/docs/reference/api/Groups", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/Groups", "anchor": "Request Body"}, {"href": "https://www.braintrust.dev/docs/reference/api/Groups", "anchor": "Create or replace group"}, {"href": "https://www.braintrust.dev/docs/reference/api/Groups", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/Groups", "anchor": "Request Body"}, {"href": "https://www.braintrust.dev/docs/reference/api/Groups", "anchor": "Get group"}, {"href": "https://www.braintrust.dev/docs/reference/api/Groups", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/Groups", "anchor": "Path Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/api/Groups", "anchor": "Partially update group"}, {"href": "https://www.braintrust.dev/docs/reference/api/Groups", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/Groups", "anchor": "Request Body"}, {"href": "https://www.braintrust.dev/docs/reference/api/Groups", "anchor": "Path Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/api/Groups", "anchor": "Delete group"}, {"href": "https://www.braintrust.dev/docs/reference/api/Groups", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/Groups", "anchor": "Path Parameters"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}, {"href": "https://www.braintrust.dev/docs/reference/api/Groups", "anchor": "List groups"}, {"href": "https://www.braintrust.dev/docs/reference/api/Groups", "anchor": "Create group"}, {"href": "https://www.braintrust.dev/docs/reference/api/Groups", "anchor": "Create or replace group"}, {"href": "https://www.braintrust.dev/docs/reference/api/Groups", "anchor": "Get group"}, {"href": "https://www.braintrust.dev/docs/reference/api/Groups", "anchor": "Partially update group"}, {"href": "https://www.braintrust.dev/docs/reference/api/Groups", "anchor": "Delete group"}], "depth": 4}, "https://www.braintrust.dev/docs/reference/api/Logs": {"url": "https://www.braintrust.dev/docs/reference/api/Logs", "title": "Logs - Docs - Reference - Braintrust", "text": "Insert project logs events\nInsert a set of events into the project logs\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nRequest Body\napplication/json\nOptionalAn array of project logs events to insert\nevents\nA list of project logs events to insert\nPath Parameters\nproject_id\nProject id\n\"uuid\"\nReturns the inserted row ids\nFetch project logs (GET form)\nFetch the events in a project logs. Equivalent to the POST form of the same path, but with the parameters in the URL query rather than in the request body. For more complex queries, use the POST /btql\nendpoint.\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nPath Parameters\nproject_id\nProject id\n\"uuid\"\nQuery Parameters\nlimit\nintegerlimit the number of traces fetched\nFetch queries may be paginated if the total result size is expected to be large (e.g. project_logs which accumulate over a long time). Note that fetch queries only support pagination in descending time order (from latest to earliest _xact_id\n. Furthermore, later pages may return rows which showed up in earlier pages, except with an earlier _xact_id\n. This happens because pagination occurs over the whole version history of the event log. You will most likely want to exclude any such duplicate, outdated rows (by id\n) from your combined result set.\nThe limit\nparameter controls the number of full traces to return. So you may end up with more individual rows than the specified limit if you are fetching events containing traces.\n0\nmax_xact_id\nstringDEPRECATION NOTICE: The manually-constructed pagination cursor is deprecated in favor of the explicit 'cursor' returned by object fetch requests. Please prefer the 'cursor' argument going forwards.\nTogether, max_xact_id\nand max_root_span_id\nform a pagination cursor\nSince a paginated fetch query returns results in order from latest to earliest, the cursor for the next page can be found as the row with the minimum (earliest) value of the tuple (_xact_id, root_span_id)\n. See the documentation of limit\nfor an overview of paginating fetch queries.\nmax_root_span_id\nstringDEPRECATION NOTICE: The manually-constructed pagination cursor is deprecated in favor of the explicit 'cursor' returned by object fetch requests. Please prefer the 'cursor' argument going forwards.\nTogether, max_xact_id\nand max_root_span_id\nform a pagination cursor\nSince a paginated fetch query returns results in order from latest to earliest, the cursor for the next page can be found as the row with the minimum (earliest) value of the tuple (_xact_id, root_span_id)\n. See the documentation of limit\nfor an overview of paginating fetch queries.\nversion\nstringRetrieve a snapshot of events from a past time\nThe version id is essentially a filter on the latest event transaction id. You can use the max_xact_id\nreturned by a past fetch as the version to reproduce that exact fetch.\nReturns the fetched rows\nFetch project logs (POST form)\nFetch the events in a project logs. Equivalent to the GET form of the same path, but with the parameters in the request body rather than in the URL query. For more complex queries, use the POST /btql\nendpoint.\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nRequest Body\napplication/json\nOptionalFilters for the fetch query\nlimit\nintegerlimit the number of traces fetched\nFetch queries may be paginated if the total result size is expected to be large (e.g. project_logs which accumulate over a long time). Note that fetch queries only support pagination in descending time order (from latest to earliest _xact_id\n. Furthermore, later pages may return rows which showed up in earlier pages, except with an earlier _xact_id\n. This happens because pagination occurs over the whole version history of the event log. You will most likely want to exclude any such duplicate, outdated rows (by id\n) from your combined result set.\nThe limit\nparameter controls the number of full traces to return. So you may end up with more individual rows than the specified limit if you are fetching events containing traces.\n0\ncursor\nstringAn opaque string to be used as a cursor for the next page of results, in order from latest to earliest.\nThe string can be obtained directly from the cursor\nproperty of the previous fetch query\nmax_xact_id\nstringDEPRECATION NOTICE: The manually-constructed pagination cursor is deprecated in favor of the explicit 'cursor' returned by object fetch requests. Please prefer the 'cursor' argument going forwards.\nTogether, max_xact_id\nand max_root_span_id\nform a pagination cursor\nSince a paginated fetch query returns results in order from latest to earliest, the cursor for the next page can be found as the row with the minimum (earliest) value of the tuple (_xact_id, root_span_id)\n. See the documentation of limit\nfor an overview of paginating fetch queries.\nmax_root_span_id\nstringDEPRECATION NOTICE: The manually-constructed pagination cursor is deprecated in favor of the explicit 'cursor' returned by object fetch requests. Please prefer the 'cursor' argument going forwards.\nTogether, max_xact_id\nand max_root_span_id\nform a pagination cursor\nSince a paginated fetch query returns results in order from latest to earliest, the cursor for the next page can be found as the row with the minimum (earliest) value of the tuple (_xact_id, root_span_id)\n. See the documentation of limit\nfor an overview of paginating fetch queries.\nversion\nstringRetrieve a snapshot of events from a past time\nThe version id is essentially a filter on the latest event transaction id. You can use the max_xact_id\nreturned by a past fetch as the version to reproduce that exact fetch.\nPath Parameters\nproject_id\nProject id\n\"uuid\"\nReturns the fetched rows\nFeedback for project logs events\nLog feedback for a set of project logs events\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nRequest Body\napplication/json\nOptionalAn array of feedback objects\nfeedback\nA list of project logs feedback items\nPath Parameters\nproject_id\nProject id\n\"uuid\"\nReturns a success status", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/reference/api", "anchor": "API"}, {"href": "https://www.braintrust.dev/docs/reference/api/Acls", "anchor": "Acls"}, {"href": "https://www.braintrust.dev/docs/reference/api/AiSecrets", "anchor": "Ai Secrets"}, {"href": "https://www.braintrust.dev/docs/reference/api/ApiKeys", "anchor": "Api Keys"}, {"href": "https://www.braintrust.dev/docs/reference/api/CrossObject", "anchor": "Cross Object"}, {"href": "https://www.braintrust.dev/docs/reference/api/Datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/reference/api/EnvVars", "anchor": "Env Vars"}, {"href": "https://www.braintrust.dev/docs/reference/api/Evals", "anchor": "Evals"}, {"href": "https://www.braintrust.dev/docs/reference/api/Experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/reference/api/Functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/reference/api/Groups", "anchor": "Groups"}, {"href": "https://www.braintrust.dev/docs/reference/api/Logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/reference/api/Organizations", "anchor": "Organizations"}, {"href": "https://www.braintrust.dev/docs/reference/api/Other", "anchor": "Other"}, {"href": "https://www.braintrust.dev/docs/reference/api/Projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectScores", "anchor": "Project Scores"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectTags", "anchor": "Project Tags"}, {"href": "https://www.braintrust.dev/docs/reference/api/Prompts", "anchor": "Prompts"}, {"href": "https://www.braintrust.dev/docs/reference/api/Proxy", "anchor": "Proxy"}, {"href": "https://www.braintrust.dev/docs/reference/api/Roles", "anchor": "Roles"}, {"href": "https://www.braintrust.dev/docs/reference/api/ServiceTokens", "anchor": "Service tokens"}, {"href": "https://www.braintrust.dev/docs/reference/api/SpanIframes", "anchor": "Span Iframes"}, {"href": "https://www.braintrust.dev/docs/reference/api/Users", "anchor": "Users"}, {"href": "https://www.braintrust.dev/docs/reference/api/Views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/reference/autoevals", "anchor": "Autoevals library"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "BTQL query syntax"}, {"href": "https://www.braintrust.dev/docs/reference/streaming", "anchor": "Streaming"}, {"href": "https://www.braintrust.dev/docs/reference/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/reference/reasoning", "anchor": "Reasoning"}, {"href": "https://www.braintrust.dev/docs/reference/organizations", "anchor": "Organizations"}, {"href": "https://www.braintrust.dev/docs/reference/mcp", "anchor": "Model Context Protocol (MCP)"}, {"href": "https://www.braintrust.dev/docs/reference/object-links", "anchor": "Sharing via URL"}, {"href": "https://www.braintrust.dev/docs/reference/glossary", "anchor": "Glossary"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/reference/api", "anchor": "API"}, {"href": "https://www.braintrust.dev/docs/reference/api/Logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/reference/api/Logs", "anchor": "Insert project logs events"}, {"href": "https://www.braintrust.dev/docs/reference/api/Logs", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/Logs", "anchor": "Request Body"}, {"href": "https://www.braintrust.dev/docs/reference/api/Logs", "anchor": "Path Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/api/Logs", "anchor": "Fetch project logs (GET form)"}, {"href": "https://www.braintrust.dev/docs/reference/api/Logs", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/Logs", "anchor": "Path Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/api/Logs", "anchor": "Query Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/api/Logs", "anchor": "Fetch project logs (POST form)"}, {"href": "https://www.braintrust.dev/docs/reference/api/Logs", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/Logs", "anchor": "Request Body"}, {"href": "https://www.braintrust.dev/docs/reference/api/Logs", "anchor": "Path Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/api/Logs", "anchor": "Feedback for project logs events"}, {"href": "https://www.braintrust.dev/docs/reference/api/Logs", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/Logs", "anchor": "Request Body"}, {"href": "https://www.braintrust.dev/docs/reference/api/Logs", "anchor": "Path Parameters"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}, {"href": "https://www.braintrust.dev/docs/reference/api/Logs", "anchor": "Insert project logs events"}, {"href": "https://www.braintrust.dev/docs/reference/api/Logs", "anchor": "Fetch project logs (GET form)"}, {"href": "https://www.braintrust.dev/docs/reference/api/Logs", "anchor": "Fetch project logs (POST form)"}, {"href": "https://www.braintrust.dev/docs/reference/api/Logs", "anchor": "Feedback for project logs events"}], "depth": 4}, "https://www.braintrust.dev/docs/reference/api/Organizations": {"url": "https://www.braintrust.dev/docs/reference/api/Organizations", "title": "Organizations - Docs - Reference - Braintrust", "text": "List organizations\nList out all organizations. The organizations are sorted by creation date, with the most recently-created organizations coming first\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nQuery Parameters\nlimit\nintegerLimit the number of objects to return\n0\nstarting_after\nstringPagination cursor id.\nFor example, if the final item in the last page you fetched had an id of foo\n, pass starting_after=foo\nto fetch the next page. Note: you may only pass one of starting_after\nand ending_before\n\"uuid\"\nending_before\nstringPagination cursor id.\nFor example, if the initial item in the last page you fetched had an id of foo\n, pass ending_before=foo\nto fetch the previous page. Note: you may only pass one of starting_after\nand ending_before\n\"uuid\"\nids\nAny properties in string, array<string>Filter search results to a particular set of object IDs. To specify a list of IDs, include the query param multiple times\norg_name\nstringFilter search results to within a particular organization\nReturns a list of organization objects\nGet organization\nGet an organization object by its id\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nPath Parameters\norganization_id\nOrganization id\n\"uuid\"\nReturns the organization object\nPartially update organization\nPartially update an organization object. Specify the fields to update in the payload. Any object-type fields will be deep-merged with existing content. Currently we do not support removing fields or setting them to null.\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nRequest Body\napplication/json\nOptionalFields to update\nname\nstringName of the organization\napi_url\nstringis_universal_api\nbooleanproxy_url\nstringrealtime_url\nstringPath Parameters\norganization_id\nOrganization id\n\"uuid\"\nReturns the organization object\nModify organization membership\nModify organization membership\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nRequest Body\napplication/json\nOptionalMembers to add/remove\ninvite_users\nobjectUsers to invite to the organization\nremove_users\nobjectUsers to remove from the organization\norg_name\nstringFor nearly all users, this parameter should be unnecessary. But in the rare case that your API key belongs to multiple organizations, or in case you want to explicitly assert the organization you are modifying, you may specify the name of the organization.\norg_id\nstringFor nearly all users, this parameter should be unnecessary. But in the rare case that your API key belongs to multiple organizations, or in case you want to explicitly assert the organization you are modifying, you may specify the id of the organization.\nA success status", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/reference/api", "anchor": "API"}, {"href": "https://www.braintrust.dev/docs/reference/api/Acls", "anchor": "Acls"}, {"href": "https://www.braintrust.dev/docs/reference/api/AiSecrets", "anchor": "Ai Secrets"}, {"href": "https://www.braintrust.dev/docs/reference/api/ApiKeys", "anchor": "Api Keys"}, {"href": "https://www.braintrust.dev/docs/reference/api/CrossObject", "anchor": "Cross Object"}, {"href": "https://www.braintrust.dev/docs/reference/api/Datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/reference/api/EnvVars", "anchor": "Env Vars"}, {"href": "https://www.braintrust.dev/docs/reference/api/Evals", "anchor": "Evals"}, {"href": "https://www.braintrust.dev/docs/reference/api/Experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/reference/api/Functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/reference/api/Groups", "anchor": "Groups"}, {"href": "https://www.braintrust.dev/docs/reference/api/Logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/reference/api/Organizations", "anchor": "Organizations"}, {"href": "https://www.braintrust.dev/docs/reference/api/Other", "anchor": "Other"}, {"href": "https://www.braintrust.dev/docs/reference/api/Projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectScores", "anchor": "Project Scores"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectTags", "anchor": "Project Tags"}, {"href": "https://www.braintrust.dev/docs/reference/api/Prompts", "anchor": "Prompts"}, {"href": "https://www.braintrust.dev/docs/reference/api/Proxy", "anchor": "Proxy"}, {"href": "https://www.braintrust.dev/docs/reference/api/Roles", "anchor": "Roles"}, {"href": "https://www.braintrust.dev/docs/reference/api/ServiceTokens", "anchor": "Service tokens"}, {"href": "https://www.braintrust.dev/docs/reference/api/SpanIframes", "anchor": "Span Iframes"}, {"href": "https://www.braintrust.dev/docs/reference/api/Users", "anchor": "Users"}, {"href": "https://www.braintrust.dev/docs/reference/api/Views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/reference/autoevals", "anchor": "Autoevals library"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "BTQL query syntax"}, {"href": "https://www.braintrust.dev/docs/reference/streaming", "anchor": "Streaming"}, {"href": "https://www.braintrust.dev/docs/reference/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/reference/reasoning", "anchor": "Reasoning"}, {"href": "https://www.braintrust.dev/docs/reference/organizations", "anchor": "Organizations"}, {"href": "https://www.braintrust.dev/docs/reference/mcp", "anchor": "Model Context Protocol (MCP)"}, {"href": "https://www.braintrust.dev/docs/reference/object-links", "anchor": "Sharing via URL"}, {"href": "https://www.braintrust.dev/docs/reference/glossary", "anchor": "Glossary"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/reference/api", "anchor": "API"}, {"href": "https://www.braintrust.dev/docs/reference/api/Organizations", "anchor": "Organizations"}, {"href": "https://www.braintrust.dev/docs/reference/api/Organizations", "anchor": "List organizations"}, {"href": "https://www.braintrust.dev/docs/reference/api/Organizations", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/Organizations", "anchor": "Query Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/api/Organizations", "anchor": "Get organization"}, {"href": "https://www.braintrust.dev/docs/reference/api/Organizations", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/Organizations", "anchor": "Path Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/api/Organizations", "anchor": "Partially update organization"}, {"href": "https://www.braintrust.dev/docs/reference/api/Organizations", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/Organizations", "anchor": "Request Body"}, {"href": "https://www.braintrust.dev/docs/reference/api/Organizations", "anchor": "Path Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/api/Organizations", "anchor": "Modify organization membership"}, {"href": "https://www.braintrust.dev/docs/reference/api/Organizations", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/Organizations", "anchor": "Request Body"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}, {"href": "https://www.braintrust.dev/docs/reference/api/Organizations", "anchor": "List organizations"}, {"href": "https://www.braintrust.dev/docs/reference/api/Organizations", "anchor": "Get organization"}, {"href": "https://www.braintrust.dev/docs/reference/api/Organizations", "anchor": "Partially update organization"}, {"href": "https://www.braintrust.dev/docs/reference/api/Organizations", "anchor": "Modify organization membership"}], "depth": 4}, "https://www.braintrust.dev/docs/reference/api/Other": {"url": "https://www.braintrust.dev/docs/reference/api/Other", "title": "Other - Docs - Reference - Braintrust", "text": "Hello world endpoint\nDefault endpoint. Simply replies with 'Hello, World!'. Authorization is not required\nGET /v1\nAuthorization\nHello world string\nDefault endpoint. Simply replies with 'Hello, World!'. Authorization is not required\nHello world string", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/reference/api", "anchor": "API"}, {"href": "https://www.braintrust.dev/docs/reference/api/Acls", "anchor": "Acls"}, {"href": "https://www.braintrust.dev/docs/reference/api/AiSecrets", "anchor": "Ai Secrets"}, {"href": "https://www.braintrust.dev/docs/reference/api/ApiKeys", "anchor": "Api Keys"}, {"href": "https://www.braintrust.dev/docs/reference/api/CrossObject", "anchor": "Cross Object"}, {"href": "https://www.braintrust.dev/docs/reference/api/Datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/reference/api/EnvVars", "anchor": "Env Vars"}, {"href": "https://www.braintrust.dev/docs/reference/api/Evals", "anchor": "Evals"}, {"href": "https://www.braintrust.dev/docs/reference/api/Experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/reference/api/Functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/reference/api/Groups", "anchor": "Groups"}, {"href": "https://www.braintrust.dev/docs/reference/api/Logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/reference/api/Organizations", "anchor": "Organizations"}, {"href": "https://www.braintrust.dev/docs/reference/api/Other", "anchor": "Other"}, {"href": "https://www.braintrust.dev/docs/reference/api/Projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectScores", "anchor": "Project Scores"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectTags", "anchor": "Project Tags"}, {"href": "https://www.braintrust.dev/docs/reference/api/Prompts", "anchor": "Prompts"}, {"href": "https://www.braintrust.dev/docs/reference/api/Proxy", "anchor": "Proxy"}, {"href": "https://www.braintrust.dev/docs/reference/api/Roles", "anchor": "Roles"}, {"href": "https://www.braintrust.dev/docs/reference/api/ServiceTokens", "anchor": "Service tokens"}, {"href": "https://www.braintrust.dev/docs/reference/api/SpanIframes", "anchor": "Span Iframes"}, {"href": "https://www.braintrust.dev/docs/reference/api/Users", "anchor": "Users"}, {"href": "https://www.braintrust.dev/docs/reference/api/Views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/reference/autoevals", "anchor": "Autoevals library"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "BTQL query syntax"}, {"href": "https://www.braintrust.dev/docs/reference/streaming", "anchor": "Streaming"}, {"href": "https://www.braintrust.dev/docs/reference/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/reference/reasoning", "anchor": "Reasoning"}, {"href": "https://www.braintrust.dev/docs/reference/organizations", "anchor": "Organizations"}, {"href": "https://www.braintrust.dev/docs/reference/mcp", "anchor": "Model Context Protocol (MCP)"}, {"href": "https://www.braintrust.dev/docs/reference/object-links", "anchor": "Sharing via URL"}, {"href": "https://www.braintrust.dev/docs/reference/glossary", "anchor": "Glossary"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/reference/api", "anchor": "API"}, {"href": "https://www.braintrust.dev/docs/reference/api/Other", "anchor": "Other"}, {"href": "https://www.braintrust.dev/docs/reference/api/Other", "anchor": "Hello world endpoint"}, {"href": "https://www.braintrust.dev/docs/reference/api/Other", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}, {"href": "https://www.braintrust.dev/docs/reference/api/Other", "anchor": "Hello world endpoint"}], "depth": 4}, "https://www.braintrust.dev/docs/reference/api/Projects": {"url": "https://www.braintrust.dev/docs/reference/api/Projects", "title": "Projects - Docs - Reference - Braintrust", "text": "List projects\nList out all projects. The projects are sorted by creation date, with the most recently-created projects coming first\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nQuery Parameters\nlimit\nintegerLimit the number of objects to return\n0\nstarting_after\nstringPagination cursor id.\nFor example, if the final item in the last page you fetched had an id of foo\n, pass starting_after=foo\nto fetch the next page. Note: you may only pass one of starting_after\nand ending_before\n\"uuid\"\nending_before\nstringPagination cursor id.\nFor example, if the initial item in the last page you fetched had an id of foo\n, pass ending_before=foo\nto fetch the previous page. Note: you may only pass one of starting_after\nand ending_before\n\"uuid\"\nids\nAny properties in string, array<string>Filter search results to a particular set of object IDs. To specify a list of IDs, include the query param multiple times\nproject_name\nstringName of the project to search for\norg_name\nstringFilter search results to within a particular organization\nReturns a list of project objects\nCreate project\nCreate a new project. If there is an existing project with the same name as the one specified in the request, will return the existing project unmodified\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nRequest Body\napplication/json\nOptionalAny desired information about the new project object\nname\nName of the project\n1\norg_name\nstringFor nearly all users, this parameter should be unnecessary. But in the rare case that your API key belongs to multiple organizations, you may specify the name of the organization the project belongs in.\nReturns the new project object\nGet project\nGet a project object by its id\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nPath Parameters\nproject_id\nProject id\n\"uuid\"\nReturns the project object\nPartially update project\nPartially update a project object. Specify the fields to update in the payload. Any object-type fields will be deep-merged with existing content. Currently we do not support removing fields or setting them to null.\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nRequest Body\napplication/json\nOptionalFields to update\nname\nstringName of the project\nsettings\nobjectPath Parameters\nproject_id\nProject id\n\"uuid\"\nReturns the project object\nDelete project\nDelete a project object by its id\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nPath Parameters\nproject_id\nProject id\n\"uuid\"\nReturns the deleted project object", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/reference/api", "anchor": "API"}, {"href": "https://www.braintrust.dev/docs/reference/api/Acls", "anchor": "Acls"}, {"href": "https://www.braintrust.dev/docs/reference/api/AiSecrets", "anchor": "Ai Secrets"}, {"href": "https://www.braintrust.dev/docs/reference/api/ApiKeys", "anchor": "Api Keys"}, {"href": "https://www.braintrust.dev/docs/reference/api/CrossObject", "anchor": "Cross Object"}, {"href": "https://www.braintrust.dev/docs/reference/api/Datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/reference/api/EnvVars", "anchor": "Env Vars"}, {"href": "https://www.braintrust.dev/docs/reference/api/Evals", "anchor": "Evals"}, {"href": "https://www.braintrust.dev/docs/reference/api/Experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/reference/api/Functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/reference/api/Groups", "anchor": "Groups"}, {"href": "https://www.braintrust.dev/docs/reference/api/Logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/reference/api/Organizations", "anchor": "Organizations"}, {"href": "https://www.braintrust.dev/docs/reference/api/Other", "anchor": "Other"}, {"href": "https://www.braintrust.dev/docs/reference/api/Projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectScores", "anchor": "Project Scores"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectTags", "anchor": "Project Tags"}, {"href": "https://www.braintrust.dev/docs/reference/api/Prompts", "anchor": "Prompts"}, {"href": "https://www.braintrust.dev/docs/reference/api/Proxy", "anchor": "Proxy"}, {"href": "https://www.braintrust.dev/docs/reference/api/Roles", "anchor": "Roles"}, {"href": "https://www.braintrust.dev/docs/reference/api/ServiceTokens", "anchor": "Service tokens"}, {"href": "https://www.braintrust.dev/docs/reference/api/SpanIframes", "anchor": "Span Iframes"}, {"href": "https://www.braintrust.dev/docs/reference/api/Users", "anchor": "Users"}, {"href": "https://www.braintrust.dev/docs/reference/api/Views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/reference/autoevals", "anchor": "Autoevals library"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "BTQL query syntax"}, {"href": "https://www.braintrust.dev/docs/reference/streaming", "anchor": "Streaming"}, {"href": "https://www.braintrust.dev/docs/reference/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/reference/reasoning", "anchor": "Reasoning"}, {"href": "https://www.braintrust.dev/docs/reference/organizations", "anchor": "Organizations"}, {"href": "https://www.braintrust.dev/docs/reference/mcp", "anchor": "Model Context Protocol (MCP)"}, {"href": "https://www.braintrust.dev/docs/reference/object-links", "anchor": "Sharing via URL"}, {"href": "https://www.braintrust.dev/docs/reference/glossary", "anchor": "Glossary"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/reference/api", "anchor": "API"}, {"href": "https://www.braintrust.dev/docs/reference/api/Projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/reference/api/Projects", "anchor": "List projects"}, {"href": "https://www.braintrust.dev/docs/reference/api/Projects", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/Projects", "anchor": "Query Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/api/Projects", "anchor": "Create project"}, {"href": "https://www.braintrust.dev/docs/reference/api/Projects", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/Projects", "anchor": "Request Body"}, {"href": "https://www.braintrust.dev/docs/reference/api/Projects", "anchor": "Get project"}, {"href": "https://www.braintrust.dev/docs/reference/api/Projects", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/Projects", "anchor": "Path Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/api/Projects", "anchor": "Partially update project"}, {"href": "https://www.braintrust.dev/docs/reference/api/Projects", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/Projects", "anchor": "Request Body"}, {"href": "https://www.braintrust.dev/docs/reference/api/Projects", "anchor": "Path Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/api/Projects", "anchor": "Delete project"}, {"href": "https://www.braintrust.dev/docs/reference/api/Projects", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/Projects", "anchor": "Path Parameters"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}, {"href": "https://www.braintrust.dev/docs/reference/api/Projects", "anchor": "List projects"}, {"href": "https://www.braintrust.dev/docs/reference/api/Projects", "anchor": "Create project"}, {"href": "https://www.braintrust.dev/docs/reference/api/Projects", "anchor": "Get project"}, {"href": "https://www.braintrust.dev/docs/reference/api/Projects", "anchor": "Partially update project"}, {"href": "https://www.braintrust.dev/docs/reference/api/Projects", "anchor": "Delete project"}], "depth": 4}, "https://www.braintrust.dev/docs/reference/api/ProjectScores": {"url": "https://www.braintrust.dev/docs/reference/api/ProjectScores", "title": "Project Scores - Docs - Reference - Braintrust", "text": "List project_scores\nList out all project_scores. The project_scores are sorted by creation date, with the most recently-created project_scores coming first\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nQuery Parameters\nlimit\nintegerLimit the number of objects to return\n0\nstarting_after\nstringPagination cursor id.\nFor example, if the final item in the last page you fetched had an id of foo\n, pass starting_after=foo\nto fetch the next page. Note: you may only pass one of starting_after\nand ending_before\n\"uuid\"\nending_before\nstringPagination cursor id.\nFor example, if the initial item in the last page you fetched had an id of foo\n, pass ending_before=foo\nto fetch the previous page. Note: you may only pass one of starting_after\nand ending_before\n\"uuid\"\nids\nAny properties in string, array<string>Filter search results to a particular set of object IDs. To specify a list of IDs, include the query param multiple times\nproject_score_name\nstringName of the project_score to search for\nproject_name\nstringName of the project to search for\nproject_id\nstringProject id\n\"uuid\"\norg_name\nstringFilter search results to within a particular organization\nscore_type\nAny properties in project_score_type_single, array<project_score_type_single & project_score_type>Returns a list of project_score objects\nCreate project_score\nCreate a new project_score. If there is an existing project_score in the project with the same name as the one specified in the request, will return the existing project_score unmodified\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nRequest Body\napplication/json\nOptionalAny desired information about the new project_score object\nproject_id\nUnique identifier for the project that the project score belongs under\n\"uuid\"\nname\nName of the project score\ndescription\nstringTextual description of the project score\nscore_type\nThe type of the configured score\n\"slider\" | \"categorical\" | \"weighted\" | \"minimum\" | \"maximum\" | \"online\" | \"free-form\"\ncategories\nAny properties in categorical, weighted, minimumconfig\nobjectReturns the new project_score object\nCreate or replace project_score\nCreate or replace project_score. If there is an existing project_score in the project with the same name as the one specified in the request, will replace the existing project_score with the provided fields\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nRequest Body\napplication/json\nOptionalAny desired information about the new project_score object\nproject_id\nUnique identifier for the project that the project score belongs under\n\"uuid\"\nname\nName of the project score\ndescription\nstringTextual description of the project score\nscore_type\nThe type of the configured score\n\"slider\" | \"categorical\" | \"weighted\" | \"minimum\" | \"maximum\" | \"online\" | \"free-form\"\ncategories\nAny properties in categorical, weighted, minimumconfig\nobjectReturns the new project_score object\nGet project_score\nGet a project_score object by its id\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nPath Parameters\nproject_score_id\nProjectScore id\n\"uuid\"\nReturns the project_score object\nPartially update project_score\nPartially update a project_score object. Specify the fields to update in the payload. Any object-type fields will be deep-merged with existing content. Currently we do not support removing fields or setting them to null.\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nRequest Body\napplication/json\nOptionalFields to update\nname\nstringName of the project score\ndescription\nstringTextual description of the project score\nscore_type\nproject_score_type_singleThe type of the configured score\n\"slider\" | \"categorical\" | \"weighted\" | \"minimum\" | \"maximum\" | \"online\" | \"free-form\"\ncategories\nAny properties in categorical, weighted, minimumconfig\nobjectPath Parameters\nproject_score_id\nProjectScore id\n\"uuid\"\nReturns the project_score object\nDelete project_score\nDelete a project_score object by its id\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nPath Parameters\nproject_score_id\nProjectScore id\n\"uuid\"\nReturns the deleted project_score object", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/reference/api", "anchor": "API"}, {"href": "https://www.braintrust.dev/docs/reference/api/Acls", "anchor": "Acls"}, {"href": "https://www.braintrust.dev/docs/reference/api/AiSecrets", "anchor": "Ai Secrets"}, {"href": "https://www.braintrust.dev/docs/reference/api/ApiKeys", "anchor": "Api Keys"}, {"href": "https://www.braintrust.dev/docs/reference/api/CrossObject", "anchor": "Cross Object"}, {"href": "https://www.braintrust.dev/docs/reference/api/Datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/reference/api/EnvVars", "anchor": "Env Vars"}, {"href": "https://www.braintrust.dev/docs/reference/api/Evals", "anchor": "Evals"}, {"href": "https://www.braintrust.dev/docs/reference/api/Experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/reference/api/Functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/reference/api/Groups", "anchor": "Groups"}, {"href": "https://www.braintrust.dev/docs/reference/api/Logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/reference/api/Organizations", "anchor": "Organizations"}, {"href": "https://www.braintrust.dev/docs/reference/api/Other", "anchor": "Other"}, {"href": "https://www.braintrust.dev/docs/reference/api/Projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectScores", "anchor": "Project Scores"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectTags", "anchor": "Project Tags"}, {"href": "https://www.braintrust.dev/docs/reference/api/Prompts", "anchor": "Prompts"}, {"href": "https://www.braintrust.dev/docs/reference/api/Proxy", "anchor": "Proxy"}, {"href": "https://www.braintrust.dev/docs/reference/api/Roles", "anchor": "Roles"}, {"href": "https://www.braintrust.dev/docs/reference/api/ServiceTokens", "anchor": "Service tokens"}, {"href": "https://www.braintrust.dev/docs/reference/api/SpanIframes", "anchor": "Span Iframes"}, {"href": "https://www.braintrust.dev/docs/reference/api/Users", "anchor": "Users"}, {"href": "https://www.braintrust.dev/docs/reference/api/Views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/reference/autoevals", "anchor": "Autoevals library"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "BTQL query syntax"}, {"href": "https://www.braintrust.dev/docs/reference/streaming", "anchor": "Streaming"}, {"href": "https://www.braintrust.dev/docs/reference/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/reference/reasoning", "anchor": "Reasoning"}, {"href": "https://www.braintrust.dev/docs/reference/organizations", "anchor": "Organizations"}, {"href": "https://www.braintrust.dev/docs/reference/mcp", "anchor": "Model Context Protocol (MCP)"}, {"href": "https://www.braintrust.dev/docs/reference/object-links", "anchor": "Sharing via URL"}, {"href": "https://www.braintrust.dev/docs/reference/glossary", "anchor": "Glossary"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/reference/api", "anchor": "API"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectScores", "anchor": "Project Scores"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectScores", "anchor": "List project_scores"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectScores", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectScores", "anchor": "Query Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectScores", "anchor": "Create project_score"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectScores", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectScores", "anchor": "Request Body"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectScores", "anchor": "Create or replace project_score"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectScores", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectScores", "anchor": "Request Body"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectScores", "anchor": "Get project_score"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectScores", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectScores", "anchor": "Path Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectScores", "anchor": "Partially update project_score"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectScores", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectScores", "anchor": "Request Body"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectScores", "anchor": "Path Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectScores", "anchor": "Delete project_score"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectScores", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectScores", "anchor": "Path Parameters"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectScores", "anchor": "List project_scores"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectScores", "anchor": "Create project_score"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectScores", "anchor": "Create or replace project_score"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectScores", "anchor": "Get project_score"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectScores", "anchor": "Partially update project_score"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectScores", "anchor": "Delete project_score"}], "depth": 4}, "https://www.braintrust.dev/docs/reference/api/ProjectTags": {"url": "https://www.braintrust.dev/docs/reference/api/ProjectTags", "title": "Project Tags - Docs - Reference - Braintrust", "text": "List project_tags\nList out all project_tags. The project_tags are sorted by creation date, with the most recently-created project_tags coming first\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nQuery Parameters\nlimit\nintegerLimit the number of objects to return\n0\nstarting_after\nstringPagination cursor id.\nFor example, if the final item in the last page you fetched had an id of foo\n, pass starting_after=foo\nto fetch the next page. Note: you may only pass one of starting_after\nand ending_before\n\"uuid\"\nending_before\nstringPagination cursor id.\nFor example, if the initial item in the last page you fetched had an id of foo\n, pass ending_before=foo\nto fetch the previous page. Note: you may only pass one of starting_after\nand ending_before\n\"uuid\"\nids\nAny properties in string, array<string>Filter search results to a particular set of object IDs. To specify a list of IDs, include the query param multiple times\nproject_tag_name\nstringName of the project_tag to search for\nproject_name\nstringName of the project to search for\nproject_id\nstringProject id\n\"uuid\"\norg_name\nstringFilter search results to within a particular organization\nReturns a list of project_tag objects\nCreate project_tag\nCreate a new project_tag. If there is an existing project_tag in the project with the same name as the one specified in the request, will return the existing project_tag unmodified\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nRequest Body\napplication/json\nOptionalAny desired information about the new project_tag object\nproject_id\nUnique identifier for the project that the project tag belongs under\n\"uuid\"\nname\nName of the project tag\ndescription\nstringTextual description of the project tag\ncolor\nstringColor of the tag for the UI\nReturns the new project_tag object\nCreate or replace project_tag\nCreate or replace project_tag. If there is an existing project_tag in the project with the same name as the one specified in the request, will replace the existing project_tag with the provided fields\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nRequest Body\napplication/json\nOptionalAny desired information about the new project_tag object\nproject_id\nUnique identifier for the project that the project tag belongs under\n\"uuid\"\nname\nName of the project tag\ndescription\nstringTextual description of the project tag\ncolor\nstringColor of the tag for the UI\nReturns the new project_tag object\nGet project_tag\nGet a project_tag object by its id\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nPath Parameters\nproject_tag_id\nProjectTag id\n\"uuid\"\nReturns the project_tag object\nPartially update project_tag\nPartially update a project_tag object. Specify the fields to update in the payload. Any object-type fields will be deep-merged with existing content. Currently we do not support removing fields or setting them to null.\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nRequest Body\napplication/json\nOptionalFields to update\nname\nstringName of the project tag\ndescription\nstringTextual description of the project tag\ncolor\nstringColor of the tag for the UI\nPath Parameters\nproject_tag_id\nProjectTag id\n\"uuid\"\nReturns the project_tag object\nDelete project_tag\nDelete a project_tag object by its id\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nPath Parameters\nproject_tag_id\nProjectTag id\n\"uuid\"\nReturns the deleted project_tag object", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/reference/api", "anchor": "API"}, {"href": "https://www.braintrust.dev/docs/reference/api/Acls", "anchor": "Acls"}, {"href": "https://www.braintrust.dev/docs/reference/api/AiSecrets", "anchor": "Ai Secrets"}, {"href": "https://www.braintrust.dev/docs/reference/api/ApiKeys", "anchor": "Api Keys"}, {"href": "https://www.braintrust.dev/docs/reference/api/CrossObject", "anchor": "Cross Object"}, {"href": "https://www.braintrust.dev/docs/reference/api/Datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/reference/api/EnvVars", "anchor": "Env Vars"}, {"href": "https://www.braintrust.dev/docs/reference/api/Evals", "anchor": "Evals"}, {"href": "https://www.braintrust.dev/docs/reference/api/Experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/reference/api/Functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/reference/api/Groups", "anchor": "Groups"}, {"href": "https://www.braintrust.dev/docs/reference/api/Logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/reference/api/Organizations", "anchor": "Organizations"}, {"href": "https://www.braintrust.dev/docs/reference/api/Other", "anchor": "Other"}, {"href": "https://www.braintrust.dev/docs/reference/api/Projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectScores", "anchor": "Project Scores"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectTags", "anchor": "Project Tags"}, {"href": "https://www.braintrust.dev/docs/reference/api/Prompts", "anchor": "Prompts"}, {"href": "https://www.braintrust.dev/docs/reference/api/Proxy", "anchor": "Proxy"}, {"href": "https://www.braintrust.dev/docs/reference/api/Roles", "anchor": "Roles"}, {"href": "https://www.braintrust.dev/docs/reference/api/ServiceTokens", "anchor": "Service tokens"}, {"href": "https://www.braintrust.dev/docs/reference/api/SpanIframes", "anchor": "Span Iframes"}, {"href": "https://www.braintrust.dev/docs/reference/api/Users", "anchor": "Users"}, {"href": "https://www.braintrust.dev/docs/reference/api/Views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/reference/autoevals", "anchor": "Autoevals library"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "BTQL query syntax"}, {"href": "https://www.braintrust.dev/docs/reference/streaming", "anchor": "Streaming"}, {"href": "https://www.braintrust.dev/docs/reference/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/reference/reasoning", "anchor": "Reasoning"}, {"href": "https://www.braintrust.dev/docs/reference/organizations", "anchor": "Organizations"}, {"href": "https://www.braintrust.dev/docs/reference/mcp", "anchor": "Model Context Protocol (MCP)"}, {"href": "https://www.braintrust.dev/docs/reference/object-links", "anchor": "Sharing via URL"}, {"href": "https://www.braintrust.dev/docs/reference/glossary", "anchor": "Glossary"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/reference/api", "anchor": "API"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectTags", "anchor": "Project Tags"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectTags", "anchor": "List project_tags"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectTags", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectTags", "anchor": "Query Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectTags", "anchor": "Create project_tag"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectTags", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectTags", "anchor": "Request Body"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectTags", "anchor": "Create or replace project_tag"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectTags", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectTags", "anchor": "Request Body"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectTags", "anchor": "Get project_tag"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectTags", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectTags", "anchor": "Path Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectTags", "anchor": "Partially update project_tag"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectTags", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectTags", "anchor": "Request Body"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectTags", "anchor": "Path Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectTags", "anchor": "Delete project_tag"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectTags", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectTags", "anchor": "Path Parameters"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectTags", "anchor": "List project_tags"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectTags", "anchor": "Create project_tag"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectTags", "anchor": "Create or replace project_tag"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectTags", "anchor": "Get project_tag"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectTags", "anchor": "Partially update project_tag"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectTags", "anchor": "Delete project_tag"}], "depth": 4}, "https://www.braintrust.dev/docs/reference/api/Prompts": {"url": "https://www.braintrust.dev/docs/reference/api/Prompts", "title": "Prompts - Docs - Reference - Braintrust", "text": "List prompts\nList out all prompts. The prompts are sorted by creation date, with the most recently-created prompts coming first\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nQuery Parameters\nlimit\nintegerLimit the number of objects to return\n0\nstarting_after\nstringPagination cursor id.\nFor example, if the final item in the last page you fetched had an id of foo\n, pass starting_after=foo\nto fetch the next page. Note: you may only pass one of starting_after\nand ending_before\n\"uuid\"\nending_before\nstringPagination cursor id.\nFor example, if the initial item in the last page you fetched had an id of foo\n, pass ending_before=foo\nto fetch the previous page. Note: you may only pass one of starting_after\nand ending_before\n\"uuid\"\nids\nAny properties in string, array<string>Filter search results to a particular set of object IDs. To specify a list of IDs, include the query param multiple times\nprompt_name\nstringName of the prompt to search for\nproject_name\nstringName of the project to search for\nproject_id\nstringProject id\n\"uuid\"\nslug\nstringRetrieve prompt with a specific slug\nversion\nstringRetrieve prompt at a specific version.\nThe version id can either be a transaction id (e.g. '1000192656880881099') or a version identifier (e.g. '81cd05ee665fdfb3').\norg_name\nstringFilter search results to within a particular organization\nReturns a list of prompt objects\nCreate prompt\nCreate a new prompt. If there is an existing prompt in the project with the same slug as the one specified in the request, will return the existing prompt unmodified\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nRequest Body\napplication/json\nOptionalAny desired information about the new prompt object\nproject_id\nUnique identifier for the project that the prompt belongs under\n\"uuid\"\nname\nName of the prompt\n1\nslug\nUnique identifier for the prompt\n1\ndescription\nstringTextual description of the prompt\nprompt_data\nobjectThe prompt, model, and its parameters\ntags\narray<string>A list of tags for the prompt\nfunction_type\nstring\"llm\" | \"scorer\" | \"task\" | \"tool\" | null\nReturns the new prompt object\nCreate or replace prompt\nCreate or replace prompt. If there is an existing prompt in the project with the same slug as the one specified in the request, will replace the existing prompt with the provided fields\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nRequest Body\napplication/json\nOptionalAny desired information about the new prompt object\nproject_id\nUnique identifier for the project that the prompt belongs under\n\"uuid\"\nname\nName of the prompt\n1\nslug\nUnique identifier for the prompt\n1\ndescription\nstringTextual description of the prompt\nprompt_data\nobjectThe prompt, model, and its parameters\ntags\narray<string>A list of tags for the prompt\nfunction_type\nstring\"llm\" | \"scorer\" | \"task\" | \"tool\" | null\nReturns the new prompt object\nGet prompt\nGet a prompt object by its id\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nPath Parameters\nprompt_id\nPrompt id\n\"uuid\"\nQuery Parameters\nversion\nstringRetrieve prompt at a specific version.\nThe version id can either be a transaction id (e.g. '1000192656880881099') or a version identifier (e.g. '81cd05ee665fdfb3').\nReturns the prompt object\nPartially update prompt\nPartially update a prompt object. Specify the fields to update in the payload. Any object-type fields will be deep-merged with existing content. Currently we do not support removing fields or setting them to null.\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nRequest Body\napplication/json\nOptionalFields to update\nname\nstringName of the prompt\nslug\nstringUnique identifier for the prompt\ndescription\nstringTextual description of the prompt\nprompt_data\nobjectThe prompt, model, and its parameters\ntags\narray<string>A list of tags for the prompt\nPath Parameters\nprompt_id\nPrompt id\n\"uuid\"\nReturns the prompt object\nDelete prompt\nDelete a prompt object by its id\nAuthorization\nAuthorization\nMost Braintrust endpoints are authenticated by providing your API key as a header Authorization: Bearer [api_key]\nto your HTTP request. You can create an API key in the Braintrust organization settings page.\nIn: header\nPath Parameters\nprompt_id\nPrompt id\n\"uuid\"\nReturns the deleted prompt object", "links": [{"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs", "anchor": ""}, {"href": "https://www.braintrust.dev/docs/start", "anchor": "Get started"}, {"href": "https://www.braintrust.dev/docs/start/eval-ui", "anchor": "Eval via UI"}, {"href": "https://www.braintrust.dev/docs/start/eval-sdk", "anchor": "Eval via SDK"}, {"href": "https://www.braintrust.dev/docs/start/frameworks", "anchor": "Frameworks"}, {"href": "https://www.braintrust.dev/docs/providers", "anchor": "Model providers"}, {"href": "https://www.braintrust.dev/docs/guides", "anchor": "Guides"}, {"href": "https://www.braintrust.dev/docs/guides/experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/guides/logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/guides/traces", "anchor": "Traces"}, {"href": "https://www.braintrust.dev/docs/guides/playground", "anchor": "Playgrounds"}, {"href": "https://www.braintrust.dev/docs/guides/loop", "anchor": "Loop"}, {"href": "https://www.braintrust.dev/docs/guides/datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/guides/views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/guides/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/guides/attachments", "anchor": "Attachments"}, {"href": "https://www.braintrust.dev/docs/guides/projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/guides/environments", "anchor": "Environments"}, {"href": "https://www.braintrust.dev/docs/guides/monitor", "anchor": "Monitor"}, {"href": "https://www.braintrust.dev/docs/guides/assignment", "anchor": "Assignment and mentions"}, {"href": "https://www.braintrust.dev/docs/guides/human-review", "anchor": "Human review"}, {"href": "https://www.braintrust.dev/docs/guides/remote-evals", "anchor": "Remote evals"}, {"href": "https://www.braintrust.dev/docs/guides/automations", "anchor": "Automations"}, {"href": "https://www.braintrust.dev/docs/guides/access-control", "anchor": "Access control"}, {"href": "https://www.braintrust.dev/docs/guides/proxy", "anchor": "AI proxy"}, {"href": "https://www.braintrust.dev/docs/guides/self-hosting", "anchor": "Self-hosting"}, {"href": "https://www.braintrust.dev/docs/guides/api", "anchor": "API walkthrough"}, {"href": "https://www.braintrust.dev/docs/best-practices/scorers", "anchor": "Writing scorers"}, {"href": "https://www.braintrust.dev/docs/best-practices/agents", "anchor": "Evaluating agents"}, {"href": "https://www.braintrust.dev/docs/cookbook", "anchor": "Cookbook"}, {"href": "https://www.braintrust.dev/docs/reference/api", "anchor": "API"}, {"href": "https://www.braintrust.dev/docs/reference/api/Acls", "anchor": "Acls"}, {"href": "https://www.braintrust.dev/docs/reference/api/AiSecrets", "anchor": "Ai Secrets"}, {"href": "https://www.braintrust.dev/docs/reference/api/ApiKeys", "anchor": "Api Keys"}, {"href": "https://www.braintrust.dev/docs/reference/api/CrossObject", "anchor": "Cross Object"}, {"href": "https://www.braintrust.dev/docs/reference/api/Datasets", "anchor": "Datasets"}, {"href": "https://www.braintrust.dev/docs/reference/api/EnvVars", "anchor": "Env Vars"}, {"href": "https://www.braintrust.dev/docs/reference/api/Evals", "anchor": "Evals"}, {"href": "https://www.braintrust.dev/docs/reference/api/Experiments", "anchor": "Experiments"}, {"href": "https://www.braintrust.dev/docs/reference/api/Functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/reference/api/Groups", "anchor": "Groups"}, {"href": "https://www.braintrust.dev/docs/reference/api/Logs", "anchor": "Logs"}, {"href": "https://www.braintrust.dev/docs/reference/api/Organizations", "anchor": "Organizations"}, {"href": "https://www.braintrust.dev/docs/reference/api/Other", "anchor": "Other"}, {"href": "https://www.braintrust.dev/docs/reference/api/Projects", "anchor": "Projects"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectScores", "anchor": "Project Scores"}, {"href": "https://www.braintrust.dev/docs/reference/api/ProjectTags", "anchor": "Project Tags"}, {"href": "https://www.braintrust.dev/docs/reference/api/Prompts", "anchor": "Prompts"}, {"href": "https://www.braintrust.dev/docs/reference/api/Proxy", "anchor": "Proxy"}, {"href": "https://www.braintrust.dev/docs/reference/api/Roles", "anchor": "Roles"}, {"href": "https://www.braintrust.dev/docs/reference/api/ServiceTokens", "anchor": "Service tokens"}, {"href": "https://www.braintrust.dev/docs/reference/api/SpanIframes", "anchor": "Span Iframes"}, {"href": "https://www.braintrust.dev/docs/reference/api/Users", "anchor": "Users"}, {"href": "https://www.braintrust.dev/docs/reference/api/Views", "anchor": "Views"}, {"href": "https://www.braintrust.dev/docs/reference/autoevals", "anchor": "Autoevals library"}, {"href": "https://www.braintrust.dev/docs/reference/btql", "anchor": "BTQL query syntax"}, {"href": "https://www.braintrust.dev/docs/reference/streaming", "anchor": "Streaming"}, {"href": "https://www.braintrust.dev/docs/reference/functions", "anchor": "Functions"}, {"href": "https://www.braintrust.dev/docs/reference/reasoning", "anchor": "Reasoning"}, {"href": "https://www.braintrust.dev/docs/reference/organizations", "anchor": "Organizations"}, {"href": "https://www.braintrust.dev/docs/reference/mcp", "anchor": "Model Context Protocol (MCP)"}, {"href": "https://www.braintrust.dev/docs/reference/object-links", "anchor": "Sharing via URL"}, {"href": "https://www.braintrust.dev/docs/reference/glossary", "anchor": "Glossary"}, {"href": "https://www.braintrust.dev/docs/changelog", "anchor": "Changelog"}, {"href": "https://www.braintrust.dev/docs/reference/api", "anchor": "API"}, {"href": "https://www.braintrust.dev/docs/reference/api/Prompts", "anchor": "Prompts"}, {"href": "https://www.braintrust.dev/docs/reference/api/Prompts", "anchor": "List prompts"}, {"href": "https://www.braintrust.dev/docs/reference/api/Prompts", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/Prompts", "anchor": "Query Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/api/Prompts", "anchor": "Create prompt"}, {"href": "https://www.braintrust.dev/docs/reference/api/Prompts", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/Prompts", "anchor": "Request Body"}, {"href": "https://www.braintrust.dev/docs/reference/api/Prompts", "anchor": "Create or replace prompt"}, {"href": "https://www.braintrust.dev/docs/reference/api/Prompts", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/Prompts", "anchor": "Request Body"}, {"href": "https://www.braintrust.dev/docs/reference/api/Prompts", "anchor": "Get prompt"}, {"href": "https://www.braintrust.dev/docs/reference/api/Prompts", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/Prompts", "anchor": "Path Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/api/Prompts", "anchor": "Query Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/api/Prompts", "anchor": "Partially update prompt"}, {"href": "https://www.braintrust.dev/docs/reference/api/Prompts", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/Prompts", "anchor": "Request Body"}, {"href": "https://www.braintrust.dev/docs/reference/api/Prompts", "anchor": "Path Parameters"}, {"href": "https://www.braintrust.dev/docs/reference/api/Prompts", "anchor": "Delete prompt"}, {"href": "https://www.braintrust.dev/docs/reference/api/Prompts", "anchor": "Authorization"}, {"href": "https://www.braintrust.dev/docs/reference/api/Prompts", "anchor": "Path Parameters"}, {"href": "https://www.braintrust.dev/", "anchor": ""}, {"href": "https://www.braintrust.dev/home", "anchor": "Home"}, {"href": "https://www.braintrust.dev/docs", "anchor": "Docs"}, {"href": "https://www.braintrust.dev/pricing", "anchor": "Pricing"}, {"href": "https://www.braintrust.dev/blog", "anchor": "Blog"}, {"href": "https://www.braintrust.dev/careers", "anchor": "Careers"}, {"href": "https://www.braintrust.dev/contact", "anchor": "Chat with us"}, {"href": "https://www.braintrust.dev/docs/reference/changelog", "anchor": "Changelog"}, {"href": "https://status.braintrust.dev/", "anchor": "Status"}, {"href": "https://trust.braintrust.dev/", "anchor": "Trust center"}, {"href": "https://www.braintrust.dev/legal/privacy-policy", "anchor": "Privacy"}, {"href": "https://www.braintrust.dev/legal/terms-of-service", "anchor": "Terms"}, {"href": "https://www.braintrust.dev/docs/reference/api/Prompts", "anchor": "List prompts"}, {"href": "https://www.braintrust.dev/docs/reference/api/Prompts", "anchor": "Create prompt"}, {"href": "https://www.braintrust.dev/docs/reference/api/Prompts", "anchor": "Create or replace prompt"}, {"href": "https://www.braintrust.dev/docs/reference/api/Prompts", "anchor": "Get prompt"}, {"href": "https://www.braintrust.dev/docs/reference/api/Prompts", "anchor": "Partially update prompt"}, {"href": "https://www.braintrust.dev/docs/reference/api/Prompts", "anchor": "Delete prompt"}], "depth": 4}}